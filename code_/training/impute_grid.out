/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
Using dataset with NaNs
n numeric features: 13
Using imputer: mean
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.06
Filename: RF_mean imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_mean imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_mean imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: median
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.06
Filename: RF_median imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_median imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_median imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: most-frequent
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.06
Filename: RF_most-frequent imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_most-frequent imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_most-frequent imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: uniform KNN
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.06
Filename: RF_uniform KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_uniform KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_uniform KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: distance KNN
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.06
Filename: RF_distance KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_distance KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_distance KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: iterative
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.06
Filename: RF_iterative imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_iterative imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/RF_iterative imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: mean
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Filename: XGB_mean imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_mean imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_mean imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: median
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Filename: XGB_median imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_median imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_median imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: most-frequent
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Filename: XGB_most-frequent imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_most-frequent imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_most-frequent imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: uniform KNN
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Filename: XGB_uniform KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_uniform KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_uniform KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: distance KNN
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Filename: XGB_distance KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_distance KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_distance KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: iterative
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Filename: XGB_iterative imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_iterative imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/XGB_iterative imputer_predictions.csv
Using dataset with NaNs
n numeric features: 19
Using imputer: mean
Average scores:	 r: 0.79±0.04	 r2: 0.61±0.07
Filename: HGB_mean imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_mean imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_mean imputer_predictions.csv
Using dataset with NaNs
n numeric features: 19
Using imputer: median
Average scores:	 r: 0.79±0.04	 r2: 0.61±0.07
Filename: HGB_median imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_median imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_median imputer_predictions.csv
Using dataset with NaNs
n numeric features: 19
Using imputer: most-frequent
Average scores:	 r: 0.79±0.04	 r2: 0.61±0.07
Filename: HGB_most-frequent imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_most-frequent imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_most-frequent imputer_predictions.csv
Using dataset with NaNs
n numeric features: 19
Using imputer: uniform KNN
Average scores:	 r: 0.79±0.04	 r2: 0.61±0.07
Filename: HGB_uniform KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_uniform KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_uniform KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 19
Using imputer: distance KNN
Average scores:	 r: 0.78±0.04	 r2: 0.61±0.06
Filename: HGB_distance KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_distance KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_distance KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 19
Using imputer: iterative
Average scores:	 r: 0.78±0.04	 r2: 0.61±0.07
Filename: HGB_iterative imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_iterative imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/HGB_iterative imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: mean
Average scores:	 r: 0.76±0.04	 r2: 0.57±0.06
Filename: NGB_mean imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_mean imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_mean imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: median
Average scores:	 r: 0.76±0.04	 r2: 0.57±0.06
Filename: NGB_median imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_median imputer_scores.json
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9928 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.7027 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 300] loss=0.4398 val_loss=0.0000 scale=2.0000 norm=1.0372
[iter 400] loss=0.2753 val_loss=0.0000 scale=1.0000 norm=0.4933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9669 val_loss=0.0000 scale=1.0000 norm=0.6809
[iter 200] loss=0.6248 val_loss=0.0000 scale=1.0000 norm=0.5601
[iter 300] loss=0.3713 val_loss=0.0000 scale=2.0000 norm=1.0158
[iter 400] loss=0.2260 val_loss=0.0000 scale=1.0000 norm=0.4808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 200] loss=0.7032 val_loss=0.0000 scale=1.0000 norm=0.5838
[iter 300] loss=0.4478 val_loss=0.0000 scale=1.0000 norm=0.5198
[iter 400] loss=0.2861 val_loss=0.0000 scale=1.0000 norm=0.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 200] loss=0.6570 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4064 val_loss=0.0000 scale=1.0000 norm=0.5140
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 400] loss=0.2398 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9726 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 200] loss=0.6506 val_loss=0.0000 scale=2.0000 norm=1.1258
[iter 300] loss=0.3935 val_loss=0.0000 scale=1.0000 norm=0.5053
[iter 400] loss=0.2287 val_loss=0.0000 scale=1.0000 norm=0.4754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9715 val_loss=0.0000 scale=1.0000 norm=0.6808
[iter 200] loss=0.6544 val_loss=0.0000 scale=1.0000 norm=0.5629
[iter 300] loss=0.3660 val_loss=0.0000 scale=2.0000 norm=0.9854
[iter 400] loss=0.2078 val_loss=0.0000 scale=2.0000 norm=0.9238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9825 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6737 val_loss=0.0000 scale=1.0000 norm=0.5723
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.0245
[iter 400] loss=0.2431 val_loss=0.0000 scale=1.0000 norm=0.4863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=1.0000 norm=0.6840
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.3597
[iter 200] loss=0.6370 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3690 val_loss=0.0000 scale=1.0000 norm=0.5019
[iter 400] loss=0.1949 val_loss=0.0000 scale=1.0000 norm=0.4688
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.3488
[iter 200] loss=0.6230 val_loss=0.0000 scale=2.0000 norm=1.1154
[iter 300] loss=0.3510 val_loss=0.0000 scale=1.0000 norm=0.4980
[iter 400] loss=0.2094 val_loss=0.0000 scale=2.0000 norm=0.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9840 val_loss=0.0000 scale=1.0000 norm=0.6894
[iter 200] loss=0.6758 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 300] loss=0.4423 val_loss=0.0000 scale=1.0000 norm=0.5196
[iter 400] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=0.9865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 200] loss=0.6308 val_loss=0.0000 scale=1.0000 norm=0.5575
[iter 300] loss=0.3637 val_loss=0.0000 scale=2.0000 norm=0.9921
[iter 400] loss=0.1844 val_loss=0.0000 scale=1.0000 norm=0.4669
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9772 val_loss=0.0000 scale=1.0000 norm=0.6821
[iter 200] loss=0.6688 val_loss=0.0000 scale=2.0000 norm=1.1351
[iter 300] loss=0.4000 val_loss=0.0000 scale=1.0000 norm=0.5069
[iter 400] loss=0.2397 val_loss=0.0000 scale=1.0000 norm=0.4795
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.3823
[iter 200] loss=0.6733 val_loss=0.0000 scale=2.0000 norm=1.1440
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5093
[iter 400] loss=0.2487 val_loss=0.0000 scale=1.0000 norm=0.4784
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6874
[iter 200] loss=0.6415 val_loss=0.0000 scale=1.0000 norm=0.5661
[iter 300] loss=0.3894 val_loss=0.0000 scale=1.0000 norm=0.5094
[iter 400] loss=0.2239 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9915 val_loss=0.0000 scale=1.0000 norm=0.6929
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9960 val_loss=0.0000 scale=2.0000 norm=1.3855
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.3838 val_loss=0.0000 scale=1.0000 norm=0.5024
[iter 400] loss=0.2345 val_loss=0.0000 scale=1.0000 norm=0.4805
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9836 val_loss=0.0000 scale=1.0000 norm=0.6883
[iter 200] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.1320
[iter 300] loss=0.4041 val_loss=0.0000 scale=1.0000 norm=0.5117
[iter 400] loss=0.2683 val_loss=0.0000 scale=2.0000 norm=0.9749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9793 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 200] loss=0.6399 val_loss=0.0000 scale=1.0000 norm=0.5567
[iter 300] loss=0.3607 val_loss=0.0000 scale=2.0000 norm=0.9752
[iter 400] loss=0.1978 val_loss=0.0000 scale=1.0000 norm=0.4563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9794 val_loss=0.0000 scale=1.0000 norm=0.6866
[iter 200] loss=0.6750 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 300] loss=0.4175 val_loss=0.0000 scale=1.0000 norm=0.5105
[iter 400] loss=0.2525 val_loss=0.0000 scale=1.0000 norm=0.4780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=1.0000 norm=0.6843
[iter 200] loss=0.6522 val_loss=0.0000 scale=2.0000 norm=1.1334
[iter 300] loss=0.3760 val_loss=0.0000 scale=1.0000 norm=0.5052
[iter 400] loss=0.2136 val_loss=0.0000 scale=1.0000 norm=0.4813
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=1.0000 norm=0.6872
[iter 200] loss=0.6671 val_loss=0.0000 scale=1.0000 norm=0.5720
[iter 300] loss=0.4163 val_loss=0.0000 scale=1.0000 norm=0.5128
[iter 400] loss=0.2504 val_loss=0.0000 scale=1.0000 norm=0.4852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=1.0000 norm=0.6890
[iter 200] loss=0.6506 val_loss=0.0000 scale=1.0000 norm=0.5649
[iter 300] loss=0.3759 val_loss=0.0000 scale=2.0000 norm=1.0118
[iter 400] loss=0.2199 val_loss=0.0000 scale=1.0000 norm=0.4802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9767 val_loss=0.0000 scale=1.0000 norm=0.6858
[iter 200] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.5634
[iter 300] loss=0.3784 val_loss=0.0000 scale=1.0000 norm=0.5041
[iter 400] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=0.9533
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9795 val_loss=0.0000 scale=1.0000 norm=0.6867
[iter 200] loss=0.6778 val_loss=0.0000 scale=2.0000 norm=1.1539
[iter 300] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.0310
[iter 400] loss=0.2342 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9811 val_loss=0.0000 scale=2.0000 norm=1.3770
[iter 200] loss=0.6738 val_loss=0.0000 scale=1.0000 norm=0.5702
[iter 300] loss=0.4300 val_loss=0.0000 scale=1.0000 norm=0.5112
[iter 400] loss=0.2786 val_loss=0.0000 scale=1.0000 norm=0.4850
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9845 val_loss=0.0000 scale=1.0000 norm=0.6864
[iter 200] loss=0.6682 val_loss=0.0000 scale=1.0000 norm=0.5699
[iter 300] loss=0.3885 val_loss=0.0000 scale=1.0000 norm=0.5066
[iter 400] loss=0.2278 val_loss=0.0000 scale=1.0000 norm=0.4786
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9821 val_loss=0.0000 scale=1.0000 norm=0.6876
[iter 200] loss=0.6685 val_loss=0.0000 scale=1.0000 norm=0.5717
[iter 300] loss=0.4165 val_loss=0.0000 scale=1.0000 norm=0.5071
[iter 400] loss=0.2610 val_loss=0.0000 scale=1.0000 norm=0.4762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6665 val_loss=0.0000 scale=2.0000 norm=1.1464
[iter 300] loss=0.4225 val_loss=0.0000 scale=2.0000 norm=1.0270
[iter 400] loss=0.2400 val_loss=0.0000 scale=1.0000 norm=0.4812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_median imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: most-frequent
Average scores:	 r: 0.76±0.04	 r2: 0.57±0.06
Filename: NGB_most-frequent imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_most-frequent imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_most-frequent imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: uniform KNN
Average scores:	 r: 0.76±0.04	 r2: 0.57±0.06
Filename: NGB_uniform KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_uniform KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_uniform KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: distance KNN
Average scores:	 r: 0.76±0.04	 r2: 0.57±0.06
Filename: NGB_distance KNN imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_distance KNN imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_distance KNN imputer_predictions.csv
Using dataset with NaNs
n numeric features: 13
Using imputer: iterative
Average scores:	 r: 0.76±0.04	 r2: 0.57±0.06
Filename: NGB_iterative imputer
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_iterative imputer_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred-device architecture/NGB_iterative imputer_predictions.csv
[iter 200] loss=0.6622 val_loss=0.0000 scale=1.0000 norm=0.5691
[iter 300] loss=0.4076 val_loss=0.0000 scale=2.0000 norm=1.0193
[iter 400] loss=0.2482 val_loss=0.0000 scale=2.0000 norm=0.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 200] loss=0.6711 val_loss=0.0000 scale=1.0000 norm=0.5712
[iter 300] loss=0.4096 val_loss=0.0000 scale=1.0000 norm=0.5080
[iter 400] loss=0.2286 val_loss=0.0000 scale=2.0000 norm=0.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 200] loss=0.6498 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 300] loss=0.3924 val_loss=0.0000 scale=1.0000 norm=0.5049
[iter 400] loss=0.2319 val_loss=0.0000 scale=2.0000 norm=0.9535
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9803 val_loss=0.0000 scale=1.0000 norm=0.6877
[iter 200] loss=0.6807 val_loss=0.0000 scale=1.0000 norm=0.5741
[iter 300] loss=0.4329 val_loss=0.0000 scale=2.0000 norm=1.0200
[iter 400] loss=0.2597 val_loss=0.0000 scale=1.0000 norm=0.4747
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.7007
[iter 200] loss=0.6661 val_loss=0.0000 scale=1.0000 norm=0.5775
[iter 300] loss=0.4193 val_loss=0.0000 scale=2.0000 norm=1.0367
[iter 400] loss=0.2536 val_loss=0.0000 scale=2.0000 norm=0.9873
[iter 200] loss=0.6579 val_loss=0.0000 scale=1.0000 norm=0.5648
[iter 300] loss=0.3904 val_loss=0.0000 scale=2.0000 norm=1.0079
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=0.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9879 val_loss=0.0000 scale=1.0000 norm=0.6921
[iter 200] loss=0.6782 val_loss=0.0000 scale=2.0000 norm=1.1580
[iter 300] loss=0.4274 val_loss=0.0000 scale=1.0000 norm=0.5184
[iter 400] loss=0.2661 val_loss=0.0000 scale=2.0000 norm=0.9792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9653 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 200] loss=0.6395 val_loss=0.0000 scale=1.0000 norm=0.5632
[iter 300] loss=0.3808 val_loss=0.0000 scale=1.0000 norm=0.5044
[iter 400] loss=0.2210 val_loss=0.0000 scale=1.0000 norm=0.4799
