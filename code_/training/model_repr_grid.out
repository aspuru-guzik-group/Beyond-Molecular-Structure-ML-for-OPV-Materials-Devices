/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
Average scores:	 r: -0.01±0.11	 r2: -1.1519063527753005e+56±6.227267827869215e+56
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/MLR_predictions.csv
Average scores:	 r: 0.05±0.13	 r2: -2.0439546883967414e+167±inf
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/MLR_predictions.csv
Average scores:	 r: 0.04±0.1	 r2: -5.21269038908315e+61±3.0261885037290783e+62
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/MLR_predictions.csv
Average scores:	 r: -0.06±0.07	 r2: -3.48206490665413e+54±1.480016179392349e+55
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/MLR_predictions.csv
Average scores:	 r: -0.02±0.08	 r2: -7.960201949443884e+56±4.1760345024132025e+57
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/MLR_predictions.csv
n numeric features: 8
Average scores:	 r: 0.58±0.07	 r2: 0.32±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/MLR_predictions.csv
n numeric features: 5
Average scores:	 r: 0.17±0.21	 r2: -5.3016431810100485e+132±3.091362635210421e+133
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/MLR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/MLR_predictions.csv
Average scores:	 r: 0.7±0.06	 r2: 0.4±0.15
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/KRR_predictions.csv
Average scores:	 r: 0.1±0.2	 r2: -4.43062328527926e+83±2.5834751240644813e+84
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/KRR_predictions.csv
Average scores:	 r: 0.62±0.07	 r2: 0.37±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/KRR_predictions.csv
Average scores:	 r: 0.53±0.06	 r2: 0.01±0.36
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/KRR_predictions.csv
Average scores:	 r: 0.39±0.1	 r2: -1.6±4.32
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/KRR_predictions.csv
n numeric features: 8
Average scores:	 r: 0.58±0.07	 r2: 0.33±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/KRR_predictions.csv
n numeric features: 5
Average scores:	 r: 0.27±0.18	 r2: -1.8929840536586107e+29±9.486118326126423e+29
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/KRR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/KRR_predictions.csv
Average scores:	 r: 0.66±0.06	 r2: 0.43±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/KNN_predictions.csv
Average scores:	 r: 0.66±0.05	 r2: 0.42±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/KNN_predictions.csv
Average scores:	 r: 0.52±0.07	 r2: 0.21±0.11
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/KNN_predictions.csv
Average scores:	 r: 0.58±0.07	 r2: 0.31±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/KNN_predictions.csv
Average scores:	 r: 0.58±0.06	 r2: 0.3±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/KNN_predictions.csv
n numeric features: 8
Average scores:	 r: 0.64±0.05	 r2: 0.39±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/KNN_predictions.csv
n numeric features: 5
Average scores:	 r: 0.47±0.07	 r2: 0.17±0.1
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/KNN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/KNN_predictions.csv
Average scores:	 r: 0.74±0.05	 r2: 0.53±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/SVR_predictions.csv
Average scores:	 r: 0.74±0.04	 r2: 0.53±0.06
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/SVR_predictions.csv
Average scores:	 r: 0.62±0.06	 r2: 0.36±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/SVR_predictions.csv
Average scores:	 r: 0.64±0.06	 r2: 0.39±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/SVR_predictions.csv
Average scores:	 r: 0.64±0.06	 r2: 0.39±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/SVR_predictions.csv
n numeric features: 8
Average scores:	 r: 0.67±0.06	 r2: 0.44±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/SVR_predictions.csv
n numeric features: 5
Average scores:	 r: 0.46±0.06	 r2: 0.19±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/SVR_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/SVR_predictions.csv
Average scores:	 r: 0.77±0.05	 r2: 0.58±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/RF_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/RF_predictions.csv
Average scores:	 r: 0.77±0.04	 r2: 0.59±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/RF_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/RF_predictions.csv
Average scores:	 r: 0.59±0.06	 r2: 0.29±0.1
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/RF_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/RF_predictions.csv
Average scores:	 r: 0.68±0.05	 r2: 0.44±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/RF_scores.json
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=1.0000 norm=0.7507
[iter 200] loss=0.8751 val_loss=0.0000 scale=1.0000 norm=0.6612
[iter 300] loss=0.6927 val_loss=0.0000 scale=1.0000 norm=0.6152
[iter 400] loss=0.5906 val_loss=0.0000 scale=1.0000 norm=0.5884
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=1.0000 norm=0.7507
[iter 200] loss=0.8751 val_loss=0.0000 scale=1.0000 norm=0.6612
[iter 300] loss=0.6927 val_loss=0.0000 scale=1.0000 norm=0.6152
[iter 400] loss=0.5906 val_loss=0.0000 scale=1.0000 norm=0.5884
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0791 val_loss=0.0000 scale=1.0000 norm=0.7448
[iter 200] loss=0.8243 val_loss=0.0000 scale=2.0000 norm=1.2849
[iter 300] loss=0.6557 val_loss=0.0000 scale=1.0000 norm=0.6020
[iter 400] loss=0.5504 val_loss=0.0000 scale=2.0000 norm=1.1505
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0913 val_loss=0.0000 scale=1.0000 norm=0.7528
[iter 200] loss=0.8432 val_loss=0.0000 scale=1.0000 norm=0.6520
[iter 300] loss=0.6749 val_loss=0.0000 scale=1.0000 norm=0.6056
[iter 400] loss=0.5671 val_loss=0.0000 scale=1.0000 norm=0.5750
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0714 val_loss=0.0000 scale=1.0000 norm=0.7409
[iter 200] loss=0.8154 val_loss=0.0000 scale=2.0000 norm=1.2813
[iter 300] loss=0.6560 val_loss=0.0000 scale=2.0000 norm=1.2006
[iter 400] loss=0.5535 val_loss=0.0000 scale=1.0000 norm=0.5730
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0802 val_loss=0.0000 scale=1.0000 norm=0.7440
[iter 200] loss=0.8285 val_loss=0.0000 scale=2.0000 norm=1.2937
[iter 300] loss=0.6555 val_loss=0.0000 scale=1.0000 norm=0.5977
[iter 400] loss=0.5477 val_loss=0.0000 scale=1.0000 norm=0.5660
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0632 val_loss=0.0000 scale=1.0000 norm=0.7380
[iter 200] loss=0.8156 val_loss=0.0000 scale=2.0000 norm=1.2820
[iter 300] loss=0.6663 val_loss=0.0000 scale=2.0000 norm=1.1977
[iter 400] loss=0.5571 val_loss=0.0000 scale=1.0000 norm=0.5698
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0810 val_loss=0.0000 scale=1.0000 norm=0.7412
[iter 200] loss=0.7992 val_loss=0.0000 scale=2.0000 norm=1.2555
[iter 300] loss=0.6295 val_loss=0.0000 scale=2.0000 norm=1.1794
[iter 400] loss=0.5329 val_loss=0.0000 scale=1.0000 norm=0.5683
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0947 val_loss=0.0000 scale=2.0000 norm=1.5075
[iter 200] loss=0.8572 val_loss=0.0000 scale=2.0000 norm=1.3115
[iter 300] loss=0.6743 val_loss=0.0000 scale=2.0000 norm=1.2111
[iter 400] loss=0.5724 val_loss=0.0000 scale=2.0000 norm=1.1566
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0980 val_loss=0.0000 scale=1.0000 norm=0.7574
[iter 200] loss=0.8876 val_loss=0.0000 scale=1.0000 norm=0.6660
[iter 300] loss=0.7267 val_loss=0.0000 scale=1.0000 norm=0.6166
[iter 400] loss=0.6226 val_loss=0.0000 scale=2.0000 norm=1.1740
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0779 val_loss=0.0000 scale=2.0000 norm=1.4859
[iter 200] loss=0.8466 val_loss=0.0000 scale=1.0000 norm=0.6519
[iter 300] loss=0.6751 val_loss=0.0000 scale=1.0000 norm=0.6083
[iter 400] loss=0.5796 val_loss=0.0000 scale=1.0000 norm=0.5857
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0591 val_loss=0.0000 scale=1.0000 norm=0.7332
[iter 200] loss=0.7919 val_loss=0.0000 scale=2.0000 norm=1.2497
[iter 300] loss=0.6381 val_loss=0.0000 scale=1.0000 norm=0.5844
[iter 400] loss=0.5437 val_loss=0.0000 scale=1.0000 norm=0.5606
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0937 val_loss=0.0000 scale=1.0000 norm=0.7561
[iter 200] loss=0.8551 val_loss=0.0000 scale=2.0000 norm=1.3157
[iter 300] loss=0.6760 val_loss=0.0000 scale=1.0000 norm=0.6100
[iter 400] loss=0.5800 val_loss=0.0000 scale=1.0000 norm=0.5843
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0880 val_loss=0.0000 scale=1.0000 norm=0.7487
[iter 200] loss=0.8483 val_loss=0.0000 scale=1.0000 norm=0.6504
[iter 300] loss=0.6982 val_loss=0.0000 scale=2.0000 norm=1.2224
[iter 400] loss=0.6032 val_loss=0.0000 scale=1.0000 norm=0.5852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0937 val_loss=0.0000 scale=1.0000 norm=0.7561
[iter 200] loss=0.8551 val_loss=0.0000 scale=2.0000 norm=1.3157
[iter 300] loss=0.6760 val_loss=0.0000 scale=1.0000 norm=0.6100
[iter 400] loss=0.5800 val_loss=0.0000 scale=1.0000 norm=0.5843
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0880 val_loss=0.0000 scale=1.0000 norm=0.7487
[iter 200] loss=0.8483 val_loss=0.0000 scale=1.0000 norm=0.6504
[iter 300] loss=0.6982 val_loss=0.0000 scale=2.0000 norm=1.2224
[iter 400] loss=0.6032 val_loss=0.0000 scale=1.0000 norm=0.5852
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0883 val_loss=0.0000 scale=1.0000 norm=0.7495
[iter 200] loss=0.8418 val_loss=0.0000 scale=1.0000 norm=0.6460
[iter 300] loss=0.6774 val_loss=0.0000 scale=1.0000 norm=0.6017
[iter 400] loss=0.5802 val_loss=0.0000 scale=1.0000 norm=0.5771
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1001 val_loss=0.0000 scale=1.0000 norm=0.7573
[iter 200] loss=0.8659 val_loss=0.0000 scale=1.0000 norm=0.6582
[iter 300] loss=0.7018 val_loss=0.0000 scale=1.0000 norm=0.6121
[iter 400] loss=0.6019 val_loss=0.0000 scale=1.0000 norm=0.5878
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0883 val_loss=0.0000 scale=1.0000 norm=0.7495
[iter 200] loss=0.8418 val_loss=0.0000 scale=1.0000 norm=0.6460
[iter 300] loss=0.6774 val_loss=0.0000 scale=1.0000 norm=0.6017
[iter 400] loss=0.5802 val_loss=0.0000 scale=1.0000 norm=0.5771
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0853 val_loss=0.0000 scale=1.0000 norm=0.7474
[iter 200] loss=0.8192 val_loss=0.0000 scale=2.0000 norm=1.2772
[iter 300] loss=0.6643 val_loss=0.0000 scale=1.0000 norm=0.5981
[iter 400] loss=0.5661 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0853 val_loss=0.0000 scale=1.0000 norm=0.7474
[iter 200] loss=0.8192 val_loss=0.0000 scale=2.0000 norm=1.2772
[iter 300] loss=0.6643 val_loss=0.0000 scale=1.0000 norm=0.5981
[iter 400] loss=0.5661 val_loss=0.0000 scale=1.0000 norm=0.5739
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0062 val_loss=0.0000 scale=1.0000 norm=0.7000
[iter 200] loss=0.6877 val_loss=0.0000 scale=2.0000 norm=1.1544
[iter 300] loss=0.4458 val_loss=0.0000 scale=1.0000 norm=0.5212
[iter 400] loss=0.2837 val_loss=0.0000 scale=1.0000 norm=0.4894
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0062 val_loss=0.0000 scale=1.0000 norm=0.7000
[iter 200] loss=0.6877 val_loss=0.0000 scale=2.0000 norm=1.1544
[iter 300] loss=0.4458 val_loss=0.0000 scale=1.0000 norm=0.5212
[iter 400] loss=0.2837 val_loss=0.0000 scale=1.0000 norm=0.4894
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0087 val_loss=0.0000 scale=1.0000 norm=0.7001
[iter 200] loss=0.6868 val_loss=0.0000 scale=1.0000 norm=0.5785
[iter 300] loss=0.4269 val_loss=0.0000 scale=2.0000 norm=1.0334
[iter 400] loss=0.2621 val_loss=0.0000 scale=1.0000 norm=0.4892
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0189 val_loss=0.0000 scale=1.0000 norm=0.7094
[iter 200] loss=0.7421 val_loss=0.0000 scale=1.0000 norm=0.5973
[iter 300] loss=0.4932 val_loss=0.0000 scale=1.0000 norm=0.5352
[iter 400] loss=0.3403 val_loss=0.0000 scale=1.0000 norm=0.5091
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0237 val_loss=0.0000 scale=1.0000 norm=0.7115
[iter 200] loss=0.7274 val_loss=0.0000 scale=1.0000 norm=0.5933
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0831 val_loss=0.0000 scale=1.0000 norm=0.7470
[iter 200] loss=0.8347 val_loss=0.0000 scale=2.0000 norm=1.2902
[iter 300] loss=0.6647 val_loss=0.0000 scale=2.0000 norm=1.2038
[iter 400] loss=0.5660 val_loss=0.0000 scale=2.0000 norm=1.1474
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0753 val_loss=0.0000 scale=2.0000 norm=1.4869
[iter 200] loss=0.8354 val_loss=0.0000 scale=2.0000 norm=1.2886
[iter 300] loss=0.6760 val_loss=0.0000 scale=2.0000 norm=1.2005
[iter 400] loss=0.5700 val_loss=0.0000 scale=2.0000 norm=1.1412
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0920 val_loss=0.0000 scale=1.0000 norm=0.7516
[iter 200] loss=0.8469 val_loss=0.0000 scale=2.0000 norm=1.2991
[iter 300] loss=0.6740 val_loss=0.0000 scale=1.0000 norm=0.5993
[iter 400] loss=0.5621 val_loss=0.0000 scale=2.0000 norm=1.1461
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0802 val_loss=0.0000 scale=1.0000 norm=0.7440
[iter 200] loss=0.8285 val_loss=0.0000 scale=2.0000 norm=1.2937
[iter 300] loss=0.6555 val_loss=0.0000 scale=1.0000 norm=0.5977
[iter 400] loss=0.5477 val_loss=0.0000 scale=1.0000 norm=0.5660
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0913 val_loss=0.0000 scale=1.0000 norm=0.7528
[iter 200] loss=0.8432 val_loss=0.0000 scale=1.0000 norm=0.6520
[iter 300] loss=0.6749 val_loss=0.0000 scale=1.0000 norm=0.6056
[iter 400] loss=0.5671 val_loss=0.0000 scale=1.0000 norm=0.5750
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0707 val_loss=0.0000 scale=1.0000 norm=0.7371
[iter 200] loss=0.7788 val_loss=0.0000 scale=2.0000 norm=1.2493
[iter 300] loss=0.6318 val_loss=0.0000 scale=1.0000 norm=0.5895
[iter 400] loss=0.5405 val_loss=0.0000 scale=1.0000 norm=0.5654
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0947 val_loss=0.0000 scale=2.0000 norm=1.5075
[iter 200] loss=0.8572 val_loss=0.0000 scale=2.0000 norm=1.3115
[iter 300] loss=0.6743 val_loss=0.0000 scale=2.0000 norm=1.2111
[iter 400] loss=0.5724 val_loss=0.0000 scale=2.0000 norm=1.1566
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0632 val_loss=0.0000 scale=1.0000 norm=0.7380
[iter 200] loss=0.8156 val_loss=0.0000 scale=2.0000 norm=1.2820
[iter 300] loss=0.6663 val_loss=0.0000 scale=2.0000 norm=1.1977
[iter 400] loss=0.5571 val_loss=0.0000 scale=1.0000 norm=0.5698
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0779 val_loss=0.0000 scale=2.0000 norm=1.4859
[iter 200] loss=0.8466 val_loss=0.0000 scale=1.0000 norm=0.6519
[iter 300] loss=0.6751 val_loss=0.0000 scale=1.0000 norm=0.6083
[iter 400] loss=0.5796 val_loss=0.0000 scale=1.0000 norm=0.5857
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0591 val_loss=0.0000 scale=1.0000 norm=0.7332
[iter 200] loss=0.7919 val_loss=0.0000 scale=2.0000 norm=1.2497
[iter 300] loss=0.6381 val_loss=0.0000 scale=1.0000 norm=0.5844
[iter 400] loss=0.5437 val_loss=0.0000 scale=1.0000 norm=0.5606
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0980 val_loss=0.0000 scale=1.0000 norm=0.7574
[iter 200] loss=0.8876 val_loss=0.0000 scale=1.0000 norm=0.6660
[iter 300] loss=0.7267 val_loss=0.0000 scale=1.0000 norm=0.6166
[iter 400] loss=0.6226 val_loss=0.0000 scale=2.0000 norm=1.1740
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0859 val_loss=0.0000 scale=1.0000 norm=0.7486
[iter 200] loss=0.8207 val_loss=0.0000 scale=2.0000 norm=1.2839
[iter 300] loss=0.6655 val_loss=0.0000 scale=2.0000 norm=1.1958
[iter 400] loss=0.5634 val_loss=0.0000 scale=2.0000 norm=1.1398
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0859 val_loss=0.0000 scale=1.0000 norm=0.7486
[iter 200] loss=0.8207 val_loss=0.0000 scale=2.0000 norm=1.2839
[iter 300] loss=0.6655 val_loss=0.0000 scale=2.0000 norm=1.1958
[iter 400] loss=0.5634 val_loss=0.0000 scale=2.0000 norm=1.1398
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0650 val_loss=0.0000 scale=1.0000 norm=0.7364
[iter 200] loss=0.7978 val_loss=0.0000 scale=2.0000 norm=1.2650
[iter 300] loss=0.6549 val_loss=0.0000 scale=2.0000 norm=1.1801
[iter 400] loss=0.5651 val_loss=0.0000 scale=2.0000 norm=1.1360
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0650 val_loss=0.0000 scale=1.0000 norm=0.7364
[iter 200] loss=0.7978 val_loss=0.0000 scale=2.0000 norm=1.2650
[iter 300] loss=0.6549 val_loss=0.0000 scale=2.0000 norm=1.1801
[iter 400] loss=0.5651 val_loss=0.0000 scale=2.0000 norm=1.1360
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1001 val_loss=0.0000 scale=1.0000 norm=0.7573
[iter 200] loss=0.8659 val_loss=0.0000 scale=1.0000 norm=0.6582
[iter 300] loss=0.7018 val_loss=0.0000 scale=1.0000 norm=0.6121
[iter 400] loss=0.6019 val_loss=0.0000 scale=1.0000 norm=0.5878
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0832 val_loss=0.0000 scale=1.0000 norm=0.7476
[iter 200] loss=0.8391 val_loss=0.0000 scale=1.0000 norm=0.6496
[iter 300] loss=0.6766 val_loss=0.0000 scale=1.0000 norm=0.6044
[iter 400] loss=0.5728 val_loss=0.0000 scale=1.0000 norm=0.5774
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0838 val_loss=0.0000 scale=1.0000 norm=0.7459
[iter 200] loss=0.8282 val_loss=0.0000 scale=2.0000 norm=1.2885
[iter 300] loss=0.6809 val_loss=0.0000 scale=2.0000 norm=1.2042
[iter 400] loss=0.5911 val_loss=0.0000 scale=1.0000 norm=0.5787
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0832 val_loss=0.0000 scale=1.0000 norm=0.7476
[iter 200] loss=0.8391 val_loss=0.0000 scale=1.0000 norm=0.6496
[iter 300] loss=0.6766 val_loss=0.0000 scale=1.0000 norm=0.6044
[iter 400] loss=0.5728 val_loss=0.0000 scale=1.0000 norm=0.5774
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0838 val_loss=0.0000 scale=1.0000 norm=0.7459
[iter 200] loss=0.8282 val_loss=0.0000 scale=2.0000 norm=1.2885
[iter 300] loss=0.6809 val_loss=0.0000 scale=2.0000 norm=1.2042
[iter 400] loss=0.5911 val_loss=0.0000 scale=1.0000 norm=0.5787
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0146 val_loss=0.0000 scale=2.0000 norm=1.4118
[iter 200] loss=0.7125 val_loss=0.0000 scale=1.0000 norm=0.5921
[iter 300] loss=0.4815 val_loss=0.0000 scale=1.0000 norm=0.5396
[iter 400] loss=0.3409 val_loss=0.0000 scale=1.0000 norm=0.5141
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0061 val_loss=0.0000 scale=1.0000 norm=0.7019
[iter 200] loss=0.6952 val_loss=0.0000 scale=2.0000 norm=1.1616
[iter 300] loss=0.4535 val_loss=0.0000 scale=2.0000 norm=1.0450
[iter 400] loss=0.3023 val_loss=0.0000 scale=1.0000 norm=0.4959
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0177 val_loss=0.0000 scale=1.0000 norm=0.7040
[iter 200] loss=0.7334 val_loss=0.0000 scale=1.0000 norm=0.5896
[iter 300] loss=0.4877 val_loss=0.0000 scale=1.0000 norm=0.5268
[iter 400] loss=0.3280 val_loss=0.0000 scale=1.0000 norm=0.4984
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0087 val_loss=0.0000 scale=1.0000 norm=0.7001
[iter 200] loss=0.6868 val_loss=0.0000 scale=1.0000 norm=0.5785
[iter 300] loss=0.4269 val_loss=0.0000 scale=2.0000 norm=1.0334
[iter 400] loss=0.2621 val_loss=0.0000 scale=1.0000 norm=0.4892
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0189 val_loss=0.0000 scale=1.0000 norm=0.7094
[iter 200] loss=0.7421 val_loss=0.0000 scale=1.0000 norm=0.5973
[iter 300] loss=0.4932 val_loss=0.0000 scale=1.0000 norm=0.5352
[iter 400] loss=0.3403 val_loss=0.0000 scale=1.0000 norm=0.5091
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9987 val_loss=0.0000 scale=1.0000 norm=0.6954
[iter 200] loss=0.6879 val_loss=0.0000 scale=1.0000 norm=0.5750
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0689 val_loss=0.0000 scale=1.0000 norm=0.7359
[iter 200] loss=0.7935 val_loss=0.0000 scale=2.0000 norm=1.2653
[iter 300] loss=0.6371 val_loss=0.0000 scale=2.0000 norm=1.1854
[iter 400] loss=0.5315 val_loss=0.0000 scale=1.0000 norm=0.5637
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0791 val_loss=0.0000 scale=1.0000 norm=0.7448
[iter 200] loss=0.8243 val_loss=0.0000 scale=2.0000 norm=1.2849
[iter 300] loss=0.6557 val_loss=0.0000 scale=1.0000 norm=0.6020
[iter 400] loss=0.5504 val_loss=0.0000 scale=2.0000 norm=1.1505
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0831 val_loss=0.0000 scale=1.0000 norm=0.7470
[iter 200] loss=0.8347 val_loss=0.0000 scale=2.0000 norm=1.2902
[iter 300] loss=0.6647 val_loss=0.0000 scale=2.0000 norm=1.2038
[iter 400] loss=0.5660 val_loss=0.0000 scale=2.0000 norm=1.1474
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0714 val_loss=0.0000 scale=1.0000 norm=0.7409
[iter 200] loss=0.8154 val_loss=0.0000 scale=2.0000 norm=1.2813
[iter 300] loss=0.6560 val_loss=0.0000 scale=2.0000 norm=1.2006
[iter 400] loss=0.5535 val_loss=0.0000 scale=1.0000 norm=0.5730
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0920 val_loss=0.0000 scale=1.0000 norm=0.7516
[iter 200] loss=0.8469 val_loss=0.0000 scale=2.0000 norm=1.2991
[iter 300] loss=0.6740 val_loss=0.0000 scale=1.0000 norm=0.5993
[iter 400] loss=0.5621 val_loss=0.0000 scale=2.0000 norm=1.1461
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=1.0000 norm=0.7519
[iter 200] loss=0.8544 val_loss=0.0000 scale=2.0000 norm=1.3065
[iter 300] loss=0.6840 val_loss=0.0000 scale=1.0000 norm=0.6014
[iter 400] loss=0.5901 val_loss=0.0000 scale=1.0000 norm=0.5787
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=1.0000 norm=0.7519
[iter 200] loss=0.8544 val_loss=0.0000 scale=2.0000 norm=1.3065
[iter 300] loss=0.6840 val_loss=0.0000 scale=1.0000 norm=0.6014
[iter 400] loss=0.5901 val_loss=0.0000 scale=1.0000 norm=0.5787
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0771 val_loss=0.0000 scale=1.0000 norm=0.7401
[iter 200] loss=0.7968 val_loss=0.0000 scale=2.0000 norm=1.2573
[iter 300] loss=0.6366 val_loss=0.0000 scale=1.0000 norm=0.5859
[iter 400] loss=0.5272 val_loss=0.0000 scale=1.0000 norm=0.5559
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0771 val_loss=0.0000 scale=1.0000 norm=0.7401
[iter 200] loss=0.7968 val_loss=0.0000 scale=2.0000 norm=1.2573
[iter 300] loss=0.6366 val_loss=0.0000 scale=1.0000 norm=0.5859
[iter 400] loss=0.5272 val_loss=0.0000 scale=1.0000 norm=0.5559
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0738 val_loss=0.0000 scale=1.0000 norm=0.7418
[iter 200] loss=0.8496 val_loss=0.0000 scale=1.0000 norm=0.6497
[iter 300] loss=0.6842 val_loss=0.0000 scale=1.0000 norm=0.6039
[iter 400] loss=0.5810 val_loss=0.0000 scale=1.0000 norm=0.5751
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0738 val_loss=0.0000 scale=1.0000 norm=0.7418
[iter 200] loss=0.8496 val_loss=0.0000 scale=1.0000 norm=0.6497
[iter 300] loss=0.6842 val_loss=0.0000 scale=1.0000 norm=0.6039
[iter 400] loss=0.5810 val_loss=0.0000 scale=1.0000 norm=0.5751
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0642 val_loss=0.0000 scale=1.0000 norm=0.7342
[iter 200] loss=0.7860 val_loss=0.0000 scale=2.0000 norm=1.2561
[iter 300] loss=0.6216 val_loss=0.0000 scale=2.0000 norm=1.1702
[iter 400] loss=0.5166 val_loss=0.0000 scale=2.0000 norm=1.1185
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0642 val_loss=0.0000 scale=1.0000 norm=0.7342
[iter 200] loss=0.7860 val_loss=0.0000 scale=2.0000 norm=1.2561
[iter 300] loss=0.6216 val_loss=0.0000 scale=2.0000 norm=1.1702
[iter 400] loss=0.5166 val_loss=0.0000 scale=2.0000 norm=1.1185
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0879 val_loss=0.0000 scale=2.0000 norm=1.5035
[iter 200] loss=0.8493 val_loss=0.0000 scale=2.0000 norm=1.3013
[iter 300] loss=0.6816 val_loss=0.0000 scale=2.0000 norm=1.2123
[iter 400] loss=0.5764 val_loss=0.0000 scale=1.0000 norm=0.5801
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0879 val_loss=0.0000 scale=2.0000 norm=1.5035
[iter 200] loss=0.8493 val_loss=0.0000 scale=2.0000 norm=1.3013
[iter 300] loss=0.6816 val_loss=0.0000 scale=2.0000 norm=1.2123
[iter 400] loss=0.5764 val_loss=0.0000 scale=1.0000 norm=0.5801
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0061 val_loss=0.0000 scale=1.0000 norm=0.7019
[iter 200] loss=0.6952 val_loss=0.0000 scale=2.0000 norm=1.1616
[iter 300] loss=0.4535 val_loss=0.0000 scale=2.0000 norm=1.0450
[iter 400] loss=0.3023 val_loss=0.0000 scale=1.0000 norm=0.4959
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0171 val_loss=0.0000 scale=1.0000 norm=0.7018
[iter 200] loss=0.7139 val_loss=0.0000 scale=1.0000 norm=0.5821
[iter 300] loss=0.4540 val_loss=0.0000 scale=2.0000 norm=1.0391
[iter 400] loss=0.3135 val_loss=0.0000 scale=1.0000 norm=0.4927
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0000 val_loss=0.0000 scale=1.0000 norm=0.6950
[iter 200] loss=0.6866 val_loss=0.0000 scale=1.0000 norm=0.5752
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5188
[iter 400] loss=0.2664 val_loss=0.0000 scale=1.0000 norm=0.4935
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0237 val_loss=0.0000 scale=1.0000 norm=0.7115
[iter 200] loss=0.7274 val_loss=0.0000 scale=1.0000 norm=0.5933
[iter 300] loss=0.4928 val_loss=0.0000 scale=1.0000 norm=0.5379
[iter 400] loss=0.3299 val_loss=0.0000 scale=1.0000 norm=0.5087
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6697 val_loss=0.0000 scale=1.0000 norm=0.5707
[iter 300] loss=0.3954 val_loss=0.0000 scale=2.0000 norm=1.0231
[iter 400] loss=0.2474 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0094 val_loss=0.0000 scale=1.0000 norm=0.7034
[iter 200] loss=0.6912 val_loss=0.0000 scale=1.0000 norm=0.5822
[iter 300] loss=0.4473 val_loss=0.0000 scale=1.0000 norm=0.5222
[iter 400] loss=0.2870 val_loss=0.0000 scale=1.0000 norm=0.4947
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9909 val_loss=0.0000 scale=1.0000 norm=0.6893
[iter 200] loss=0.6865 val_loss=0.0000 scale=2.0000 norm=1.1491
[iter 300] loss=0.4279 val_loss=0.0000 scale=1.0000 norm=0.5151
[iter 400] loss=0.2763 val_loss=0.0000 scale=1.0000 norm=0.4872
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0143 val_loss=0.0000 scale=1.0000 norm=0.7072
[iter 200] loss=0.7399 val_loss=0.0000 scale=1.0000 norm=0.6010
[iter 300] loss=0.5179 val_loss=0.0000 scale=2.0000 norm=1.0875
[iter 400] loss=0.3595 val_loss=0.0000 scale=2.0000 norm=1.0238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0197 val_loss=0.0000 scale=1.0000 norm=0.7057
[iter 200] loss=0.7195 val_loss=0.0000 scale=2.0000 norm=1.1799
[iter 300] loss=0.4559 val_loss=0.0000 scale=2.0000 norm=1.0490
[iter 400] loss=0.3120 val_loss=0.0000 scale=1.0000 norm=0.4971
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0034 val_loss=0.0000 scale=1.0000 norm=0.6972
[iter 200] loss=0.6910 val_loss=0.0000 scale=1.0000 norm=0.5782
[iter 300] loss=0.4408 val_loss=0.0000 scale=2.0000 norm=1.0329
[iter 400] loss=0.2700 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9926 val_loss=0.0000 scale=1.0000 norm=0.6900
[iter 200] loss=0.6857 val_loss=0.0000 scale=2.0000 norm=1.1532
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0753 val_loss=0.0000 scale=2.0000 norm=1.4869
[iter 200] loss=0.8354 val_loss=0.0000 scale=2.0000 norm=1.2886
[iter 300] loss=0.6760 val_loss=0.0000 scale=2.0000 norm=1.2005
[iter 400] loss=0.5700 val_loss=0.0000 scale=2.0000 norm=1.1412
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0689 val_loss=0.0000 scale=1.0000 norm=0.7359
[iter 200] loss=0.7935 val_loss=0.0000 scale=2.0000 norm=1.2653
[iter 300] loss=0.6371 val_loss=0.0000 scale=2.0000 norm=1.1854
[iter 400] loss=0.5315 val_loss=0.0000 scale=1.0000 norm=0.5637
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0548 val_loss=0.0000 scale=2.0000 norm=1.4559
[iter 200] loss=0.7869 val_loss=0.0000 scale=2.0000 norm=1.2513
[iter 300] loss=0.6279 val_loss=0.0000 scale=1.0000 norm=0.5864
[iter 400] loss=0.5359 val_loss=0.0000 scale=1.0000 norm=0.5641
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0548 val_loss=0.0000 scale=2.0000 norm=1.4559
[iter 200] loss=0.7869 val_loss=0.0000 scale=2.0000 norm=1.2513
[iter 300] loss=0.6279 val_loss=0.0000 scale=1.0000 norm=0.5864
[iter 400] loss=0.5359 val_loss=0.0000 scale=1.0000 norm=0.5641
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0810 val_loss=0.0000 scale=1.0000 norm=0.7412
[iter 200] loss=0.7992 val_loss=0.0000 scale=2.0000 norm=1.2555
[iter 300] loss=0.6295 val_loss=0.0000 scale=2.0000 norm=1.1794
[iter 400] loss=0.5329 val_loss=0.0000 scale=1.0000 norm=0.5683
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0707 val_loss=0.0000 scale=1.0000 norm=0.7371
[iter 200] loss=0.7788 val_loss=0.0000 scale=2.0000 norm=1.2493
[iter 300] loss=0.6318 val_loss=0.0000 scale=1.0000 norm=0.5895
[iter 400] loss=0.5405 val_loss=0.0000 scale=1.0000 norm=0.5654
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0711 val_loss=0.0000 scale=1.0000 norm=0.7406
[iter 200] loss=0.8044 val_loss=0.0000 scale=2.0000 norm=1.2829
[iter 300] loss=0.6534 val_loss=0.0000 scale=1.0000 norm=0.5998
[iter 400] loss=0.5593 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0711 val_loss=0.0000 scale=1.0000 norm=0.7406
[iter 200] loss=0.8044 val_loss=0.0000 scale=2.0000 norm=1.2829
[iter 300] loss=0.6534 val_loss=0.0000 scale=1.0000 norm=0.5998
[iter 400] loss=0.5593 val_loss=0.0000 scale=1.0000 norm=0.5759
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0662 val_loss=0.0000 scale=1.0000 norm=0.7346
[iter 200] loss=0.8063 val_loss=0.0000 scale=1.0000 norm=0.6275
[iter 300] loss=0.6039 val_loss=0.0000 scale=2.0000 norm=1.1568
[iter 400] loss=0.4986 val_loss=0.0000 scale=2.0000 norm=1.1115
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0662 val_loss=0.0000 scale=1.0000 norm=0.7346
[iter 200] loss=0.8063 val_loss=0.0000 scale=1.0000 norm=0.6275
[iter 300] loss=0.6039 val_loss=0.0000 scale=2.0000 norm=1.1568
[iter 400] loss=0.4986 val_loss=0.0000 scale=2.0000 norm=1.1115
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0752 val_loss=0.0000 scale=1.0000 norm=0.7437
[iter 200] loss=0.8144 val_loss=0.0000 scale=2.0000 norm=1.2864
[iter 300] loss=0.6598 val_loss=0.0000 scale=2.0000 norm=1.2052
[iter 400] loss=0.5548 val_loss=0.0000 scale=1.0000 norm=0.5749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0752 val_loss=0.0000 scale=1.0000 norm=0.7437
[iter 200] loss=0.8144 val_loss=0.0000 scale=2.0000 norm=1.2864
[iter 300] loss=0.6598 val_loss=0.0000 scale=2.0000 norm=1.2052
[iter 400] loss=0.5548 val_loss=0.0000 scale=1.0000 norm=0.5749
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0512 val_loss=0.0000 scale=1.0000 norm=0.7250
[iter 200] loss=0.7942 val_loss=0.0000 scale=2.0000 norm=1.2600
[iter 300] loss=0.6250 val_loss=0.0000 scale=2.0000 norm=1.1769
[iter 400] loss=0.5326 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0512 val_loss=0.0000 scale=1.0000 norm=0.7250
[iter 200] loss=0.7942 val_loss=0.0000 scale=2.0000 norm=1.2600
[iter 300] loss=0.6250 val_loss=0.0000 scale=2.0000 norm=1.1769
[iter 400] loss=0.5326 val_loss=0.0000 scale=1.0000 norm=0.5644
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0171 val_loss=0.0000 scale=1.0000 norm=0.7018
[iter 200] loss=0.7139 val_loss=0.0000 scale=1.0000 norm=0.5821
[iter 300] loss=0.4540 val_loss=0.0000 scale=2.0000 norm=1.0391
[iter 400] loss=0.3135 val_loss=0.0000 scale=1.0000 norm=0.4927
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0000 val_loss=0.0000 scale=1.0000 norm=0.6950
[iter 200] loss=0.6866 val_loss=0.0000 scale=1.0000 norm=0.5752
[iter 300] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5188
[iter 400] loss=0.2664 val_loss=0.0000 scale=1.0000 norm=0.4935
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0146 val_loss=0.0000 scale=2.0000 norm=1.4118
[iter 200] loss=0.7125 val_loss=0.0000 scale=1.0000 norm=0.5921
[iter 300] loss=0.4815 val_loss=0.0000 scale=1.0000 norm=0.5396
[iter 400] loss=0.3409 val_loss=0.0000 scale=1.0000 norm=0.5141
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9842 val_loss=0.0000 scale=1.0000 norm=0.6875
[iter 200] loss=0.6697 val_loss=0.0000 scale=1.0000 norm=0.5707
[iter 300] loss=0.3954 val_loss=0.0000 scale=2.0000 norm=1.0231
[iter 400] loss=0.2474 val_loss=0.0000 scale=1.0000 norm=0.4879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0177 val_loss=0.0000 scale=1.0000 norm=0.7040
[iter 200] loss=0.7334 val_loss=0.0000 scale=1.0000 norm=0.5896
[iter 300] loss=0.4877 val_loss=0.0000 scale=1.0000 norm=0.5268
[iter 400] loss=0.3280 val_loss=0.0000 scale=1.0000 norm=0.4984
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9909 val_loss=0.0000 scale=1.0000 norm=0.6893
[iter 200] loss=0.6865 val_loss=0.0000 scale=2.0000 norm=1.1491
[iter 300] loss=0.4279 val_loss=0.0000 scale=1.0000 norm=0.5151
[iter 400] loss=0.2763 val_loss=0.0000 scale=1.0000 norm=0.4872
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0143 val_loss=0.0000 scale=1.0000 norm=0.7072
[iter 200] loss=0.7399 val_loss=0.0000 scale=1.0000 norm=0.6010
[iter 300] loss=0.5179 val_loss=0.0000 scale=2.0000 norm=1.0875
[iter 400] loss=0.3595 val_loss=0.0000 scale=2.0000 norm=1.0238
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9987 val_loss=0.0000 scale=1.0000 norm=0.6954
[iter 200] loss=0.6879 val_loss=0.0000 scale=1.0000 norm=0.5750
[iter 300] loss=0.4500 val_loss=0.0000 scale=1.0000 norm=0.5214
[iter 400] loss=0.2890 val_loss=0.0000 scale=1.0000 norm=0.4975
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0034 val_loss=0.0000 scale=1.0000 norm=0.6972
[iter 200] loss=0.6910 val_loss=0.0000 scale=1.0000 norm=0.5782
[iter 300] loss=0.4408 val_loss=0.0000 scale=2.0000 norm=1.0329
[iter 400] loss=0.2700 val_loss=0.0000 scale=1.0000 norm=0.4835
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0171 val_loss=0.0000 scale=2.0000 norm=1.4184
[iter 200] loss=0.7195 val_loss=0.0000 scale=1.0000 norm=0.5941
[iter 300] loss=0.4803 val_loss=0.0000 scale=1.0000 norm=0.5389
[iter 400] loss=0.3328 val_loss=0.0000 scale=1.0000 norm=0.5088
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0122 val_loss=0.0000 scale=1.0000 norm=0.7045
[iter 200] loss=0.7270 val_loss=0.0000 scale=1.0000 norm=0.5905
[iter 300] loss=0.4718 val_loss=0.0000 scale=2.0000 norm=1.0589
[iter 400] loss=0.3123 val_loss=0.0000 scale=1.0000 norm=0.5016
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0176 val_loss=0.0000 scale=1.0000 norm=0.7054
[iter 200] loss=0.7206 val_loss=0.0000 scale=1.0000 norm=0.5885
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
[iter 300] loss=0.4500 val_loss=0.0000 scale=1.0000 norm=0.5214
[iter 400] loss=0.2890 val_loss=0.0000 scale=1.0000 norm=0.4975
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0094 val_loss=0.0000 scale=1.0000 norm=0.7034
[iter 200] loss=0.6912 val_loss=0.0000 scale=1.0000 norm=0.5822
[iter 300] loss=0.4473 val_loss=0.0000 scale=1.0000 norm=0.5222
[iter 400] loss=0.2870 val_loss=0.0000 scale=1.0000 norm=0.4947
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.6965
[iter 200] loss=0.7282 val_loss=0.0000 scale=1.0000 norm=0.5905
[iter 300] loss=0.4878 val_loss=0.0000 scale=2.0000 norm=1.0632
[iter 400] loss=0.3257 val_loss=0.0000 scale=1.0000 norm=0.5055
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9910 val_loss=0.0000 scale=1.0000 norm=0.6904
[iter 200] loss=0.6607 val_loss=0.0000 scale=1.0000 norm=0.5678
[iter 300] loss=0.3958 val_loss=0.0000 scale=2.0000 norm=1.0212
[iter 400] loss=0.2387 val_loss=0.0000 scale=1.0000 norm=0.4825
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0197 val_loss=0.0000 scale=1.0000 norm=0.7057
[iter 200] loss=0.7195 val_loss=0.0000 scale=2.0000 norm=1.1799
[iter 300] loss=0.4559 val_loss=0.0000 scale=2.0000 norm=1.0490
[iter 400] loss=0.3120 val_loss=0.0000 scale=1.0000 norm=0.4971
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0105 val_loss=0.0000 scale=1.0000 norm=0.7023
[iter 200] loss=0.7028 val_loss=0.0000 scale=1.0000 norm=0.5841
[iter 300] loss=0.4330 val_loss=0.0000 scale=2.0000 norm=1.0468
[iter 400] loss=0.2697 val_loss=0.0000 scale=1.0000 norm=0.4943
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9926 val_loss=0.0000 scale=1.0000 norm=0.6900
[iter 200] loss=0.6857 val_loss=0.0000 scale=2.0000 norm=1.1532
[iter 300] loss=0.4207 val_loss=0.0000 scale=2.0000 norm=1.0378
[iter 400] loss=0.2609 val_loss=0.0000 scale=1.0000 norm=0.4938
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0073 val_loss=0.0000 scale=1.0000 norm=0.6973
[iter 200] loss=0.6683 val_loss=0.0000 scale=2.0000 norm=1.1447
[iter 300] loss=0.4353 val_loss=0.0000 scale=2.0000 norm=1.0358
[iter 400] loss=0.2959 val_loss=0.0000 scale=1.0000 norm=0.4881
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0267 val_loss=0.0000 scale=1.0000 norm=0.7108
[iter 200] loss=0.7213 val_loss=0.0000 scale=1.0000 norm=0.5912
[iter 300] loss=0.4904 val_loss=0.0000 scale=2.0000 norm=1.0597
[iter 400] loss=0.3215 val_loss=0.0000 scale=1.0000 norm=0.4982
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0043 val_loss=0.0000 scale=1.0000 norm=0.6969
[iter 200] loss=0.7023 val_loss=0.0000 scale=1.0000 norm=0.5814
[iter 300] loss=0.4377 val_loss=0.0000 scale=2.0000 norm=1.0467
[iter 400] loss=0.2871 val_loss=0.0000 scale=2.0000 norm=1.0008
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0204 val_loss=0.0000 scale=1.0000 norm=0.7085
[iter 200] loss=0.7275 val_loss=0.0000 scale=2.0000 norm=1.1806
[iter 300] loss=0.4799 val_loss=0.0000 scale=1.0000 norm=0.5260
[iter 400] loss=0.3113 val_loss=0.0000 scale=2.0000 norm=0.9895
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9965 val_loss=0.0000 scale=1.0000 norm=0.6940
[iter 200] loss=0.6615 val_loss=0.0000 scale=2.0000 norm=1.1394
[iter 300] loss=0.4314 val_loss=0.0000 scale=1.0000 norm=0.5154
[iter 400] loss=0.2824 val_loss=0.0000 scale=1.0000 norm=0.4910
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2345 val_loss=0.0000 scale=2.0000 norm=1.7260
[iter 200] loss=1.1331 val_loss=0.0000 scale=2.0000 norm=1.6425
[iter 300] loss=1.0849 val_loss=0.0000 scale=1.0000 norm=0.8002
[iter 400] loss=1.0359 val_loss=0.0000 scale=2.0000 norm=1.5549
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2345 val_loss=0.0000 scale=2.0000 norm=1.7260
[iter 200] loss=1.1331 val_loss=0.0000 scale=2.0000 norm=1.6425
[iter 300] loss=1.0849 val_loss=0.0000 scale=1.0000 norm=0.8002
[iter 400] loss=1.0359 val_loss=0.0000 scale=2.0000 norm=1.5549
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2080 val_loss=0.0000 scale=2.0000 norm=1.6928
[iter 200] loss=1.1058 val_loss=0.0000 scale=2.0000 norm=1.6094
[iter 300] loss=1.0510 val_loss=0.0000 scale=1.0000 norm=0.7810
[iter 400] loss=1.0018 val_loss=0.0000 scale=1.0000 norm=0.7580
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2099 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=1.1085 val_loss=0.0000 scale=1.0000 norm=0.8087
[iter 300] loss=1.0570 val_loss=0.0000 scale=2.0000 norm=1.5737
[iter 400] loss=0.9961 val_loss=0.0000 scale=1.0000 norm=0.7590
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2587 val_loss=0.0000 scale=1.0000 norm=0.8757
[iter 200] loss=1.1511 val_loss=0.0000 scale=2.0000 norm=1.6549
[iter 300] loss=1.0965 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 400] loss=1.0389 val_loss=0.0000 scale=2.0000 norm=1.5583
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2123 val_loss=0.0000 scale=2.0000 norm=1.7038
[iter 200] loss=1.1198 val_loss=0.0000 scale=1.0000 norm=0.8129
[iter 300] loss=1.0656 val_loss=0.0000 scale=2.0000 norm=1.5784
[iter 400] loss=1.0117 val_loss=0.0000 scale=2.0000 norm=1.5283
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2463 val_loss=0.0000 scale=1.0000 norm=0.8643
[iter 200] loss=1.1232 val_loss=0.0000 scale=2.0000 norm=1.6224
[iter 300] loss=1.0692 val_loss=0.0000 scale=2.0000 norm=1.5771
[iter 400] loss=1.0089 val_loss=0.0000 scale=1.0000 norm=0.7614
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2387 val_loss=0.0000 scale=2.0000 norm=1.7240
[iter 200] loss=1.1256 val_loss=0.0000 scale=1.0000 norm=0.8126
[iter 300] loss=1.0716 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 400] loss=1.0255 val_loss=0.0000 scale=2.0000 norm=1.5377
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2463 val_loss=0.0000 scale=1.0000 norm=0.8643
[iter 200] loss=1.1232 val_loss=0.0000 scale=2.0000 norm=1.6224
[iter 300] loss=1.0692 val_loss=0.0000 scale=2.0000 norm=1.5771
[iter 400] loss=1.0089 val_loss=0.0000 scale=1.0000 norm=0.7614
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2630 val_loss=0.0000 scale=2.0000 norm=1.7568
[iter 200] loss=1.1459 val_loss=0.0000 scale=2.0000 norm=1.6504
[iter 300] loss=1.0911 val_loss=0.0000 scale=1.0000 norm=0.8014
[iter 400] loss=1.0367 val_loss=0.0000 scale=2.0000 norm=1.5526
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1927 val_loss=0.0000 scale=2.0000 norm=1.6880
[iter 200] loss=1.1119 val_loss=0.0000 scale=1.0000 norm=0.8127
[iter 300] loss=1.0571 val_loss=0.0000 scale=1.0000 norm=0.7893
[iter 400] loss=1.0147 val_loss=0.0000 scale=1.0000 norm=0.7690
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2253 val_loss=0.0000 scale=2.0000 norm=1.7106
[iter 200] loss=1.1044 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 300] loss=1.0484 val_loss=0.0000 scale=1.0000 norm=0.7821
[iter 400] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5145
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2253 val_loss=0.0000 scale=2.0000 norm=1.7106
[iter 200] loss=1.1044 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 300] loss=1.0484 val_loss=0.0000 scale=1.0000 norm=0.7821
[iter 400] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5145
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2237 val_loss=0.0000 scale=2.0000 norm=1.7061
[iter 200] loss=1.1118 val_loss=0.0000 scale=2.0000 norm=1.6134
[iter 300] loss=1.0590 val_loss=0.0000 scale=1.0000 norm=0.7847
[iter 400] loss=1.0072 val_loss=0.0000 scale=1.0000 norm=0.7619
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 300] loss=0.4928 val_loss=0.0000 scale=1.0000 norm=0.5379
[iter 400] loss=0.3299 val_loss=0.0000 scale=1.0000 norm=0.5087
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0060 val_loss=0.0000 scale=1.0000 norm=0.6980
[iter 200] loss=0.6966 val_loss=0.0000 scale=2.0000 norm=1.1646
[iter 300] loss=0.4593 val_loss=0.0000 scale=2.0000 norm=1.0616
[iter 400] loss=0.2937 val_loss=0.0000 scale=1.0000 norm=0.5027
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0060 val_loss=0.0000 scale=1.0000 norm=0.6980
[iter 200] loss=0.6966 val_loss=0.0000 scale=2.0000 norm=1.1646
[iter 300] loss=0.4593 val_loss=0.0000 scale=2.0000 norm=1.0616
[iter 400] loss=0.2937 val_loss=0.0000 scale=1.0000 norm=0.5027
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0171 val_loss=0.0000 scale=2.0000 norm=1.4184
[iter 200] loss=0.7195 val_loss=0.0000 scale=1.0000 norm=0.5941
[iter 300] loss=0.4803 val_loss=0.0000 scale=1.0000 norm=0.5389
[iter 400] loss=0.3328 val_loss=0.0000 scale=1.0000 norm=0.5088
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=1.0000 norm=0.6965
[iter 200] loss=0.7282 val_loss=0.0000 scale=1.0000 norm=0.5905
[iter 300] loss=0.4878 val_loss=0.0000 scale=2.0000 norm=1.0632
[iter 400] loss=0.3257 val_loss=0.0000 scale=1.0000 norm=0.5055
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9910 val_loss=0.0000 scale=1.0000 norm=0.6904
[iter 200] loss=0.6607 val_loss=0.0000 scale=1.0000 norm=0.5678
[iter 300] loss=0.3958 val_loss=0.0000 scale=2.0000 norm=1.0212
[iter 400] loss=0.2387 val_loss=0.0000 scale=1.0000 norm=0.4825
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0218 val_loss=0.0000 scale=1.0000 norm=0.7100
[iter 200] loss=0.7165 val_loss=0.0000 scale=1.0000 norm=0.5912
[iter 300] loss=0.4712 val_loss=0.0000 scale=1.0000 norm=0.5312
[iter 400] loss=0.3123 val_loss=0.0000 scale=1.0000 norm=0.5006
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0105 val_loss=0.0000 scale=1.0000 norm=0.7023
[iter 200] loss=0.7028 val_loss=0.0000 scale=1.0000 norm=0.5841
[iter 300] loss=0.4330 val_loss=0.0000 scale=2.0000 norm=1.0468
[iter 400] loss=0.2697 val_loss=0.0000 scale=1.0000 norm=0.4943
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0073 val_loss=0.0000 scale=2.0000 norm=1.4028
[iter 200] loss=0.6909 val_loss=0.0000 scale=1.0000 norm=0.5834
[iter 300] loss=0.4289 val_loss=0.0000 scale=1.0000 norm=0.5249
[iter 400] loss=0.2759 val_loss=0.0000 scale=1.0000 norm=0.4983
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0073 val_loss=0.0000 scale=1.0000 norm=0.6973
[iter 200] loss=0.6683 val_loss=0.0000 scale=2.0000 norm=1.1447
[iter 300] loss=0.4353 val_loss=0.0000 scale=2.0000 norm=1.0358
[iter 400] loss=0.2959 val_loss=0.0000 scale=1.0000 norm=0.4881
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0099 val_loss=0.0000 scale=1.0000 norm=0.7018
[iter 200] loss=0.6828 val_loss=0.0000 scale=2.0000 norm=1.1575
[iter 300] loss=0.4358 val_loss=0.0000 scale=1.0000 norm=0.5257
[iter 400] loss=0.2925 val_loss=0.0000 scale=1.0000 norm=0.4983
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=1.0000 norm=0.7037
[iter 200] loss=0.6989 val_loss=0.0000 scale=2.0000 norm=1.1685
[iter 300] loss=0.4514 val_loss=0.0000 scale=1.0000 norm=0.5250
[iter 400] loss=0.3058 val_loss=0.0000 scale=1.0000 norm=0.4982
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0118 val_loss=0.0000 scale=1.0000 norm=0.7043
[iter 200] loss=0.7029 val_loss=0.0000 scale=1.0000 norm=0.5878
[iter 300] loss=0.4614 val_loss=0.0000 scale=1.0000 norm=0.5324
[iter 400] loss=0.2985 val_loss=0.0000 scale=1.0000 norm=0.5043
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2564 val_loss=0.0000 scale=2.0000 norm=1.7541
[iter 200] loss=1.1521 val_loss=0.0000 scale=1.0000 norm=0.8307
[iter 300] loss=1.1021 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 400] loss=1.0457 val_loss=0.0000 scale=2.0000 norm=1.5613
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=1.1119 val_loss=0.0000 scale=1.0000 norm=0.8019
[iter 300] loss=1.0534 val_loss=0.0000 scale=2.0000 norm=1.5574
[iter 400] loss=0.9954 val_loss=0.0000 scale=2.0000 norm=1.5073
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2504 val_loss=0.0000 scale=2.0000 norm=1.7388
[iter 200] loss=1.1365 val_loss=0.0000 scale=1.0000 norm=0.8200
[iter 300] loss=1.0799 val_loss=0.0000 scale=2.0000 norm=1.5911
[iter 400] loss=1.0320 val_loss=0.0000 scale=1.0000 norm=0.7729
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2250 val_loss=0.0000 scale=2.0000 norm=1.7149
[iter 200] loss=1.1229 val_loss=0.0000 scale=2.0000 norm=1.6272
[iter 300] loss=1.0706 val_loss=0.0000 scale=2.0000 norm=1.5831
[iter 400] loss=1.0165 val_loss=0.0000 scale=2.0000 norm=1.5369
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2207 val_loss=0.0000 scale=2.0000 norm=1.7119
[iter 200] loss=1.1241 val_loss=0.0000 scale=2.0000 norm=1.6301
[iter 300] loss=1.0599 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 400] loss=1.0112 val_loss=0.0000 scale=1.0000 norm=0.7653
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2207 val_loss=0.0000 scale=2.0000 norm=1.7119
[iter 200] loss=1.1241 val_loss=0.0000 scale=2.0000 norm=1.6301
[iter 300] loss=1.0599 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 400] loss=1.0112 val_loss=0.0000 scale=1.0000 norm=0.7653
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2529 val_loss=0.0000 scale=2.0000 norm=1.7426
[iter 200] loss=1.1422 val_loss=0.0000 scale=1.0000 norm=0.8225
[iter 300] loss=1.0792 val_loss=0.0000 scale=2.0000 norm=1.5923
[iter 400] loss=1.0186 val_loss=0.0000 scale=1.0000 norm=0.7691
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1979 val_loss=0.0000 scale=2.0000 norm=1.6822
[iter 200] loss=1.0971 val_loss=0.0000 scale=1.0000 norm=0.8004
[iter 300] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.5534
[iter 400] loss=0.9863 val_loss=0.0000 scale=2.0000 norm=1.5043
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2630 val_loss=0.0000 scale=2.0000 norm=1.7568
[iter 200] loss=1.1459 val_loss=0.0000 scale=2.0000 norm=1.6504
[iter 300] loss=1.0911 val_loss=0.0000 scale=1.0000 norm=0.8014
[iter 400] loss=1.0367 val_loss=0.0000 scale=2.0000 norm=1.5526
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1979 val_loss=0.0000 scale=2.0000 norm=1.6822
[iter 200] loss=1.0971 val_loss=0.0000 scale=1.0000 norm=0.8004
[iter 300] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.5534
[iter 400] loss=0.9863 val_loss=0.0000 scale=2.0000 norm=1.5043
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2479 val_loss=0.0000 scale=2.0000 norm=1.7461
[iter 200] loss=1.1568 val_loss=0.0000 scale=1.0000 norm=0.8333
[iter 300] loss=1.1077 val_loss=0.0000 scale=1.0000 norm=0.8114
[iter 400] loss=1.0603 val_loss=0.0000 scale=2.0000 norm=1.5785
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2290 val_loss=0.0000 scale=2.0000 norm=1.7099
[iter 200] loss=1.1160 val_loss=0.0000 scale=1.0000 norm=0.8053
[iter 300] loss=1.0620 val_loss=0.0000 scale=1.0000 norm=0.7832
[iter 400] loss=1.0124 val_loss=0.0000 scale=2.0000 norm=1.5214
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2209 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=1.1214 val_loss=0.0000 scale=1.0000 norm=0.8122
[iter 300] loss=1.0687 val_loss=0.0000 scale=1.0000 norm=0.7894
[iter 400] loss=1.0186 val_loss=0.0000 scale=2.0000 norm=1.5322
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 300] loss=0.4207 val_loss=0.0000 scale=2.0000 norm=1.0378
[iter 400] loss=0.2609 val_loss=0.0000 scale=1.0000 norm=0.4938
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0122 val_loss=0.0000 scale=1.0000 norm=0.7045
[iter 200] loss=0.7270 val_loss=0.0000 scale=1.0000 norm=0.5905
[iter 300] loss=0.4718 val_loss=0.0000 scale=2.0000 norm=1.0589
[iter 400] loss=0.3123 val_loss=0.0000 scale=1.0000 norm=0.5016
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0176 val_loss=0.0000 scale=1.0000 norm=0.7054
[iter 200] loss=0.7206 val_loss=0.0000 scale=1.0000 norm=0.5885
[iter 300] loss=0.4761 val_loss=0.0000 scale=2.0000 norm=1.0553
[iter 400] loss=0.3068 val_loss=0.0000 scale=2.0000 norm=0.9969
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0043 val_loss=0.0000 scale=1.0000 norm=0.6969
[iter 200] loss=0.7023 val_loss=0.0000 scale=1.0000 norm=0.5814
[iter 300] loss=0.4377 val_loss=0.0000 scale=2.0000 norm=1.0467
[iter 400] loss=0.2871 val_loss=0.0000 scale=2.0000 norm=1.0008
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0073 val_loss=0.0000 scale=2.0000 norm=1.4028
[iter 200] loss=0.6909 val_loss=0.0000 scale=1.0000 norm=0.5834
[iter 300] loss=0.4289 val_loss=0.0000 scale=1.0000 norm=0.5249
[iter 400] loss=0.2759 val_loss=0.0000 scale=1.0000 norm=0.4983
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0267 val_loss=0.0000 scale=1.0000 norm=0.7108
[iter 200] loss=0.7213 val_loss=0.0000 scale=1.0000 norm=0.5912
[iter 300] loss=0.4904 val_loss=0.0000 scale=2.0000 norm=1.0597
[iter 400] loss=0.3215 val_loss=0.0000 scale=1.0000 norm=0.4982
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0118 val_loss=0.0000 scale=1.0000 norm=0.7043
[iter 200] loss=0.7029 val_loss=0.0000 scale=1.0000 norm=0.5878
[iter 300] loss=0.4614 val_loss=0.0000 scale=1.0000 norm=0.5324
[iter 400] loss=0.2985 val_loss=0.0000 scale=1.0000 norm=0.5043
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0204 val_loss=0.0000 scale=1.0000 norm=0.7085
[iter 200] loss=0.7275 val_loss=0.0000 scale=2.0000 norm=1.1806
[iter 300] loss=0.4799 val_loss=0.0000 scale=1.0000 norm=0.5260
[iter 400] loss=0.3113 val_loss=0.0000 scale=2.0000 norm=0.9895
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=1.1119 val_loss=0.0000 scale=1.0000 norm=0.8019
[iter 300] loss=1.0534 val_loss=0.0000 scale=2.0000 norm=1.5574
[iter 400] loss=0.9954 val_loss=0.0000 scale=2.0000 norm=1.5073
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2379 val_loss=0.0000 scale=2.0000 norm=1.7250
[iter 200] loss=1.1181 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 300] loss=1.0637 val_loss=0.0000 scale=2.0000 norm=1.5775
[iter 400] loss=1.0111 val_loss=0.0000 scale=2.0000 norm=1.5331
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2165 val_loss=0.0000 scale=2.0000 norm=1.7037
[iter 200] loss=1.1136 val_loss=0.0000 scale=1.0000 norm=0.8110
[iter 300] loss=1.0583 val_loss=0.0000 scale=1.0000 norm=0.7868
[iter 400] loss=1.0125 val_loss=0.0000 scale=2.0000 norm=1.5327
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2379 val_loss=0.0000 scale=2.0000 norm=1.7250
[iter 200] loss=1.1181 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 300] loss=1.0637 val_loss=0.0000 scale=2.0000 norm=1.5775
[iter 400] loss=1.0111 val_loss=0.0000 scale=2.0000 norm=1.5331
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2587 val_loss=0.0000 scale=1.0000 norm=0.8757
[iter 200] loss=1.1511 val_loss=0.0000 scale=2.0000 norm=1.6549
[iter 300] loss=1.0965 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 400] loss=1.0389 val_loss=0.0000 scale=2.0000 norm=1.5583
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2080 val_loss=0.0000 scale=2.0000 norm=1.6928
[iter 200] loss=1.1058 val_loss=0.0000 scale=2.0000 norm=1.6094
[iter 300] loss=1.0510 val_loss=0.0000 scale=1.0000 norm=0.7810
[iter 400] loss=1.0018 val_loss=0.0000 scale=1.0000 norm=0.7580
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2099 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=1.1085 val_loss=0.0000 scale=1.0000 norm=0.8087
[iter 300] loss=1.0570 val_loss=0.0000 scale=2.0000 norm=1.5737
[iter 400] loss=0.9961 val_loss=0.0000 scale=1.0000 norm=0.7590
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2387 val_loss=0.0000 scale=2.0000 norm=1.7240
[iter 200] loss=1.1256 val_loss=0.0000 scale=1.0000 norm=0.8126
[iter 300] loss=1.0716 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 400] loss=1.0255 val_loss=0.0000 scale=2.0000 norm=1.5377
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2396 val_loss=0.0000 scale=2.0000 norm=1.7289
[iter 200] loss=1.1311 val_loss=0.0000 scale=1.0000 norm=0.8182
[iter 300] loss=1.0803 val_loss=0.0000 scale=1.0000 norm=0.7966
[iter 400] loss=1.0240 val_loss=0.0000 scale=2.0000 norm=1.5431
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1927 val_loss=0.0000 scale=2.0000 norm=1.6880
[iter 200] loss=1.1119 val_loss=0.0000 scale=1.0000 norm=0.8127
[iter 300] loss=1.0571 val_loss=0.0000 scale=1.0000 norm=0.7893
[iter 400] loss=1.0147 val_loss=0.0000 scale=1.0000 norm=0.7690
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2529 val_loss=0.0000 scale=2.0000 norm=1.7426
[iter 200] loss=1.1422 val_loss=0.0000 scale=1.0000 norm=0.8225
[iter 300] loss=1.0792 val_loss=0.0000 scale=2.0000 norm=1.5923
[iter 400] loss=1.0186 val_loss=0.0000 scale=1.0000 norm=0.7691
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2290 val_loss=0.0000 scale=2.0000 norm=1.7099
[iter 200] loss=1.1160 val_loss=0.0000 scale=1.0000 norm=0.8053
[iter 300] loss=1.0620 val_loss=0.0000 scale=1.0000 norm=0.7832
[iter 400] loss=1.0124 val_loss=0.0000 scale=2.0000 norm=1.5214
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2237 val_loss=0.0000 scale=2.0000 norm=1.7061
[iter 200] loss=1.1118 val_loss=0.0000 scale=2.0000 norm=1.6134
[iter 300] loss=1.0590 val_loss=0.0000 scale=1.0000 norm=0.7847
[iter 400] loss=1.0072 val_loss=0.0000 scale=1.0000 norm=0.7619
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2479 val_loss=0.0000 scale=2.0000 norm=1.7461
[iter 200] loss=1.1568 val_loss=0.0000 scale=1.0000 norm=0.8333
[iter 300] loss=1.1077 val_loss=0.0000 scale=1.0000 norm=0.8114
[iter 400] loss=1.0603 val_loss=0.0000 scale=2.0000 norm=1.5785
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2205 val_loss=0.0000 scale=2.0000 norm=1.7101
[iter 200] loss=1.1271 val_loss=0.0000 scale=1.0000 norm=0.8163
[iter 300] loss=1.0785 val_loss=0.0000 scale=2.0000 norm=1.5911
[iter 400] loss=1.0302 val_loss=0.0000 scale=2.0000 norm=1.5496
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2209 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=1.1214 val_loss=0.0000 scale=1.0000 norm=0.8122
[iter 300] loss=1.0687 val_loss=0.0000 scale=1.0000 norm=0.7894
[iter 400] loss=1.0186 val_loss=0.0000 scale=2.0000 norm=1.5322
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2465 val_loss=0.0000 scale=2.0000 norm=1.7369
[iter 200] loss=1.1390 val_loss=0.0000 scale=1.0000 norm=0.8224
[iter 300] loss=1.0930 val_loss=0.0000 scale=1.0000 norm=0.8017
[iter 400] loss=1.0424 val_loss=0.0000 scale=2.0000 norm=1.5557
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2225 val_loss=0.0000 scale=2.0000 norm=1.7066
[iter 200] loss=1.1116 val_loss=0.0000 scale=1.0000 norm=0.8080
[iter 300] loss=1.0598 val_loss=0.0000 scale=2.0000 norm=1.5741
[iter 400] loss=1.0153 val_loss=0.0000 scale=2.0000 norm=1.5344
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 300] loss=0.4761 val_loss=0.0000 scale=2.0000 norm=1.0553
[iter 400] loss=0.3068 val_loss=0.0000 scale=2.0000 norm=0.9969
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0218 val_loss=0.0000 scale=1.0000 norm=0.7100
[iter 200] loss=0.7165 val_loss=0.0000 scale=1.0000 norm=0.5912
[iter 300] loss=0.4712 val_loss=0.0000 scale=1.0000 norm=0.5312
[iter 400] loss=0.3123 val_loss=0.0000 scale=1.0000 norm=0.5006
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=0.6909
[iter 200] loss=0.6885 val_loss=0.0000 scale=1.0000 norm=0.5744
[iter 300] loss=0.4595 val_loss=0.0000 scale=2.0000 norm=1.0376
[iter 400] loss=0.2901 val_loss=0.0000 scale=1.0000 norm=0.4875
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=0.6909
[iter 200] loss=0.6885 val_loss=0.0000 scale=1.0000 norm=0.5744
[iter 300] loss=0.4595 val_loss=0.0000 scale=2.0000 norm=1.0376
[iter 400] loss=0.2901 val_loss=0.0000 scale=1.0000 norm=0.4875
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9965 val_loss=0.0000 scale=1.0000 norm=0.6940
[iter 200] loss=0.6615 val_loss=0.0000 scale=2.0000 norm=1.1394
[iter 300] loss=0.4314 val_loss=0.0000 scale=1.0000 norm=0.5154
[iter 400] loss=0.2824 val_loss=0.0000 scale=1.0000 norm=0.4910
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0099 val_loss=0.0000 scale=1.0000 norm=0.7018
[iter 200] loss=0.6828 val_loss=0.0000 scale=2.0000 norm=1.1575
[iter 300] loss=0.4358 val_loss=0.0000 scale=1.0000 norm=0.5257
[iter 400] loss=0.2925 val_loss=0.0000 scale=1.0000 norm=0.4983
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=1.0000 norm=0.7037
[iter 200] loss=0.6989 val_loss=0.0000 scale=2.0000 norm=1.1685
[iter 300] loss=0.4514 val_loss=0.0000 scale=1.0000 norm=0.5250
[iter 400] loss=0.3058 val_loss=0.0000 scale=1.0000 norm=0.4982
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2165 val_loss=0.0000 scale=2.0000 norm=1.7037
[iter 200] loss=1.1136 val_loss=0.0000 scale=1.0000 norm=0.8110
[iter 300] loss=1.0583 val_loss=0.0000 scale=1.0000 norm=0.7868
[iter 400] loss=1.0125 val_loss=0.0000 scale=2.0000 norm=1.5327
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2564 val_loss=0.0000 scale=2.0000 norm=1.7541
[iter 200] loss=1.1521 val_loss=0.0000 scale=1.0000 norm=0.8307
[iter 300] loss=1.1021 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 400] loss=1.0457 val_loss=0.0000 scale=2.0000 norm=1.5613
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2250 val_loss=0.0000 scale=2.0000 norm=1.7149
[iter 200] loss=1.1229 val_loss=0.0000 scale=2.0000 norm=1.6272
[iter 300] loss=1.0706 val_loss=0.0000 scale=2.0000 norm=1.5831
[iter 400] loss=1.0165 val_loss=0.0000 scale=2.0000 norm=1.5369
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2504 val_loss=0.0000 scale=2.0000 norm=1.7388
[iter 200] loss=1.1365 val_loss=0.0000 scale=1.0000 norm=0.8200
[iter 300] loss=1.0799 val_loss=0.0000 scale=2.0000 norm=1.5911
[iter 400] loss=1.0320 val_loss=0.0000 scale=1.0000 norm=0.7729
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2396 val_loss=0.0000 scale=2.0000 norm=1.7289
[iter 200] loss=1.1311 val_loss=0.0000 scale=1.0000 norm=0.8182
[iter 300] loss=1.0803 val_loss=0.0000 scale=1.0000 norm=0.7966
[iter 400] loss=1.0240 val_loss=0.0000 scale=2.0000 norm=1.5431
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2123 val_loss=0.0000 scale=2.0000 norm=1.7038
[iter 200] loss=1.1198 val_loss=0.0000 scale=1.0000 norm=0.8129
[iter 300] loss=1.0656 val_loss=0.0000 scale=2.0000 norm=1.5784
[iter 400] loss=1.0117 val_loss=0.0000 scale=2.0000 norm=1.5283
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2236 val_loss=0.0000 scale=2.0000 norm=1.7050
[iter 200] loss=1.1189 val_loss=0.0000 scale=1.0000 norm=0.8082
[iter 300] loss=1.0678 val_loss=0.0000 scale=2.0000 norm=1.5742
[iter 400] loss=1.0148 val_loss=0.0000 scale=1.0000 norm=0.7642
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2236 val_loss=0.0000 scale=2.0000 norm=1.7050
[iter 200] loss=1.1189 val_loss=0.0000 scale=1.0000 norm=0.8082
[iter 300] loss=1.0678 val_loss=0.0000 scale=2.0000 norm=1.5742
[iter 400] loss=1.0148 val_loss=0.0000 scale=1.0000 norm=0.7642
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2275 val_loss=0.0000 scale=2.0000 norm=1.7245
[iter 200] loss=1.1327 val_loss=0.0000 scale=2.0000 norm=1.6429
[iter 300] loss=1.0762 val_loss=0.0000 scale=2.0000 norm=1.5949
[iter 400] loss=1.0286 val_loss=0.0000 scale=2.0000 norm=1.5503
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2275 val_loss=0.0000 scale=2.0000 norm=1.7245
[iter 200] loss=1.1327 val_loss=0.0000 scale=2.0000 norm=1.6429
[iter 300] loss=1.0762 val_loss=0.0000 scale=2.0000 norm=1.5949
[iter 400] loss=1.0286 val_loss=0.0000 scale=2.0000 norm=1.5503
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1914 val_loss=0.0000 scale=2.0000 norm=1.6738
[iter 200] loss=1.0937 val_loss=0.0000 scale=1.0000 norm=0.7984
[iter 300] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.5518
[iter 400] loss=0.9839 val_loss=0.0000 scale=1.0000 norm=0.7510
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2205 val_loss=0.0000 scale=2.0000 norm=1.7101
[iter 200] loss=1.1271 val_loss=0.0000 scale=1.0000 norm=0.8163
[iter 300] loss=1.0785 val_loss=0.0000 scale=2.0000 norm=1.5911
[iter 400] loss=1.0302 val_loss=0.0000 scale=2.0000 norm=1.5496
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2151 val_loss=0.0000 scale=2.0000 norm=1.6998
[iter 200] loss=1.1127 val_loss=0.0000 scale=2.0000 norm=1.6148
[iter 300] loss=1.0575 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 400] loss=0.9975 val_loss=0.0000 scale=1.0000 norm=0.7568
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2465 val_loss=0.0000 scale=2.0000 norm=1.7369
[iter 200] loss=1.1390 val_loss=0.0000 scale=1.0000 norm=0.8224
[iter 300] loss=1.0930 val_loss=0.0000 scale=1.0000 norm=0.8017
[iter 400] loss=1.0424 val_loss=0.0000 scale=2.0000 norm=1.5557
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=1.0000 norm=0.7545
[iter 200] loss=0.8820 val_loss=0.0000 scale=1.0000 norm=0.6615
[iter 300] loss=0.7194 val_loss=0.0000 scale=1.0000 norm=0.6065
[iter 400] loss=0.5959 val_loss=0.0000 scale=1.0000 norm=0.5744
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1053 val_loss=0.0000 scale=1.0000 norm=0.7688
[iter 200] loss=0.8852 val_loss=0.0000 scale=1.0000 norm=0.6713
[iter 300] loss=0.7291 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6198 val_loss=0.0000 scale=1.0000 norm=0.5906
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1062 val_loss=0.0000 scale=1.0000 norm=0.7694
[iter 200] loss=0.8766 val_loss=0.0000 scale=1.0000 norm=0.6695
[iter 300] loss=0.7025 val_loss=0.0000 scale=1.0000 norm=0.6204
[iter 400] loss=0.5942 val_loss=0.0000 scale=1.0000 norm=0.5908
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1097 val_loss=0.0000 scale=1.0000 norm=0.7689
[iter 200] loss=0.9058 val_loss=0.0000 scale=1.0000 norm=0.6750
[iter 300] loss=0.7405 val_loss=0.0000 scale=1.0000 norm=0.6178
[iter 400] loss=0.6174 val_loss=0.0000 scale=1.0000 norm=0.5815
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0839 val_loss=0.0000 scale=1.0000 norm=0.7574
[iter 200] loss=0.8620 val_loss=0.0000 scale=1.0000 norm=0.6592
[iter 300] loss=0.6928 val_loss=0.0000 scale=1.0000 norm=0.6027
[iter 400] loss=0.5666 val_loss=0.0000 scale=1.0000 norm=0.5667
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2419 val_loss=0.0000 scale=2.0000 norm=1.7281
[iter 200] loss=1.1335 val_loss=0.0000 scale=2.0000 norm=1.6355
[iter 300] loss=1.0723 val_loss=0.0000 scale=1.0000 norm=0.7917
[iter 400] loss=1.0198 val_loss=0.0000 scale=1.0000 norm=0.7673
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1914 val_loss=0.0000 scale=2.0000 norm=1.6738
[iter 200] loss=1.0937 val_loss=0.0000 scale=1.0000 norm=0.7984
[iter 300] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.5518
[iter 400] loss=0.9839 val_loss=0.0000 scale=1.0000 norm=0.7510
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2602 val_loss=0.0000 scale=2.0000 norm=1.7535
[iter 200] loss=1.1466 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 300] loss=1.0955 val_loss=0.0000 scale=1.0000 norm=0.8041
[iter 400] loss=1.0461 val_loss=0.0000 scale=2.0000 norm=1.5613
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2597 val_loss=0.0000 scale=1.0000 norm=0.8752
[iter 200] loss=1.1484 val_loss=0.0000 scale=1.0000 norm=0.8232
[iter 300] loss=1.0915 val_loss=0.0000 scale=1.0000 norm=0.7993
[iter 400] loss=1.0330 val_loss=0.0000 scale=2.0000 norm=1.5486
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1996 val_loss=0.0000 scale=2.0000 norm=1.6965
[iter 200] loss=1.1129 val_loss=0.0000 scale=1.0000 norm=0.8133
[iter 300] loss=1.0514 val_loss=0.0000 scale=2.0000 norm=1.5730
[iter 400] loss=1.0071 val_loss=0.0000 scale=2.0000 norm=1.5304
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2597 val_loss=0.0000 scale=1.0000 norm=0.8752
[iter 200] loss=1.1484 val_loss=0.0000 scale=1.0000 norm=0.8232
[iter 300] loss=1.0915 val_loss=0.0000 scale=1.0000 norm=0.7993
[iter 400] loss=1.0330 val_loss=0.0000 scale=2.0000 norm=1.5486
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1996 val_loss=0.0000 scale=2.0000 norm=1.6965
[iter 200] loss=1.1129 val_loss=0.0000 scale=1.0000 norm=0.8133
[iter 300] loss=1.0514 val_loss=0.0000 scale=2.0000 norm=1.5730
[iter 400] loss=1.0071 val_loss=0.0000 scale=2.0000 norm=1.5304
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1062 val_loss=0.0000 scale=1.0000 norm=0.7694
[iter 200] loss=0.8766 val_loss=0.0000 scale=1.0000 norm=0.6695
[iter 300] loss=0.7025 val_loss=0.0000 scale=1.0000 norm=0.6204
[iter 400] loss=0.5942 val_loss=0.0000 scale=1.0000 norm=0.5908
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=1.0000 norm=0.7545
[iter 200] loss=0.8820 val_loss=0.0000 scale=1.0000 norm=0.6615
[iter 300] loss=0.7194 val_loss=0.0000 scale=1.0000 norm=0.6065
[iter 400] loss=0.5959 val_loss=0.0000 scale=1.0000 norm=0.5744
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1042 val_loss=0.0000 scale=1.0000 norm=0.7667
[iter 200] loss=0.8953 val_loss=0.0000 scale=1.0000 norm=0.6719
[iter 300] loss=0.7199 val_loss=0.0000 scale=2.0000 norm=1.2280
[iter 400] loss=0.5894 val_loss=0.0000 scale=1.0000 norm=0.5789
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1038 val_loss=0.0000 scale=1.0000 norm=0.7648
[iter 200] loss=0.8833 val_loss=0.0000 scale=2.0000 norm=1.3338
[iter 300] loss=0.7220 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6017 val_loss=0.0000 scale=1.0000 norm=0.5844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1131 val_loss=0.0000 scale=1.0000 norm=0.7749
[iter 200] loss=0.9094 val_loss=0.0000 scale=1.0000 norm=0.6814
[iter 300] loss=0.7514 val_loss=0.0000 scale=1.0000 norm=0.6302
[iter 400] loss=0.6277 val_loss=0.0000 scale=1.0000 norm=0.5947
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0980 val_loss=0.0000 scale=1.0000 norm=0.7639
[iter 200] loss=0.8855 val_loss=0.0000 scale=1.0000 norm=0.6702
[iter 300] loss=0.7259 val_loss=0.0000 scale=1.0000 norm=0.6181
[iter 400] loss=0.6118 val_loss=0.0000 scale=1.0000 norm=0.5862
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1026 val_loss=0.0000 scale=1.0000 norm=0.7630
[iter 200] loss=0.9017 val_loss=0.0000 scale=1.0000 norm=0.6746
[iter 300] loss=0.7343 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6135 val_loss=0.0000 scale=1.0000 norm=0.5849
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1045 val_loss=0.0000 scale=1.0000 norm=0.7676
[iter 200] loss=0.8937 val_loss=0.0000 scale=2.0000 norm=1.3452
[iter 300] loss=0.7268 val_loss=0.0000 scale=1.0000 norm=0.6179
[iter 400] loss=0.6053 val_loss=0.0000 scale=1.0000 norm=0.5851
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1081 val_loss=0.0000 scale=1.0000 norm=0.7715
[iter 200] loss=0.9094 val_loss=0.0000 scale=1.0000 norm=0.6789
[iter 300] loss=0.7477 val_loss=0.0000 scale=1.0000 norm=0.6260
[iter 400] loss=0.6284 val_loss=0.0000 scale=1.0000 norm=0.5928
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0778 val_loss=0.0000 scale=1.0000 norm=0.7513
[iter 200] loss=0.8584 val_loss=0.0000 scale=1.0000 norm=0.6546
[iter 300] loss=0.6835 val_loss=0.0000 scale=1.0000 norm=0.6015
[iter 400] loss=0.5679 val_loss=0.0000 scale=1.0000 norm=0.5718
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1115 val_loss=0.0000 scale=1.0000 norm=0.7709
[iter 200] loss=0.8822 val_loss=0.0000 scale=1.0000 norm=0.6689
[iter 300] loss=0.7089 val_loss=0.0000 scale=1.0000 norm=0.6143
[iter 400] loss=0.5955 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1124 val_loss=0.0000 scale=1.0000 norm=0.7745
[iter 200] loss=0.9122 val_loss=0.0000 scale=1.0000 norm=0.6785
[iter 300] loss=0.7469 val_loss=0.0000 scale=1.0000 norm=0.6196
[iter 400] loss=0.6256 val_loss=0.0000 scale=1.0000 norm=0.5818
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0947 val_loss=0.0000 scale=1.0000 norm=0.7608
[iter 200] loss=0.8988 val_loss=0.0000 scale=1.0000 norm=0.6744
[iter 300] loss=0.7334 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6204 val_loss=0.0000 scale=1.0000 norm=0.5879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1033 val_loss=0.0000 scale=1.0000 norm=0.7693
[iter 200] loss=0.8935 val_loss=0.0000 scale=2.0000 norm=1.3506
[iter 300] loss=0.7362 val_loss=0.0000 scale=2.0000 norm=1.2481
[iter 400] loss=0.6040 val_loss=0.0000 scale=1.0000 norm=0.5888
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.8927 val_loss=0.0000 scale=1.0000 norm=0.6735
[iter 300] loss=0.7214 val_loss=0.0000 scale=1.0000 norm=0.6196
[iter 400] loss=0.5833 val_loss=0.0000 scale=1.0000 norm=0.5790
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1137 val_loss=0.0000 scale=1.0000 norm=0.7741
[iter 200] loss=0.9064 val_loss=0.0000 scale=1.0000 norm=0.6787
[iter 300] loss=0.7279 val_loss=0.0000 scale=2.0000 norm=1.2375
[iter 400] loss=0.6184 val_loss=0.0000 scale=1.0000 norm=0.5887
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1007 val_loss=0.0000 scale=1.0000 norm=0.7627
[iter 200] loss=0.8871 val_loss=0.0000 scale=2.0000 norm=1.3364
[iter 300] loss=0.7247 val_loss=0.0000 scale=2.0000 norm=1.2321
[iter 400] loss=0.6036 val_loss=0.0000 scale=1.0000 norm=0.5828
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0944 val_loss=0.0000 scale=1.0000 norm=0.7593
[iter 200] loss=0.8835 val_loss=0.0000 scale=1.0000 norm=0.6645
[iter 300] loss=0.7129 val_loss=0.0000 scale=1.0000 norm=0.6131
[iter 400] loss=0.5792 val_loss=0.0000 scale=1.0000 norm=0.5802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0944 val_loss=0.0000 scale=1.0000 norm=0.7593
[iter 200] loss=0.8835 val_loss=0.0000 scale=1.0000 norm=0.6645
[iter 300] loss=0.7129 val_loss=0.0000 scale=1.0000 norm=0.6131
[iter 100] loss=1.2602 val_loss=0.0000 scale=2.0000 norm=1.7535
[iter 200] loss=1.1466 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 300] loss=1.0955 val_loss=0.0000 scale=1.0000 norm=0.8041
[iter 400] loss=1.0461 val_loss=0.0000 scale=2.0000 norm=1.5613
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2419 val_loss=0.0000 scale=2.0000 norm=1.7281
[iter 200] loss=1.1335 val_loss=0.0000 scale=2.0000 norm=1.6355
[iter 300] loss=1.0723 val_loss=0.0000 scale=1.0000 norm=0.7917
[iter 400] loss=1.0198 val_loss=0.0000 scale=1.0000 norm=0.7673
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2225 val_loss=0.0000 scale=2.0000 norm=1.7066
[iter 200] loss=1.1116 val_loss=0.0000 scale=1.0000 norm=0.8080
[iter 300] loss=1.0598 val_loss=0.0000 scale=2.0000 norm=1.5741
[iter 400] loss=1.0153 val_loss=0.0000 scale=2.0000 norm=1.5344
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2151 val_loss=0.0000 scale=2.0000 norm=1.6998
[iter 200] loss=1.1127 val_loss=0.0000 scale=2.0000 norm=1.6148
[iter 300] loss=1.0575 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 400] loss=0.9975 val_loss=0.0000 scale=1.0000 norm=0.7568
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0976 val_loss=0.0000 scale=1.0000 norm=0.7648
[iter 200] loss=0.8764 val_loss=0.0000 scale=2.0000 norm=1.3293
[iter 300] loss=0.6900 val_loss=0.0000 scale=1.0000 norm=0.6057
[iter 400] loss=0.5669 val_loss=0.0000 scale=1.0000 norm=0.5772
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7640
[iter 200] loss=0.8575 val_loss=0.0000 scale=1.0000 norm=0.6597
[iter 300] loss=0.7054 val_loss=0.0000 scale=2.0000 norm=1.2216
[iter 400] loss=0.5944 val_loss=0.0000 scale=1.0000 norm=0.5803
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0839 val_loss=0.0000 scale=1.0000 norm=0.7574
[iter 200] loss=0.8620 val_loss=0.0000 scale=1.0000 norm=0.6592
[iter 300] loss=0.6928 val_loss=0.0000 scale=1.0000 norm=0.6027
[iter 400] loss=0.5666 val_loss=0.0000 scale=1.0000 norm=0.5667
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1097 val_loss=0.0000 scale=1.0000 norm=0.7689
[iter 200] loss=0.9058 val_loss=0.0000 scale=1.0000 norm=0.6750
[iter 300] loss=0.7405 val_loss=0.0000 scale=1.0000 norm=0.6178
[iter 400] loss=0.6174 val_loss=0.0000 scale=1.0000 norm=0.5815
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1038 val_loss=0.0000 scale=1.0000 norm=0.7648
[iter 200] loss=0.8833 val_loss=0.0000 scale=2.0000 norm=1.3338
[iter 300] loss=0.7220 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6017 val_loss=0.0000 scale=1.0000 norm=0.5844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1045 val_loss=0.0000 scale=1.0000 norm=0.7676
[iter 200] loss=0.8937 val_loss=0.0000 scale=2.0000 norm=1.3452
[iter 300] loss=0.7268 val_loss=0.0000 scale=1.0000 norm=0.6179
[iter 400] loss=0.6053 val_loss=0.0000 scale=1.0000 norm=0.5851
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1053 val_loss=0.0000 scale=1.0000 norm=0.7645
[iter 200] loss=0.8624 val_loss=0.0000 scale=1.0000 norm=0.6557
[iter 300] loss=0.6850 val_loss=0.0000 scale=1.0000 norm=0.6045
[iter 400] loss=0.5753 val_loss=0.0000 scale=1.0000 norm=0.5796
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1026 val_loss=0.0000 scale=1.0000 norm=0.7630
[iter 200] loss=0.9017 val_loss=0.0000 scale=1.0000 norm=0.6746
[iter 300] loss=0.7343 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6135 val_loss=0.0000 scale=1.0000 norm=0.5849
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1115 val_loss=0.0000 scale=1.0000 norm=0.7709
[iter 200] loss=0.8822 val_loss=0.0000 scale=1.0000 norm=0.6689
[iter 300] loss=0.7089 val_loss=0.0000 scale=1.0000 norm=0.6143
[iter 400] loss=0.5955 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0898 val_loss=0.0000 scale=1.0000 norm=0.7539
[iter 200] loss=0.8703 val_loss=0.0000 scale=1.0000 norm=0.6598
[iter 300] loss=0.6931 val_loss=0.0000 scale=1.0000 norm=0.6039
[iter 400] loss=0.5757 val_loss=0.0000 scale=1.0000 norm=0.5695
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0931 val_loss=0.0000 scale=1.0000 norm=0.7586
[iter 200] loss=0.8723 val_loss=0.0000 scale=1.0000 norm=0.6608
[iter 300] loss=0.7030 val_loss=0.0000 scale=1.0000 norm=0.6083
[iter 400] loss=0.5746 val_loss=0.0000 scale=1.0000 norm=0.5803
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=0.7586
[iter 200] loss=0.8698 val_loss=0.0000 scale=1.0000 norm=0.6630
[iter 300] loss=0.7107 val_loss=0.0000 scale=1.0000 norm=0.6150
[iter 400] loss=0.5871 val_loss=0.0000 scale=1.0000 norm=0.5792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1009 val_loss=0.0000 scale=1.0000 norm=0.7629
[iter 200] loss=0.8896 val_loss=0.0000 scale=1.0000 norm=0.6671
[iter 300] loss=0.7226 val_loss=0.0000 scale=1.0000 norm=0.6123
[iter 400] loss=0.5923 val_loss=0.0000 scale=1.0000 norm=0.5770
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1009 val_loss=0.0000 scale=1.0000 norm=0.7629
[iter 200] loss=0.8896 val_loss=0.0000 scale=1.0000 norm=0.6671
[iter 300] loss=0.7226 val_loss=0.0000 scale=1.0000 norm=0.6123
[iter 400] loss=0.5923 val_loss=0.0000 scale=1.0000 norm=0.5770
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1072 val_loss=0.0000 scale=1.0000 norm=0.7695
[iter 200] loss=0.8986 val_loss=0.0000 scale=1.0000 norm=0.6747
[iter 300] loss=0.7458 val_loss=0.0000 scale=1.0000 norm=0.6259
[iter 400] loss=0.6257 val_loss=0.0000 scale=1.0000 norm=0.5927
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0968 val_loss=0.0000 scale=1.0000 norm=0.7636
[iter 200] loss=0.8842 val_loss=0.0000 scale=1.0000 norm=0.6690
[iter 300] loss=0.7106 val_loss=0.0000 scale=1.0000 norm=0.6102
[iter 400] loss=0.5873 val_loss=0.0000 scale=1.0000 norm=0.5761
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1002 val_loss=0.0000 scale=1.0000 norm=0.7656
[iter 200] loss=0.8923 val_loss=0.0000 scale=1.0000 norm=0.6736
[iter 300] loss=0.7247 val_loss=0.0000 scale=2.0000 norm=1.2399
[iter 400] loss=0.5957 val_loss=0.0000 scale=1.0000 norm=0.5808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0980 val_loss=0.0000 scale=1.0000 norm=0.7592
[iter 200] loss=0.8776 val_loss=0.0000 scale=1.0000 norm=0.6631
[iter 300] loss=0.7313 val_loss=0.0000 scale=1.0000 norm=0.6256
[iter 400] loss=0.6306 val_loss=0.0000 scale=1.0000 norm=0.5993
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0972 val_loss=0.0000 scale=1.0000 norm=0.7642
[iter 200] loss=0.8965 val_loss=0.0000 scale=1.0000 norm=0.6782
[iter 300] loss=0.7495 val_loss=0.0000 scale=1.0000 norm=0.6342
[iter 400] loss=0.6480 val_loss=0.0000 scale=1.0000 norm=0.6076
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 200] loss=0.8937 val_loss=0.0000 scale=2.0000 norm=1.3600
[iter 300] loss=0.7486 val_loss=0.0000 scale=1.0000 norm=0.6386
[iter 400] loss=0.6558 val_loss=0.0000 scale=1.0000 norm=0.6119
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1097 val_loss=0.0000 scale=1.0000 norm=0.7719
[iter 200] loss=0.9067 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 300] loss=0.7437 val_loss=0.0000 scale=2.0000 norm=1.2624
[iter 400] loss=0.6290 val_loss=0.0000 scale=1.0000 norm=0.5997
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0758 val_loss=0.0000 scale=1.0000 norm=0.7499
[iter 200] loss=0.8520 val_loss=0.0000 scale=1.0000 norm=0.6529
[iter 300] loss=0.6955 val_loss=0.0000 scale=1.0000 norm=0.6090
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7640
[iter 200] loss=0.8575 val_loss=0.0000 scale=1.0000 norm=0.6597
[iter 300] loss=0.7054 val_loss=0.0000 scale=2.0000 norm=1.2216
[iter 400] loss=0.5944 val_loss=0.0000 scale=1.0000 norm=0.5803
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0976 val_loss=0.0000 scale=1.0000 norm=0.7648
[iter 200] loss=0.8764 val_loss=0.0000 scale=2.0000 norm=1.3293
[iter 300] loss=0.6900 val_loss=0.0000 scale=1.0000 norm=0.6057
[iter 400] loss=0.5669 val_loss=0.0000 scale=1.0000 norm=0.5772
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1053 val_loss=0.0000 scale=1.0000 norm=0.7688
[iter 200] loss=0.8852 val_loss=0.0000 scale=1.0000 norm=0.6713
[iter 300] loss=0.7291 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6198 val_loss=0.0000 scale=1.0000 norm=0.5906
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1131 val_loss=0.0000 scale=1.0000 norm=0.7749
[iter 200] loss=0.9094 val_loss=0.0000 scale=1.0000 norm=0.6814
[iter 300] loss=0.7514 val_loss=0.0000 scale=1.0000 norm=0.6302
[iter 400] loss=0.6277 val_loss=0.0000 scale=1.0000 norm=0.5947
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1042 val_loss=0.0000 scale=1.0000 norm=0.7667
[iter 200] loss=0.8953 val_loss=0.0000 scale=1.0000 norm=0.6719
[iter 300] loss=0.7199 val_loss=0.0000 scale=2.0000 norm=1.2280
[iter 400] loss=0.5894 val_loss=0.0000 scale=1.0000 norm=0.5789
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1072 val_loss=0.0000 scale=1.0000 norm=0.7716
[iter 200] loss=0.9039 val_loss=0.0000 scale=1.0000 norm=0.6811
[iter 300] loss=0.7403 val_loss=0.0000 scale=1.0000 norm=0.6260
[iter 400] loss=0.6215 val_loss=0.0000 scale=1.0000 norm=0.5933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1072 val_loss=0.0000 scale=1.0000 norm=0.7716
[iter 200] loss=0.9039 val_loss=0.0000 scale=1.0000 norm=0.6811
[iter 300] loss=0.7403 val_loss=0.0000 scale=1.0000 norm=0.6260
[iter 400] loss=0.6215 val_loss=0.0000 scale=1.0000 norm=0.5933
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0898 val_loss=0.0000 scale=1.0000 norm=0.7539
[iter 200] loss=0.8703 val_loss=0.0000 scale=1.0000 norm=0.6598
[iter 300] loss=0.6931 val_loss=0.0000 scale=1.0000 norm=0.6039
[iter 400] loss=0.5757 val_loss=0.0000 scale=1.0000 norm=0.5695
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1081 val_loss=0.0000 scale=1.0000 norm=0.7715
[iter 200] loss=0.9094 val_loss=0.0000 scale=1.0000 norm=0.6789
[iter 300] loss=0.7477 val_loss=0.0000 scale=1.0000 norm=0.6260
[iter 400] loss=0.6284 val_loss=0.0000 scale=1.0000 norm=0.5928
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=0.7586
[iter 200] loss=0.8698 val_loss=0.0000 scale=1.0000 norm=0.6630
[iter 300] loss=0.7107 val_loss=0.0000 scale=1.0000 norm=0.6150
[iter 400] loss=0.5871 val_loss=0.0000 scale=1.0000 norm=0.5792
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0931 val_loss=0.0000 scale=1.0000 norm=0.7586
[iter 200] loss=0.8723 val_loss=0.0000 scale=1.0000 norm=0.6608
[iter 300] loss=0.7030 val_loss=0.0000 scale=1.0000 norm=0.6083
[iter 400] loss=0.5746 val_loss=0.0000 scale=1.0000 norm=0.5803
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0947 val_loss=0.0000 scale=1.0000 norm=0.7608
[iter 200] loss=0.8988 val_loss=0.0000 scale=1.0000 norm=0.6744
[iter 300] loss=0.7334 val_loss=0.0000 scale=1.0000 norm=0.6208
[iter 400] loss=0.6204 val_loss=0.0000 scale=1.0000 norm=0.5879
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1007 val_loss=0.0000 scale=1.0000 norm=0.7627
[iter 200] loss=0.8871 val_loss=0.0000 scale=2.0000 norm=1.3364
[iter 300] loss=0.7247 val_loss=0.0000 scale=2.0000 norm=1.2321
[iter 400] loss=0.6036 val_loss=0.0000 scale=1.0000 norm=0.5828
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0915 val_loss=0.0000 scale=1.0000 norm=0.7592
[iter 200] loss=0.8812 val_loss=0.0000 scale=1.0000 norm=0.6652
[iter 300] loss=0.7042 val_loss=0.0000 scale=1.0000 norm=0.6107
[iter 400] loss=0.5865 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=0.7610
[iter 200] loss=0.8725 val_loss=0.0000 scale=1.0000 norm=0.6657
[iter 300] loss=0.7066 val_loss=0.0000 scale=1.0000 norm=0.6147
[iter 400] loss=0.5758 val_loss=0.0000 scale=2.0000 norm=1.1601
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0926 val_loss=0.0000 scale=1.0000 norm=0.7610
[iter 200] loss=0.8725 val_loss=0.0000 scale=1.0000 norm=0.6657
[iter 300] loss=0.7066 val_loss=0.0000 scale=1.0000 norm=0.6147
[iter 400] loss=0.5758 val_loss=0.0000 scale=2.0000 norm=1.1601
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1016 val_loss=0.0000 scale=1.0000 norm=0.7638
[iter 200] loss=0.8670 val_loss=0.0000 scale=1.0000 norm=0.6680
[iter 300] loss=0.6949 val_loss=0.0000 scale=1.0000 norm=0.6220
[iter 400] loss=0.5966 val_loss=0.0000 scale=1.0000 norm=0.5952
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0980 val_loss=0.0000 scale=1.0000 norm=0.7592
[iter 200] loss=0.8776 val_loss=0.0000 scale=1.0000 norm=0.6631
[iter 300] loss=0.7313 val_loss=0.0000 scale=1.0000 norm=0.6256
[iter 400] loss=0.6306 val_loss=0.0000 scale=1.0000 norm=0.5993
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0972 val_loss=0.0000 scale=1.0000 norm=0.7642
[iter 200] loss=0.8965 val_loss=0.0000 scale=1.0000 norm=0.6782
[iter 300] loss=0.7495 val_loss=0.0000 scale=1.0000 norm=0.6342
[iter 400] loss=0.6480 val_loss=0.0000 scale=1.0000 norm=0.6076
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0896 val_loss=0.0000 scale=1.0000 norm=0.7652
[iter 200] loss=0.8981 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 300] loss=0.7763 val_loss=0.0000 scale=1.0000 norm=0.6444
[iter 400] loss=0.6782 val_loss=0.0000 scale=1.0000 norm=0.6172
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1097 val_loss=0.0000 scale=1.0000 norm=0.7719
[iter 200] loss=0.9067 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 300] loss=0.7437 val_loss=0.0000 scale=2.0000 norm=1.2624
[iter 400] loss=0.6290 val_loss=0.0000 scale=1.0000 norm=0.5997
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1118 val_loss=0.0000 scale=1.0000 norm=0.7718
[iter 200] loss=0.8986 val_loss=0.0000 scale=1.0000 norm=0.6768
[iter 300] loss=0.7468 val_loss=0.0000 scale=1.0000 norm=0.6304
[iter 400] loss=0.6294 val_loss=0.0000 scale=1.0000 norm=0.5979
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1001 val_loss=0.0000 scale=1.0000 norm=0.7629
[iter 200] loss=0.8878 val_loss=0.0000 scale=1.0000 norm=0.6725
[iter 300] loss=0.7372 val_loss=0.0000 scale=1.0000 norm=0.6294
[iter 400] loss=0.6314 val_loss=0.0000 scale=2.0000 norm=1.2015
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1041 val_loss=0.0000 scale=1.0000 norm=0.7682
[iter 200] loss=0.9024 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 300] loss=0.7606 val_loss=0.0000 scale=2.0000 norm=1.2772
[iter 400] loss=0.6656 val_loss=0.0000 scale=1.0000 norm=0.6155
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1001 val_loss=0.0000 scale=1.0000 norm=0.7629
[iter 200] loss=0.8878 val_loss=0.0000 scale=1.0000 norm=0.6725
[iter 300] loss=0.7372 val_loss=0.0000 scale=1.0000 norm=0.6294
[iter 400] loss=0.6314 val_loss=0.0000 scale=2.0000 norm=1.2015
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1017 val_loss=0.0000 scale=1.0000 norm=0.7663
[iter 200] loss=0.8840 val_loss=0.0000 scale=1.0000 norm=0.6725
[iter 300] loss=0.7439 val_loss=0.0000 scale=1.0000 norm=0.6290
[iter 100] loss=1.1053 val_loss=0.0000 scale=1.0000 norm=0.7645
[iter 200] loss=0.8624 val_loss=0.0000 scale=1.0000 norm=0.6557
[iter 300] loss=0.6850 val_loss=0.0000 scale=1.0000 norm=0.6045
[iter 400] loss=0.5753 val_loss=0.0000 scale=1.0000 norm=0.5796
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0980 val_loss=0.0000 scale=1.0000 norm=0.7639
[iter 200] loss=0.8855 val_loss=0.0000 scale=1.0000 norm=0.6702
[iter 300] loss=0.7259 val_loss=0.0000 scale=1.0000 norm=0.6181
[iter 400] loss=0.6118 val_loss=0.0000 scale=1.0000 norm=0.5862
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0999 val_loss=0.0000 scale=1.0000 norm=0.7677
[iter 200] loss=0.8843 val_loss=0.0000 scale=1.0000 norm=0.6719
[iter 300] loss=0.7147 val_loss=0.0000 scale=1.0000 norm=0.6171
[iter 400] loss=0.5959 val_loss=0.0000 scale=1.0000 norm=0.5873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0999 val_loss=0.0000 scale=1.0000 norm=0.7677
[iter 200] loss=0.8843 val_loss=0.0000 scale=1.0000 norm=0.6719
[iter 300] loss=0.7147 val_loss=0.0000 scale=1.0000 norm=0.6171
[iter 400] loss=0.5959 val_loss=0.0000 scale=1.0000 norm=0.5873
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0778 val_loss=0.0000 scale=1.0000 norm=0.7513
[iter 200] loss=0.8584 val_loss=0.0000 scale=1.0000 norm=0.6546
[iter 300] loss=0.6835 val_loss=0.0000 scale=1.0000 norm=0.6015
[iter 400] loss=0.5679 val_loss=0.0000 scale=1.0000 norm=0.5718
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1033 val_loss=0.0000 scale=1.0000 norm=0.7693
[iter 200] loss=0.8935 val_loss=0.0000 scale=2.0000 norm=1.3506
[iter 300] loss=0.7362 val_loss=0.0000 scale=2.0000 norm=1.2481
[iter 400] loss=0.6040 val_loss=0.0000 scale=1.0000 norm=0.5888
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1124 val_loss=0.0000 scale=1.0000 norm=0.7745
[iter 200] loss=0.9122 val_loss=0.0000 scale=1.0000 norm=0.6785
[iter 300] loss=0.7469 val_loss=0.0000 scale=1.0000 norm=0.6196
[iter 400] loss=0.6256 val_loss=0.0000 scale=1.0000 norm=0.5818
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0915 val_loss=0.0000 scale=1.0000 norm=0.7592
[iter 200] loss=0.8812 val_loss=0.0000 scale=1.0000 norm=0.6652
[iter 300] loss=0.7042 val_loss=0.0000 scale=1.0000 norm=0.6107
[iter 400] loss=0.5865 val_loss=0.0000 scale=1.0000 norm=0.5812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.8927 val_loss=0.0000 scale=1.0000 norm=0.6735
[iter 300] loss=0.7214 val_loss=0.0000 scale=1.0000 norm=0.6196
[iter 400] loss=0.5833 val_loss=0.0000 scale=1.0000 norm=0.5790
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1137 val_loss=0.0000 scale=1.0000 norm=0.7741
[iter 200] loss=0.9064 val_loss=0.0000 scale=1.0000 norm=0.6787
[iter 300] loss=0.7279 val_loss=0.0000 scale=2.0000 norm=1.2375
[iter 400] loss=0.6184 val_loss=0.0000 scale=1.0000 norm=0.5887
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1002 val_loss=0.0000 scale=1.0000 norm=0.7656
[iter 200] loss=0.8923 val_loss=0.0000 scale=1.0000 norm=0.6736
[iter 300] loss=0.7247 val_loss=0.0000 scale=2.0000 norm=1.2399
[iter 400] loss=0.5957 val_loss=0.0000 scale=1.0000 norm=0.5808
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1072 val_loss=0.0000 scale=1.0000 norm=0.7695
[iter 200] loss=0.8986 val_loss=0.0000 scale=1.0000 norm=0.6747
[iter 300] loss=0.7458 val_loss=0.0000 scale=1.0000 norm=0.6259
[iter 400] loss=0.6257 val_loss=0.0000 scale=1.0000 norm=0.5927
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1067 val_loss=0.0000 scale=1.0000 norm=0.7714
[iter 200] loss=0.9033 val_loss=0.0000 scale=1.0000 norm=0.6797
[iter 300] loss=0.7442 val_loss=0.0000 scale=2.0000 norm=1.2590
[iter 400] loss=0.6331 val_loss=0.0000 scale=1.0000 norm=0.5985
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1016 val_loss=0.0000 scale=1.0000 norm=0.7638
[iter 200] loss=0.8670 val_loss=0.0000 scale=1.0000 norm=0.6680
[iter 300] loss=0.6949 val_loss=0.0000 scale=1.0000 norm=0.6220
[iter 400] loss=0.5966 val_loss=0.0000 scale=1.0000 norm=0.5952
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0758 val_loss=0.0000 scale=1.0000 norm=0.7499
[iter 200] loss=0.8520 val_loss=0.0000 scale=1.0000 norm=0.6529
[iter 300] loss=0.6955 val_loss=0.0000 scale=1.0000 norm=0.6090
[iter 400] loss=0.5929 val_loss=0.0000 scale=1.0000 norm=0.5844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1070 val_loss=0.0000 scale=1.0000 norm=0.7693
[iter 200] loss=0.8927 val_loss=0.0000 scale=1.0000 norm=0.6768
[iter 300] loss=0.7409 val_loss=0.0000 scale=2.0000 norm=1.2625
[iter 400] loss=0.6406 val_loss=0.0000 scale=1.0000 norm=0.6027
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0986 val_loss=0.0000 scale=2.0000 norm=1.5297
[iter 200] loss=0.8976 val_loss=0.0000 scale=1.0000 norm=0.6786
[iter 300] loss=0.7451 val_loss=0.0000 scale=1.0000 norm=0.6326
[iter 400] loss=0.6321 val_loss=0.0000 scale=1.0000 norm=0.5997
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1118 val_loss=0.0000 scale=1.0000 norm=0.7718
[iter 200] loss=0.8986 val_loss=0.0000 scale=1.0000 norm=0.6768
[iter 300] loss=0.7468 val_loss=0.0000 scale=1.0000 norm=0.6304
[iter 400] loss=0.6294 val_loss=0.0000 scale=1.0000 norm=0.5979
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1065 val_loss=0.0000 scale=1.0000 norm=0.7689
[iter 200] loss=0.9079 val_loss=0.0000 scale=1.0000 norm=0.6798
[iter 300] loss=0.7644 val_loss=0.0000 scale=2.0000 norm=1.2737
[iter 400] loss=0.6608 val_loss=0.0000 scale=1.0000 norm=0.6090
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1007 val_loss=0.0000 scale=1.0000 norm=0.7693
[iter 200] loss=0.8995 val_loss=0.0000 scale=1.0000 norm=0.6810
[iter 300] loss=0.7537 val_loss=0.0000 scale=1.0000 norm=0.6380
[iter 400] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.6095
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1102 val_loss=0.0000 scale=1.0000 norm=0.7720
[iter 200] loss=0.9141 val_loss=0.0000 scale=1.0000 norm=0.6828
[iter 300] loss=0.7658 val_loss=0.0000 scale=1.0000 norm=0.6355
[iter 400] loss=0.6679 val_loss=0.0000 scale=1.0000 norm=0.6107
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0891 val_loss=0.0000 scale=1.0000 norm=0.7569
[iter 200] loss=0.8671 val_loss=0.0000 scale=1.0000 norm=0.6604
[iter 300] loss=0.7139 val_loss=0.0000 scale=1.0000 norm=0.6166
[iter 400] loss=0.6031 val_loss=0.0000 scale=1.0000 norm=0.5878
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0984 val_loss=0.0000 scale=1.0000 norm=0.7628
[iter 200] loss=0.8944 val_loss=0.0000 scale=1.0000 norm=0.6716
[iter 300] loss=0.7283 val_loss=0.0000 scale=1.0000 norm=0.6174
[iter 400] loss=0.6080 val_loss=0.0000 scale=2.0000 norm=1.1753
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1045 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 200] loss=0.8889 val_loss=0.0000 scale=1.0000 norm=0.6730
[iter 300] loss=0.7444 val_loss=0.0000 scale=1.0000 norm=0.6299
[iter 400] loss=0.6481 val_loss=0.0000 scale=1.0000 norm=0.6034
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0899 val_loss=0.0000 scale=2.0000 norm=1.5154
[iter 200] loss=0.8647 val_loss=0.0000 scale=1.0000 norm=0.6645
[iter 300] loss=0.7056 val_loss=0.0000 scale=1.0000 norm=0.6184
[iter 400] loss=0.6005 val_loss=0.0000 scale=1.0000 norm=0.5927
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0899 val_loss=0.0000 scale=2.0000 norm=1.5154
[iter 200] loss=0.8647 val_loss=0.0000 scale=1.0000 norm=0.6645
[iter 300] loss=0.7056 val_loss=0.0000 scale=1.0000 norm=0.6184
[iter 400] loss=0.5792 val_loss=0.0000 scale=1.0000 norm=0.5802
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0968 val_loss=0.0000 scale=1.0000 norm=0.7636
[iter 200] loss=0.8842 val_loss=0.0000 scale=1.0000 norm=0.6690
[iter 300] loss=0.7106 val_loss=0.0000 scale=1.0000 norm=0.6102
[iter 400] loss=0.5873 val_loss=0.0000 scale=1.0000 norm=0.5761
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 200] loss=0.8937 val_loss=0.0000 scale=2.0000 norm=1.3600
[iter 300] loss=0.7486 val_loss=0.0000 scale=1.0000 norm=0.6386
[iter 400] loss=0.6558 val_loss=0.0000 scale=1.0000 norm=0.6119
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1067 val_loss=0.0000 scale=1.0000 norm=0.7714
[iter 200] loss=0.9033 val_loss=0.0000 scale=1.0000 norm=0.6797
[iter 300] loss=0.7442 val_loss=0.0000 scale=2.0000 norm=1.2590
[iter 400] loss=0.6331 val_loss=0.0000 scale=1.0000 norm=0.5985
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1070 val_loss=0.0000 scale=1.0000 norm=0.7693
[iter 200] loss=0.8927 val_loss=0.0000 scale=1.0000 norm=0.6768
[iter 300] loss=0.7409 val_loss=0.0000 scale=2.0000 norm=1.2625
[iter 400] loss=0.6406 val_loss=0.0000 scale=1.0000 norm=0.6027
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0992 val_loss=0.0000 scale=2.0000 norm=1.5301
[iter 200] loss=0.8916 val_loss=0.0000 scale=1.0000 norm=0.6742
[iter 300] loss=0.7394 val_loss=0.0000 scale=1.0000 norm=0.6291
[iter 400] loss=0.6324 val_loss=0.0000 scale=1.0000 norm=0.6022
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0896 val_loss=0.0000 scale=1.0000 norm=0.7652
[iter 200] loss=0.8981 val_loss=0.0000 scale=1.0000 norm=0.6827
[iter 300] loss=0.7763 val_loss=0.0000 scale=1.0000 norm=0.6444
[iter 400] loss=0.6782 val_loss=0.0000 scale=1.0000 norm=0.6172
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0922 val_loss=0.0000 scale=1.0000 norm=0.7606
[iter 200] loss=0.8841 val_loss=0.0000 scale=1.0000 norm=0.6701
[iter 300] loss=0.7309 val_loss=0.0000 scale=1.0000 norm=0.6271
[iter 400] loss=0.6299 val_loss=0.0000 scale=1.0000 norm=0.5998
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0986 val_loss=0.0000 scale=2.0000 norm=1.5297
[iter 200] loss=0.8976 val_loss=0.0000 scale=1.0000 norm=0.6786
[iter 300] loss=0.7451 val_loss=0.0000 scale=1.0000 norm=0.6326
[iter 400] loss=0.6321 val_loss=0.0000 scale=1.0000 norm=0.5997
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1007 val_loss=0.0000 scale=1.0000 norm=0.7693
[iter 200] loss=0.8995 val_loss=0.0000 scale=1.0000 norm=0.6810
[iter 300] loss=0.7537 val_loss=0.0000 scale=1.0000 norm=0.6380
[iter 400] loss=0.6429 val_loss=0.0000 scale=1.0000 norm=0.6095
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0864 val_loss=0.0000 scale=1.0000 norm=0.7560
[iter 200] loss=0.8902 val_loss=0.0000 scale=1.0000 norm=0.6708
[iter 300] loss=0.7444 val_loss=0.0000 scale=1.0000 norm=0.6264
[iter 400] loss=0.6359 val_loss=0.0000 scale=1.0000 norm=0.5978
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1017 val_loss=0.0000 scale=1.0000 norm=0.7663
[iter 200] loss=0.8840 val_loss=0.0000 scale=1.0000 norm=0.6725
[iter 300] loss=0.7439 val_loss=0.0000 scale=1.0000 norm=0.6290
[iter 400] loss=0.6486 val_loss=0.0000 scale=1.0000 norm=0.6010
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1005 val_loss=0.0000 scale=1.0000 norm=0.7665
[iter 200] loss=0.8898 val_loss=0.0000 scale=1.0000 norm=0.6781
[iter 300] loss=0.7329 val_loss=0.0000 scale=1.0000 norm=0.6278
[iter 400] loss=0.6311 val_loss=0.0000 scale=1.0000 norm=0.5967
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0903 val_loss=0.0000 scale=1.0000 norm=0.7575
[iter 200] loss=0.8799 val_loss=0.0000 scale=2.0000 norm=1.3344
[iter 300] loss=0.7190 val_loss=0.0000 scale=2.0000 norm=1.2427
[iter 400] loss=0.6227 val_loss=0.0000 scale=1.0000 norm=0.5976
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.9080 val_loss=0.0000 scale=1.0000 norm=0.6851
[iter 300] loss=0.7777 val_loss=0.0000 scale=1.0000 norm=0.6462
[iter 400] loss=0.6877 val_loss=0.0000 scale=1.0000 norm=0.6213
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1045 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 200] loss=0.8889 val_loss=0.0000 scale=1.0000 norm=0.6730
[iter 300] loss=0.7444 val_loss=0.0000 scale=1.0000 norm=0.6299
[iter 400] loss=0.6481 val_loss=0.0000 scale=1.0000 norm=0.6034
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1162 val_loss=0.0000 scale=1.0000 norm=0.7776
[iter 200] loss=0.9179 val_loss=0.0000 scale=1.0000 norm=0.6899
[iter 300] loss=0.7760 val_loss=0.0000 scale=1.0000 norm=0.6504
[iter 400] loss=0.6828 val_loss=0.0000 scale=1.0000 norm=0.6240
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0873 val_loss=0.0000 scale=1.0000 norm=0.7555
[iter 200] loss=0.8756 val_loss=0.0000 scale=1.0000 norm=0.6641
[iter 300] loss=0.7320 val_loss=0.0000 scale=1.0000 norm=0.6210
[iter 400] loss=0.6260 val_loss=0.0000 scale=1.0000 norm=0.5952
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1162 val_loss=0.0000 scale=1.0000 norm=0.7776
[iter 200] loss=0.9179 val_loss=0.0000 scale=1.0000 norm=0.6899
[iter 300] loss=0.7760 val_loss=0.0000 scale=1.0000 norm=0.6504
[iter 400] loss=0.6828 val_loss=0.0000 scale=1.0000 norm=0.6240
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1118 val_loss=0.0000 scale=1.0000 norm=0.7725
[iter 200] loss=0.8862 val_loss=0.0000 scale=2.0000 norm=1.3479
[iter 300] loss=0.7299 val_loss=0.0000 scale=1.0000 norm=0.6268
[iter 400] loss=0.6261 val_loss=0.0000 scale=1.0000 norm=0.6003
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0925 val_loss=0.0000 scale=1.0000 norm=0.7582
[iter 200] loss=0.8878 val_loss=0.0000 scale=1.0000 norm=0.6701
[iter 300] loss=0.7344 val_loss=0.0000 scale=1.0000 norm=0.6266
[iter 400] loss=0.6250 val_loss=0.0000 scale=1.0000 norm=0.5953
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0804 val_loss=0.0000 scale=1.0000 norm=0.7533
[iter 200] loss=0.8684 val_loss=0.0000 scale=1.0000 norm=0.6621
[iter 300] loss=0.7121 val_loss=0.0000 scale=1.0000 norm=0.6210
[iter 400] loss=0.6070 val_loss=0.0000 scale=1.0000 norm=0.5955
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1208 val_loss=0.0000 scale=1.0000 norm=0.7761
[iter 200] loss=0.9262 val_loss=0.0000 scale=1.0000 norm=0.7062
[iter 300] loss=0.8220 val_loss=0.0000 scale=1.0000 norm=0.6749
[iter 400] loss=0.7460 val_loss=0.0000 scale=2.0000 norm=1.3037
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1084 val_loss=0.0000 scale=1.0000 norm=0.7689
[iter 200] loss=0.9358 val_loss=0.0000 scale=1.0000 norm=0.7018
[iter 300] loss=0.8121 val_loss=0.0000 scale=2.0000 norm=1.3368
[iter 400] loss=0.7391 val_loss=0.0000 scale=2.0000 norm=1.3024
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1208 val_loss=0.0000 scale=1.0000 norm=0.7761
[iter 200] loss=0.9262 val_loss=0.0000 scale=1.0000 norm=0.7062
[iter 300] loss=0.8220 val_loss=0.0000 scale=1.0000 norm=0.6749
[iter 400] loss=0.7460 val_loss=0.0000 scale=2.0000 norm=1.3037
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1084 val_loss=0.0000 scale=1.0000 norm=0.7689
[iter 200] loss=0.9358 val_loss=0.0000 scale=1.0000 norm=0.7018
[iter 300] loss=0.8121 val_loss=0.0000 scale=2.0000 norm=1.3368
[iter 400] loss=0.7391 val_loss=0.0000 scale=2.0000 norm=1.3024
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1431 val_loss=0.0000 scale=1.0000 norm=0.7908
[iter 400] loss=0.6486 val_loss=0.0000 scale=1.0000 norm=0.6010
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1065 val_loss=0.0000 scale=1.0000 norm=0.7689
[iter 200] loss=0.9079 val_loss=0.0000 scale=1.0000 norm=0.6798
[iter 300] loss=0.7644 val_loss=0.0000 scale=2.0000 norm=1.2737
[iter 400] loss=0.6608 val_loss=0.0000 scale=1.0000 norm=0.6090
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0864 val_loss=0.0000 scale=1.0000 norm=0.7560
[iter 200] loss=0.8902 val_loss=0.0000 scale=1.0000 norm=0.6708
[iter 300] loss=0.7444 val_loss=0.0000 scale=1.0000 norm=0.6264
[iter 400] loss=0.6359 val_loss=0.0000 scale=1.0000 norm=0.5978
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1021 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.9080 val_loss=0.0000 scale=1.0000 norm=0.6851
[iter 300] loss=0.7777 val_loss=0.0000 scale=1.0000 norm=0.6462
[iter 400] loss=0.6877 val_loss=0.0000 scale=1.0000 norm=0.6213
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1005 val_loss=0.0000 scale=1.0000 norm=0.7665
[iter 200] loss=0.8898 val_loss=0.0000 scale=1.0000 norm=0.6781
[iter 300] loss=0.7329 val_loss=0.0000 scale=1.0000 norm=0.6278
[iter 400] loss=0.6311 val_loss=0.0000 scale=1.0000 norm=0.5967
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0903 val_loss=0.0000 scale=1.0000 norm=0.7575
[iter 200] loss=0.8799 val_loss=0.0000 scale=2.0000 norm=1.3344
[iter 300] loss=0.7190 val_loss=0.0000 scale=2.0000 norm=1.2427
[iter 400] loss=0.6227 val_loss=0.0000 scale=1.0000 norm=0.5976
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0873 val_loss=0.0000 scale=1.0000 norm=0.7555
[iter 200] loss=0.8756 val_loss=0.0000 scale=1.0000 norm=0.6641
[iter 300] loss=0.7320 val_loss=0.0000 scale=1.0000 norm=0.6210
[iter 400] loss=0.6260 val_loss=0.0000 scale=1.0000 norm=0.5952
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0984 val_loss=0.0000 scale=1.0000 norm=0.7628
[iter 200] loss=0.8944 val_loss=0.0000 scale=1.0000 norm=0.6716
[iter 300] loss=0.7283 val_loss=0.0000 scale=1.0000 norm=0.6174
[iter 400] loss=0.6080 val_loss=0.0000 scale=2.0000 norm=1.1753
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0925 val_loss=0.0000 scale=1.0000 norm=0.7582
[iter 200] loss=0.8878 val_loss=0.0000 scale=1.0000 norm=0.6701
[iter 300] loss=0.7344 val_loss=0.0000 scale=1.0000 norm=0.6266
[iter 400] loss=0.6250 val_loss=0.0000 scale=1.0000 norm=0.5953
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0804 val_loss=0.0000 scale=1.0000 norm=0.7533
[iter 200] loss=0.8684 val_loss=0.0000 scale=1.0000 norm=0.6621
[iter 300] loss=0.7121 val_loss=0.0000 scale=1.0000 norm=0.6210
[iter 400] loss=0.6070 val_loss=0.0000 scale=1.0000 norm=0.5955
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1118 val_loss=0.0000 scale=1.0000 norm=0.7725
[iter 200] loss=0.8862 val_loss=0.0000 scale=2.0000 norm=1.3479
[iter 300] loss=0.7299 val_loss=0.0000 scale=1.0000 norm=0.6268
[iter 400] loss=0.6261 val_loss=0.0000 scale=1.0000 norm=0.6003
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1519 val_loss=0.0000 scale=1.0000 norm=0.8007
[iter 200] loss=0.9705 val_loss=0.0000 scale=2.0000 norm=1.4661
[iter 300] loss=0.8569 val_loss=0.0000 scale=2.0000 norm=1.3973
[iter 400] loss=0.7701 val_loss=0.0000 scale=1.0000 norm=0.6736
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1239 val_loss=0.0000 scale=2.0000 norm=1.5563
[iter 200] loss=0.9435 val_loss=0.0000 scale=2.0000 norm=1.4243
[iter 300] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.3658
[iter 400] loss=0.7566 val_loss=0.0000 scale=1.0000 norm=0.6614
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1126 val_loss=0.0000 scale=1.0000 norm=0.7742
[iter 200] loss=0.9440 val_loss=0.0000 scale=2.0000 norm=1.4237
[iter 300] loss=0.8272 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 400] loss=0.7557 val_loss=0.0000 scale=1.0000 norm=0.6573
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1126 val_loss=0.0000 scale=1.0000 norm=0.7742
[iter 200] loss=0.9440 val_loss=0.0000 scale=2.0000 norm=1.4237
[iter 300] loss=0.8272 val_loss=0.0000 scale=1.0000 norm=0.6788
[iter 400] loss=0.7557 val_loss=0.0000 scale=1.0000 norm=0.6573
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1395 val_loss=0.0000 scale=1.0000 norm=0.7893
[iter 200] loss=0.9699 val_loss=0.0000 scale=1.0000 norm=0.7223
[iter 300] loss=0.8530 val_loss=0.0000 scale=1.0000 norm=0.6888
[iter 400] loss=0.7765 val_loss=0.0000 scale=2.0000 norm=1.3336
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1392 val_loss=0.0000 scale=1.0000 norm=0.7887
[iter 200] loss=0.9729 val_loss=0.0000 scale=1.0000 norm=0.7203
[iter 300] loss=0.8565 val_loss=0.0000 scale=1.0000 norm=0.6884
[iter 400] loss=0.7785 val_loss=0.0000 scale=1.0000 norm=0.6659
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0913 val_loss=0.0000 scale=2.0000 norm=1.5246
[iter 200] loss=0.9044 val_loss=0.0000 scale=1.0000 norm=0.6978
[iter 300] loss=0.7841 val_loss=0.0000 scale=1.0000 norm=0.6638
[iter 400] loss=0.7009 val_loss=0.0000 scale=2.0000 norm=1.2844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1545 val_loss=0.0000 scale=1.0000 norm=0.7979
[iter 200] loss=0.9953 val_loss=0.0000 scale=2.0000 norm=1.4719
[iter 300] loss=0.8844 val_loss=0.0000 scale=1.0000 norm=0.7021
[iter 400] loss=0.8112 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1220 val_loss=0.0000 scale=2.0000 norm=1.5536
[iter 200] loss=0.9372 val_loss=0.0000 scale=1.0000 norm=0.7067
[iter 300] loss=0.8269 val_loss=0.0000 scale=1.0000 norm=0.6775
[iter 400] loss=0.7523 val_loss=0.0000 scale=1.0000 norm=0.6540
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0933 val_loss=0.0000 scale=2.0000 norm=1.5270
[iter 200] loss=0.8930 val_loss=0.0000 scale=2.0000 norm=1.3942
[iter 300] loss=0.7837 val_loss=0.0000 scale=2.0000 norm=1.3393
[iter 400] loss=0.7157 val_loss=0.0000 scale=1.0000 norm=0.6504
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1128 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.9119 val_loss=0.0000 scale=1.0000 norm=0.6940
[iter 300] loss=0.8000 val_loss=0.0000 scale=1.0000 norm=0.6655
[iter 400] loss=0.7109 val_loss=0.0000 scale=1.0000 norm=0.6374
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1311 val_loss=0.0000 scale=2.0000 norm=1.5660
[iter 200] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.7200
[iter 300] loss=0.8516 val_loss=0.0000 scale=1.0000 norm=0.6824
[iter 400] loss=0.7612 val_loss=0.0000 scale=1.0000 norm=0.6559
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1182 val_loss=0.0000 scale=1.0000 norm=0.7772
[iter 200] loss=0.9278 val_loss=0.0000 scale=2.0000 norm=1.4130
[iter 300] loss=0.8103 val_loss=0.0000 scale=1.0000 norm=0.6713
[iter 400] loss=0.7337 val_loss=0.0000 scale=1.0000 norm=0.6484
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1083 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.9202 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 300] loss=0.7947 val_loss=0.0000 scale=1.0000 norm=0.6637
[iter 400] loss=0.7092 val_loss=0.0000 scale=1.0000 norm=0.6404
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1231 val_loss=0.0000 scale=2.0000 norm=1.5646
[iter 200] loss=0.9578 val_loss=0.0000 scale=1.0000 norm=0.7254
[iter 300] loss=0.8560 val_loss=0.0000 scale=1.0000 norm=0.6994
[iter 400] loss=0.7947 val_loss=0.0000 scale=1.0000 norm=0.6831
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1397 val_loss=0.0000 scale=2.0000 norm=1.5776
[iter 400] loss=0.5929 val_loss=0.0000 scale=1.0000 norm=0.5844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0992 val_loss=0.0000 scale=2.0000 norm=1.5301
[iter 200] loss=0.8916 val_loss=0.0000 scale=1.0000 norm=0.6742
[iter 300] loss=0.7394 val_loss=0.0000 scale=1.0000 norm=0.6291
[iter 400] loss=0.6324 val_loss=0.0000 scale=1.0000 norm=0.6022
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1041 val_loss=0.0000 scale=1.0000 norm=0.7682
[iter 200] loss=0.9024 val_loss=0.0000 scale=1.0000 norm=0.6806
[iter 300] loss=0.7606 val_loss=0.0000 scale=2.0000 norm=1.2772
[iter 400] loss=0.6656 val_loss=0.0000 scale=1.0000 norm=0.6155
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0922 val_loss=0.0000 scale=1.0000 norm=0.7606
[iter 200] loss=0.8841 val_loss=0.0000 scale=1.0000 norm=0.6701
[iter 300] loss=0.7309 val_loss=0.0000 scale=1.0000 norm=0.6271
[iter 400] loss=0.6299 val_loss=0.0000 scale=1.0000 norm=0.5998
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0977 val_loss=0.0000 scale=1.0000 norm=0.7618
[iter 200] loss=0.8799 val_loss=0.0000 scale=2.0000 norm=1.3412
[iter 300] loss=0.7282 val_loss=0.0000 scale=1.0000 norm=0.6281
[iter 400] loss=0.6173 val_loss=0.0000 scale=1.0000 norm=0.5945
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0977 val_loss=0.0000 scale=1.0000 norm=0.7618
[iter 200] loss=0.8799 val_loss=0.0000 scale=2.0000 norm=1.3412
[iter 300] loss=0.7282 val_loss=0.0000 scale=1.0000 norm=0.6281
[iter 400] loss=0.6173 val_loss=0.0000 scale=1.0000 norm=0.5945
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0891 val_loss=0.0000 scale=1.0000 norm=0.7569
[iter 200] loss=0.8671 val_loss=0.0000 scale=1.0000 norm=0.6604
[iter 300] loss=0.7139 val_loss=0.0000 scale=1.0000 norm=0.6166
[iter 400] loss=0.6031 val_loss=0.0000 scale=1.0000 norm=0.5878
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1102 val_loss=0.0000 scale=1.0000 norm=0.7720
[iter 200] loss=0.9141 val_loss=0.0000 scale=1.0000 norm=0.6828
[iter 300] loss=0.7658 val_loss=0.0000 scale=1.0000 norm=0.6355
[iter 400] loss=0.6679 val_loss=0.0000 scale=1.0000 norm=0.6107
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0899 val_loss=0.0000 scale=1.0000 norm=0.7595
[iter 200] loss=0.8740 val_loss=0.0000 scale=2.0000 norm=1.3390
[iter 300] loss=0.7164 val_loss=0.0000 scale=1.0000 norm=0.6273
[iter 400] loss=0.6042 val_loss=0.0000 scale=1.0000 norm=0.6000
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0899 val_loss=0.0000 scale=1.0000 norm=0.7595
[iter 200] loss=0.8740 val_loss=0.0000 scale=2.0000 norm=1.3390
[iter 300] loss=0.7164 val_loss=0.0000 scale=1.0000 norm=0.6273
[iter 400] loss=0.6042 val_loss=0.0000 scale=1.0000 norm=0.6000
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1077 val_loss=0.0000 scale=1.0000 norm=0.7728
[iter 200] loss=0.9174 val_loss=0.0000 scale=1.0000 norm=0.6891
[iter 300] loss=0.7847 val_loss=0.0000 scale=1.0000 norm=0.6506
[iter 400] loss=0.6918 val_loss=0.0000 scale=1.0000 norm=0.6223
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1077 val_loss=0.0000 scale=1.0000 norm=0.7728
[iter 200] loss=0.9174 val_loss=0.0000 scale=1.0000 norm=0.6891
[iter 300] loss=0.7847 val_loss=0.0000 scale=1.0000 norm=0.6506
[iter 400] loss=0.6918 val_loss=0.0000 scale=1.0000 norm=0.6223
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1338 val_loss=0.0000 scale=2.0000 norm=1.5689
[iter 200] loss=0.9546 val_loss=0.0000 scale=2.0000 norm=1.4301
[iter 300] loss=0.8425 val_loss=0.0000 scale=1.0000 norm=0.6844
[iter 400] loss=0.7696 val_loss=0.0000 scale=1.0000 norm=0.6615
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1338 val_loss=0.0000 scale=2.0000 norm=1.5689
[iter 200] loss=0.9546 val_loss=0.0000 scale=2.0000 norm=1.4301
[iter 300] loss=0.8425 val_loss=0.0000 scale=1.0000 norm=0.6844
[iter 400] loss=0.7696 val_loss=0.0000 scale=1.0000 norm=0.6615
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1308 val_loss=0.0000 scale=1.0000 norm=0.7824
[iter 200] loss=0.9592 val_loss=0.0000 scale=1.0000 norm=0.7195
[iter 300] loss=0.8427 val_loss=0.0000 scale=2.0000 norm=1.3692
[iter 400] loss=0.7475 val_loss=0.0000 scale=2.0000 norm=1.3180
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1504 val_loss=0.0000 scale=1.0000 norm=0.7963
[iter 200] loss=0.9436 val_loss=0.0000 scale=2.0000 norm=1.4179
[iter 300] loss=0.8361 val_loss=0.0000 scale=1.0000 norm=0.6802
[iter 400] loss=0.7703 val_loss=0.0000 scale=1.0000 norm=0.6605
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1438 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.9569 val_loss=0.0000 scale=2.0000 norm=1.4342
[iter 300] loss=0.8437 val_loss=0.0000 scale=1.0000 norm=0.6885
[iter 400] loss=0.7627 val_loss=0.0000 scale=1.0000 norm=0.6671
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1395 val_loss=0.0000 scale=1.0000 norm=0.7893
[iter 200] loss=0.9699 val_loss=0.0000 scale=1.0000 norm=0.7223
[iter 300] loss=0.8530 val_loss=0.0000 scale=1.0000 norm=0.6888
[iter 400] loss=0.7765 val_loss=0.0000 scale=2.0000 norm=1.3336
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1278 val_loss=0.0000 scale=1.0000 norm=0.7800
[iter 200] loss=0.9474 val_loss=0.0000 scale=2.0000 norm=1.4181
[iter 300] loss=0.8477 val_loss=0.0000 scale=1.0000 norm=0.6842
[iter 400] loss=0.7677 val_loss=0.0000 scale=1.0000 norm=0.6591
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1195 val_loss=0.0000 scale=1.0000 norm=0.7809
[iter 200] loss=0.9396 val_loss=0.0000 scale=1.0000 norm=0.7187
[iter 300] loss=0.8363 val_loss=0.0000 scale=1.0000 norm=0.6889
[iter 400] loss=0.7561 val_loss=0.0000 scale=1.0000 norm=0.6643
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1351 val_loss=0.0000 scale=1.0000 norm=0.7889
[iter 200] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.4477
[iter 300] loss=0.8561 val_loss=0.0000 scale=1.0000 norm=0.6928
[iter 400] loss=0.7838 val_loss=0.0000 scale=1.0000 norm=0.6722
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0933 val_loss=0.0000 scale=2.0000 norm=1.5270
[iter 200] loss=0.8930 val_loss=0.0000 scale=2.0000 norm=1.3942
[iter 300] loss=0.7837 val_loss=0.0000 scale=2.0000 norm=1.3393
[iter 400] loss=0.7157 val_loss=0.0000 scale=1.0000 norm=0.6504
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1083 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.9202 val_loss=0.0000 scale=1.0000 norm=0.6999
[iter 300] loss=0.7947 val_loss=0.0000 scale=1.0000 norm=0.6637
[iter 400] loss=0.7092 val_loss=0.0000 scale=1.0000 norm=0.6404
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1231 val_loss=0.0000 scale=2.0000 norm=1.5646
[iter 200] loss=0.9578 val_loss=0.0000 scale=1.0000 norm=0.7254
[iter 300] loss=0.8560 val_loss=0.0000 scale=1.0000 norm=0.6994
[iter 400] loss=0.7947 val_loss=0.0000 scale=1.0000 norm=0.6831
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1182 val_loss=0.0000 scale=1.0000 norm=0.7772
[iter 200] loss=0.9278 val_loss=0.0000 scale=2.0000 norm=1.4130
[iter 300] loss=0.8103 val_loss=0.0000 scale=1.0000 norm=0.6713
[iter 400] loss=0.7337 val_loss=0.0000 scale=1.0000 norm=0.6484
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1372 val_loss=0.0000 scale=1.0000 norm=0.7889
[iter 200] loss=0.9505 val_loss=0.0000 scale=1.0000 norm=0.7172
[iter 300] loss=0.8293 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 400] loss=0.7488 val_loss=0.0000 scale=2.0000 norm=1.3231
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1372 val_loss=0.0000 scale=1.0000 norm=0.7889
[iter 200] loss=0.9815 val_loss=0.0000 scale=1.0000 norm=0.7271
[iter 300] loss=0.8721 val_loss=0.0000 scale=1.0000 norm=0.6981
[iter 400] loss=0.7990 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1260 val_loss=0.0000 scale=1.0000 norm=0.7781
[iter 200] loss=0.9102 val_loss=0.0000 scale=2.0000 norm=1.4038
[iter 300] loss=0.7918 val_loss=0.0000 scale=1.0000 norm=0.6700
[iter 400] loss=0.7107 val_loss=0.0000 scale=1.0000 norm=0.6458
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1431 val_loss=0.0000 scale=1.0000 norm=0.7908
[iter 200] loss=0.9815 val_loss=0.0000 scale=1.0000 norm=0.7271
[iter 300] loss=0.8721 val_loss=0.0000 scale=1.0000 norm=0.6981
[iter 400] loss=0.7990 val_loss=0.0000 scale=1.0000 norm=0.6771
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1260 val_loss=0.0000 scale=1.0000 norm=0.7781
[iter 200] loss=0.9102 val_loss=0.0000 scale=2.0000 norm=1.4038
[iter 300] loss=0.7918 val_loss=0.0000 scale=1.0000 norm=0.6700
[iter 400] loss=0.7107 val_loss=0.0000 scale=1.0000 norm=0.6458
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1110 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 200] loss=0.9078 val_loss=0.0000 scale=2.0000 norm=1.3842
[iter 300] loss=0.7936 val_loss=0.0000 scale=1.0000 norm=0.6645
[iter 400] loss=0.7189 val_loss=0.0000 scale=1.0000 norm=0.6429
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0913 val_loss=0.0000 scale=2.0000 norm=1.5246
[iter 200] loss=0.9044 val_loss=0.0000 scale=1.0000 norm=0.6978
[iter 300] loss=0.7841 val_loss=0.0000 scale=1.0000 norm=0.6638
[iter 400] loss=0.7009 val_loss=0.0000 scale=2.0000 norm=1.2844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1110 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 200] loss=0.9078 val_loss=0.0000 scale=2.0000 norm=1.3842
[iter 300] loss=0.7936 val_loss=0.0000 scale=1.0000 norm=0.6645
[iter 400] loss=0.7189 val_loss=0.0000 scale=1.0000 norm=0.6429
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1220 val_loss=0.0000 scale=2.0000 norm=1.5536
[iter 200] loss=0.9372 val_loss=0.0000 scale=1.0000 norm=0.7067
[iter 300] loss=0.8269 val_loss=0.0000 scale=1.0000 norm=0.6775
[iter 400] loss=0.7523 val_loss=0.0000 scale=1.0000 norm=0.6540
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1082 val_loss=0.0000 scale=2.0000 norm=1.5400
[iter 200] loss=0.9166 val_loss=0.0000 scale=1.0000 norm=0.7001
[iter 300] loss=0.7983 val_loss=0.0000 scale=1.0000 norm=0.6688
[iter 400] loss=0.7225 val_loss=0.0000 scale=1.0000 norm=0.6495
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1545 val_loss=0.0000 scale=1.0000 norm=0.7979
[iter 200] loss=0.9953 val_loss=0.0000 scale=2.0000 norm=1.4719
[iter 300] loss=0.8844 val_loss=0.0000 scale=1.0000 norm=0.7021
[iter 400] loss=0.8112 val_loss=0.0000 scale=1.0000 norm=0.6774
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1082 val_loss=0.0000 scale=2.0000 norm=1.5400
[iter 200] loss=0.9166 val_loss=0.0000 scale=1.0000 norm=0.7001
[iter 300] loss=0.7983 val_loss=0.0000 scale=1.0000 norm=0.6688
[iter 400] loss=0.7225 val_loss=0.0000 scale=1.0000 norm=0.6495
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1321 val_loss=0.0000 scale=2.0000 norm=1.5785
[iter 200] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.7261
[iter 300] loss=0.8498 val_loss=0.0000 scale=1.0000 norm=0.6948
[iter 400] loss=0.7736 val_loss=0.0000 scale=1.0000 norm=0.6748
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1311 val_loss=0.0000 scale=2.0000 norm=1.5660
[iter 200] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.7200
[iter 300] loss=0.8516 val_loss=0.0000 scale=1.0000 norm=0.6824
[iter 400] loss=0.7612 val_loss=0.0000 scale=1.0000 norm=0.6559
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1321 val_loss=0.0000 scale=2.0000 norm=1.5785
[iter 200] loss=0.9692 val_loss=0.0000 scale=1.0000 norm=0.7261
[iter 300] loss=0.8498 val_loss=0.0000 scale=1.0000 norm=0.6948
[iter 400] loss=0.7736 val_loss=0.0000 scale=1.0000 norm=0.6748
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1297 val_loss=0.0000 scale=1.0000 norm=0.7821
[iter 200] loss=0.9382 val_loss=0.0000 scale=2.0000 norm=1.4122
[iter 300] loss=0.8183 val_loss=0.0000 scale=1.0000 norm=0.6730
[iter 400] loss=0.7306 val_loss=0.0000 scale=1.0000 norm=0.6516
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1297 val_loss=0.0000 scale=1.0000 norm=0.7821
[iter 200] loss=0.9382 val_loss=0.0000 scale=2.0000 norm=1.4122
[iter 300] loss=0.8183 val_loss=0.0000 scale=1.0000 norm=0.6730
[iter 400] loss=0.7306 val_loss=0.0000 scale=1.0000 norm=0.6516
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1177 val_loss=0.0000 scale=1.0000 norm=0.7736
[iter 200] loss=0.9320 val_loss=0.0000 scale=1.0000 norm=0.7028
[iter 300] loss=0.8164 val_loss=0.0000 scale=2.0000 norm=1.3455
[iter 400] loss=0.7403 val_loss=0.0000 scale=1.0000 norm=0.6536
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1104 val_loss=0.0000 scale=1.0000 norm=0.7731
[iter 200] loss=0.9232 val_loss=0.0000 scale=1.0000 norm=0.7094
[iter 300] loss=0.8287 val_loss=0.0000 scale=1.0000 norm=0.6818
[iter 400] loss=0.7474 val_loss=0.0000 scale=1.0000 norm=0.6544
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1397 val_loss=0.0000 scale=2.0000 norm=1.5776
[iter 200] loss=0.9516 val_loss=0.0000 scale=2.0000 norm=1.4384
[iter 300] loss=0.8260 val_loss=0.0000 scale=2.0000 norm=1.3672
[iter 400] loss=0.7319 val_loss=0.0000 scale=2.0000 norm=1.3093
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2366 val_loss=0.0000 scale=1.0000 norm=0.8581
[iter 200] loss=1.1231 val_loss=0.0000 scale=2.0000 norm=1.6120
[iter 300] loss=1.0417 val_loss=0.0000 scale=1.0000 norm=0.7822
[iter 400] loss=0.9885 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2366 val_loss=0.0000 scale=1.0000 norm=0.8581
[iter 200] loss=1.1231 val_loss=0.0000 scale=2.0000 norm=1.6120
[iter 300] loss=1.0417 val_loss=0.0000 scale=1.0000 norm=0.7822
[iter 400] loss=0.9885 val_loss=0.0000 scale=1.0000 norm=0.7687
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1963 val_loss=0.0000 scale=1.0000 norm=0.8269
[iter 200] loss=1.0649 val_loss=0.0000 scale=2.0000 norm=1.5615
[iter 300] loss=0.9911 val_loss=0.0000 scale=1.0000 norm=0.7663
[iter 400] loss=0.9488 val_loss=0.0000 scale=1.0000 norm=0.7562
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2210 val_loss=0.0000 scale=2.0000 norm=1.6970
[iter 200] loss=1.1083 val_loss=0.0000 scale=2.0000 norm=1.5929
[iter 300] loss=1.0318 val_loss=0.0000 scale=2.0000 norm=1.5507
[iter 400] loss=0.9793 val_loss=0.0000 scale=2.0000 norm=1.5232
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2363 val_loss=0.0000 scale=1.0000 norm=0.8587
[iter 200] loss=1.1204 val_loss=0.0000 scale=1.0000 norm=0.8032
[iter 300] loss=1.0356 val_loss=0.0000 scale=1.0000 norm=0.7796
[iter 400] loss=0.9878 val_loss=0.0000 scale=1.0000 norm=0.7695
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2157 val_loss=0.0000 scale=1.0000 norm=0.8442
[iter 200] loss=1.1061 val_loss=0.0000 scale=2.0000 norm=1.5949
[iter 300] loss=1.0389 val_loss=0.0000 scale=1.0000 norm=0.7770
[iter 400] loss=0.9961 val_loss=0.0000 scale=1.0000 norm=0.7659
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2166 val_loss=0.0000 scale=1.0000 norm=0.8449
[iter 200] loss=1.1041 val_loss=0.0000 scale=2.0000 norm=1.5912
[iter 300] loss=1.0214 val_loss=0.0000 scale=1.0000 norm=0.7733
[iter 400] loss=0.9765 val_loss=0.0000 scale=1.0000 norm=0.7638
[iter 400] loss=0.6005 val_loss=0.0000 scale=1.0000 norm=0.5927
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1239 val_loss=0.0000 scale=2.0000 norm=1.5563
[iter 200] loss=0.9435 val_loss=0.0000 scale=2.0000 norm=1.4243
[iter 300] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.3658
[iter 400] loss=0.7566 val_loss=0.0000 scale=1.0000 norm=0.6614
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1519 val_loss=0.0000 scale=1.0000 norm=0.8007
[iter 200] loss=0.9705 val_loss=0.0000 scale=2.0000 norm=1.4661
[iter 300] loss=0.8569 val_loss=0.0000 scale=2.0000 norm=1.3973
[iter 400] loss=0.7701 val_loss=0.0000 scale=1.0000 norm=0.6736
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1504 val_loss=0.0000 scale=1.0000 norm=0.7963
[iter 200] loss=0.9436 val_loss=0.0000 scale=2.0000 norm=1.4179
[iter 300] loss=0.8361 val_loss=0.0000 scale=1.0000 norm=0.6802
[iter 400] loss=0.7703 val_loss=0.0000 scale=1.0000 norm=0.6605
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1308 val_loss=0.0000 scale=1.0000 norm=0.7824
[iter 200] loss=0.9592 val_loss=0.0000 scale=1.0000 norm=0.7195
[iter 300] loss=0.8427 val_loss=0.0000 scale=2.0000 norm=1.3692
[iter 400] loss=0.7475 val_loss=0.0000 scale=2.0000 norm=1.3180
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1392 val_loss=0.0000 scale=1.0000 norm=0.7887
[iter 200] loss=0.9729 val_loss=0.0000 scale=1.0000 norm=0.7203
[iter 300] loss=0.8565 val_loss=0.0000 scale=1.0000 norm=0.6884
[iter 400] loss=0.7785 val_loss=0.0000 scale=1.0000 norm=0.6659
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1438 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.9569 val_loss=0.0000 scale=2.0000 norm=1.4342
[iter 300] loss=0.8437 val_loss=0.0000 scale=1.0000 norm=0.6885
[iter 400] loss=0.7627 val_loss=0.0000 scale=1.0000 norm=0.6671
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1195 val_loss=0.0000 scale=1.0000 norm=0.7809
[iter 200] loss=0.9396 val_loss=0.0000 scale=1.0000 norm=0.7187
[iter 300] loss=0.8363 val_loss=0.0000 scale=1.0000 norm=0.6889
[iter 400] loss=0.7561 val_loss=0.0000 scale=1.0000 norm=0.6643
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1278 val_loss=0.0000 scale=1.0000 norm=0.7800
[iter 200] loss=0.9474 val_loss=0.0000 scale=2.0000 norm=1.4181
[iter 300] loss=0.8477 val_loss=0.0000 scale=1.0000 norm=0.6842
[iter 400] loss=0.7677 val_loss=0.0000 scale=1.0000 norm=0.6591
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1128 val_loss=0.0000 scale=1.0000 norm=0.7674
[iter 200] loss=0.9119 val_loss=0.0000 scale=1.0000 norm=0.6940
[iter 300] loss=0.8000 val_loss=0.0000 scale=1.0000 norm=0.6655
[iter 400] loss=0.7109 val_loss=0.0000 scale=1.0000 norm=0.6374
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1351 val_loss=0.0000 scale=1.0000 norm=0.7889
[iter 200] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.4477
[iter 300] loss=0.8561 val_loss=0.0000 scale=1.0000 norm=0.6928
[iter 400] loss=0.7838 val_loss=0.0000 scale=1.0000 norm=0.6722
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1311 val_loss=0.0000 scale=1.0000 norm=0.7851
[iter 200] loss=0.9626 val_loss=0.0000 scale=2.0000 norm=1.4441
[iter 300] loss=0.8575 val_loss=0.0000 scale=1.0000 norm=0.6907
[iter 400] loss=0.7828 val_loss=0.0000 scale=1.0000 norm=0.6678
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1311 val_loss=0.0000 scale=1.0000 norm=0.7851
[iter 200] loss=0.9626 val_loss=0.0000 scale=2.0000 norm=1.4441
[iter 300] loss=0.8575 val_loss=0.0000 scale=1.0000 norm=0.6907
[iter 400] loss=0.7828 val_loss=0.0000 scale=1.0000 norm=0.6678
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1455 val_loss=0.0000 scale=1.0000 norm=0.7907
[iter 200] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.4415
[iter 300] loss=0.8604 val_loss=0.0000 scale=2.0000 norm=1.3830
[iter 400] loss=0.7829 val_loss=0.0000 scale=1.0000 norm=0.6687
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1177 val_loss=0.0000 scale=1.0000 norm=0.7736
[iter 200] loss=0.9320 val_loss=0.0000 scale=1.0000 norm=0.7028
[iter 300] loss=0.8164 val_loss=0.0000 scale=2.0000 norm=1.3455
[iter 400] loss=0.7403 val_loss=0.0000 scale=1.0000 norm=0.6536
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2314 val_loss=0.0000 scale=1.0000 norm=0.8550
[iter 200] loss=1.1262 val_loss=0.0000 scale=1.0000 norm=0.8083
[iter 300] loss=1.0381 val_loss=0.0000 scale=1.0000 norm=0.7780
[iter 400] loss=0.9856 val_loss=0.0000 scale=1.0000 norm=0.7623
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2314 val_loss=0.0000 scale=1.0000 norm=0.8550
[iter 200] loss=1.1262 val_loss=0.0000 scale=1.0000 norm=0.8083
[iter 300] loss=1.0381 val_loss=0.0000 scale=1.0000 norm=0.7780
[iter 400] loss=0.9856 val_loss=0.0000 scale=1.0000 norm=0.7623
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2210 val_loss=0.0000 scale=2.0000 norm=1.6970
[iter 200] loss=1.1083 val_loss=0.0000 scale=2.0000 norm=1.5929
[iter 300] loss=1.0318 val_loss=0.0000 scale=2.0000 norm=1.5507
[iter 400] loss=0.9793 val_loss=0.0000 scale=2.0000 norm=1.5232
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1963 val_loss=0.0000 scale=1.0000 norm=0.8269
[iter 200] loss=1.0649 val_loss=0.0000 scale=2.0000 norm=1.5615
[iter 300] loss=0.9911 val_loss=0.0000 scale=1.0000 norm=0.7663
[iter 400] loss=0.9488 val_loss=0.0000 scale=1.0000 norm=0.7562
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2175 val_loss=0.0000 scale=1.0000 norm=0.8442
[iter 200] loss=1.1044 val_loss=0.0000 scale=1.0000 norm=0.7986
[iter 300] loss=1.0330 val_loss=0.0000 scale=1.0000 norm=0.7779
[iter 400] loss=0.9850 val_loss=0.0000 scale=1.0000 norm=0.7639
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2175 val_loss=0.0000 scale=1.0000 norm=0.8442
[iter 200] loss=1.1044 val_loss=0.0000 scale=1.0000 norm=0.7986
[iter 300] loss=1.0330 val_loss=0.0000 scale=1.0000 norm=0.7779
[iter 400] loss=0.9850 val_loss=0.0000 scale=1.0000 norm=0.7639
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2284 val_loss=0.0000 scale=1.0000 norm=0.8531
[iter 200] loss=1.1247 val_loss=0.0000 scale=1.0000 norm=0.8074
[iter 300] loss=1.0540 val_loss=0.0000 scale=1.0000 norm=0.7878
[iter 400] loss=1.0079 val_loss=0.0000 scale=1.0000 norm=0.7747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2284 val_loss=0.0000 scale=1.0000 norm=0.8531
[iter 200] loss=1.1247 val_loss=0.0000 scale=1.0000 norm=0.8074
[iter 300] loss=1.0540 val_loss=0.0000 scale=1.0000 norm=0.7878
[iter 400] loss=1.0079 val_loss=0.0000 scale=1.0000 norm=0.7747
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2212 val_loss=0.0000 scale=1.0000 norm=0.8476
[iter 200] loss=1.1161 val_loss=0.0000 scale=2.0000 norm=1.6033
[iter 300] loss=1.0375 val_loss=0.0000 scale=1.0000 norm=0.7800
[iter 400] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=0.7673
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2212 val_loss=0.0000 scale=1.0000 norm=0.8476
[iter 200] loss=1.1161 val_loss=0.0000 scale=2.0000 norm=1.6033
[iter 300] loss=1.0375 val_loss=0.0000 scale=1.0000 norm=0.7800
[iter 400] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=0.7673
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2172 val_loss=0.0000 scale=1.0000 norm=0.8428
[iter 200] loss=1.0947 val_loss=0.0000 scale=1.0000 norm=0.7879
[iter 300] loss=1.0137 val_loss=0.0000 scale=2.0000 norm=1.5305
[iter 400] loss=0.9624 val_loss=0.0000 scale=1.0000 norm=0.7528
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2363 val_loss=0.0000 scale=1.0000 norm=0.8602
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
[iter 200] loss=0.9516 val_loss=0.0000 scale=2.0000 norm=1.4384
[iter 300] loss=0.8260 val_loss=0.0000 scale=2.0000 norm=1.3672
[iter 400] loss=0.7319 val_loss=0.0000 scale=2.0000 norm=1.3093
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1455 val_loss=0.0000 scale=1.0000 norm=0.7907
[iter 200] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.4415
[iter 300] loss=0.8604 val_loss=0.0000 scale=2.0000 norm=1.3830
[iter 400] loss=0.7829 val_loss=0.0000 scale=1.0000 norm=0.6687
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1104 val_loss=0.0000 scale=1.0000 norm=0.7731
[iter 200] loss=0.9232 val_loss=0.0000 scale=1.0000 norm=0.7094
[iter 300] loss=0.8287 val_loss=0.0000 scale=1.0000 norm=0.6818
[iter 400] loss=0.7474 val_loss=0.0000 scale=1.0000 norm=0.6544
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2363 val_loss=0.0000 scale=1.0000 norm=0.8577
[iter 200] loss=1.1182 val_loss=0.0000 scale=1.0000 norm=0.8006
[iter 300] loss=1.0265 val_loss=0.0000 scale=2.0000 norm=1.5515
[iter 400] loss=0.9818 val_loss=0.0000 scale=1.0000 norm=0.7640
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2222 val_loss=0.0000 scale=1.0000 norm=0.8475
[iter 200] loss=1.1101 val_loss=0.0000 scale=2.0000 norm=1.5971
[iter 300] loss=1.0309 val_loss=0.0000 scale=2.0000 norm=1.5553
[iter 400] loss=0.9814 val_loss=0.0000 scale=1.0000 norm=0.7675
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2363 val_loss=0.0000 scale=1.0000 norm=0.8577
[iter 200] loss=1.1182 val_loss=0.0000 scale=1.0000 norm=0.8006
[iter 300] loss=1.0265 val_loss=0.0000 scale=2.0000 norm=1.5515
[iter 400] loss=0.9818 val_loss=0.0000 scale=1.0000 norm=0.7640
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2418 val_loss=0.0000 scale=1.0000 norm=0.8628
[iter 200] loss=1.1383 val_loss=0.0000 scale=1.0000 norm=0.8132
[iter 300] loss=1.0715 val_loss=0.0000 scale=1.0000 norm=0.7904
[iter 400] loss=1.0216 val_loss=0.0000 scale=1.0000 norm=0.7762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2363 val_loss=0.0000 scale=1.0000 norm=0.8587
[iter 200] loss=1.1204 val_loss=0.0000 scale=1.0000 norm=0.8032
[iter 300] loss=1.0356 val_loss=0.0000 scale=1.0000 norm=0.7796
[iter 400] loss=0.9878 val_loss=0.0000 scale=1.0000 norm=0.7695
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2380 val_loss=0.0000 scale=1.0000 norm=0.8593
[iter 200] loss=1.1397 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=1.0679 val_loss=0.0000 scale=1.0000 norm=0.7897
[iter 400] loss=1.0238 val_loss=0.0000 scale=1.0000 norm=0.7780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2166 val_loss=0.0000 scale=1.0000 norm=0.8449
[iter 200] loss=1.1041 val_loss=0.0000 scale=2.0000 norm=1.5912
[iter 300] loss=1.0214 val_loss=0.0000 scale=1.0000 norm=0.7733
[iter 400] loss=0.9765 val_loss=0.0000 scale=1.0000 norm=0.7638
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2221 val_loss=0.0000 scale=2.0000 norm=1.6918
[iter 200] loss=1.1077 val_loss=0.0000 scale=1.0000 norm=0.7956
[iter 300] loss=1.0316 val_loss=0.0000 scale=1.0000 norm=0.7764
[iter 400] loss=0.9811 val_loss=0.0000 scale=1.0000 norm=0.7635
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2157 val_loss=0.0000 scale=1.0000 norm=0.8442
[iter 200] loss=1.1061 val_loss=0.0000 scale=2.0000 norm=1.5949
[iter 300] loss=1.0389 val_loss=0.0000 scale=1.0000 norm=0.7770
[iter 400] loss=0.9961 val_loss=0.0000 scale=1.0000 norm=0.7659
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2221 val_loss=0.0000 scale=2.0000 norm=1.6918
[iter 200] loss=1.1077 val_loss=0.0000 scale=1.0000 norm=0.7956
[iter 300] loss=1.0316 val_loss=0.0000 scale=1.0000 norm=0.7764
[iter 400] loss=0.9811 val_loss=0.0000 scale=1.0000 norm=0.7635
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2312 val_loss=0.0000 scale=2.0000 norm=1.7059
[iter 200] loss=1.1234 val_loss=0.0000 scale=1.0000 norm=0.8040
[iter 300] loss=1.0547 val_loss=0.0000 scale=1.0000 norm=0.7867
[iter 400] loss=1.0087 val_loss=0.0000 scale=2.0000 norm=1.5554
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2213 val_loss=0.0000 scale=1.0000 norm=0.8473
[iter 200] loss=1.1144 val_loss=0.0000 scale=1.0000 norm=0.7976
[iter 300] loss=1.0333 val_loss=0.0000 scale=2.0000 norm=1.5449
[iter 400] loss=0.9855 val_loss=0.0000 scale=1.0000 norm=0.7592
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2312 val_loss=0.0000 scale=2.0000 norm=1.7059
[iter 200] loss=1.1234 val_loss=0.0000 scale=1.0000 norm=0.8040
[iter 300] loss=1.0547 val_loss=0.0000 scale=1.0000 norm=0.7867
[iter 400] loss=1.0087 val_loss=0.0000 scale=2.0000 norm=1.5554
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2213 val_loss=0.0000 scale=1.0000 norm=0.8473
[iter 200] loss=1.1144 val_loss=0.0000 scale=1.0000 norm=0.7976
[iter 300] loss=1.0333 val_loss=0.0000 scale=2.0000 norm=1.5449
[iter 400] loss=0.9855 val_loss=0.0000 scale=1.0000 norm=0.7592
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2106 val_loss=0.0000 scale=1.0000 norm=0.8397
[iter 200] loss=1.1010 val_loss=0.0000 scale=1.0000 norm=0.7928
[iter 300] loss=1.0240 val_loss=0.0000 scale=2.0000 norm=1.5470
[iter 400] loss=0.9729 val_loss=0.0000 scale=1.0000 norm=0.7613
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2300 val_loss=0.0000 scale=1.0000 norm=0.8520
[iter 200] loss=1.1204 val_loss=0.0000 scale=2.0000 norm=1.6098
[iter 300] loss=1.0511 val_loss=0.0000 scale=1.0000 norm=0.7859
[iter 400] loss=1.0002 val_loss=0.0000 scale=1.0000 norm=0.7713
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2363 val_loss=0.0000 scale=1.0000 norm=0.8602
[iter 200] loss=1.1287 val_loss=0.0000 scale=1.0000 norm=0.8099
[iter 300] loss=1.0529 val_loss=0.0000 scale=1.0000 norm=0.7881
[iter 400] loss=1.0095 val_loss=0.0000 scale=1.0000 norm=0.7782
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2172 val_loss=0.0000 scale=1.0000 norm=0.8428
[iter 200] loss=1.0947 val_loss=0.0000 scale=1.0000 norm=0.7879
[iter 300] loss=1.0137 val_loss=0.0000 scale=2.0000 norm=1.5305
[iter 400] loss=0.9624 val_loss=0.0000 scale=1.0000 norm=0.7528
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1984 val_loss=0.0000 scale=1.0000 norm=0.8327
[iter 200] loss=1.1047 val_loss=0.0000 scale=1.0000 norm=0.7978
[iter 300] loss=1.0255 val_loss=0.0000 scale=1.0000 norm=0.7755
[iter 400] loss=0.9783 val_loss=0.0000 scale=1.0000 norm=0.7625
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2321 val_loss=0.0000 scale=1.0000 norm=0.8554
[iter 200] loss=1.1290 val_loss=0.0000 scale=1.0000 norm=0.8052
[iter 300] loss=1.0567 val_loss=0.0000 scale=1.0000 norm=0.7849
[iter 400] loss=1.0027 val_loss=0.0000 scale=1.0000 norm=0.7722
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2301 val_loss=0.0000 scale=1.0000 norm=0.8547
[iter 200] loss=1.1100 val_loss=0.0000 scale=2.0000 norm=1.6032
[iter 300] loss=1.0410 val_loss=0.0000 scale=1.0000 norm=0.7816
[iter 400] loss=0.9895 val_loss=0.0000 scale=2.0000 norm=1.5352
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2321 val_loss=0.0000 scale=1.0000 norm=0.8554
[iter 200] loss=1.1290 val_loss=0.0000 scale=1.0000 norm=0.8052
[iter 300] loss=1.0567 val_loss=0.0000 scale=1.0000 norm=0.7849
[iter 400] loss=1.0027 val_loss=0.0000 scale=1.0000 norm=0.7722
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2301 val_loss=0.0000 scale=1.0000 norm=0.8547
[iter 200] loss=1.1100 val_loss=0.0000 scale=2.0000 norm=1.6032
[iter 300] loss=1.0410 val_loss=0.0000 scale=1.0000 norm=0.7816
[iter 400] loss=0.9895 val_loss=0.0000 scale=2.0000 norm=1.5352
LOSS: 1.2120500802993774
LOSS: 1.2049823999404907
[iter 200] loss=0.9505 val_loss=0.0000 scale=1.0000 norm=0.7172
[iter 300] loss=0.8293 val_loss=0.0000 scale=1.0000 norm=0.6847
[iter 400] loss=0.7488 val_loss=0.0000 scale=2.0000 norm=1.3231
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2278 val_loss=0.0000 scale=1.0000 norm=0.8503
[iter 200] loss=1.1253 val_loss=0.0000 scale=2.0000 norm=1.6115
[iter 300] loss=1.0575 val_loss=0.0000 scale=1.0000 norm=0.7892
[iter 400] loss=1.0143 val_loss=0.0000 scale=1.0000 norm=0.7758
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2278 val_loss=0.0000 scale=1.0000 norm=0.8503
[iter 200] loss=1.1253 val_loss=0.0000 scale=2.0000 norm=1.6115
[iter 300] loss=1.0575 val_loss=0.0000 scale=1.0000 norm=0.7892
[iter 400] loss=1.0143 val_loss=0.0000 scale=1.0000 norm=0.7758
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2222 val_loss=0.0000 scale=1.0000 norm=0.8475
[iter 200] loss=1.1101 val_loss=0.0000 scale=2.0000 norm=1.5971
[iter 300] loss=1.0309 val_loss=0.0000 scale=2.0000 norm=1.5553
[iter 400] loss=0.9814 val_loss=0.0000 scale=1.0000 norm=0.7675
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2380 val_loss=0.0000 scale=1.0000 norm=0.8593
[iter 200] loss=1.1397 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=1.0679 val_loss=0.0000 scale=1.0000 norm=0.7897
[iter 400] loss=1.0238 val_loss=0.0000 scale=1.0000 norm=0.7780
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2418 val_loss=0.0000 scale=1.0000 norm=0.8628
[iter 200] loss=1.1383 val_loss=0.0000 scale=1.0000 norm=0.8132
[iter 300] loss=1.0715 val_loss=0.0000 scale=1.0000 norm=0.7904
[iter 400] loss=1.0216 val_loss=0.0000 scale=1.0000 norm=0.7762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2412 val_loss=0.0000 scale=1.0000 norm=0.8667
[iter 200] loss=1.1293 val_loss=0.0000 scale=1.0000 norm=0.8140
[iter 300] loss=1.0620 val_loss=0.0000 scale=1.0000 norm=0.7951
[iter 400] loss=1.0132 val_loss=0.0000 scale=1.0000 norm=0.7834
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2412 val_loss=0.0000 scale=1.0000 norm=0.8667
[iter 200] loss=1.1293 val_loss=0.0000 scale=1.0000 norm=0.8140
[iter 300] loss=1.0620 val_loss=0.0000 scale=1.0000 norm=0.7951
[iter 400] loss=1.0132 val_loss=0.0000 scale=1.0000 norm=0.7834
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2179 val_loss=0.0000 scale=1.0000 norm=0.8471
[iter 200] loss=1.1144 val_loss=0.0000 scale=1.0000 norm=0.8066
[iter 300] loss=1.0316 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 400] loss=0.9846 val_loss=0.0000 scale=1.0000 norm=0.7718
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2179 val_loss=0.0000 scale=1.0000 norm=0.8471
[iter 200] loss=1.1144 val_loss=0.0000 scale=1.0000 norm=0.8066
[iter 300] loss=1.0316 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 400] loss=0.9846 val_loss=0.0000 scale=1.0000 norm=0.7718
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2290 val_loss=0.0000 scale=1.0000 norm=0.8555
[iter 200] loss=1.1276 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 300] loss=1.0495 val_loss=0.0000 scale=2.0000 norm=1.5680
[iter 400] loss=0.9964 val_loss=0.0000 scale=1.0000 norm=0.7684
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2290 val_loss=0.0000 scale=1.0000 norm=0.8555
[iter 200] loss=1.1276 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 300] loss=1.0495 val_loss=0.0000 scale=2.0000 norm=1.5680
[iter 400] loss=0.9964 val_loss=0.0000 scale=1.0000 norm=0.7684
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2258 val_loss=0.0000 scale=2.0000 norm=1.7077
[iter 200] loss=1.1146 val_loss=0.0000 scale=2.0000 norm=1.6063
[iter 300] loss=1.0436 val_loss=0.0000 scale=1.0000 norm=0.7810
[iter 400] loss=0.9950 val_loss=0.0000 scale=1.0000 norm=0.7680
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2258 val_loss=0.0000 scale=2.0000 norm=1.7077
[iter 200] loss=1.1146 val_loss=0.0000 scale=2.0000 norm=1.6063
[iter 300] loss=1.0436 val_loss=0.0000 scale=1.0000 norm=0.7810
[iter 400] loss=0.9950 val_loss=0.0000 scale=1.0000 norm=0.7680
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2204 val_loss=0.0000 scale=2.0000 norm=1.6918
[iter 200] loss=1.1190 val_loss=0.0000 scale=1.0000 norm=0.8018
[iter 300] loss=1.0412 val_loss=0.0000 scale=2.0000 norm=1.5544
[iter 400] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=0.7657
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2337 val_loss=0.0000 scale=1.0000 norm=0.8555
[iter 200] loss=1.1141 val_loss=0.0000 scale=1.0000 norm=0.8005
[iter 300] loss=1.0310 val_loss=0.0000 scale=1.0000 norm=0.7794
[iter 400] loss=0.9799 val_loss=0.0000 scale=1.0000 norm=0.7660
LOSS: 1.202947735786438
LOSS: 1.1956803798675537
LOSS: 1.188751220703125
LOSS: 1.1821917295455933
LOSS: 1.1760247945785522
LOSS: 1.170253872871399
LOSS: 1.1648510694503784
LOSS: 1.159754753112793
LOSS: 1.1548911333084106
LOSS: 1.150204062461853
LOSS: 1.1456656455993652
LOSS: 1.1412687301635742
LOSS: 1.137015700340271
LOSS: 1.132912516593933
LOSS: 1.1289653778076172
LOSS: 1.1251795291900635
LOSS: 1.1215587854385376
LOSS: 1.1181062459945679
LOSS: 1.1148234605789185
LOSS: 1.1117113828659058
LOSS: 1.1087698936462402
LOSS: 1.1059986352920532
LOSS: 1.1033955812454224
LOSS: 1.1009588241577148
LOSS: 1.0986851453781128
LOSS: 1.0965710878372192
LOSS: 1.0946115255355835
LOSS: 1.0928014516830444
LOSS: 1.0911346673965454
LOSS: 1.0896035432815552
LOSS: 1.0882015228271484
LOSS: 1.086920142173767
LOSS: 1.085752248764038
LOSS: 1.0846898555755615
LOSS: 1.083725094795227
LOSS: 1.0828512907028198
LOSS: 1.0820621252059937
LOSS: 1.081351399421692
LOSS: 1.0807135105133057
LOSS: 1.0801441669464111
LOSS: 1.0796388387680054
LOSS: 1.0791925191879272
LOSS: 1.0788018703460693
LOSS: 1.0784622430801392
LOSS: 1.0781692266464233
LOSS: 1.077919840812683
LOSS: 1.0777087211608887
LOSS: 1.0775319337844849
LOSS: 1.0773850679397583
LOSS: 1.0772651433944702
LOSS: 1.0771679878234863
LOSS: 1.077089786529541
LOSS: 1.0770288705825806
LOSS: 1.0769819021224976
LOSS: 1.0769466161727905
LOSS: 1.076921820640564
LOSS: 1.0769054889678955
LOSS: 1.0768955945968628
LOSS: 1.076891541481018
LOSS: 1.0768921375274658
LOSS: 1.0768959522247314
LOSS: 1.076902985572815
LOSS: 1.0769108533859253
LOSS: 1.076919436454773
LOSS: 1.0769292116165161
LOSS: 1.0769376754760742
LOSS: 1.0769457817077637
LOSS: 1.0769531726837158
LOSS: 1.0769588947296143
LOSS: 1.0769635438919067
LOSS: 1.0769670009613037
LOSS: 1.0769689083099365
LOSS: 1.0769705772399902
LOSS: 1.0769703388214111
LOSS: 1.076969861984253
LOSS: 1.0769681930541992
LOSS: 1.0769660472869873
LOSS: 1.0769637823104858
LOSS: 1.0769600868225098
LOSS: 1.0769563913345337
LOSS: 1.0769520998001099
LOSS: 1.0769476890563965
LOSS: 1.0769431591033936
LOSS: 1.076938271522522
LOSS: 1.0769340991973877
LOSS: 1.0769293308258057
LOSS: 1.0769249200820923
LOSS: 1.076920509338379
LOSS: 1.0769169330596924
LOSS: 1.0769128799438477
LOSS: 1.0769100189208984
LOSS: 1.0769072771072388
LOSS: 1.076904296875
LOSS: 1.076902151107788
LOSS: 1.076899766921997
LOSS: 1.0768979787826538
LOSS: 1.0768965482711792
LOSS: 1.076894998550415
LOSS: 1.0768941640853882
LOSS: 1.0768928527832031
[iter 200] loss=1.1287 val_loss=0.0000 scale=1.0000 norm=0.8099
[iter 300] loss=1.0529 val_loss=0.0000 scale=1.0000 norm=0.7881
[iter 400] loss=1.0095 val_loss=0.0000 scale=1.0000 norm=0.7782
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2134 val_loss=0.0000 scale=1.0000 norm=0.8444
[iter 200] loss=1.1115 val_loss=0.0000 scale=1.0000 norm=0.8043
[iter 300] loss=1.0362 val_loss=0.0000 scale=1.0000 norm=0.7827
[iter 400] loss=0.9908 val_loss=0.0000 scale=1.0000 norm=0.7719
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2204 val_loss=0.0000 scale=2.0000 norm=1.6918
[iter 200] loss=1.1190 val_loss=0.0000 scale=1.0000 norm=0.8018
[iter 300] loss=1.0412 val_loss=0.0000 scale=2.0000 norm=1.5544
[iter 400] loss=0.9951 val_loss=0.0000 scale=1.0000 norm=0.7657
LOSS: 1.214086651802063
LOSS: 1.2071738243103027
LOSS: 1.2006150484085083
LOSS: 1.194420337677002
LOSS: 1.1885409355163574
LOSS: 1.1828666925430298
LOSS: 1.1773097515106201
LOSS: 1.1718426942825317
LOSS: 1.1664680242538452
LOSS: 1.1611964702606201
LOSS: 1.1560381650924683
LOSS: 1.1510025262832642
LOSS: 1.1460970640182495
LOSS: 1.1413283348083496
LOSS: 1.1367017030715942
LOSS: 1.1322214603424072
LOSS: 1.1278916597366333
LOSS: 1.123714804649353
LOSS: 1.1196937561035156
LOSS: 1.1158294677734375
LOSS: 1.1121225357055664
LOSS: 1.1085729598999023
LOSS: 1.1051790714263916
LOSS: 1.1019388437271118
LOSS: 1.0988494157791138
LOSS: 1.0959067344665527
LOSS: 1.0931073427200317
LOSS: 1.0904464721679688
LOSS: 1.0879194736480713
LOSS: 1.0855231285095215
LOSS: 1.0832535028457642
LOSS: 1.0811066627502441
LOSS: 1.07908034324646
LOSS: 1.0771712064743042
LOSS: 1.0753763914108276
LOSS: 1.073692798614502
LOSS: 1.0721169710159302
LOSS: 1.0706448554992676
LOSS: 1.0692731142044067
LOSS: 1.0679959058761597
LOSS: 1.0668092966079712
LOSS: 1.0657086372375488
LOSS: 1.064689040184021
LOSS: 1.0637463331222534
LOSS: 1.0628767013549805
LOSS: 1.0620760917663574
LOSS: 1.061340570449829
LOSS: 1.0606681108474731
LOSS: 1.0600531101226807
LOSS: 1.0594936609268188
LOSS: 1.058985948562622
LOSS: 1.0585267543792725
LOSS: 1.0581119060516357
LOSS: 1.0577383041381836
LOSS: 1.057403564453125
LOSS: 1.0571038722991943
LOSS: 1.0568362474441528
LOSS: 1.0565986633300781
LOSS: 1.056388258934021
LOSS: 1.0562036037445068
LOSS: 1.0560424327850342
LOSS: 1.0559011697769165
LOSS: 1.0557794570922852
LOSS: 1.0556753873825073
LOSS: 1.0555857419967651
LOSS: 1.0555094480514526
LOSS: 1.0554455518722534
LOSS: 1.0553921461105347
LOSS: 1.0553479194641113
LOSS: 1.0553122758865356
LOSS: 1.0552831888198853
LOSS: 1.055260181427002
LOSS: 1.055242896080017
LOSS: 1.0552293062210083
LOSS: 1.0552197694778442
LOSS: 1.0552133321762085
LOSS: 1.0552088022232056
LOSS: 1.0552071332931519
LOSS: 1.0552061796188354
LOSS: 1.0552061796188354
LOSS: 1.0552079677581787
LOSS: 1.055209755897522
LOSS: 1.055212140083313
LOSS: 1.0552141666412354
LOSS: 1.0552172660827637
LOSS: 1.0552195310592651
LOSS: 1.0552213191986084
LOSS: 1.0552237033843994
LOSS: 1.0552254915237427
LOSS: 1.0552273988723755
LOSS: 1.0552282333374023
LOSS: 1.0552290678024292
LOSS: 1.0552295446395874
LOSS: 1.0552294254302979
LOSS: 1.0552293062210083
LOSS: 1.0552294254302979
LOSS: 1.055228590965271
LOSS: 1.055228352546692
LOSS: 1.0552269220352173
LOSS: 1.0552263259887695
LOSS: 1.1982630491256714
LOSS: 1.1919211149215698
LOSS: 1.1859705448150635
LOSS: 1.1803929805755615
LOSS: 1.1751267910003662
LOSS: 1.1700870990753174
LOSS: 1.1652071475982666
LOSS: 1.1604565382003784
LOSS: 1.155829668045044
LOSS: 1.1513323783874512
LOSS: 1.1469733715057373
LOSS: 1.1427618265151978
LOSS: 1.138706088066101
LOSS: 1.1348130702972412
LOSS: 1.1310888528823853
LOSS: 1.1275384426116943
LOSS: 1.124165415763855
LOSS: 1.120972752571106
LOSS: 1.1179622411727905
LOSS: 1.1151347160339355
LOSS: 1.1124894618988037
LOSS: 1.1100245714187622
LOSS: 1.1077368259429932
LOSS: 1.1056219339370728
LOSS: 1.1036735773086548
LOSS: 1.1018848419189453
LOSS: 1.1002482175827026
LOSS: 1.0987552404403687
LOSS: 1.0973973274230957
LOSS: 1.0961661338806152
LOSS: 1.0950535535812378
LOSS: 1.0940521955490112
LOSS: 1.0931546688079834
LOSS: 1.0923548936843872
LOSS: 1.0916467905044556
LOSS: 1.0910241603851318
LOSS: 1.0904821157455444
LOSS: 1.0900144577026367
LOSS: 1.089614987373352
LOSS: 1.089277982711792
LOSS: 1.0889971256256104
LOSS: 1.088766098022461
LOSS: 1.088578224182129
LOSS: 1.0884283781051636
LOSS: 1.088310956954956
LOSS: 1.0882205963134766
LOSS: 1.0881532430648804
LOSS: 1.088105320930481
LOSS: 1.088072657585144
LOSS: 1.0880537033081055
LOSS: 1.0880458354949951
LOSS: 1.0880463123321533
LOSS: 1.0880534648895264
LOSS: 1.088065266609192
LOSS: 1.088080644607544
LOSS: 1.0880978107452393
LOSS: 1.0881152153015137
LOSS: 1.0881325006484985
LOSS: 1.0881476402282715
LOSS: 1.088161826133728
LOSS: 1.0881729125976562
LOSS: 1.0881816148757935
LOSS: 1.0881880521774292
LOSS: 1.0881918668746948
LOSS: 1.0881941318511963
LOSS: 1.0881937742233276
LOSS: 1.0881917476654053
LOSS: 1.088188648223877
LOSS: 1.088183045387268
LOSS: 1.0881770849227905
LOSS: 1.088170051574707
LOSS: 1.0881619453430176
LOSS: 1.08815336227417
LOSS: 1.0881441831588745
LOSS: 1.0881348848342896
LOSS: 1.0881255865097046
LOSS: 1.0881167650222778
LOSS: 1.0881075859069824
LOSS: 1.088099479675293
LOSS: 1.0880917310714722
LOSS: 1.0880846977233887
LOSS: 1.0880781412124634
LOSS: 1.088072419166565
LOSS: 1.0880670547485352
LOSS: 1.0880624055862427
LOSS: 1.088058352470398
LOSS: 1.0880546569824219
LOSS: 1.0880519151687622
LOSS: 1.088049292564392
LOSS: 1.0880476236343384
LOSS: 1.0880457162857056
LOSS: 1.0880446434020996
LOSS: 1.0880438089370728
LOSS: 1.088043451309204
LOSS: 1.088042974472046
LOSS: 1.0880426168441772
LOSS: 1.088042974472046
LOSS: 1.0880428552627563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2245 val_loss=0.0000 scale=1.0000 norm=0.8502
[iter 200] loss=1.1043 val_loss=0.0000 scale=1.0000 norm=0.7925
[iter 300] loss=1.0218 val_loss=0.0000 scale=1.0000 norm=0.7701
[iter 400] loss=0.9747 val_loss=0.0000 scale=2.0000 norm=1.5220
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2245 val_loss=0.0000 scale=1.0000 norm=0.8502
[iter 200] loss=1.1043 val_loss=0.0000 scale=1.0000 norm=0.7925
[iter 300] loss=1.0218 val_loss=0.0000 scale=1.0000 norm=0.7701
[iter 400] loss=0.9747 val_loss=0.0000 scale=2.0000 norm=1.5220
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2300 val_loss=0.0000 scale=1.0000 norm=0.8520
[iter 200] loss=1.1204 val_loss=0.0000 scale=2.0000 norm=1.6098
[iter 300] loss=1.0511 val_loss=0.0000 scale=1.0000 norm=0.7859
[iter 400] loss=1.0002 val_loss=0.0000 scale=1.0000 norm=0.7713
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2248 val_loss=0.0000 scale=1.0000 norm=0.8505
[iter 200] loss=1.1203 val_loss=0.0000 scale=2.0000 norm=1.6140
[iter 300] loss=1.0507 val_loss=0.0000 scale=1.0000 norm=0.7873
[iter 400] loss=1.0029 val_loss=0.0000 scale=1.0000 norm=0.7764
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2106 val_loss=0.0000 scale=1.0000 norm=0.8397
[iter 200] loss=1.1010 val_loss=0.0000 scale=1.0000 norm=0.7928
[iter 300] loss=1.0240 val_loss=0.0000 scale=2.0000 norm=1.5470
[iter 400] loss=0.9729 val_loss=0.0000 scale=1.0000 norm=0.7613
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2248 val_loss=0.0000 scale=1.0000 norm=0.8505
[iter 200] loss=1.1203 val_loss=0.0000 scale=2.0000 norm=1.6140
[iter 300] loss=1.0507 val_loss=0.0000 scale=1.0000 norm=0.7873
[iter 400] loss=1.0029 val_loss=0.0000 scale=1.0000 norm=0.7764
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2258 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 200] loss=1.1167 val_loss=0.0000 scale=1.0000 norm=0.7998
[iter 300] loss=1.0522 val_loss=0.0000 scale=1.0000 norm=0.7841
[iter 400] loss=1.0065 val_loss=0.0000 scale=1.0000 norm=0.7762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1984 val_loss=0.0000 scale=1.0000 norm=0.8327
[iter 200] loss=1.1047 val_loss=0.0000 scale=1.0000 norm=0.7978
[iter 300] loss=1.0255 val_loss=0.0000 scale=1.0000 norm=0.7755
[iter 400] loss=0.9783 val_loss=0.0000 scale=1.0000 norm=0.7625
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2258 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 200] loss=1.1167 val_loss=0.0000 scale=1.0000 norm=0.7998
[iter 300] loss=1.0522 val_loss=0.0000 scale=1.0000 norm=0.7841
[iter 400] loss=1.0065 val_loss=0.0000 scale=1.0000 norm=0.7762
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2337 val_loss=0.0000 scale=1.0000 norm=0.8555
[iter 200] loss=1.1141 val_loss=0.0000 scale=1.0000 norm=0.8005
[iter 300] loss=1.0310 val_loss=0.0000 scale=1.0000 norm=0.7794
[iter 400] loss=0.9799 val_loss=0.0000 scale=1.0000 norm=0.7660
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2134 val_loss=0.0000 scale=1.0000 norm=0.8444
[iter 200] loss=1.1115 val_loss=0.0000 scale=1.0000 norm=0.8043
[iter 300] loss=1.0362 val_loss=0.0000 scale=1.0000 norm=0.7827
[iter 400] loss=0.9908 val_loss=0.0000 scale=1.0000 norm=0.7719
LOSS: 1.2084391117095947
LOSS: 1.2012025117874146
LOSS: 1.194305658340454
LOSS: 1.1877774000167847
LOSS: 1.181631088256836
LOSS: 1.175848364830017
LOSS: 1.1703683137893677
LOSS: 1.1651066541671753
LOSS: 1.1599963903427124
LOSS: 1.1550074815750122
LOSS: 1.1501344442367554
LOSS: 1.145383596420288
LOSS: 1.1407636404037476
LOSS: 1.1362839937210083
LOSS: 1.1319527626037598
LOSS: 1.1277766227722168
LOSS: 1.1237609386444092
LOSS: 1.1199101209640503
LOSS: 1.1162270307540894
LOSS: 1.1127142906188965
LOSS: 1.1093729734420776
LOSS: 1.1062028408050537
LOSS: 1.1032036542892456
LOSS: 1.1003729104995728
LOSS: 1.0977075099945068
LOSS: 1.095203161239624
LOSS: 1.0928549766540527
LOSS: 1.0906565189361572
LOSS: 1.0886011123657227
LOSS: 1.0866819620132446
LOSS: 1.0848917961120605
LOSS: 1.0832244157791138
LOSS: 1.0816731452941895
LOSS: 1.0802322626113892
LOSS: 1.0788973569869995
LOSS: 1.077663540840149
LOSS: 1.0765266418457031
LOSS: 1.075482726097107
LOSS: 1.0745280981063843
LOSS: 1.0736585855484009
LOSS: 1.0728694200515747
LOSS: 1.0721564292907715
LOSS: 1.0715136528015137
LOSS: 1.0709372758865356
LOSS: 1.0704206228256226
LOSS: 1.0699595212936401
LOSS: 1.0695494413375854
LOSS: 1.0691848993301392
LOSS: 1.0688632726669312
LOSS: 1.0685802698135376
LOSS: 1.0683329105377197
LOSS: 1.0681180953979492
LOSS: 1.0679328441619873
LOSS: 1.0677748918533325
LOSS: 1.0676409006118774
LOSS: 1.0675287246704102
LOSS: 1.0674362182617188
LOSS: 1.0673598051071167
LOSS: 1.0672976970672607
LOSS: 1.0672478675842285
LOSS: 1.0672085285186768
LOSS: 1.0671778917312622
LOSS: 1.0671552419662476
LOSS: 1.067138671875
LOSS: 1.0671275854110718
LOSS: 1.0671203136444092
LOSS: 1.0671168565750122
LOSS: 1.0671168565750122
LOSS: 1.067118525505066
LOSS: 1.0671218633651733
LOSS: 1.0671260356903076
LOSS: 1.0671309232711792
LOSS: 1.0671356916427612
LOSS: 1.067140817642212
LOSS: 1.0671449899673462
LOSS: 1.067149043083191
LOSS: 1.067152738571167
LOSS: 1.0671557188034058
LOSS: 1.0671579837799072
LOSS: 1.0671597719192505
LOSS: 1.0671610832214355
LOSS: 1.0671616792678833
LOSS: 1.0671619176864624
LOSS: 1.067162036895752
LOSS: 1.0671610832214355
LOSS: 1.06715989112854
LOSS: 1.0671577453613281
LOSS: 1.067156195640564
LOSS: 1.067153811454773
LOSS: 1.0671515464782715
LOSS: 1.0671488046646118
LOSS: 1.0671464204788208
LOSS: 1.0671441555023193
LOSS: 1.0671412944793701
LOSS: 1.0671395063400269
LOSS: 1.067136526107788
LOSS: 1.0671347379684448
LOSS: 1.067132830619812
LOSS: 1.067130446434021
LOSS: 1.0671284198760986
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/RF_predictions.csv
Average scores:	 r: 0.68±0.05	 r2: 0.44±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/RF_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/RF_predictions.csv
n numeric features: 8
Average scores:	 r: 0.68±0.06	 r2: 0.45±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/RF_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/RF_predictions.csv
n numeric features: 5
Average scores:	 r: 0.6±0.06	 r2: 0.32±0.1
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/RF_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/RF_predictions.csv
Average scores:	 r: 0.76±0.05	 r2: 0.57±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/XGB_predictions.csv
Average scores:	 r: 0.75±0.05	 r2: 0.55±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/XGB_predictions.csv
Average scores:	 r: 0.6±0.08	 r2: 0.31±0.21
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/XGB_predictions.csv
Average scores:	 r: 0.66±0.05	 r2: 0.41±0.06
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/XGB_predictions.csv
Average scores:	 r: 0.65±0.06	 r2: 0.39±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/XGB_predictions.csv
n numeric features: 8
Average scores:	 r: 0.66±0.06	 r2: 0.4±0.1
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/XGB_predictions.csv
n numeric features: 5
Average scores:	 r: 0.57±0.06	 r2: 0.22±0.14
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/XGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/XGB_predictions.csv
Average scores:	 r: 0.76±0.05	 r2: 0.57±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/HGB_predictions.csv
Average scores:	 r: 0.78±0.05	 r2: 0.6±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/HGB_predictions.csv
Average scores:	 r: 0.47±0.07	 r2: 0.21±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/HGB_predictions.csv
Average scores:	 r: 0.68±0.05	 r2: 0.44±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/HGB_predictions.csv
Average scores:	 r: 0.67±0.05	 r2: 0.43±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/HGB_predictions.csv
n numeric features: 8
Average scores:	 r: 0.69±0.05	 r2: 0.46±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/HGB_predictions.csv
n numeric features: 11
Average scores:	 r: 0.61±0.05	 r2: 0.35±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/HGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/HGB_predictions.csv
Average scores:	 r: 0.77±0.05	 r2: 0.57±0.06
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/NGB_predictions.csv
Average scores:	 r: 0.76±0.05	 r2: 0.57±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/NGB_predictions.csv
Average scores:	 r: 0.56±0.07	 r2: 0.29±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/NGB_predictions.csv
Average scores:	 r: 0.66±0.06	 r2: 0.42±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/NGB_predictions.csv
Average scores:	 r: 0.66±0.05	 r2: 0.42±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/NGB_predictions.csv
n numeric features: 8
Average scores:	 r: 0.67±0.06	 r2: 0.44±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/NGB_predictions.csv
n numeric features: 5
Average scores:	 r: 0.61±0.05	 r2: 0.35±0.05
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/NGB_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/NGB_predictions.csv
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
LOSS: 1.1990872621536255
LOSS: 1.191591501235962
LOSS: 1.184422492980957
LOSS: 1.177604079246521
LOSS: 1.1711376905441284
LOSS: 1.1649792194366455
LOSS: 1.1590434312820435
LOSS: 1.1532496213912964
LOSS: 1.1475586891174316
LOSS: 1.141964316368103
LOSS: 1.136474847793579
LOSS: 1.1311028003692627
LOSS: 1.1258604526519775
LOSS: 1.12075936794281
LOSS: 1.1158093214035034
LOSS: 1.111019253730774
LOSS: 1.1063963174819946
LOSS: 1.1019474267959595
LOSS: 1.0976771116256714
LOSS: 1.0935897827148438
LOSS: 1.089687466621399
LOSS: 1.0859713554382324
LOSS: 1.082440733909607
LOSS: 1.0790939331054688
LOSS: 1.0759276151657104
LOSS: 1.07293701171875
LOSS: 1.0701173543930054
LOSS: 1.0674632787704468
LOSS: 1.0649689435958862
LOSS: 1.0626293420791626
LOSS: 1.0604398250579834
LOSS: 1.0583959817886353
LOSS: 1.0564948320388794
LOSS: 1.0547322034835815
LOSS: 1.0531044006347656
LOSS: 1.0516078472137451
LOSS: 1.050237774848938
LOSS: 1.0489888191223145
LOSS: 1.0478547811508179
LOSS: 1.046829104423523
LOSS: 1.0459049940109253
LOSS: 1.045075535774231
LOSS: 1.044333815574646
LOSS: 1.0436736345291138
LOSS: 1.0430891513824463
LOSS: 1.0425747632980347
LOSS: 1.0421252250671387
LOSS: 1.0417360067367554
LOSS: 1.0414021015167236
LOSS: 1.0411182641983032
LOSS: 1.0408798456192017
LOSS: 1.0406819581985474
LOSS: 1.0405198335647583
LOSS: 1.040388584136963
LOSS: 1.0402847528457642
LOSS: 1.04020357131958
LOSS: 1.0401421785354614
LOSS: 1.0400975942611694
LOSS: 1.0400670766830444
LOSS: 1.040048360824585
LOSS: 1.0400395393371582
LOSS: 1.0400372743606567
LOSS: 1.0400416851043701
LOSS: 1.0400502681732178
LOSS: 1.0400617122650146
LOSS: 1.0400739908218384
LOSS: 1.0400872230529785
LOSS: 1.04010009765625
LOSS: 1.0401121377944946
LOSS: 1.0401232242584229
LOSS: 1.0401325225830078
LOSS: 1.0401402711868286
LOSS: 1.0401458740234375
LOSS: 1.0401506423950195
LOSS: 1.0401535034179688
LOSS: 1.0401543378829956
LOSS: 1.0401538610458374
LOSS: 1.040151834487915
LOSS: 1.0401480197906494
LOSS: 1.0401440858840942
LOSS: 1.0401384830474854
LOSS: 1.0401325225830078
LOSS: 1.0401257276535034
LOSS: 1.0401191711425781
LOSS: 1.0401124954223633
LOSS: 1.0401057004928589
LOSS: 1.0400985479354858
LOSS: 1.040091872215271
LOSS: 1.0400854349136353
LOSS: 1.0400795936584473
LOSS: 1.0400736331939697
LOSS: 1.0400681495666504
LOSS: 1.0400636196136475
LOSS: 1.0400593280792236
LOSS: 1.0400549173355103
LOSS: 1.040051817893982
LOSS: 1.0400488376617432
LOSS: 1.0400468111038208
LOSS: 1.0400444269180298
LOSS: 1.0400426387786865
LOSS: 1.2120500802993774
LOSS: 1.2049823999404907
LOSS: 1.1982630491256714
LOSS: 1.1919211149215698
LOSS: 1.1859705448150635
LOSS: 1.1803929805755615
LOSS: 1.1751267910003662
LOSS: 1.1700870990753174
LOSS: 1.1652071475982666
LOSS: 1.1604565382003784
LOSS: 1.155829668045044
LOSS: 1.1513323783874512
LOSS: 1.1469733715057373
LOSS: 1.1427618265151978
LOSS: 1.138706088066101
LOSS: 1.1348130702972412
LOSS: 1.1310888528823853
LOSS: 1.1275384426116943
LOSS: 1.124165415763855
LOSS: 1.120972752571106
LOSS: 1.1179622411727905
LOSS: 1.1151347160339355
LOSS: 1.1124894618988037
LOSS: 1.1100245714187622
LOSS: 1.1077368259429932
LOSS: 1.1056219339370728
LOSS: 1.1036735773086548
LOSS: 1.1018848419189453
LOSS: 1.1002482175827026
LOSS: 1.0987552404403687
LOSS: 1.0973973274230957
LOSS: 1.0961661338806152
LOSS: 1.0950535535812378
LOSS: 1.0940521955490112
LOSS: 1.0931546688079834
LOSS: 1.0923548936843872
LOSS: 1.0916467905044556
LOSS: 1.0910241603851318
LOSS: 1.0904821157455444
LOSS: 1.0900144577026367
LOSS: 1.089614987373352
LOSS: 1.089277982711792
LOSS: 1.0889971256256104
LOSS: 1.088766098022461
LOSS: 1.088578224182129
LOSS: 1.0884283781051636
LOSS: 1.088310956954956
LOSS: 1.0882205963134766
LOSS: 1.0881532430648804
LOSS: 1.088105320930481
LOSS: 1.088072657585144
LOSS: 1.0880537033081055
LOSS: 1.0880458354949951
LOSS: 1.0880463123321533
LOSS: 1.0880534648895264
LOSS: 1.088065266609192
LOSS: 1.088080644607544
LOSS: 1.0880978107452393
LOSS: 1.0881152153015137
LOSS: 1.0881325006484985
LOSS: 1.0881476402282715
LOSS: 1.088161826133728
LOSS: 1.0881729125976562
LOSS: 1.0881816148757935
LOSS: 1.0881880521774292
LOSS: 1.0881918668746948
LOSS: 1.0881941318511963
LOSS: 1.0881937742233276
LOSS: 1.0881917476654053
LOSS: 1.088188648223877
LOSS: 1.088183045387268
LOSS: 1.0881770849227905
LOSS: 1.088170051574707
LOSS: 1.0881619453430176
LOSS: 1.08815336227417
LOSS: 1.0881441831588745
LOSS: 1.0881348848342896
LOSS: 1.0881255865097046
LOSS: 1.0881167650222778
LOSS: 1.0881075859069824
LOSS: 1.088099479675293
LOSS: 1.0880917310714722
LOSS: 1.0880846977233887
LOSS: 1.0880781412124634
LOSS: 1.088072419166565
LOSS: 1.0880670547485352
LOSS: 1.0880624055862427
LOSS: 1.088058352470398
LOSS: 1.0880546569824219
LOSS: 1.0880519151687622
LOSS: 1.088049292564392
LOSS: 1.0880476236343384
LOSS: 1.0880457162857056
LOSS: 1.0880446434020996
LOSS: 1.0880438089370728
LOSS: 1.088043451309204
LOSS: 1.088042974472046
LOSS: 1.0880426168441772
LOSS: 1.088042974472046
LOSS: 1.0880428552627563
LOSS: 1.1990872621536255
LOSS: 1.191591501235962
LOSS: 1.184422492980957
LOSS: 1.177604079246521
LOSS: 1.1711376905441284
LOSS: 1.1649792194366455
LOSS: 1.1590434312820435
LOSS: 1.1532496213912964
LOSS: 1.1475586891174316
LOSS: 1.141964316368103
LOSS: 1.136474847793579
LOSS: 1.1311028003692627
LOSS: 1.1258604526519775
LOSS: 1.12075936794281
LOSS: 1.1158093214035034
LOSS: 1.111019253730774
LOSS: 1.1063963174819946
LOSS: 1.1019474267959595
LOSS: 1.0976771116256714
LOSS: 1.0935897827148438
LOSS: 1.089687466621399
LOSS: 1.0859713554382324
LOSS: 1.082440733909607
LOSS: 1.0790939331054688
LOSS: 1.0759276151657104
LOSS: 1.07293701171875
LOSS: 1.0701173543930054
LOSS: 1.0674632787704468
LOSS: 1.0649689435958862
LOSS: 1.0626293420791626
LOSS: 1.0604398250579834
LOSS: 1.0583959817886353
LOSS: 1.0564948320388794
LOSS: 1.0547322034835815
LOSS: 1.0531044006347656
LOSS: 1.0516078472137451
LOSS: 1.050237774848938
LOSS: 1.0489888191223145
LOSS: 1.0478547811508179
LOSS: 1.046829104423523
LOSS: 1.0459049940109253
LOSS: 1.045075535774231
LOSS: 1.044333815574646
LOSS: 1.0436736345291138
LOSS: 1.0430891513824463
LOSS: 1.0425747632980347
LOSS: 1.0421252250671387
LOSS: 1.0417360067367554
LOSS: 1.0414021015167236
LOSS: 1.0411182641983032
LOSS: 1.0408798456192017
LOSS: 1.0406819581985474
LOSS: 1.0405198335647583
LOSS: 1.040388584136963
LOSS: 1.0402847528457642
LOSS: 1.04020357131958
LOSS: 1.0401421785354614
LOSS: 1.0400975942611694
LOSS: 1.0400670766830444
LOSS: 1.040048360824585
LOSS: 1.0400395393371582
LOSS: 1.0400372743606567
LOSS: 1.0400416851043701
LOSS: 1.0400502681732178
LOSS: 1.0400617122650146
LOSS: 1.0400739908218384
LOSS: 1.0400872230529785
LOSS: 1.04010009765625
LOSS: 1.0401121377944946
LOSS: 1.0401232242584229
LOSS: 1.0401325225830078
LOSS: 1.0401402711868286
LOSS: 1.0401458740234375
LOSS: 1.0401506423950195
LOSS: 1.0401535034179688
LOSS: 1.0401543378829956
LOSS: 1.0401538610458374
LOSS: 1.040151834487915
LOSS: 1.0401480197906494
LOSS: 1.0401440858840942
LOSS: 1.0401384830474854
LOSS: 1.0401325225830078
LOSS: 1.0401257276535034
LOSS: 1.0401191711425781
LOSS: 1.0401124954223633
LOSS: 1.0401057004928589
LOSS: 1.0400985479354858
LOSS: 1.040091872215271
LOSS: 1.0400854349136353
LOSS: 1.0400795936584473
LOSS: 1.0400736331939697
LOSS: 1.0400681495666504
LOSS: 1.0400636196136475
LOSS: 1.0400593280792236
LOSS: 1.0400549173355103
LOSS: 1.040051817893982
LOSS: 1.0400488376617432
LOSS: 1.0400468111038208
LOSS: 1.0400444269180298
LOSS: 1.0400426387786865
LOSS: 1.211442470550537
LOSS: 1.2044309377670288
LOSS: 1.1977651119232178
LOSS: 1.191425085067749
LOSS: 1.1852997541427612
LOSS: 1.1792793273925781
LOSS: 1.1733351945877075
LOSS: 1.1674765348434448
LOSS: 1.1617186069488525
LOSS: 1.1560755968093872
LOSS: 1.1505602598190308
LOSS: 1.1451830863952637
LOSS: 1.1399532556533813
LOSS: 1.1348789930343628
LOSS: 1.1299667358398438
LOSS: 1.1252226829528809
LOSS: 1.1206507682800293
LOSS: 1.1162538528442383
LOSS: 1.1120336055755615
LOSS: 1.1079905033111572
LOSS: 1.1041229963302612
LOSS: 1.1004290580749512
LOSS: 1.096906304359436
LOSS: 1.0935521125793457
LOSS: 1.0903639793395996
LOSS: 1.087340235710144
LOSS: 1.0844788551330566
LOSS: 1.081778645515442
LOSS: 1.079237699508667
LOSS: 1.0768543481826782
LOSS: 1.07462477684021
LOSS: 1.2084391117095947
LOSS: 1.2012025117874146
LOSS: 1.194305658340454
LOSS: 1.1877774000167847
LOSS: 1.181631088256836
LOSS: 1.175848364830017
LOSS: 1.1703683137893677
LOSS: 1.1651066541671753
LOSS: 1.1599963903427124
LOSS: 1.1550074815750122
LOSS: 1.1501344442367554
LOSS: 1.145383596420288
LOSS: 1.1407636404037476
LOSS: 1.1362839937210083
LOSS: 1.1319527626037598
LOSS: 1.1277766227722168
LOSS: 1.1237609386444092
LOSS: 1.1199101209640503
LOSS: 1.1162270307540894
LOSS: 1.1127142906188965
LOSS: 1.1093729734420776
LOSS: 1.1062028408050537
LOSS: 1.1032036542892456
LOSS: 1.1003729104995728
LOSS: 1.0977075099945068
LOSS: 1.095203161239624
LOSS: 1.0928549766540527
LOSS: 1.0906565189361572
LOSS: 1.0886011123657227
LOSS: 1.0866819620132446
LOSS: 1.0848917961120605
LOSS: 1.0832244157791138
LOSS: 1.0816731452941895
LOSS: 1.0802322626113892
LOSS: 1.0788973569869995
LOSS: 1.077663540840149
LOSS: 1.0765266418457031
LOSS: 1.075482726097107
LOSS: 1.0745280981063843
LOSS: 1.0736585855484009
LOSS: 1.0728694200515747
LOSS: 1.0721564292907715
LOSS: 1.0715136528015137
LOSS: 1.0709372758865356
LOSS: 1.0704206228256226
LOSS: 1.0699595212936401
LOSS: 1.0695494413375854
LOSS: 1.0691848993301392
LOSS: 1.0688632726669312
LOSS: 1.0685802698135376
LOSS: 1.0683329105377197
LOSS: 1.0681180953979492
LOSS: 1.0679328441619873
LOSS: 1.0677748918533325
LOSS: 1.0676409006118774
LOSS: 1.0675287246704102
LOSS: 1.0674362182617188
LOSS: 1.0673598051071167
LOSS: 1.0672976970672607
LOSS: 1.0672478675842285
LOSS: 1.0672085285186768
LOSS: 1.0671778917312622
LOSS: 1.0671552419662476
LOSS: 1.067138671875
LOSS: 1.0671275854110718
LOSS: 1.0671203136444092
LOSS: 1.0671168565750122
LOSS: 1.0671168565750122
LOSS: 1.067118525505066
LOSS: 1.0671218633651733
LOSS: 1.0671260356903076
LOSS: 1.0671309232711792
LOSS: 1.0671356916427612
LOSS: 1.067140817642212
LOSS: 1.0671449899673462
LOSS: 1.067149043083191
LOSS: 1.067152738571167
LOSS: 1.0671557188034058
LOSS: 1.0671579837799072
LOSS: 1.0671597719192505
LOSS: 1.0671610832214355
LOSS: 1.0671616792678833
LOSS: 1.0671619176864624
LOSS: 1.067162036895752
LOSS: 1.0671610832214355
LOSS: 1.06715989112854
LOSS: 1.0671577453613281
LOSS: 1.067156195640564
LOSS: 1.067153811454773
LOSS: 1.0671515464782715
LOSS: 1.0671488046646118
LOSS: 1.0671464204788208
LOSS: 1.0671441555023193
LOSS: 1.0671412944793701
LOSS: 1.0671395063400269
LOSS: 1.067136526107788
LOSS: 1.0671347379684448
LOSS: 1.067132830619812
LOSS: 1.067130446434021
LOSS: 1.0671284198760986
LOSS: 1.2049267292022705
LOSS: 1.197614073753357
LOSS: 1.190637469291687
LOSS: 1.1840262413024902
LOSS: 1.1777962446212769
LOSS: 1.1719359159469604
LOSS: 1.1663928031921387
LOSS: 1.1610864400863647
LOSS: 1.155945897102356
LOSS: 1.1509337425231934
LOSS: 1.146039605140686
LOSS: 1.141266942024231
LOSS: 1.1366236209869385
LOSS: 1.132118582725525
LOSS: 1.1277596950531006
LOSS: 1.1235533952713013
LOSS: 1.1195052862167358
LOSS: 1.1156198978424072
LOSS: 1.1119006872177124
LOSS: 1.1083496809005737
LOSS: 1.1049691438674927
LOSS: 1.101759433746338
LOSS: 1.0987204313278198
LOSS: 1.0958508253097534
LOSS: 1.0931484699249268
LOSS: 1.0906097888946533
LOSS: 1.0882306098937988
LOSS: 1.0860055685043335
LOSS: 1.0839285850524902
LOSS: 1.0819929838180542
LOSS: 1.0801920890808105
LOSS: 1.0785187482833862
LOSS: 1.0769670009613037
LOSS: 1.075529932975769
LOSS: 1.074202299118042
LOSS: 1.07297945022583
LOSS: 1.0718564987182617
LOSS: 1.070829153060913
LOSS: 1.0698928833007812
LOSS: 1.069044828414917
LOSS: 1.0682790279388428
LOSS: 1.0675923824310303
LOSS: 1.0669790506362915
LOSS: 1.066434383392334
LOSS: 1.0659527778625488
LOSS: 1.0655287504196167
LOSS: 1.0651570558547974
LOSS: 1.0648332834243774
LOSS: 1.0645524263381958
LOSS: 1.064310073852539
LOSS: 1.0641027688980103
LOSS: 1.0639272928237915
LOSS: 1.0637799501419067
LOSS: 1.0636581182479858
LOSS: 1.06355881690979
LOSS: 1.063480019569397
LOSS: 1.0634180307388306
LOSS: 1.063371181488037
LOSS: 1.063336730003357
LOSS: 1.0633124113082886
LOSS: 1.0632973909378052
LOSS: 1.0632882118225098
LOSS: 1.0632845163345337
LOSS: 1.0632851123809814
LOSS: 1.0632879734039307
LOSS: 1.0632929801940918
LOSS: 1.0633001327514648
LOSS: 1.063307523727417
LOSS: 1.063315510749817
LOSS: 1.0633232593536377
LOSS: 1.0633314847946167
LOSS: 1.0633379220962524
LOSS: 1.0633442401885986
LOSS: 1.0633490085601807
LOSS: 1.0633525848388672
LOSS: 1.0633553266525269
LOSS: 1.063356876373291
LOSS: 1.0633572340011597
LOSS: 1.063356876373291
LOSS: 1.0633550882339478
LOSS: 1.063353180885315
LOSS: 1.0633502006530762
LOSS: 1.063347339630127
LOSS: 1.0633440017700195
LOSS: 1.0633398294448853
LOSS: 1.0633354187011719
LOSS: 1.0633313655853271
LOSS: 1.0633270740509033
LOSS: 1.0633230209350586
LOSS: 1.0633184909820557
LOSS: 1.06331467628479
LOSS: 1.0633105039596558
LOSS: 1.0633070468902588
LOSS: 1.0633037090301514
LOSS: 1.063300609588623
LOSS: 1.0632975101470947
LOSS: 1.0632951259613037
LOSS: 1.0632926225662231
LOSS: 1.0632908344268799
LOSS: 1.0632895231246948
LOSS: 1.2085517644882202
LOSS: 1.201398253440857
LOSS: 1.194588541984558
LOSS: 1.1881513595581055
LOSS: 1.1820976734161377
LOSS: 1.1764049530029297
LOSS: 1.1710071563720703
LOSS: 1.165819764137268
LOSS: 1.1607834100723267
LOSS: 1.1558750867843628
LOSS: 1.151093602180481
LOSS: 1.1464471817016602
LOSS: 1.14194655418396
LOSS: 1.1376008987426758
LOSS: 1.1334187984466553
LOSS: 1.1294070482254028
LOSS: 1.1255710124969482
LOSS: 1.1219148635864258
LOSS: 1.1184415817260742
LOSS: 1.1151525974273682
LOSS: 1.1120485067367554
LOSS: 1.1091282367706299
LOSS: 1.1063898801803589
LOSS: 1.103830337524414
LOSS: 1.101445198059082
LOSS: 1.099228858947754
LOSS: 1.0971750020980835
LOSS: 1.095276117324829
LOSS: 1.0935245752334595
LOSS: 1.091913104057312
LOSS: 1.0904324054718018
LOSS: 1.0890756845474243
LOSS: 1.0878351926803589
LOSS: 1.0867043733596802
LOSS: 1.0856764316558838
LOSS: 1.0847458839416504
LOSS: 1.0839065313339233
LOSS: 1.0831536054611206
LOSS: 1.0824811458587646
LOSS: 1.0818841457366943
LOSS: 1.0813571214675903
LOSS: 1.0808939933776855
LOSS: 1.0804896354675293
LOSS: 1.0801390409469604
LOSS: 1.0798358917236328
LOSS: 1.0795754194259644
LOSS: 1.079352855682373
LOSS: 1.0791641473770142
LOSS: 1.0790053606033325
LOSS: 1.0788726806640625
LOSS: 1.0787642002105713
LOSS: 1.0786755084991455
LOSS: 1.0786054134368896
LOSS: 1.0785512924194336
LOSS: 1.0785106420516968
LOSS: 1.0784807205200195
LOSS: 1.0784605741500854
LOSS: 1.078447937965393
LOSS: 1.0784419775009155
LOSS: 1.0784401893615723
LOSS: 1.078442096710205
LOSS: 1.0784461498260498
LOSS: 1.0784519910812378
LOSS: 1.0784584283828735
LOSS: 1.0784661769866943
LOSS: 1.0784730911254883
LOSS: 1.0784809589385986
LOSS: 1.0784881114959717
LOSS: 1.0784939527511597
LOSS: 1.0784995555877686
LOSS: 1.0785043239593506
LOSS: 1.0785073041915894
LOSS: 1.078510046005249
LOSS: 1.078511357307434
LOSS: 1.0785119533538818
LOSS: 1.0785114765167236
LOSS: 1.07850980758667
LOSS: 1.078507423400879
LOSS: 1.078505039215088
LOSS: 1.0785024166107178
LOSS: 1.0784987211227417
LOSS: 1.0784952640533447
LOSS: 1.0784913301467896
LOSS: 1.078487515449524
LOSS: 1.0784835815429688
LOSS: 1.0784794092178345
LOSS: 1.0784752368927002
LOSS: 1.0784716606140137
LOSS: 1.078467607498169
LOSS: 1.0784639120101929
LOSS: 1.0784610509872437
LOSS: 1.0784580707550049
LOSS: 1.0784552097320557
LOSS: 1.0784529447555542
LOSS: 1.0784507989883423
LOSS: 1.0784482955932617
LOSS: 1.0784467458724976
LOSS: 1.0784454345703125
LOSS: 1.0784441232681274
LOSS: 1.0784425735473633
LOSS: 1.211442470550537
LOSS: 1.2044309377670288
LOSS: 1.1977651119232178
LOSS: 1.191425085067749
LOSS: 1.1852997541427612
LOSS: 1.1792793273925781
LOSS: 1.1733351945877075
LOSS: 1.1674765348434448
LOSS: 1.1617186069488525
LOSS: 1.1560755968093872
LOSS: 1.1505602598190308
LOSS: 1.1451830863952637
LOSS: 1.1399532556533813
LOSS: 1.1348789930343628
LOSS: 1.1299667358398438
LOSS: 1.1252226829528809
LOSS: 1.1206507682800293
LOSS: 1.1162538528442383
LOSS: 1.1120336055755615
LOSS: 1.1079905033111572
LOSS: 1.1041229963302612
LOSS: 1.1004290580749512
LOSS: 1.096906304359436
LOSS: 1.0935521125793457
LOSS: 1.0903639793395996
LOSS: 1.087340235710144
LOSS: 1.0844788551330566
LOSS: 1.081778645515442
LOSS: 1.079237699508667
LOSS: 1.0768543481826782
LOSS: 1.07462477684021LOSS: 1.214086651802063
LOSS: 1.2071738243103027
LOSS: 1.2006150484085083
LOSS: 1.194420337677002
LOSS: 1.1885409355163574
LOSS: 1.1828666925430298
LOSS: 1.1773097515106201
LOSS: 1.1718426942825317
LOSS: 1.1664680242538452
LOSS: 1.1611964702606201
LOSS: 1.1560381650924683
LOSS: 1.1510025262832642
LOSS: 1.1460970640182495
LOSS: 1.1413283348083496
LOSS: 1.1367017030715942
LOSS: 1.1322214603424072
LOSS: 1.1278916597366333
LOSS: 1.123714804649353
LOSS: 1.1196937561035156
LOSS: 1.1158294677734375
LOSS: 1.1121225357055664
LOSS: 1.1085729598999023
LOSS: 1.1051790714263916
LOSS: 1.1019388437271118
LOSS: 1.0988494157791138
LOSS: 1.0959067344665527
LOSS: 1.0931073427200317
LOSS: 1.0904464721679688
LOSS: 1.0879194736480713
LOSS: 1.0855231285095215
LOSS: 1.0832535028457642
LOSS: 1.0811066627502441
LOSS: 1.07908034324646
LOSS: 1.0771712064743042
LOSS: 1.0753763914108276
LOSS: 1.073692798614502
LOSS: 1.0721169710159302
LOSS: 1.0706448554992676
LOSS: 1.0692731142044067
LOSS: 1.0679959058761597
LOSS: 1.0668092966079712
LOSS: 1.0657086372375488
LOSS: 1.064689040184021
LOSS: 1.0637463331222534
LOSS: 1.0628767013549805
LOSS: 1.0620760917663574
LOSS: 1.061340570449829
LOSS: 1.0606681108474731
LOSS: 1.0600531101226807
LOSS: 1.0594936609268188
LOSS: 1.058985948562622
LOSS: 1.0585267543792725
LOSS: 1.0581119060516357
LOSS: 1.0577383041381836
LOSS: 1.057403564453125
LOSS: 1.0571038722991943
LOSS: 1.0568362474441528
LOSS: 1.0565986633300781
LOSS: 1.056388258934021
LOSS: 1.0562036037445068
LOSS: 1.0560424327850342
LOSS: 1.0559011697769165
LOSS: 1.0557794570922852
LOSS: 1.0556753873825073
LOSS: 1.0555857419967651
LOSS: 1.0555094480514526
LOSS: 1.0554455518722534
LOSS: 1.0553921461105347
LOSS: 1.0553479194641113
LOSS: 1.0553122758865356
LOSS: 1.0552831888198853
LOSS: 1.055260181427002
LOSS: 1.055242896080017
LOSS: 1.0552293062210083
LOSS: 1.0552197694778442
LOSS: 1.0552133321762085
LOSS: 1.0552088022232056
LOSS: 1.0552071332931519
LOSS: 1.0552061796188354
LOSS: 1.0552061796188354
LOSS: 1.0552079677581787
LOSS: 1.055209755897522
LOSS: 1.055212140083313
LOSS: 1.0552141666412354
LOSS: 1.0552172660827637
LOSS: 1.0552195310592651
LOSS: 1.0552213191986084
LOSS: 1.0552237033843994
LOSS: 1.0552254915237427
LOSS: 1.0552273988723755
LOSS: 1.0552282333374023
LOSS: 1.0552290678024292
LOSS: 1.0552295446395874
LOSS: 1.0552294254302979
LOSS: 1.0552293062210083
LOSS: 1.0552294254302979
LOSS: 1.055228590965271
LOSS: 1.055228352546692
LOSS: 1.0552269220352173
LOSS: 1.0552263259887695
LOSS: 1.2121368646621704
LOSS: 1.205133080482483
LOSS: 1.1984808444976807
LOSS: 1.1922087669372559
LOSS: 1.1863291263580322
LOSS: 1.180820107460022
LOSS: 1.175614595413208
LOSS: 1.170625925064087
LOSS: 1.16579008102417
LOSS: 1.161078929901123
LOSS: 1.1564875841140747
LOSS: 1.152020812034607
LOSS: 1.1476848125457764
LOSS: 1.1434863805770874
LOSS: 1.1394309997558594
LOSS: 1.135522723197937
LOSS: 1.1317648887634277
LOSS: 1.1281605958938599
LOSS: 1.1247111558914185
LOSS: 1.1214183568954468
LOSS: 1.118282675743103
LOSS: 1.1153048276901245
LOSS: 1.1124837398529053
LOSS: 1.109818935394287
LOSS: 1.1073081493377686
LOSS: 1.1049484014511108
LOSS: 1.102736234664917
LOSS: 1.1006675958633423
LOSS: 1.0987361669540405
LOSS: 1.0969367027282715
LOSS: 1.095262885093689
LOSS: 1.0937080383300781
LOSS: 1.0922659635543823
LOSS: 1.0909305810928345
LOSS: 1.089695692062378
LOSS: 1.0885564088821411
LOSS: 1.087507963180542
LOSS: 1.0865463018417358
LOSS: 1.0856672525405884
LOSS: 1.0848668813705444
LOSS: 1.0841418504714966
LOSS: 1.0834881067276
LOSS: 1.0829015970230103
LOSS: 1.0823782682418823
LOSS: 1.0819138288497925
LOSS: 1.0815032720565796
LOSS: 1.0811421871185303
LOSS: 1.0808255672454834
LOSS: 1.080549716949463
LOSS: 1.0803104639053345
LOSS: 1.080104112625122
LOSS: 1.0799263715744019
LOSS: 1.0797760486602783
LOSS: 1.0796490907669067
LOSS: 1.0795434713363647
LOSS: 1.0794568061828613
LOSS: 1.0793862342834473
LOSS: 1.0793312788009644
LOSS: 1.0792887210845947
LOSS: 1.0792570114135742
LOSS: 1.0792343616485596
LOSS: 1.0792193412780762
LOSS: 1.0792096853256226
LOSS: 1.079204797744751
LOSS: 1.0792031288146973
LOSS: 1.0792043209075928
LOSS: 1.0792075395584106
LOSS: 1.0792123079299927
LOSS: 1.079217553138733
LOSS: 1.079223394393921
LOSS: 1.0792292356491089
LOSS: 1.0792351961135864
LOSS: 1.0792409181594849
LOSS: 1.0792458057403564
LOSS: 1.0792502164840698
LOSS: 1.0792540311813354
LOSS: 1.0792567729949951
LOSS: 1.079258680343628
LOSS: 1.0792593955993652
LOSS: 1.0792597532272339
LOSS: 1.079258918762207
LOSS: 1.0792574882507324
LOSS: 1.0792558193206787
LOSS: 1.0792537927627563
LOSS: 1.0792510509490967
LOSS: 1.0792487859725952
LOSS: 1.0792450904846191
LOSS: 1.079242467880249
LOSS: 1.0792391300201416
LOSS: 1.0792356729507446
LOSS: 1.0792322158813477
LOSS: 1.0792295932769775
LOSS: 1.0792261362075806
LOSS: 1.0792232751846313
LOSS: 1.0792206525802612
LOSS: 1.0792181491851807
LOSS: 1.0792160034179688
LOSS: 1.079214096069336
LOSS: 1.079211950302124
LOSS: 1.0792096853256226
LOSS: 1.2005735635757446
LOSS: 1.1931188106536865
LOSS: 1.1859928369522095
LOSS: 1.1792244911193848
LOSS: 1.1728283166885376
LOSS: 1.1667897701263428
LOSS: 1.16105318069458
LOSS: 1.15553617477417
LOSS: 1.1501697301864624
LOSS: 1.14491868019104
LOSS: 1.1397747993469238
LOSS: 1.1347423791885376
LOSS: 1.1298303604125977
LOSS: 1.1250483989715576
LOSS: 1.120404839515686
LOSS: 1.115907073020935
LOSS: 1.111561894416809
LOSS: 1.107374668121338
LOSS: 1.1033499240875244
LOSS: 1.0994911193847656
LOSS: 1.0958012342453003
LOSS: 1.0922820568084717
LOSS: 1.0889339447021484
LOSS: 1.0857571363449097
LOSS: 1.0827497243881226
LOSS: 1.079908847808838
LOSS: 1.077230453491211
LOSS: 1.0747100114822388
LOSS: 1.0723421573638916
LOSS: 1.0701199769973755
LOSS: 1.068037986755371
LOSS: 1.066089391708374
LOSS: 1.0642691850662231
LOSS: 1.0625715255737305
LOSS: 1.0609921216964722
LOSS: 1.0595262050628662
LOSS: 1.0581711530685425
LOSS: 1.0569227933883667
LOSS: 1.0557774305343628
LOSS: 1.0547311305999756
LOSS: 1.0537797212600708
LOSS: 1.052918553352356
LOSS: 1.0521422624588013
LOSS: 1.051445484161377
LOSS: 1.0508224964141846
LOSS: 1.0502671003341675
LOSS: 1.0497750043869019
LOSS: 1.0493401288986206
LOSS: 1.0489578247070312
LOSS: 1.048624038696289
LOSS: 1.0483343601226807
LOSS: 1.0480854511260986
LOSS: 1.0478737354278564
LOSS: 1.047695517539978
LOSS: 1.047547698020935
LOSS: 1.0474261045455933
LOSS: 1.0473288297653198
LOSS: 1.04725182056427
LOSS: 1.047191858291626
LOSS: 1.0471469163894653
LOSS: 1.0471142530441284
LOSS: 1.0470914840698242
LOSS: 1.0470774173736572
LOSS: 1.047069787979126
LOSS: 1.047067403793335
LOSS: 1.0470691919326782
LOSS: 1.0470744371414185
LOSS: 1.0470818281173706
LOSS: 1.0470902919769287
LOSS: 1.0470997095108032
LOSS: 1.0471094846725464
LOSS: 1.0471181869506836
LOSS: 1.0471265316009521
LOSS: 1.0471335649490356
LOSS: 1.0471396446228027
LOSS: 1.0471439361572266
LOSS: 1.047147274017334
LOSS: 1.0471489429473877
LOSS: 1.047149658203125
LOSS: 1.0471495389938354
LOSS: 1.0471481084823608
LOSS: 1.0471463203430176
LOSS: 1.0471434593200684
LOSS: 1.0471400022506714
LOSS: 1.0471361875534058
LOSS: 1.0471322536468506
LOSS: 1.0471271276474
LOSS: 1.0471223592758179
LOSS: 1.0471171140670776
LOSS: 1.0471128225326538
LOSS: 1.047107458114624
LOSS: 1.047102689743042
LOSS: 1.0470978021621704
LOSS: 1.047094464302063
LOSS: 1.0470902919769287
LOSS: 1.0470867156982422
LOSS: 1.0470832586288452
LOSS: 1.0470808744430542
LOSS: 1.0470783710479736
LOSS: 1.0470759868621826
LOSS: 1.203023076057434
LOSS: 1.1957604885101318
LOSS: 1.1888362169265747
LOSS: 1.1822717189788818
LOSS: 1.1760585308074951
LOSS: 1.170134425163269
LOSS: 1.1644059419631958
LOSS: 1.1588060855865479
LOSS: 1.1533128023147583
LOSS: 1.1479294300079346
LOSS: 1.1426671743392944
LOSS: 1.137538194656372
LOSS: 1.132553219795227
LOSS: 1.1277217864990234
LOSS: 1.123051643371582
LOSS: 1.11854887008667
LOSS: 1.114219069480896
LOSS: 1.1100659370422363
LOSS: 1.1060925722122192
LOSS: 1.1023005247116089
LOSS: 1.0986905097961426
LOSS: 1.0952616930007935
LOSS: 1.0920124053955078
LOSS: 1.088940143585205
LOSS: 1.086040735244751
LOSS: 1.083309292793274
LOSS: 1.080741286277771
LOSS: 1.0783302783966064
LOSS: 1.07607102394104
LOSS: 1.0739576816558838
LOSS: 1.071984887123108
LOSS: 1.202947735786438
LOSS: 1.1956803798675537
LOSS: 1.188751220703125
LOSS: 1.1821917295455933
LOSS: 1.1760247945785522
LOSS: 1.170253872871399
LOSS: 1.1648510694503784
LOSS: 1.159754753112793
LOSS: 1.1548911333084106
LOSS: 1.150204062461853
LOSS: 1.1456656455993652
LOSS: 1.1412687301635742
LOSS: 1.137015700340271
LOSS: 1.132912516593933
LOSS: 1.1289653778076172
LOSS: 1.1251795291900635
LOSS: 1.1215587854385376
LOSS: 1.1181062459945679
LOSS: 1.1148234605789185
LOSS: 1.1117113828659058
LOSS: 1.1087698936462402
LOSS: 1.1059986352920532
LOSS: 1.1033955812454224
LOSS: 1.1009588241577148
LOSS: 1.0986851453781128
LOSS: 1.0965710878372192
LOSS: 1.0946115255355835
LOSS: 1.0928014516830444
LOSS: 1.0911346673965454
LOSS: 1.0896035432815552
LOSS: 1.0882015228271484
LOSS: 1.086920142173767
LOSS: 1.085752248764038
LOSS: 1.0846898555755615
LOSS: 1.083725094795227
LOSS: 1.0828512907028198
LOSS: 1.0820621252059937
LOSS: 1.081351399421692
LOSS: 1.0807135105133057
LOSS: 1.0801441669464111
LOSS: 1.0796388387680054
LOSS: 1.0791925191879272
LOSS: 1.0788018703460693
LOSS: 1.0784622430801392
LOSS: 1.0781692266464233
LOSS: 1.077919840812683
LOSS: 1.0777087211608887
LOSS: 1.0775319337844849
LOSS: 1.0773850679397583
LOSS: 1.0772651433944702
LOSS: 1.0771679878234863
LOSS: 1.077089786529541
LOSS: 1.0770288705825806
LOSS: 1.0769819021224976
LOSS: 1.0769466161727905
LOSS: 1.076921820640564
LOSS: 1.0769054889678955
LOSS: 1.0768955945968628
LOSS: 1.076891541481018
LOSS: 1.0768921375274658
LOSS: 1.0768959522247314
LOSS: 1.076902985572815
LOSS: 1.0769108533859253
LOSS: 1.076919436454773
LOSS: 1.0769292116165161
LOSS: 1.0769376754760742
LOSS: 1.0769457817077637
LOSS: 1.0769531726837158
LOSS: 1.0769588947296143
LOSS: 1.0769635438919067
LOSS: 1.0769670009613037
LOSS: 1.0769689083099365
LOSS: 1.0769705772399902
LOSS: 1.0769703388214111
LOSS: 1.076969861984253
LOSS: 1.0769681930541992
LOSS: 1.0769660472869873
LOSS: 1.0769637823104858
LOSS: 1.0769600868225098
LOSS: 1.0769563913345337
LOSS: 1.0769520998001099
LOSS: 1.0769476890563965
LOSS: 1.0769431591033936
LOSS: 1.076938271522522
LOSS: 1.0769340991973877
LOSS: 1.0769293308258057
LOSS: 1.0769249200820923
LOSS: 1.076920509338379
LOSS: 1.0769169330596924
LOSS: 1.0769128799438477
LOSS: 1.0769100189208984
LOSS: 1.0769072771072388
LOSS: 1.076904296875
LOSS: 1.076902151107788
LOSS: 1.076899766921997
LOSS: 1.0768979787826538
LOSS: 1.0768965482711792
LOSS: 1.076894998550415
LOSS: 1.0768941640853882
LOSS: 1.0768928527832031
LOSS: 1.2005735635757446
LOSS: 1.1931188106536865
LOSS: 1.1859928369522095
LOSS: 1.1792244911193848
LOSS: 1.1728283166885376
LOSS: 1.1667897701263428
LOSS: 1.16105318069458
LOSS: 1.15553617477417
LOSS: 1.1501697301864624
LOSS: 1.14491868019104
LOSS: 1.1397747993469238
LOSS: 1.1347423791885376
LOSS: 1.1298303604125977
LOSS: 1.1250483989715576
LOSS: 1.120404839515686
LOSS: 1.115907073020935
LOSS: 1.111561894416809
LOSS: 1.107374668121338
LOSS: 1.1033499240875244
LOSS: 1.0994911193847656
LOSS: 1.0958012342453003
LOSS: 1.0922820568084717
LOSS: 1.0889339447021484
LOSS: 1.0857571363449097
LOSS: 1.0827497243881226
LOSS: 1.079908847808838
LOSS: 1.077230453491211
LOSS: 1.0747100114822388
LOSS: 1.0723421573638916
LOSS: 1.0701199769973755
LOSS: 1.068037986755371
LOSS: 1.066089391708374
LOSS: 1.0642691850662231
LOSS: 1.0625715255737305
LOSS: 1.0609921216964722
LOSS: 1.0595262050628662
LOSS: 1.0581711530685425
LOSS: 1.0569227933883667
LOSS: 1.0557774305343628
LOSS: 1.0547311305999756
LOSS: 1.0537797212600708
LOSS: 1.052918553352356
LOSS: 1.0521422624588013
LOSS: 1.051445484161377
LOSS: 1.0508224964141846
LOSS: 1.0502671003341675
LOSS: 1.0497750043869019
LOSS: 1.0493401288986206
LOSS: 1.0489578247070312
LOSS: 1.048624038696289
LOSS: 1.0483343601226807
LOSS: 1.0480854511260986
LOSS: 1.0478737354278564
LOSS: 1.047695517539978
LOSS: 1.047547698020935
LOSS: 1.0474261045455933
LOSS: 1.0473288297653198
LOSS: 1.04725182056427
LOSS: 1.047191858291626
LOSS: 1.0471469163894653
LOSS: 1.0471142530441284
LOSS: 1.0470914840698242
LOSS: 1.0470774173736572
LOSS: 1.047069787979126
LOSS: 1.047067403793335
LOSS: 1.0470691919326782
LOSS: 1.0470744371414185
LOSS: 1.0470818281173706
LOSS: 1.0470902919769287
LOSS: 1.0470997095108032
LOSS: 1.0471094846725464
LOSS: 1.0471181869506836
LOSS: 1.0471265316009521
LOSS: 1.0471335649490356
LOSS: 1.0471396446228027
LOSS: 1.0471439361572266
LOSS: 1.047147274017334
LOSS: 1.0471489429473877
LOSS: 1.047149658203125
LOSS: 1.0471495389938354
LOSS: 1.0471481084823608
LOSS: 1.0471463203430176
LOSS: 1.0471434593200684
LOSS: 1.0471400022506714
LOSS: 1.0471361875534058
LOSS: 1.0471322536468506
LOSS: 1.0471271276474
LOSS: 1.0471223592758179
LOSS: 1.0471171140670776
LOSS: 1.0471128225326538
LOSS: 1.047107458114624
LOSS: 1.047102689743042
LOSS: 1.0470978021621704
LOSS: 1.047094464302063
LOSS: 1.0470902919769287
LOSS: 1.0470867156982422
LOSS: 1.0470832586288452
LOSS: 1.0470808744430542
LOSS: 1.0470783710479736
LOSS: 1.0470759868621826
LOSS: 1.2049267292022705
LOSS: 1.197614073753357
LOSS: 1.190637469291687
LOSS: 1.1840262413024902
LOSS: 1.1777962446212769
LOSS: 1.1719359159469604
LOSS: 1.1663928031921387
LOSS: 1.1610864400863647
LOSS: 1.155945897102356
LOSS: 1.1509337425231934
LOSS: 1.146039605140686
LOSS: 1.141266942024231
LOSS: 1.1366236209869385
LOSS: 1.132118582725525
LOSS: 1.1277596950531006
LOSS: 1.1235533952713013
LOSS: 1.1195052862167358
LOSS: 1.1156198978424072
LOSS: 1.1119006872177124
LOSS: 1.1083496809005737
LOSS: 1.1049691438674927
LOSS: 1.101759433746338
LOSS: 1.0987204313278198
LOSS: 1.0958508253097534
LOSS: 1.0931484699249268
LOSS: 1.0906097888946533
LOSS: 1.0882306098937988
LOSS: 1.0860055685043335
LOSS: 1.0839285850524902
LOSS: 1.0819929838180542
LOSS: 1.0801920890808105
LOSS: 1.0785187482833862
LOSS: 1.0769670009613037
LOSS: 1.075529932975769
LOSS: 1.074202299118042
LOSS: 1.07297945022583
LOSS: 1.0718564987182617
LOSS: 1.070829153060913
LOSS: 1.0698928833007812
LOSS: 1.069044828414917
LOSS: 1.0682790279388428
LOSS: 1.0675923824310303
LOSS: 1.0669790506362915
LOSS: 1.066434383392334
LOSS: 1.0659527778625488
LOSS: 1.0655287504196167
LOSS: 1.0651570558547974
LOSS: 1.0648332834243774
LOSS: 1.0645524263381958
LOSS: 1.064310073852539
LOSS: 1.0641027688980103
LOSS: 1.0639272928237915
LOSS: 1.0637799501419067
LOSS: 1.0636581182479858
LOSS: 1.06355881690979
LOSS: 1.063480019569397
LOSS: 1.0634180307388306
LOSS: 1.063371181488037
LOSS: 1.063336730003357
LOSS: 1.0633124113082886
LOSS: 1.0632973909378052
LOSS: 1.0632882118225098
LOSS: 1.0632845163345337
LOSS: 1.0632851123809814
LOSS: 1.0632879734039307
LOSS: 1.0632929801940918
LOSS: 1.0633001327514648
LOSS: 1.063307523727417
LOSS: 1.063315510749817
LOSS: 1.0633232593536377
LOSS: 1.0633314847946167
LOSS: 1.0633379220962524
LOSS: 1.0633442401885986
LOSS: 1.0633490085601807
LOSS: 1.0633525848388672
LOSS: 1.0633553266525269
LOSS: 1.063356876373291
LOSS: 1.0633572340011597
LOSS: 1.063356876373291
LOSS: 1.0633550882339478
LOSS: 1.063353180885315
LOSS: 1.0633502006530762
LOSS: 1.063347339630127
LOSS: 1.0633440017700195
LOSS: 1.0633398294448853
LOSS: 1.0633354187011719
LOSS: 1.0633313655853271
LOSS: 1.0633270740509033
LOSS: 1.0633230209350586
LOSS: 1.0633184909820557
LOSS: 1.06331467628479
LOSS: 1.0633105039596558
LOSS: 1.0633070468902588
LOSS: 1.0633037090301514
LOSS: 1.063300609588623
LOSS: 1.0632975101470947
LOSS: 1.0632951259613037
LOSS: 1.0632926225662231
LOSS: 1.0632908344268799
LOSS: 1.0632895231246948
LOSS: 1.2150156497955322
LOSS: 1.2079932689666748
LOSS: 1.2013213634490967
LOSS: 1.19502592086792
LOSS: 1.1891098022460938
LOSS: 1.183531641960144
LOSS: 1.1782042980194092
LOSS: 1.1730402708053589
LOSS: 1.1679919958114624
LOSS: 1.163046956062317
LOSS: 1.1582084894180298
LOSS: 1.1534850597381592
LOSS: 1.14888596534729
LOSS: 1.1444191932678223
LOSS: 1.140092134475708
LOSS: 1.1359108686447144
LOSS: 1.131880760192871
LOSS: 1.1280063390731812
LOSS: 1.1242913007736206
LOSS: 1.1207389831542969
LOSS: 1.1173514127731323
LOSS: 1.114129900932312
LOSS: 1.111074686050415
LOSS: 1.1081844568252563
LOSS: 1.1054571866989136
LOSS: 1.1028892993927002
LOSS: 1.1004761457443237
LOSS: 1.098212718963623
LOSS: 1.0960923433303833
LOSS: 1.094109058380127
LOSS: 1.0922563076019287

LOSS: 1.0725454092025757
LOSS: 1.070610523223877
LOSS: 1.0688148736953735
LOSS: 1.0671523809432983
LOSS: 1.0656170845031738
LOSS: 1.0642035007476807
LOSS: 1.0629063844680786
LOSS: 1.061719536781311
LOSS: 1.0606398582458496
LOSS: 1.05966055393219
LOSS: 1.058777093887329
LOSS: 1.0579832792282104
LOSS: 1.0572731494903564
LOSS: 1.0566412210464478
LOSS: 1.0560805797576904
LOSS: 1.0555862188339233
LOSS: 1.0551525354385376
LOSS: 1.054775357246399
LOSS: 1.0544488430023193
LOSS: 1.0541698932647705
LOSS: 1.0539331436157227
LOSS: 1.0537344217300415
LOSS: 1.0535697937011719
LOSS: 1.0534350872039795
LOSS: 1.05332612991333
LOSS: 1.0532397031784058
LOSS: 1.0531731843948364
LOSS: 1.0531233549118042
LOSS: 1.0530871152877808
LOSS: 1.053062915802002
LOSS: 1.0530482530593872
LOSS: 1.0530411005020142
LOSS: 1.053040862083435
LOSS: 1.0530445575714111
LOSS: 1.0530518293380737
LOSS: 1.05306077003479
LOSS: 1.0530712604522705
LOSS: 1.0530812740325928
LOSS: 1.0530920028686523
LOSS: 1.0531023740768433
LOSS: 1.05311119556427
LOSS: 1.05311918258667
LOSS: 1.053126335144043
LOSS: 1.0531312227249146
LOSS: 1.0531351566314697
LOSS: 1.053137183189392
LOSS: 1.053138017654419
LOSS: 1.0531377792358398
LOSS: 1.0531357526779175
LOSS: 1.053133487701416
LOSS: 1.0531296730041504
LOSS: 1.0531258583068848
LOSS: 1.053120732307434
LOSS: 1.0531154870986938
LOSS: 1.053109884262085
LOSS: 1.0531036853790283
LOSS: 1.0530983209609985
LOSS: 1.0530927181243896
LOSS: 1.0530872344970703
LOSS: 1.0530823469161987
LOSS: 1.0530763864517212
LOSS: 1.053072214126587
LOSS: 1.0530673265457153
LOSS: 1.0530633926391602
LOSS: 1.0530601739883423
LOSS: 1.053056240081787
LOSS: 1.0530534982681274
LOSS: 1.0530509948730469
LOSS: 1.0530489683151245
LOSS: 1.2085517644882202
LOSS: 1.201398253440857
LOSS: 1.194588541984558
LOSS: 1.1881513595581055
LOSS: 1.1820976734161377
LOSS: 1.1764049530029297
LOSS: 1.1710071563720703
LOSS: 1.165819764137268
LOSS: 1.1607834100723267
LOSS: 1.1558750867843628
LOSS: 1.151093602180481
LOSS: 1.1464471817016602
LOSS: 1.14194655418396
LOSS: 1.1376008987426758
LOSS: 1.1334187984466553
LOSS: 1.1294070482254028
LOSS: 1.1255710124969482
LOSS: 1.1219148635864258
LOSS: 1.1184415817260742
LOSS: 1.1151525974273682
LOSS: 1.1120485067367554
LOSS: 1.1091282367706299
LOSS: 1.1063898801803589
LOSS: 1.103830337524414
LOSS: 1.101445198059082
LOSS: 1.099228858947754
LOSS: 1.0971750020980835
LOSS: 1.095276117324829
LOSS: 1.0935245752334595
LOSS: 1.091913104057312
LOSS: 1.0904324054718018
LOSS: 1.0890756845474243
LOSS: 1.0878351926803589
LOSS: 1.0867043733596802
LOSS: 1.0856764316558838
LOSS: 1.0847458839416504
LOSS: 1.0839065313339233
LOSS: 1.0831536054611206
LOSS: 1.0824811458587646
LOSS: 1.0818841457366943
LOSS: 1.0813571214675903
LOSS: 1.0808939933776855
LOSS: 1.0804896354675293
LOSS: 1.0801390409469604
LOSS: 1.0798358917236328
LOSS: 1.0795754194259644
LOSS: 1.079352855682373
LOSS: 1.0791641473770142
LOSS: 1.0790053606033325
LOSS: 1.0788726806640625
LOSS: 1.0787642002105713
LOSS: 1.0786755084991455
LOSS: 1.0786054134368896
LOSS: 1.0785512924194336
LOSS: 1.0785106420516968
LOSS: 1.0784807205200195
LOSS: 1.0784605741500854
LOSS: 1.078447937965393
LOSS: 1.0784419775009155
LOSS: 1.0784401893615723
LOSS: 1.078442096710205
LOSS: 1.0784461498260498
LOSS: 1.0784519910812378
LOSS: 1.0784584283828735
LOSS: 1.0784661769866943
LOSS: 1.0784730911254883
LOSS: 1.0784809589385986
LOSS: 1.0784881114959717
LOSS: 1.0784939527511597
LOSS: 1.0784995555877686
LOSS: 1.0785043239593506
LOSS: 1.0785073041915894
LOSS: 1.078510046005249
LOSS: 1.078511357307434
LOSS: 1.0785119533538818
LOSS: 1.0785114765167236
LOSS: 1.07850980758667
LOSS: 1.078507423400879
LOSS: 1.078505039215088
LOSS: 1.0785024166107178
LOSS: 1.0784987211227417
LOSS: 1.0784952640533447
LOSS: 1.0784913301467896
LOSS: 1.078487515449524
LOSS: 1.0784835815429688
LOSS: 1.0784794092178345
LOSS: 1.0784752368927002
LOSS: 1.0784716606140137
LOSS: 1.078467607498169
LOSS: 1.0784639120101929
LOSS: 1.0784610509872437
LOSS: 1.0784580707550049
LOSS: 1.0784552097320557
LOSS: 1.0784529447555542
LOSS: 1.0784507989883423
LOSS: 1.0784482955932617
LOSS: 1.0784467458724976
LOSS: 1.0784454345703125
LOSS: 1.0784441232681274
LOSS: 1.0784425735473633
LOSS: 1.2041581869125366
LOSS: 1.1968941688537598
LOSS: 1.1899683475494385
LOSS: 1.1834042072296143
LOSS: 1.1771981716156006
LOSS: 1.1712971925735474
LOSS: 1.1656081676483154
LOSS: 1.1600531339645386
LOSS: 1.154598355293274
LOSS: 1.1492397785186768
LOSS: 1.1439851522445679
LOSS: 1.1388442516326904
LOSS: 1.1338268518447876
LOSS: 1.1289417743682861
LOSS: 1.12419593334198
LOSS: 1.1195961236953735
LOSS: 1.1151479482650757
LOSS: 1.1108564138412476
LOSS: 1.1067253351211548
LOSS: 1.102758526802063
LOSS: 1.0989582538604736
LOSS: 1.09532630443573
LOSS: 1.0918631553649902
LOSS: 1.088567852973938
LOSS: 1.0854390859603882
LOSS: 1.082473635673523
LOSS: 1.0796680450439453
LOSS: 1.0770174264907837
LOSS: 1.0745165348052979
LOSS: 1.072161078453064
LOSS: 1.069946050643921
LOSS: 1.0678671598434448
LOSS: 1.065920114517212
LOSS: 1.0641021728515625
LOSS: 1.0624103546142578
LOSS: 1.0608412027359009
LOSS: 1.0593920946121216
LOSS: 1.0580593347549438
LOSS: 1.056838870048523
LOSS: 1.055726170539856
LOSS: 1.054715871810913
LOSS: 1.0538018941879272
LOSS: 1.052978515625
LOSS: 1.0522394180297852
LOSS: 1.0515787601470947
LOSS: 1.050991177558899
LOSS: 1.050471305847168
LOSS: 1.0500143766403198
LOSS: 1.0496153831481934
LOSS: 1.0492706298828125
LOSS: 1.0489745140075684
LOSS: 1.048723816871643
LOSS: 1.048513650894165
LOSS: 1.048340082168579
LOSS: 1.0481979846954346
LOSS: 1.0480836629867554
LOSS: 1.0479940176010132
LOSS: 1.0479240417480469
LOSS: 1.047872543334961
LOSS: 1.0478354692459106
LOSS: 1.0478103160858154
LOSS: 1.0477956533432007
LOSS: 1.047789216041565
LOSS: 1.047789454460144
LOSS: 1.0477941036224365
LOSS: 1.0478030443191528
LOSS: 1.0478146076202393
LOSS: 1.0478262901306152
LOSS: 1.047838807106018
LOSS: 1.047850489616394
LOSS: 1.0478614568710327
LOSS: 1.0478715896606445
LOSS: 1.0478794574737549
LOSS: 1.0478863716125488
LOSS: 1.04789137840271
LOSS: 1.0478949546813965
LOSS: 1.0478967428207397
LOSS: 1.0478968620300293
LOSS: 1.0478960275650024
LOSS: 1.0478932857513428
LOSS: 1.0478899478912354
LOSS: 1.0478851795196533
LOSS: 1.0478808879852295
LOSS: 1.0478743314743042
LOSS: 1.047868251800537
LOSS: 1.0478620529174805
LOSS: 1.047855019569397
LOSS: 1.0478490591049194
LOSS: 1.0478427410125732
LOSS: 1.0478363037109375
LOSS: 1.04783034324646
LOSS: 1.0478252172470093
LOSS: 1.0478200912475586
LOSS: 1.0478148460388184
LOSS: 1.047810435295105
LOSS: 1.047806739807129
LOSS: 1.0478034019470215
LOSS: 1.0478004217147827
LOSS: 1.0477980375289917
LOSS: 1.0477956533432007
LOSS: 1.203023076057434
LOSS: 1.1957604885101318
LOSS: 1.1888362169265747
LOSS: 1.1822717189788818
LOSS: 1.1760585308074951
LOSS: 1.170134425163269
LOSS: 1.1644059419631958
LOSS: 1.1588060855865479
LOSS: 1.1533128023147583
LOSS: 1.1479294300079346
LOSS: 1.1426671743392944
LOSS: 1.137538194656372
LOSS: 1.132553219795227
LOSS: 1.1277217864990234
LOSS: 1.123051643371582
LOSS: 1.11854887008667
LOSS: 1.114219069480896
LOSS: 1.1100659370422363
LOSS: 1.1060925722122192
LOSS: 1.1023005247116089
LOSS: 1.0986905097961426
LOSS: 1.0952616930007935
LOSS: 1.0920124053955078
LOSS: 1.088940143585205
LOSS: 1.086040735244751
LOSS: 1.083309292793274
LOSS: 1.080741286277771
LOSS: 1.0783302783966064
LOSS: 1.07607102394104
LOSS: 1.0739576816558838
LOSS: 1.071984887123108
LOSS: 1.0701476335525513
LOSS: 1.0684411525726318
LOSS: 1.0668609142303467
LOSS: 1.065402626991272
LOSS: 1.064061164855957
LOSS: 1.0628321170806885
LOSS: 1.061710238456726
LOSS: 1.060689091682434
LOSS: 1.0597641468048096
LOSS: 1.058928370475769
LOSS: 1.058176040649414
LOSS: 1.0575004816055298
LOSS: 1.0568970441818237
LOSS: 1.0563595294952393
LOSS: 1.0558842420578003
LOSS: 1.0554648637771606
LOSS: 1.055098056793213
LOSS: 1.054778814315796
LOSS: 1.0545036792755127
LOSS: 1.0542678833007812
LOSS: 1.0540680885314941
LOSS: 1.0539003610610962
LOSS: 1.0537608861923218
LOSS: 1.0536465644836426
LOSS: 1.0535533428192139
LOSS: 1.053478717803955
LOSS: 1.053420901298523
LOSS: 1.053376317024231
LOSS: 1.0533435344696045
LOSS: 1.0533206462860107
LOSS: 1.0533058643341064
LOSS: 1.053297758102417
LOSS: 1.0725454092025757
LOSS: 1.070610523223877
LOSS: 1.0688148736953735
LOSS: 1.0671523809432983
LOSS: 1.0656170845031738
LOSS: 1.0642035007476807
LOSS: 1.0629063844680786
LOSS: 1.061719536781311
LOSS: 1.0606398582458496
LOSS: 1.05966055393219
LOSS: 1.058777093887329
LOSS: 1.0579832792282104
LOSS: 1.0572731494903564
LOSS: 1.0566412210464478
LOSS: 1.0560805797576904
LOSS: 1.0555862188339233
LOSS: 1.0551525354385376
LOSS: 1.054775357246399
LOSS: 1.0544488430023193
LOSS: 1.0541698932647705
LOSS: 1.0539331436157227
LOSS: 1.0537344217300415
LOSS: 1.0535697937011719
LOSS: 1.0534350872039795
LOSS: 1.05332612991333
LOSS: 1.0532397031784058
LOSS: 1.0531731843948364
LOSS: 1.0531233549118042
LOSS: 1.0530871152877808
LOSS: 1.053062915802002
LOSS: 1.0530482530593872
LOSS: 1.0530411005020142
LOSS: 1.053040862083435
LOSS: 1.0530445575714111
LOSS: 1.0530518293380737
LOSS: 1.05306077003479
LOSS: 1.0530712604522705
LOSS: 1.0530812740325928
LOSS: 1.0530920028686523
LOSS: 1.0531023740768433
LOSS: 1.05311119556427
LOSS: 1.05311918258667
LOSS: 1.053126335144043
LOSS: 1.0531312227249146
LOSS: 1.0531351566314697
LOSS: 1.053137183189392
LOSS: 1.053138017654419
LOSS: 1.0531377792358398
LOSS: 1.0531357526779175
LOSS: 1.053133487701416
LOSS: 1.0531296730041504
LOSS: 1.0531258583068848
LOSS: 1.053120732307434
LOSS: 1.0531154870986938
LOSS: 1.053109884262085
LOSS: 1.0531036853790283
LOSS: 1.0530983209609985
LOSS: 1.0530927181243896
LOSS: 1.0530872344970703
LOSS: 1.0530823469161987
LOSS: 1.0530763864517212
LOSS: 1.053072214126587
LOSS: 1.0530673265457153
LOSS: 1.0530633926391602
LOSS: 1.0530601739883423
LOSS: 1.053056240081787
LOSS: 1.0530534982681274
LOSS: 1.0530509948730469
LOSS: 1.0530489683151245
LOSS: 1.2121368646621704
LOSS: 1.205133080482483
LOSS: 1.1984808444976807
LOSS: 1.1922087669372559
LOSS: 1.1863291263580322
LOSS: 1.180820107460022
LOSS: 1.175614595413208
LOSS: 1.170625925064087
LOSS: 1.16579008102417
LOSS: 1.161078929901123
LOSS: 1.1564875841140747
LOSS: 1.152020812034607
LOSS: 1.1476848125457764
LOSS: 1.1434863805770874
LOSS: 1.1394309997558594
LOSS: 1.135522723197937
LOSS: 1.1317648887634277
LOSS: 1.1281605958938599
LOSS: 1.1247111558914185
LOSS: 1.1214183568954468
LOSS: 1.118282675743103
LOSS: 1.1153048276901245
LOSS: 1.1124837398529053
LOSS: 1.109818935394287
LOSS: 1.1073081493377686
LOSS: 1.1049484014511108
LOSS: 1.102736234664917
LOSS: 1.1006675958633423
LOSS: 1.0987361669540405
LOSS: 1.0969367027282715
LOSS: 1.095262885093689
LOSS: 1.0937080383300781
LOSS: 1.0922659635543823
LOSS: 1.0909305810928345
LOSS: 1.089695692062378
LOSS: 1.0885564088821411
LOSS: 1.087507963180542
LOSS: 1.0865463018417358
LOSS: 1.0856672525405884
LOSS: 1.0848668813705444
LOSS: 1.0841418504714966
LOSS: 1.0834881067276
LOSS: 1.0829015970230103
LOSS: 1.0823782682418823
LOSS: 1.0819138288497925
LOSS: 1.0815032720565796
LOSS: 1.0811421871185303
LOSS: 1.0808255672454834
LOSS: 1.080549716949463
LOSS: 1.0803104639053345
LOSS: 1.080104112625122
LOSS: 1.0799263715744019
LOSS: 1.0797760486602783
LOSS: 1.0796490907669067
LOSS: 1.0795434713363647
LOSS: 1.0794568061828613
LOSS: 1.0793862342834473
LOSS: 1.0793312788009644
LOSS: 1.0792887210845947
LOSS: 1.0792570114135742
LOSS: 1.0792343616485596
LOSS: 1.0792193412780762
LOSS: 1.0792096853256226
LOSS: 1.079204797744751
LOSS: 1.0792031288146973
LOSS: 1.0792043209075928
LOSS: 1.0792075395584106
LOSS: 1.0792123079299927
LOSS: 1.079217553138733
LOSS: 1.079223394393921
LOSS: 1.0792292356491089
LOSS: 1.0792351961135864
LOSS: 1.0792409181594849
LOSS: 1.0792458057403564
LOSS: 1.0792502164840698
LOSS: 1.0792540311813354
LOSS: 1.0792567729949951
LOSS: 1.079258680343628
LOSS: 1.0792593955993652
LOSS: 1.0792597532272339
LOSS: 1.079258918762207
LOSS: 1.0792574882507324
LOSS: 1.0792558193206787
LOSS: 1.0792537927627563
LOSS: 1.0792510509490967
LOSS: 1.0792487859725952
LOSS: 1.0792450904846191
LOSS: 1.079242467880249
LOSS: 1.0792391300201416
LOSS: 1.0792356729507446
LOSS: 1.0792322158813477
LOSS: 1.0792295932769775
LOSS: 1.0792261362075806
LOSS: 1.0792232751846313
LOSS: 1.0792206525802612
LOSS: 1.0792181491851807
LOSS: 1.0792160034179688
LOSS: 1.079214096069336
LOSS: 1.079211950302124
LOSS: 1.0792096853256226
LOSS: 1.2056111097335815
LOSS: 1.1983487606048584
LOSS: 1.1914249658584595
LOSS: 1.1848692893981934
LOSS: 1.1786986589431763
LOSS: 1.172902226448059
LOSS: 1.1674295663833618
LOSS: 1.162202000617981
LOSS: 1.1571485996246338
LOSS: 1.1522307395935059
LOSS: 1.1474385261535645
LOSS: 1.1427749395370483
LOSS: 1.1382484436035156
LOSS: 1.1338677406311035
LOSS: 1.129641056060791
LOSS: 1.1255747079849243
LOSS: 1.1216740608215332
LOSS: 1.1179428100585938
LOSS: 1.1143834590911865
LOSS: 1.1109974384307861
LOSS: 1.1077853441238403
LOSS: 1.1047462224960327
LOSS: 1.1018779277801514
LOSS: 1.0991781949996948
LOSS: 1.0966427326202393
LOSS: 1.0942670106887817
LOSS: 1.0920449495315552
LOSS: 1.0899702310562134
LOSS: 1.0880358219146729
LOSS: 1.086234450340271
LOSS: 1.0845584869384766
LOSS: 1.0830007791519165
LOSS: 1.0815541744232178
LOSS: 1.0802124738693237
LOSS: 1.0789694786071777
LOSS: 1.0778203010559082
LOSS: 1.0767598152160645
LOSS: 1.0757840871810913
LOSS: 1.0748882293701172
LOSS: 1.074068546295166
LOSS: 1.073320984840393
LOSS: 1.0726416110992432
LOSS: 1.0720255374908447
LOSS: 1.0714693069458008
LOSS: 1.0709677934646606
LOSS: 1.0705164670944214
LOSS: 1.0701115131378174
LOSS: 1.069749116897583
LOSS: 1.069425106048584
LOSS: 1.0691369771957397
LOSS: 1.0688811540603638
LOSS: 1.0686551332473755
LOSS: 1.0684564113616943
LOSS: 1.0682823657989502
LOSS: 1.0681312084197998
LOSS: 1.0680005550384521
LOSS: 1.067888855934143
LOSS: 1.067793607711792
LOSS: 1.0677132606506348
LOSS: 1.067645788192749
LOSS: 1.0675896406173706
LOSS: 1.0675432682037354
LOSS: 1.0675055980682373
LOSS: 1.0674748420715332
LOSS: 1.0674502849578857
LOSS: 1.0674314498901367
LOSS: 1.067417025566101
LOSS: 1.0674067735671997
LOSS: 1.067399024963379
LOSS: 1.0673952102661133
LOSS: 1.0673927068710327
LOSS: 1.0673922300338745
LOSS: 1.0673928260803223
LOSS: 1.067394495010376
LOSS: 1.0673967599868774
LOSS: 1.0673998594284058
LOSS: 1.0674021244049072
LOSS: 1.0674046277999878
LOSS: 1.0674070119857788
LOSS: 1.0674092769622803
LOSS: 1.0674116611480713
LOSS: 1.067413330078125
LOSS: 1.0674148797988892
LOSS: 1.0674161911010742
LOSS: 1.0674172639846802
LOSS: 1.0674176216125488
LOSS: 1.0674177408218384
LOSS: 1.0674176216125488
LOSS: 1.0674172639846802
LOSS: 1.067417025566101
LOSS: 1.0674164295196533
LOSS: 1.0674152374267578
LOSS: 1.0674136877059937
LOSS: 1.067413091659546
LOSS: 1.0674113035202026
LOSS: 1.0674103498458862
LOSS: 1.0674090385437012
LOSS: 1.0674079656600952
LOSS: 1.0674062967300415
LOSS: 1.067405104637146
LOSS: 1.2150156497955322
LOSS: 1.2079932689666748
LOSS: 1.2013213634490967
LOSS: 1.19502592086792
LOSS: 1.1891098022460938
LOSS: 1.183531641960144
LOSS: 1.1782042980194092
LOSS: 1.1730402708053589
LOSS: 1.1679919958114624
LOSS: 1.163046956062317
LOSS: 1.1582084894180298
LOSS: 1.1534850597381592
LOSS: 1.14888596534729
LOSS: 1.1444191932678223
LOSS: 1.140092134475708
LOSS: 1.1359108686447144
LOSS: 1.131880760192871
LOSS: 1.1280063390731812
LOSS: 1.1242913007736206
LOSS: 1.1207389831542969
LOSS: 1.1173514127731323
LOSS: 1.114129900932312
LOSS: 1.111074686050415
LOSS: 1.1081844568252563
LOSS: 1.1054571866989136
LOSS: 1.1028892993927002
LOSS: 1.1004761457443237
LOSS: 1.098212718963623
LOSS: 1.0960923433303833
LOSS: 1.094109058380127
LOSS: 1.0922563076019287
LOSS: 1.0905280113220215
LOSS: 1.0889188051223755
LOSS: 1.0874236822128296
LOSS: 1.0860378742218018
LOSS: 1.084757685661316
LOSS: 1.0835793018341064
LOSS: 1.0824990272521973
LOSS: 1.0815125703811646
LOSS: 1.080615520477295
LOSS: 1.0798031091690063
LOSS: 1.0790698528289795
LOSS: 1.0784107446670532
LOSS: 1.0778191089630127
LOSS: 1.0772907733917236
LOSS: 1.0768194198608398
LOSS: 1.0764003992080688
LOSS: 1.0760291814804077
LOSS: 1.0757020711898804
LOSS: 1.0754153728485107
LOSS: 1.0751655101776123
LOSS: 1.0749492645263672
LOSS: 1.0747642517089844
LOSS: 1.0746067762374878
LOSS: 1.0744743347167969
LOSS: 1.0743638277053833
LOSS: 1.0742725133895874
LOSS: 1.0741982460021973
LOSS: 1.0741381645202637
LOSS: 1.0740896463394165
LOSS: 1.0740524530410767
LOSS: 1.0740234851837158
LOSS: 1.0701476335525513
LOSS: 1.0684411525726318
LOSS: 1.0668609142303467
LOSS: 1.065402626991272
LOSS: 1.064061164855957
LOSS: 1.0628321170806885
LOSS: 1.061710238456726
LOSS: 1.060689091682434
LOSS: 1.0597641468048096
LOSS: 1.058928370475769
LOSS: 1.058176040649414
LOSS: 1.0575004816055298
LOSS: 1.0568970441818237
LOSS: 1.0563595294952393
LOSS: 1.0558842420578003
LOSS: 1.0554648637771606
LOSS: 1.055098056793213
LOSS: 1.054778814315796
LOSS: 1.0545036792755127
LOSS: 1.0542678833007812
LOSS: 1.0540680885314941
LOSS: 1.0539003610610962
LOSS: 1.0537608861923218
LOSS: 1.0536465644836426
LOSS: 1.0535533428192139
LOSS: 1.053478717803955
LOSS: 1.053420901298523
LOSS: 1.053376317024231
LOSS: 1.0533435344696045
LOSS: 1.0533206462860107
LOSS: 1.0533058643341064
LOSS: 1.053297758102417
LOSS: 1.0532951354980469
LOSS: 1.0532965660095215
LOSS: 1.0533010959625244
LOSS: 1.0533076524734497
LOSS: 1.0533148050308228
LOSS: 1.0533233880996704
LOSS: 1.053331732749939
LOSS: 1.0533392429351807
LOSS: 1.0533467531204224
LOSS: 1.053353190422058
LOSS: 1.0533586740493774
LOSS: 1.0533630847930908
LOSS: 1.0533664226531982
LOSS: 1.053369402885437
LOSS: 1.0533703565597534
LOSS: 1.0533705949783325
LOSS: 1.0533697605133057
LOSS: 1.0533688068389893
LOSS: 1.0533668994903564
LOSS: 1.0533639192581177
LOSS: 1.0533604621887207
LOSS: 1.0533570051193237
LOSS: 1.0533528327941895
LOSS: 1.0533485412597656
LOSS: 1.0533443689346313
LOSS: 1.0533397197723389
LOSS: 1.0533360242843628
LOSS: 1.0533318519592285
LOSS: 1.0533274412155151
LOSS: 1.05332350730896
LOSS: 1.053320288658142
LOSS: 1.0533170700073242
LOSS: 1.053313970565796
LOSS: 1.0533108711242676
LOSS: 1.053308367729187
LOSS: 1.053305983543396
LOSS: 1.0533039569854736
LOSS: 1.2098358869552612
LOSS: 1.202711582183838
LOSS: 1.1959325075149536
LOSS: 1.1895281076431274
LOSS: 1.1835120916366577
LOSS: 1.177866816520691
LOSS: 1.1725319623947144
LOSS: 1.1674232482910156
LOSS: 1.1624747514724731
LOSS: 1.1576564311981201
LOSS: 1.152962565422058
LOSS: 1.1483995914459229
LOSS: 1.1439759731292725
LOSS: 1.139701247215271
LOSS: 1.135583758354187
LOSS: 1.131630301475525
LOSS: 1.127846598625183
LOSS: 1.1242377758026123
LOSS: 1.120807409286499
LOSS: 1.1175581216812134
LOSS: 1.114491581916809
LOSS: 1.1116083860397339
LOSS: 1.108907699584961
LOSS: 1.1063878536224365
LOSS: 1.1040455102920532
LOSS: 1.1018762588500977
LOSS: 1.0998742580413818
LOSS: 1.09803307056427
LOSS: 1.0963447093963623
LOSS: 1.0948015451431274
LOSS: 1.0933952331542969
LOSS: 1.092117190361023
LOSS: 1.0909596681594849
LOSS: 1.0899152755737305
LOSS: 1.0889770984649658
LOSS: 1.088138222694397
LOSS: 1.0873931646347046
LOSS: 1.086735725402832
LOSS: 1.0861607789993286
LOSS: 1.0856616497039795
LOSS: 1.0852330923080444
LOSS: 1.0848687887191772
LOSS: 1.0845625400543213
LOSS: 1.084308385848999
LOSS: 1.084099531173706
LOSS: 1.0839309692382812
LOSS: 1.0837960243225098
LOSS: 1.0836912393569946
LOSS: 1.0836108922958374
LOSS: 1.0835514068603516
LOSS: 1.0835095643997192
LOSS: 1.0834826231002808
LOSS: 1.0834678411483765
LOSS: 1.0834625959396362
LOSS: 1.0834654569625854
LOSS: 1.083473801612854
LOSS: 1.083486557006836
LOSS: 1.083501935005188
LOSS: 1.0835185050964355
LOSS: 1.0835354328155518
LOSS: 1.0835511684417725
LOSS: 1.0835657119750977
LOSS: 1.0835775136947632
LOSS: 1.0835875272750854
LOSS: 1.083595633506775
LOSS: 1.0836012363433838
LOSS: 1.083604335784912
LOSS: 1.0836056470870972
LOSS: 1.0836050510406494
LOSS: 1.0836031436920166
LOSS: 1.0835994482040405
LOSS: 1.0835944414138794
LOSS: 1.0835886001586914
LOSS: 1.0835810899734497
LOSS: 1.0835734605789185
LOSS: 1.0835648775100708
LOSS: 1.0835561752319336
LOSS: 1.0835473537445068
LOSS: 1.0835388898849487
LOSS: 1.0835301876068115
LOSS: 1.083522081375122
LOSS: 1.0835139751434326
LOSS: 1.08350670337677
LOSS: 1.083499789237976
LOSS: 1.0834940671920776
LOSS: 1.0834890604019165
LOSS: 1.0834839344024658
LOSS: 1.083479404449463
LOSS: 1.0834754705429077
LOSS: 1.0834723711013794
LOSS: 1.0834696292877197
LOSS: 1.0834672451019287
LOSS: 1.0834654569625854
LOSS: 1.0834641456604004
LOSS: 1.0834629535675049
LOSS: 1.083462119102478
LOSS: 1.0834615230560303
LOSS: 1.0834612846374512
LOSS: 1.0834606885910034
LOSS: 1.083461046218872
LOSS: 1.2041581869125366
LOSS: 1.1968941688537598
LOSS: 1.1899683475494385
LOSS: 1.1834042072296143
LOSS: 1.1771981716156006
LOSS: 1.1712971925735474
LOSS: 1.1656081676483154
LOSS: 1.1600531339645386
LOSS: 1.154598355293274
LOSS: 1.1492397785186768
LOSS: 1.1439851522445679
LOSS: 1.1388442516326904
LOSS: 1.1338268518447876
LOSS: 1.1289417743682861
LOSS: 1.12419593334198
LOSS: 1.1195961236953735
LOSS: 1.1151479482650757
LOSS: 1.1108564138412476
LOSS: 1.1067253351211548
LOSS: 1.102758526802063
LOSS: 1.0989582538604736
LOSS: 1.09532630443573
LOSS: 1.0918631553649902
LOSS: 1.088567852973938
LOSS: 1.0854390859603882
LOSS: 1.082473635673523
LOSS: 1.0796680450439453
LOSS: 1.0770174264907837
LOSS: 1.0745165348052979
LOSS: 1.072161078453064
LOSS: 1.069946050643921
LOSS: 1.0678671598434448
LOSS: 1.065920114517212
LOSS: 1.0641021728515625
LOSS: 1.0624103546142578
LOSS: 1.0608412027359009
LOSS: 1.0593920946121216
LOSS: 1.0580593347549438
LOSS: 1.056838870048523
LOSS: 1.055726170539856
LOSS: 1.054715871810913
LOSS: 1.0538018941879272
LOSS: 1.052978515625
LOSS: 1.0522394180297852
LOSS: 1.0515787601470947
LOSS: 1.050991177558899
LOSS: 1.050471305847168
LOSS: 1.0500143766403198
LOSS: 1.0496153831481934
LOSS: 1.0492706298828125
LOSS: 1.0489745140075684
LOSS: 1.048723816871643
LOSS: 1.048513650894165
LOSS: 1.048340082168579
LOSS: 1.0481979846954346
LOSS: 1.0480836629867554
LOSS: 1.0479940176010132
LOSS: 1.0479240417480469
LOSS: 1.047872543334961
LOSS: 1.0478354692459106
LOSS: 1.0478103160858154
LOSS: 1.0477956533432007
LOSS: 1.047789216041565
LOSS: 1.047789454460144
LOSS: 1.0477941036224365
LOSS: 1.0478030443191528
LOSS: 1.0478146076202393
LOSS: 1.0478262901306152
LOSS: 1.047838807106018
LOSS: 1.047850489616394
LOSS: 1.0478614568710327
LOSS: 1.0478715896606445
LOSS: 1.0478794574737549
LOSS: 1.0478863716125488
LOSS: 1.04789137840271
LOSS: 1.0478949546813965
LOSS: 1.0478967428207397
LOSS: 1.0478968620300293
LOSS: 1.0478960275650024
LOSS: 1.0478932857513428
LOSS: 1.0478899478912354
LOSS: 1.0478851795196533
LOSS: 1.0478808879852295
LOSS: 1.0478743314743042
LOSS: 1.047868251800537
LOSS: 1.0478620529174805
LOSS: 1.047855019569397
LOSS: 1.0478490591049194
LOSS: 1.0478427410125732
LOSS: 1.0478363037109375
LOSS: 1.04783034324646
LOSS: 1.0478252172470093
LOSS: 1.0478200912475586
LOSS: 1.0478148460388184
LOSS: 1.047810435295105
LOSS: 1.047806739807129
LOSS: 1.0478034019470215
LOSS: 1.0478004217147827
LOSS: 1.0477980375289917
LOSS: 1.0477956533432007
LOSS: 1.197387456893921
LOSS: 1.1898107528686523
LOSS: 1.1825568675994873
LOSS: 1.1756547689437866
LOSS: 1.1691211462020874
LOSS: 1.1629470586776733
LOSS: 1.1570842266082764
LOSS: 1.1514544486999512
LOSS: 1.1459838151931763
LOSS: 1.1406285762786865
LOSS: 1.1353747844696045
LOSS: 1.1302235126495361
LOSS: 1.1251825094223022
LOSS: 1.1202605962753296
LOSS: 1.1154661178588867
LOSS: 1.1108068227767944
LOSS: 1.106289267539978
LOSS: 1.1019195318222046
LOSS: 1.0977023839950562
LOSS: 1.0936416387557983
LOSS: 1.08974027633667
LOSS: 1.0860010385513306
LOSS: 1.0824247598648071
LOSS: 1.07901132106781
LOSS: 1.0757598876953125
LOSS: 1.0726680755615234
LOSS: 1.0697320699691772
LOSS: 1.0669479370117188
LOSS: 1.06430983543396
LOSS: 1.061812162399292
LOSS: 1.0594482421875
LOSS: 1.0572129487991333
LOSS: 1.0550988912582397
LOSS: 1.0531026124954224
LOSS: 1.0512183904647827
LOSS: 1.049443244934082
LOSS: 1.047772765159607
LOSS: 1.046204686164856
LOSS: 1.0447360277175903
LOSS: 1.04336416721344
LOSS: 1.0420849323272705
LOSS: 1.0408953428268433
LOSS: 1.039791464805603
LOSS: 1.0387682914733887
LOSS: 1.0378217697143555
LOSS: 1.036946415901184
LOSS: 1.0361380577087402
LOSS: 1.0353928804397583
LOSS: 1.0347062349319458
LOSS: 1.0340744256973267
LOSS: 1.033494472503662
LOSS: 1.0329632759094238
LOSS: 1.0324783325195312
LOSS: 1.0320372581481934
LOSS: 1.0316362380981445
LOSS: 1.0312734842300415
LOSS: 1.0309460163116455
LOSS: 1.0306506156921387
LOSS: 1.0303863286972046
LOSS: 1.0301487445831299
LOSS: 1.0299365520477295
LOSS: 1.0297468900680542
LOSS: 1.0905280113220215
LOSS: 1.0889188051223755
LOSS: 1.0874236822128296
LOSS: 1.0860378742218018
LOSS: 1.084757685661316
LOSS: 1.0835793018341064
LOSS: 1.0824990272521973
LOSS: 1.0815125703811646
LOSS: 1.080615520477295
LOSS: 1.0798031091690063
LOSS: 1.0790698528289795
LOSS: 1.0784107446670532
LOSS: 1.0778191089630127
LOSS: 1.0772907733917236
LOSS: 1.0768194198608398
LOSS: 1.0764003992080688
LOSS: 1.0760291814804077
LOSS: 1.0757020711898804
LOSS: 1.0754153728485107
LOSS: 1.0751655101776123
LOSS: 1.0749492645263672
LOSS: 1.0747642517089844
LOSS: 1.0746067762374878
LOSS: 1.0744743347167969
LOSS: 1.0743638277053833
LOSS: 1.0742725133895874
LOSS: 1.0741982460021973
LOSS: 1.0741381645202637
LOSS: 1.0740896463394165
LOSS: 1.0740524530410767
LOSS: 1.0740234851837158
LOSS: 1.0740022659301758
LOSS: 1.0739867687225342
LOSS: 1.0739772319793701
LOSS: 1.0739719867706299
LOSS: 1.0739697217941284
LOSS: 1.0739707946777344
LOSS: 1.0739737749099731
LOSS: 1.073978304862976
LOSS: 1.0739829540252686
LOSS: 1.0739880800247192
LOSS: 1.0739933252334595
LOSS: 1.0739984512329102
LOSS: 1.074002742767334
LOSS: 1.0740071535110474
LOSS: 1.0740106105804443
LOSS: 1.074013352394104
LOSS: 1.074015736579895
LOSS: 1.0740177631378174
LOSS: 1.0740185976028442
LOSS: 1.0740190744400024
LOSS: 1.0740188360214233
LOSS: 1.0740183591842651
LOSS: 1.0740172863006592
LOSS: 1.0740153789520264
LOSS: 1.074013590812683
LOSS: 1.0740116834640503
LOSS: 1.0740091800689697
LOSS: 1.0740066766738892
LOSS: 1.07400381565094
LOSS: 1.0740011930465698
LOSS: 1.0739988088607788
LOSS: 1.0739963054656982
LOSS: 1.0739930868148804
LOSS: 1.0739909410476685
LOSS: 1.0739890336990356
LOSS: 1.073986291885376
LOSS: 1.0739846229553223
LOSS: 1.0739827156066895
LOSS: 1.2056111097335815
LOSS: 1.1983487606048584
LOSS: 1.1914249658584595
LOSS: 1.1848692893981934
LOSS: 1.1786986589431763
LOSS: 1.172902226448059
LOSS: 1.1674295663833618
LOSS: 1.162202000617981
LOSS: 1.1571485996246338
LOSS: 1.1522307395935059
LOSS: 1.1474385261535645
LOSS: 1.1427749395370483
LOSS: 1.1382484436035156
LOSS: 1.1338677406311035
LOSS: 1.129641056060791
LOSS: 1.1255747079849243
LOSS: 1.1216740608215332
LOSS: 1.1179428100585938
LOSS: 1.1143834590911865
LOSS: 1.1109974384307861
LOSS: 1.1077853441238403
LOSS: 1.1047462224960327
LOSS: 1.1018779277801514
LOSS: 1.0991781949996948
LOSS: 1.0966427326202393
LOSS: 1.0942670106887817
LOSS: 1.0920449495315552
LOSS: 1.0899702310562134
LOSS: 1.0880358219146729
LOSS: 1.086234450340271
LOSS: 1.0845584869384766
LOSS: 1.0830007791519165
LOSS: 1.0815541744232178
LOSS: 1.0802124738693237
LOSS: 1.0789694786071777
LOSS: 1.0778203010559082
LOSS: 1.0767598152160645
LOSS: 1.0757840871810913
LOSS: 1.0748882293701172
LOSS: 1.074068546295166
LOSS: 1.073320984840393
LOSS: 1.0726416110992432
LOSS: 1.0720255374908447
LOSS: 1.0714693069458008
LOSS: 1.0709677934646606
LOSS: 1.0705164670944214
LOSS: 1.0701115131378174
LOSS: 1.069749116897583
LOSS: 1.069425106048584
LOSS: 1.0691369771957397
LOSS: 1.0688811540603638
LOSS: 1.0686551332473755
LOSS: 1.0684564113616943
LOSS: 1.0682823657989502
LOSS: 1.0681312084197998
LOSS: 1.0680005550384521
LOSS: 1.067888855934143
LOSS: 1.067793607711792
LOSS: 1.0677132606506348
LOSS: 1.067645788192749
LOSS: 1.0675896406173706
LOSS: 1.0675432682037354
LOSS: 1.0675055980682373
LOSS: 1.0674748420715332
LOSS: 1.0674502849578857
LOSS: 1.0674314498901367
LOSS: 1.067417025566101
LOSS: 1.0674067735671997
LOSS: 1.067399024963379
LOSS: 1.0673952102661133
LOSS: 1.0673927068710327
LOSS: 1.0673922300338745
LOSS: 1.0673928260803223
LOSS: 1.067394495010376
LOSS: 1.0673967599868774
LOSS: 1.0673998594284058
LOSS: 1.0674021244049072
LOSS: 1.0674046277999878
LOSS: 1.0674070119857788
LOSS: 1.0674092769622803
LOSS: 1.0674116611480713
LOSS: 1.067413330078125
LOSS: 1.0674148797988892
LOSS: 1.0674161911010742
LOSS: 1.0674172639846802
LOSS: 1.0674176216125488
LOSS: 1.0674177408218384
LOSS: 1.0674176216125488
LOSS: 1.0674172639846802
LOSS: 1.067417025566101
LOSS: 1.0674164295196533
LOSS: 1.0674152374267578
LOSS: 1.0674136877059937
LOSS: 1.067413091659546
LOSS: 1.0674113035202026
LOSS: 1.0674103498458862
LOSS: 1.0674090385437012
LOSS: 1.0674079656600952
LOSS: 1.0674062967300415
LOSS: 1.067405104637146
LOSS: 1.2098358869552612
LOSS: 1.202711582183838
LOSS: 1.1959325075149536
LOSS: 1.1895281076431274
LOSS: 1.1835120916366577
LOSS: 1.177866816520691
LOSS: 1.1725319623947144
LOSS: 1.1674232482910156
LOSS: 1.1624747514724731
LOSS: 1.1576564311981201
LOSS: 1.152962565422058
LOSS: 1.1483995914459229
LOSS: 1.1439759731292725
LOSS: 1.139701247215271
LOSS: 1.135583758354187
LOSS: 1.131630301475525
LOSS: 1.127846598625183
LOSS: 1.1242377758026123
LOSS: 1.120807409286499
LOSS: 1.1175581216812134
LOSS: 1.114491581916809
LOSS: 1.1116083860397339
LOSS: 1.108907699584961
LOSS: 1.1063878536224365
LOSS: 1.1040455102920532
LOSS: 1.1018762588500977
LOSS: 1.0998742580413818
LOSS: 1.09803307056427
LOSS: 1.0963447093963623
LOSS: 1.0948015451431274
LOSS: 1.0933952331542969
LOSS: 1.092117190361023
LOSS: 1.0909596681594849
LOSS: 1.0899152755737305
LOSS: 1.0889770984649658
LOSS: 1.088138222694397
LOSS: 1.0873931646347046
LOSS: 1.086735725402832
LOSS: 1.0861607789993286
LOSS: 1.0856616497039795
LOSS: 1.0852330923080444
LOSS: 1.0848687887191772
LOSS: 1.0845625400543213
LOSS: 1.084308385848999
LOSS: 1.084099531173706
LOSS: 1.0839309692382812
LOSS: 1.0837960243225098
LOSS: 1.0836912393569946
LOSS: 1.0836108922958374
LOSS: 1.0835514068603516
LOSS: 1.0835095643997192
LOSS: 1.0834826231002808
LOSS: 1.0834678411483765
LOSS: 1.0834625959396362
LOSS: 1.0834654569625854
LOSS: 1.083473801612854
LOSS: 1.083486557006836
LOSS: 1.083501935005188
LOSS: 1.0835185050964355
LOSS: 1.0835354328155518
LOSS: 1.0835511684417725
LOSS: 1.0835657119750977
LOSS: 1.0835775136947632
LOSS: 1.0835875272750854
LOSS: 1.083595633506775
LOSS: 1.0836012363433838
LOSS: 1.083604335784912
LOSS: 1.0836056470870972
LOSS: 1.0836050510406494
LOSS: 1.0836031436920166
LOSS: 1.0835994482040405
LOSS: 1.0835944414138794
LOSS: 1.0835886001586914
LOSS: 1.0835810899734497
LOSS: 1.0835734605789185
LOSS: 1.0835648775100708
LOSS: 1.0835561752319336
LOSS: 1.0835473537445068
LOSS: 1.0835388898849487
LOSS: 1.0835301876068115
LOSS: 1.083522081375122
LOSS: 1.0835139751434326
LOSS: 1.08350670337677
LOSS: 1.083499789237976
LOSS: 1.0834940671920776
LOSS: 1.0834890604019165
LOSS: 1.0834839344024658
LOSS: 1.083479404449463
LOSS: 1.0834754705429077
LOSS: 1.0834723711013794
LOSS: 1.0834696292877197
LOSS: 1.0834672451019287
LOSS: 1.0834654569625854
LOSS: 1.0834641456604004
LOSS: 1.0834629535675049
LOSS: 1.083462119102478
LOSS: 1.0834615230560303
LOSS: 1.0834612846374512
LOSS: 1.0834606885910034
LOSS: 1.083461046218872
LOSS: 1.2195827960968018
LOSS: 1.212821125984192
LOSS: 1.206422209739685
LOSS: 1.2004083395004272
LOSS: 1.1947664022445679
LOSS: 1.1894261837005615
LOSS: 1.1842880249023438
LOSS: 1.1792868375778198
LOSS: 1.174403429031372
LOSS: 1.1696418523788452
LOSS: 1.1650121212005615
LOSS: 1.1605255603790283
LOSS: 1.1561918258666992
LOSS: 1.1520189046859741
LOSS: 1.1480134725570679
LOSS: 1.1441813707351685
LOSS: 1.1405268907546997
LOSS: 1.1370530128479004
LOSS: 1.133762240409851
LOSS: 1.1306558847427368
LOSS: 1.127733588218689
LOSS: 1.1249943971633911
LOSS: 1.1224360466003418
LOSS: 1.1200544834136963
LOSS: 1.117845058441162
LOSS: 1.1158020496368408
LOSS: 1.1139189004898071
LOSS: 1.1121879816055298
LOSS: 1.1106020212173462
LOSS: 1.1091539859771729
LOSS: 1.1078356504440308
LOSS: 1.106641173362732
LOSS: 1.1055630445480347
LOSS: 1.10459566116333
LOSS: 1.1037328243255615
LOSS: 1.102968692779541
LOSS: 1.102297067642212
LOSS: 1.1017119884490967
LOSS: 1.1012060642242432
LOSS: 1.1007734537124634
LOSS: 1.100406289100647
LOSS: 1.1000988483428955
LOSS: 1.0998437404632568
LOSS: 1.0996344089508057
LOSS: 1.0994656085968018
LOSS: 1.0993317365646362
LOSS: 1.0992282629013062
LOSS: 1.0991510152816772
LOSS: 1.0990957021713257
LOSS: 1.0990592241287231
LOSS: 1.0990381240844727
LOSS: 1.0990302562713623
LOSS: 1.099031925201416
LOSS: 1.0990405082702637
LOSS: 1.0990546941757202
LOSS: 1.099071979522705
LOSS: 1.0990904569625854
LOSS: 1.0991095304489136
LOSS: 1.0991275310516357
LOSS: 1.0991439819335938
LOSS: 1.0991584062576294
LOSS: 1.0991703271865845
LOSS: 1.0991804599761963
LOSS: 1.0991877317428589
LOSS: 1.0991926193237305
LOSS: 1.099195122718811
LOSS: 1.0991953611373901
LOSS: 1.0991930961608887
LOSS: 1.0991888046264648
LOSS: 1.099183201789856
LOSS: 1.0991761684417725
LOSS: 1.0991674661636353
LOSS: 1.0991582870483398
LOSS: 1.0991485118865967
LOSS: 1.0991379022598267
LOSS: 1.0991277694702148
LOSS: 1.099117636680603
LOSS: 1.0991076231002808
LOSS: 1.0990982055664062
LOSS: 1.09908926486969
LOSS: 1.099081039428711
LOSS: 1.0990735292434692
LOSS: 1.0990660190582275
LOSS: 1.0990599393844604
LOSS: 1.0990543365478516
LOSS: 1.0990492105484009
LOSS: 1.0990450382232666
LOSS: 1.099041223526001
LOSS: 1.0990387201309204
LOSS: 1.0990358591079712
LOSS: 1.099034309387207
LOSS: 1.0990325212478638
LOSS: 1.099031686782837
LOSS: 1.0990307331085205
LOSS: 1.0990303754806519
LOSS: 1.0990301370620728
LOSS: 1.0990298986434937
LOSS: 1.0990298986434937
LOSS: 1.0990296602249146
LOSS: 1.0990300178527832
LOSS: 1.2169747352600098
LOSS: 1.2101690769195557
LOSS: 1.2037243843078613
LOSS: 1.1976666450500488
LOSS: 1.1919958591461182
LOSS: 1.1866638660430908
LOSS: 1.1815801858901978
LOSS: 1.176662802696228
LOSS: 1.1718730926513672
LOSS: 1.1672029495239258
LOSS: 1.1626570224761963
LOSS: 1.1582434177398682
LOSS: 1.1539688110351562
LOSS: 1.1498394012451172
LOSS: 1.1458603143692017
LOSS: 1.1420345306396484
LOSS: 1.1383650302886963
LOSS: 1.1348539590835571
LOSS: 1.1315021514892578
LOSS: 1.1283105611801147
LOSS: 1.125279188156128
LOSS: 1.1224076747894287
LOSS: 1.1196948289871216
LOSS: 1.1171388626098633
LOSS: 1.1147366762161255
LOSS: 1.112485408782959
LOSS: 1.1103802919387817
LOSS: 1.1084166765213013
LOSS: 1.1065888404846191
LOSS: 1.1048904657363892
LOSS: 1.1033159494400024
LOSS: 1.1018582582473755
LOSS: 1.1005117893218994
LOSS: 1.0992705821990967
LOSS: 1.098129391670227
LOSS: 1.097083568572998
LOSS: 1.0961277484893799
LOSS: 1.0952582359313965
LOSS: 1.094470739364624
LOSS: 1.0937614440917969
LOSS: 1.0931254625320435
LOSS: 1.0925594568252563
LOSS: 1.0920575857162476
LOSS: 1.0916155576705933
LOSS: 1.0912290811538696
LOSS: 1.0908926725387573
LOSS: 1.0906013250350952
LOSS: 1.0903514623641968
LOSS: 1.0901379585266113
LOSS: 1.089957356452942
LOSS: 1.0898066759109497
LOSS: 1.0896813869476318
LOSS: 1.089579463005066
LOSS: 1.0894978046417236
LOSS: 1.089434027671814
LOSS: 1.089385747909546
LOSS: 1.089350938796997
LOSS: 1.089327096939087
LOSS: 1.089312195777893
LOSS: 1.0893049240112305
LOSS: 1.089302897453308
LOSS: 1.0893052816390991
LOSS: 1.0893100500106812
LOSS: 1.0893173217773438
LOSS: 1.0893257856369019
LOSS: 1.0893352031707764
LOSS: 1.089343786239624
LOSS: 1.0893521308898926
LOSS: 1.089360237121582
LOSS: 1.0893677473068237
LOSS: 1.0893733501434326
LOSS: 1.089378833770752
LOSS: 1.0893821716308594
LOSS: 1.0893847942352295
LOSS: 1.0893861055374146
LOSS: 1.0893863439559937
LOSS: 1.0893851518630981
LOSS: 1.089382529258728
LOSS: 1.089379906654358
LOSS: 1.0893759727478027
LOSS: 1.0893722772598267
LOSS: 1.0893677473068237
LOSS: 1.0893633365631104
LOSS: 1.0893585681915283
LOSS: 1.089353322982788
LOSS: 1.089348316192627
LOSS: 1.0893434286117554
LOSS: 1.0893394947052002
LOSS: 1.0893347263336182
LOSS: 1.0893299579620361
LOSS: 1.0893263816833496
LOSS: 1.0893229246139526
LOSS: 1.0893194675445557
LOSS: 1.0893166065216064
LOSS: 1.0893139839172363
LOSS: 1.0893117189407349
LOSS: 1.089309573173523
LOSS: 1.0893075466156006
LOSS: 1.089306116104126
LOSS: 1.0893055200576782
LOSS: 1.1977410316467285
LOSS: 1.1901732683181763
LOSS: 1.1829285621643066
LOSS: 1.1760294437408447
LOSS: 1.1694731712341309
LOSS: 1.1632087230682373
LOSS: 1.1571462154388428
LOSS: 1.151210069656372
LOSS: 1.145368218421936
LOSS: 1.1396195888519287
LOSS: 1.1339747905731201
LOSS: 1.1284468173980713
LOSS: 1.1230485439300537
LOSS: 1.1177911758422852
LOSS: 1.112684965133667
LOSS: 1.1077382564544678
LOSS: 1.102958083152771
LOSS: 1.0983507633209229
LOSS: 1.0939205884933472
LOSS: 1.0896711349487305
LOSS: 1.0856043100357056
LOSS: 1.0817204713821411
LOSS: 1.0780186653137207
LOSS: 1.0744969844818115
LOSS: 1.071151614189148
LOSS: 1.0679783821105957
LOSS: 1.0649728775024414
LOSS: 1.0621297359466553
LOSS: 1.059444785118103
LOSS: 1.0569136142730713
LOSS: 1.0545324087142944
LOSS: 1.0522984266281128
LOSS: 1.050208330154419
LOSS: 1.0482594966888428
LOSS: 1.0464483499526978
LOSS: 1.0447713136672974
LOSS: 1.04322350025177
LOSS: 1.0417999029159546
LOSS: 1.0404945611953735
LOSS: 1.0393013954162598
LOSS: 1.0382134914398193
LOSS: 1.0372248888015747
LOSS: 1.0363293886184692
LOSS: 1.0355217456817627
LOSS: 1.0347964763641357
LOSS: 1.0341483354568481
LOSS: 1.0335724353790283
LOSS: 1.0330644845962524
LOSS: 1.032619595527649
LOSS: 1.0322321653366089
LOSS: 1.0318971872329712
LOSS: 1.0316098928451538
LOSS: 1.0313652753829956
LOSS: 1.031158685684204
LOSS: 1.0309864282608032
LOSS: 1.0308443307876587
LOSS: 1.0307279825210571
LOSS: 1.030634880065918
LOSS: 1.0305629968643188
LOSS: 1.030508279800415
LOSS: 1.0304685831069946
LOSS: 1.0304419994354248
LOSS: 1.03042471408844
LOSS: 1.0304162502288818
LOSS: 1.0304133892059326
LOSS: 1.0304162502288818
LOSS: 1.030422329902649
LOSS: 1.030430555343628
LOSS: 1.030439853668213
LOSS: 1.0304502248764038
LOSS: 1.0304608345031738
LOSS: 1.0304712057113647
LOSS: 1.0304805040359497
LOSS: 1.0304887294769287
LOSS: 1.0304956436157227
LOSS: 1.0305018424987793
LOSS: 1.0305050611495972
LOSS: 1.0305079221725464
LOSS: 1.0305094718933105
LOSS: 1.0305088758468628
LOSS: 1.0305078029632568
LOSS: 1.0305052995681763
LOSS: 1.0305019617080688
LOSS: 1.0304982662200928
LOSS: 1.0304934978485107
LOSS: 1.0304886102676392
LOSS: 1.0304830074310303
LOSS: 1.03047776222229
LOSS: 1.0304720401763916
LOSS: 1.0304665565490723
LOSS: 1.0304608345031738
LOSS: 1.0304553508758545
LOSS: 1.0304499864578247
LOSS: 1.0304458141326904
LOSS: 1.030441164970398
LOSS: 1.0304369926452637
LOSS: 1.030433177947998
LOSS: 1.0304299592971802
LOSS: 1.0304267406463623
LOSS: 1.0304242372512817
LOSS: 1.2230162620544434
LOSS: 1.2164080142974854
LOSS: 1.2101693153381348
LOSS: 1.2043133974075317
LOSS: 1.1987979412078857
LOSS: 1.1935181617736816
LOSS: 1.1883808374404907
LOSS: 1.1833522319793701
LOSS: 1.1784312725067139
LOSS: 1.1736271381378174
LOSS: 1.1689485311508179
LOSS: 1.1644033193588257
LOSS: 1.1599974632263184
LOSS: 1.1557352542877197
LOSS: 1.151620626449585
LOSS: 1.1476553678512573
LOSS: 1.1438418626785278
LOSS: 1.1401811838150024
LOSS: 1.1366736888885498
LOSS: 1.1333198547363281
LOSS: 1.1301188468933105
LOSS: 1.1270694732666016
LOSS: 1.1241700649261475
LOSS: 1.1214179992675781
LOSS: 1.1188104152679443
LOSS: 1.1163434982299805
LOSS: 1.1140127182006836
LOSS: 1.1118135452270508
LOSS: 1.1097412109375
LOSS: 1.1077903509140015
LOSS: 1.1059561967849731
LOSS: 1.104233741760254
LOSS: 1.1026188135147095
LOSS: 1.101107120513916
LOSS: 1.0996949672698975
LOSS: 1.09837806224823
LOSS: 1.0971537828445435
LOSS: 1.096017837524414
LOSS: 1.0949671268463135
LOSS: 1.0939968824386597
LOSS: 1.093104600906372
LOSS: 1.0922845602035522
LOSS: 1.091533899307251
LOSS: 1.0908472537994385
LOSS: 1.0902215242385864
LOSS: 1.0896520614624023
LOSS: 1.089134931564331
LOSS: 1.088667392730713
LOSS: 1.0882458686828613
LOSS: 1.0878663063049316
LOSS: 1.0875271558761597
LOSS: 1.0872244834899902
LOSS: 1.0869561433792114
LOSS: 1.0867195129394531
LOSS: 1.0865113735198975
LOSS: 1.086329460144043
LOSS: 1.0861713886260986
LOSS: 1.0860341787338257
LOSS: 1.0859159231185913
LOSS: 1.0858155488967896
LOSS: 1.0857301950454712
LOSS: 1.0856579542160034
LOSS: 1.085598111152649
LOSS: 1.0855488777160645
LOSS: 1.0855088233947754
LOSS: 1.0854774713516235
LOSS: 1.0854521989822388
LOSS: 1.0854336023330688
LOSS: 1.0854194164276123
LOSS: 1.0854096412658691
LOSS: 1.0854034423828125
LOSS: 1.0853996276855469
LOSS: 1.085398554801941
LOSS: 1.0853979587554932
LOSS: 1.0853996276855469
LOSS: 1.0854014158248901
LOSS: 1.0854042768478394
LOSS: 1.0854074954986572
LOSS: 1.0854101181030273
LOSS: 1.0854138135910034
LOSS: 1.0854169130325317
LOSS: 1.0854191780090332
LOSS: 1.0854213237762451
LOSS: 1.0854238271713257
LOSS: 1.0854253768920898
LOSS: 1.0854265689849854
LOSS: 1.0854270458221436
LOSS: 1.0854274034500122
LOSS: 1.0854272842407227
LOSS: 1.085426926612854
LOSS: 1.0854265689849854
LOSS: 1.0740022659301758
LOSS: 1.0739867687225342
LOSS: 1.0739772319793701
LOSS: 1.0739719867706299
LOSS: 1.0739697217941284
LOSS: 1.0739707946777344
LOSS: 1.0739737749099731
LOSS: 1.073978304862976
LOSS: 1.0739829540252686
LOSS: 1.0739880800247192
LOSS: 1.0739933252334595
LOSS: 1.0739984512329102
LOSS: 1.074002742767334
LOSS: 1.0740071535110474
LOSS: 1.0740106105804443
LOSS: 1.074013352394104
LOSS: 1.074015736579895
LOSS: 1.0740177631378174
LOSS: 1.0740185976028442
LOSS: 1.0740190744400024
LOSS: 1.0740188360214233
LOSS: 1.0740183591842651
LOSS: 1.0740172863006592
LOSS: 1.0740153789520264
LOSS: 1.074013590812683
LOSS: 1.0740116834640503
LOSS: 1.0740091800689697
LOSS: 1.0740066766738892
LOSS: 1.07400381565094
LOSS: 1.0740011930465698
LOSS: 1.0739988088607788
LOSS: 1.0739963054656982
LOSS: 1.0739930868148804
LOSS: 1.0739909410476685
LOSS: 1.0739890336990356
LOSS: 1.073986291885376
LOSS: 1.0739846229553223
LOSS: 1.0739827156066895
LOSS: 1.2169747352600098
LOSS: 1.2101690769195557
LOSS: 1.2037243843078613
LOSS: 1.1976666450500488
LOSS: 1.1919958591461182
LOSS: 1.1866638660430908
LOSS: 1.1815801858901978
LOSS: 1.176662802696228
LOSS: 1.1718730926513672
LOSS: 1.1672029495239258
LOSS: 1.1626570224761963
LOSS: 1.1582434177398682
LOSS: 1.1539688110351562
LOSS: 1.1498394012451172
LOSS: 1.1458603143692017
LOSS: 1.1420345306396484
LOSS: 1.1383650302886963
LOSS: 1.1348539590835571
LOSS: 1.1315021514892578
LOSS: 1.1283105611801147
LOSS: 1.125279188156128
LOSS: 1.1224076747894287
LOSS: 1.1196948289871216
LOSS: 1.1171388626098633
LOSS: 1.1147366762161255
LOSS: 1.112485408782959
LOSS: 1.1103802919387817
LOSS: 1.1084166765213013
LOSS: 1.1065888404846191
LOSS: 1.1048904657363892
LOSS: 1.1033159494400024
LOSS: 1.1018582582473755
LOSS: 1.1005117893218994
LOSS: 1.0992705821990967
LOSS: 1.098129391670227
LOSS: 1.097083568572998
LOSS: 1.0961277484893799
LOSS: 1.0952582359313965
LOSS: 1.094470739364624
LOSS: 1.0937614440917969
LOSS: 1.0931254625320435
LOSS: 1.0925594568252563
LOSS: 1.0920575857162476
LOSS: 1.0916155576705933
LOSS: 1.0912290811538696
LOSS: 1.0908926725387573
LOSS: 1.0906013250350952
LOSS: 1.0903514623641968
LOSS: 1.0901379585266113
LOSS: 1.089957356452942
LOSS: 1.0898066759109497
LOSS: 1.0896813869476318
LOSS: 1.089579463005066
LOSS: 1.0894978046417236
LOSS: 1.089434027671814
LOSS: 1.089385747909546
LOSS: 1.089350938796997
LOSS: 1.089327096939087
LOSS: 1.089312195777893
LOSS: 1.0893049240112305
LOSS: 1.089302897453308
LOSS: 1.0893052816390991
LOSS: 1.0893100500106812
LOSS: 1.0893173217773438
LOSS: 1.0893257856369019
LOSS: 1.0893352031707764
LOSS: 1.089343786239624
LOSS: 1.0893521308898926
LOSS: 1.089360237121582
LOSS: 1.0893677473068237
LOSS: 1.0893733501434326
LOSS: 1.089378833770752
LOSS: 1.0893821716308594
LOSS: 1.0893847942352295
LOSS: 1.0893861055374146
LOSS: 1.0893863439559937
LOSS: 1.0893851518630981
LOSS: 1.089382529258728
LOSS: 1.089379906654358
LOSS: 1.0893759727478027
LOSS: 1.0893722772598267
LOSS: 1.0893677473068237
LOSS: 1.0893633365631104
LOSS: 1.0893585681915283
LOSS: 1.089353322982788
LOSS: 1.089348316192627
LOSS: 1.0893434286117554
LOSS: 1.0893394947052002
LOSS: 1.0893347263336182
LOSS: 1.0893299579620361
LOSS: 1.0893263816833496
LOSS: 1.0893229246139526
LOSS: 1.0893194675445557
LOSS: 1.0893166065216064
LOSS: 1.0893139839172363
LOSS: 1.0893117189407349
LOSS: 1.089309573173523
LOSS: 1.0893075466156006
LOSS: 1.089306116104126
LOSS: 1.0893055200576782
LOSS: 1.197387456893921
LOSS: 1.1898107528686523
LOSS: 1.1825568675994873
LOSS: 1.1756547689437866
LOSS: 1.1691211462020874
LOSS: 1.1629470586776733
LOSS: 1.1570842266082764
LOSS: 1.1514544486999512
LOSS: 1.1459838151931763
LOSS: 1.1406285762786865
LOSS: 1.1353747844696045
LOSS: 1.1302235126495361
LOSS: 1.1251825094223022
LOSS: 1.1202605962753296
LOSS: 1.1154661178588867
LOSS: 1.1108068227767944
LOSS: 1.106289267539978
LOSS: 1.1019195318222046
LOSS: 1.0977023839950562
LOSS: 1.0936416387557983
LOSS: 1.08974027633667
LOSS: 1.0860010385513306
LOSS: 1.0824247598648071
LOSS: 1.07901132106781
LOSS: 1.0757598876953125
LOSS: 1.0726680755615234
LOSS: 1.0697320699691772
LOSS: 1.0669479370117188
LOSS: 1.06430983543396
LOSS: 1.061812162399292
LOSS: 1.0594482421875
LOSS: 1.0572129487991333
LOSS: 1.0550988912582397
LOSS: 1.0531026124954224
LOSS: 1.0512183904647827
LOSS: 1.049443244934082
LOSS: 1.047772765159607
LOSS: 1.046204686164856
LOSS: 1.0447360277175903
LOSS: 1.04336416721344
LOSS: 1.0420849323272705
LOSS: 1.0408953428268433
LOSS: 1.039791464805603
LOSS: 1.0387682914733887
LOSS: 1.0378217697143555
LOSS: 1.036946415901184
LOSS: 1.0361380577087402
LOSS: 1.0353928804397583
LOSS: 1.0347062349319458
LOSS: 1.0340744256973267
LOSS: 1.033494472503662
LOSS: 1.0329632759094238
LOSS: 1.0324783325195312
LOSS: 1.0320372581481934
LOSS: 1.0316362380981445
LOSS: 1.0312734842300415
LOSS: 1.0309460163116455
LOSS: 1.0306506156921387
LOSS: 1.0303863286972046
LOSS: 1.0301487445831299
LOSS: 1.0299365520477295
LOSS: 1.0297468900680542
LOSS: 1.0295778512954712
LOSS: 1.0294281244277954
LOSS: 1.029296636581421
LOSS: 1.0291801691055298
LOSS: 1.0290786027908325
LOSS: 1.0289901494979858
LOSS: 1.0289136171340942
LOSS: 1.028847575187683
LOSS: 1.028791069984436
LOSS: 1.0287436246871948
LOSS: 1.0287035703659058
LOSS: 1.0286697149276733
LOSS: 1.0286413431167603
LOSS: 1.0286173820495605
LOSS: 1.0285985469818115
LOSS: 1.0285825729370117
LOSS: 1.0285706520080566
LOSS: 1.0285612344741821
LOSS: 1.028554081916809
LOSS: 1.028549075126648
LOSS: 1.0285457372665405
LOSS: 1.0285429954528809
LOSS: 1.0285425186157227
LOSS: 1.0285418033599854
LOSS: 1.0285420417785645
LOSS: 1.0285422801971436
LOSS: 1.0285431146621704
LOSS: 1.0285446643829346
LOSS: 1.0285450220108032
LOSS: 1.0285465717315674
LOSS: 1.028547763824463
LOSS: 1.0285488367080688
LOSS: 1.0285500288009644
LOSS: 1.028550386428833
LOSS: 1.0285515785217285
LOSS: 1.0285520553588867
LOSS: 1.0285524129867554
LOSS: 1.028552770614624
LOSS: 1.217702865600586
LOSS: 1.2108701467514038
LOSS: 1.2043966054916382
LOSS: 1.1983040571212769
LOSS: 1.1925780773162842
LOSS: 1.1871466636657715
LOSS: 1.1819078922271729
LOSS: 1.1767951250076294
LOSS: 1.171788215637207
LOSS: 1.1668893098831177
LOSS: 1.1621074676513672
LOSS: 1.1574522256851196
LOSS: 1.152931571006775
LOSS: 1.1485527753829956
LOSS: 1.1443216800689697
LOSS: 1.1402432918548584
LOSS: 1.1363214254379272
LOSS: 1.1325596570968628
LOSS: 1.1289607286453247
LOSS: 1.1255263090133667
LOSS: 1.1222578287124634
LOSS: 1.1191552877426147
LOSS: 1.116218090057373
LOSS: 1.113444209098816
LOSS: 1.1108310222625732
LOSS: 1.1083743572235107
LOSS: 1.106069564819336
LOSS: 1.1039111614227295
LOSS: 1.1018937826156616
LOSS: 1.1000111103057861
LOSS: 1.0982575416564941
LOSS: 1.096627950668335
LOSS: 1.0951169729232788
LOSS: 1.0937203168869019
LOSS: 1.0924338102340698
LOSS: 1.0912531614303589
LOSS: 1.0901747941970825
LOSS: 1.0891942977905273
LOSS: 1.0883071422576904
LOSS: 1.087507963180542
LOSS: 1.0867916345596313
LOSS: 1.0861523151397705
LOSS: 1.0855846405029297
LOSS: 1.0850826501846313
LOSS: 1.0846407413482666
LOSS: 1.0842534303665161
LOSS: 1.083916425704956
LOSS: 1.0836248397827148
LOSS: 1.083375096321106
LOSS: 1.083162784576416
LOSS: 1.0829850435256958
LOSS: 1.0828375816345215
LOSS: 1.0827175378799438
LOSS: 1.0826212167739868
LOSS: 1.0825456380844116
LOSS: 1.0824881792068481
LOSS: 1.0824445486068726
LOSS: 1.082414150238037
LOSS: 1.0823936462402344
LOSS: 1.082381010055542
LOSS: 1.082375168800354
LOSS: 1.0823743343353271
LOSS: 1.082377552986145
LOSS: 1.0823839902877808
LOSS: 1.0823920965194702
LOSS: 1.0824017524719238
LOSS: 1.082411527633667
LOSS: 1.0824214220046997
LOSS: 1.0824304819107056
LOSS: 1.0824389457702637
LOSS: 1.0824464559555054
LOSS: 1.0824518203735352
LOSS: 1.082456111907959
LOSS: 1.082459568977356
LOSS: 1.0824611186981201
LOSS: 1.082461953163147
LOSS: 1.0824613571166992
LOSS: 1.0824600458145142
LOSS: 1.0824573040008545
LOSS: 1.0824543237686157
LOSS: 1.0824503898620605
LOSS: 1.0824458599090576
LOSS: 1.0824412107467651
LOSS: 1.0824360847473145
LOSS: 1.0824310779571533
LOSS: 1.082425594329834
LOSS: 1.0824202299118042
LOSS: 1.0824155807495117
LOSS: 1.0824106931686401
LOSS: 1.082405686378479
LOSS: 1.0824018716812134
LOSS: 1.0823975801467896
LOSS: 1.0295778512954712
LOSS: 1.0294281244277954
LOSS: 1.029296636581421
LOSS: 1.0291801691055298
LOSS: 1.0290786027908325
LOSS: 1.0289901494979858
LOSS: 1.0289136171340942
LOSS: 1.028847575187683
LOSS: 1.028791069984436
LOSS: 1.0287436246871948
LOSS: 1.0287035703659058
LOSS: 1.0286697149276733
LOSS: 1.0286413431167603
LOSS: 1.0286173820495605
LOSS: 1.0285985469818115
LOSS: 1.0285825729370117
LOSS: 1.0285706520080566
LOSS: 1.0285612344741821
LOSS: 1.028554081916809
LOSS: 1.028549075126648
LOSS: 1.0285457372665405
LOSS: 1.0285429954528809
LOSS: 1.0285425186157227
LOSS: 1.0285418033599854
LOSS: 1.0285420417785645
LOSS: 1.0285422801971436
LOSS: 1.0285431146621704
LOSS: 1.0285446643829346
LOSS: 1.0285450220108032
LOSS: 1.0285465717315674
LOSS: 1.028547763824463
LOSS: 1.0285488367080688
LOSS: 1.0285500288009644
LOSS: 1.028550386428833
LOSS: 1.0285515785217285
LOSS: 1.0285520553588867
LOSS: 1.0285524129867554
LOSS: 1.028552770614624
LOSS: 1.1977410316467285
LOSS: 1.1901732683181763
LOSS: 1.1829285621643066
LOSS: 1.1760294437408447
LOSS: 1.1694731712341309
LOSS: 1.1632087230682373
LOSS: 1.1571462154388428
LOSS: 1.151210069656372
LOSS: 1.145368218421936
LOSS: 1.1396195888519287
LOSS: 1.1339747905731201
LOSS: 1.1284468173980713
LOSS: 1.1230485439300537
LOSS: 1.1177911758422852
LOSS: 1.112684965133667
LOSS: 1.1077382564544678
LOSS: 1.102958083152771
LOSS: 1.0983507633209229
LOSS: 1.0939205884933472
LOSS: 1.0896711349487305
LOSS: 1.0856043100357056
LOSS: 1.0817204713821411
LOSS: 1.0780186653137207
LOSS: 1.0744969844818115
LOSS: 1.071151614189148
LOSS: 1.0679783821105957
LOSS: 1.0649728775024414
LOSS: 1.0621297359466553
LOSS: 1.059444785118103
LOSS: 1.0569136142730713
LOSS: 1.0545324087142944
LOSS: 1.0522984266281128
LOSS: 1.050208330154419
LOSS: 1.0482594966888428
LOSS: 1.0464483499526978
LOSS: 1.0447713136672974
LOSS: 1.04322350025177
LOSS: 1.0417999029159546
LOSS: 1.0404945611953735
LOSS: 1.0393013954162598
LOSS: 1.0382134914398193
LOSS: 1.0372248888015747
LOSS: 1.0363293886184692
LOSS: 1.0355217456817627
LOSS: 1.0347964763641357
LOSS: 1.0341483354568481
LOSS: 1.0335724353790283
LOSS: 1.0330644845962524
LOSS: 1.032619595527649
LOSS: 1.0322321653366089
LOSS: 1.0318971872329712
LOSS: 1.0316098928451538
LOSS: 1.0313652753829956
LOSS: 1.031158685684204
LOSS: 1.0309864282608032
LOSS: 1.0308443307876587
LOSS: 1.0307279825210571
LOSS: 1.030634880065918
LOSS: 1.0305629968643188
LOSS: 1.030508279800415
LOSS: 1.0304685831069946
LOSS: 1.0304419994354248
LOSS: 1.03042471408844
LOSS: 1.0304162502288818
LOSS: 1.0304133892059326
LOSS: 1.0304162502288818
LOSS: 1.030422329902649
LOSS: 1.030430555343628
LOSS: 1.030439853668213
LOSS: 1.0304502248764038
LOSS: 1.0304608345031738
LOSS: 1.0304712057113647
LOSS: 1.0304805040359497
LOSS: 1.0304887294769287
LOSS: 1.0304956436157227
LOSS: 1.0305018424987793
LOSS: 1.0305050611495972
LOSS: 1.0305079221725464
LOSS: 1.0305094718933105
LOSS: 1.0305088758468628
LOSS: 1.0305078029632568
LOSS: 1.0305052995681763
LOSS: 1.0305019617080688
LOSS: 1.0304982662200928
LOSS: 1.0304934978485107
LOSS: 1.0304886102676392
LOSS: 1.0304830074310303
LOSS: 1.03047776222229
LOSS: 1.0304720401763916
LOSS: 1.0304665565490723
LOSS: 1.0304608345031738
LOSS: 1.0304553508758545
LOSS: 1.0304499864578247
LOSS: 1.0304458141326904
LOSS: 1.030441164970398
LOSS: 1.0304369926452637
LOSS: 1.030433177947998
LOSS: 1.0304299592971802
LOSS: 1.0304267406463623
LOSS: 1.0304242372512817
LOSS: 1.2195827960968018
LOSS: 1.212821125984192
LOSS: 1.206422209739685
LOSS: 1.2004083395004272
LOSS: 1.1947664022445679
LOSS: 1.1894261837005615
LOSS: 1.1842880249023438
LOSS: 1.1792868375778198
LOSS: 1.174403429031372
LOSS: 1.1696418523788452
LOSS: 1.1650121212005615
LOSS: 1.1605255603790283
LOSS: 1.1561918258666992
LOSS: 1.1520189046859741
LOSS: 1.1480134725570679
LOSS: 1.1441813707351685
LOSS: 1.1405268907546997
LOSS: 1.1370530128479004
LOSS: 1.133762240409851
LOSS: 1.1306558847427368
LOSS: 1.127733588218689
LOSS: 1.1249943971633911
LOSS: 1.1224360466003418
LOSS: 1.1200544834136963
LOSS: 1.117845058441162
LOSS: 1.1158020496368408
LOSS: 1.1139189004898071
LOSS: 1.1121879816055298
LOSS: 1.1106020212173462
LOSS: 1.1091539859771729
LOSS: 1.1078356504440308
LOSS: 1.106641173362732
LOSS: 1.1055630445480347
LOSS: 1.10459566116333
LOSS: 1.1037328243255615
LOSS: 1.102968692779541
LOSS: 1.102297067642212
LOSS: 1.1017119884490967
LOSS: 1.1012060642242432
LOSS: 1.1007734537124634
LOSS: 1.100406289100647
LOSS: 1.1000988483428955
LOSS: 1.0998437404632568
LOSS: 1.0996344089508057
LOSS: 1.0994656085968018
LOSS: 1.0993317365646362
LOSS: 1.0992282629013062
LOSS: 1.0991510152816772
LOSS: 1.0990957021713257
LOSS: 1.0990592241287231
LOSS: 1.0990381240844727
LOSS: 1.0990302562713623
LOSS: 1.099031925201416
LOSS: 1.0990405082702637
LOSS: 1.0990546941757202
LOSS: 1.099071979522705
LOSS: 1.0990904569625854
LOSS: 1.0991095304489136
LOSS: 1.0991275310516357
LOSS: 1.0991439819335938
LOSS: 1.0991584062576294
LOSS: 1.0991703271865845
LOSS: 1.0991804599761963
LOSS: 1.0991877317428589
LOSS: 1.0991926193237305
LOSS: 1.099195122718811
LOSS: 1.0991953611373901
LOSS: 1.0991930961608887
LOSS: 1.0991888046264648
LOSS: 1.099183201789856
LOSS: 1.0991761684417725
LOSS: 1.0991674661636353
LOSS: 1.0991582870483398
LOSS: 1.0991485118865967
LOSS: 1.0991379022598267
LOSS: 1.0991277694702148
LOSS: 1.099117636680603
LOSS: 1.0991076231002808
LOSS: 1.0990982055664062
LOSS: 1.09908926486969
LOSS: 1.099081039428711
LOSS: 1.0990735292434692
LOSS: 1.0990660190582275
LOSS: 1.0990599393844604
LOSS: 1.0990543365478516
LOSS: 1.0990492105484009
LOSS: 1.0990450382232666
LOSS: 1.099041223526001
LOSS: 1.0990387201309204
LOSS: 1.0990358591079712
LOSS: 1.099034309387207
LOSS: 1.0990325212478638
LOSS: 1.099031686782837
LOSS: 1.0990307331085205
LOSS: 1.0990303754806519
LOSS: 1.0990301370620728
LOSS: 1.0990298986434937
LOSS: 1.0990298986434937
LOSS: 1.0990296602249146
LOSS: 1.0990300178527832
LOSS: 1.1880083084106445
LOSS: 1.1801010370254517
LOSS: 1.1725003719329834
LOSS: 1.1652356386184692
LOSS: 1.1583278179168701
LOSS: 1.1517802476882935
LOSS: 1.1455659866333008
LOSS: 1.1396262645721436
LOSS: 1.1338893175125122
LOSS: 1.1282998323440552
LOSS: 1.1228315830230713
LOSS: 1.1174802780151367
LOSS: 1.1122533082962036
LOSS: 1.1071621179580688
LOSS: 1.1022189855575562
LOSS: 1.097435474395752
LOSS: 1.0928218364715576
LOSS: 1.0883866548538208
LOSS: 1.0841370820999146
LOSS: 1.0800789594650269
LOSS: 1.0762155055999756
LOSS: 1.0725494623184204
LOSS: 1.0690808296203613
LOSS: 1.0658085346221924
LOSS: 1.0627291202545166
LOSS: 1.0598382949829102
LOSS: 1.057129979133606
LOSS: 1.0545967817306519
LOSS: 1.0522311925888062
LOSS: 1.0500253438949585
LOSS: 1.0479705333709717
LOSS: 1.0460600852966309
LOSS: 1.0442866086959839
LOSS: 1.0426442623138428
LOSS: 1.0411275625228882
LOSS: 1.0397312641143799
LOSS: 1.038450837135315
LOSS: 1.0372811555862427
LOSS: 1.0362170934677124
LOSS: 1.035253882408142
LOSS: 1.0343847274780273
LOSS: 1.0336036682128906
LOSS: 1.0329041481018066
LOSS: 1.0322794914245605
LOSS: 1.0317232608795166
LOSS: 1.0312297344207764
LOSS: 1.0307931900024414
LOSS: 1.0304089784622192
LOSS: 1.0300720930099487
LOSS: 1.0297788381576538
LOSS: 1.0295251607894897
LOSS: 1.0293083190917969
LOSS: 1.02912437915802
LOSS: 1.0289702415466309
LOSS: 1.0288423299789429
LOSS: 1.0287370681762695
LOSS: 1.0286521911621094
LOSS: 1.0285840034484863
LOSS: 1.0285303592681885
LOSS: 1.028489112854004
LOSS: 1.0284578800201416
LOSS: 1.0284357070922852
LOSS: 1.028420090675354
LOSS: 1.028410792350769
LOSS: 1.028406023979187
LOSS: 1.028405785560608
LOSS: 1.0284080505371094
LOSS: 1.028412938117981
LOSS: 1.0284186601638794
LOSS: 1.0284254550933838
LOSS: 1.028432846069336
LOSS: 1.0284397602081299
LOSS: 1.0284461975097656
LOSS: 1.0284515619277954
LOSS: 1.0284563302993774
LOSS: 1.028460144996643
LOSS: 1.028463363647461
LOSS: 1.0284662246704102
LOSS: 1.0284680128097534
LOSS: 1.0284686088562012
LOSS: 1.0284686088562012
LOSS: 1.0284678936004639
LOSS: 1.0284667015075684
LOSS: 1.0284651517868042
LOSS: 1.0284626483917236
LOSS: 1.0284600257873535
LOSS: 1.0284565687179565
LOSS: 1.0284533500671387
LOSS: 1.0284497737884521
LOSS: 1.028445839881897
LOSS: 1.0284429788589478
LOSS: 1.0284395217895508
LOSS: 1.0532951354980469
LOSS: 1.0532965660095215
LOSS: 1.0533010959625244
LOSS: 1.0533076524734497
LOSS: 1.0533148050308228
LOSS: 1.0533233880996704
LOSS: 1.053331732749939
LOSS: 1.0533392429351807
LOSS: 1.0533467531204224
LOSS: 1.053353190422058
LOSS: 1.0533586740493774
LOSS: 1.0533630847930908
LOSS: 1.0533664226531982
LOSS: 1.053369402885437
LOSS: 1.0533703565597534
LOSS: 1.0533705949783325
LOSS: 1.0533697605133057
LOSS: 1.0533688068389893
LOSS: 1.0533668994903564
LOSS: 1.0533639192581177
LOSS: 1.0533604621887207
LOSS: 1.0533570051193237
LOSS: 1.0533528327941895
LOSS: 1.0533485412597656
LOSS: 1.0533443689346313
LOSS: 1.0533397197723389
LOSS: 1.0533360242843628
LOSS: 1.0533318519592285
LOSS: 1.0533274412155151
LOSS: 1.05332350730896
LOSS: 1.053320288658142
LOSS: 1.0533170700073242
LOSS: 1.053313970565796
LOSS: 1.0533108711242676
LOSS: 1.053308367729187
LOSS: 1.053305983543396
LOSS: 1.0533039569854736
LOSS: 1.2102243900299072
LOSS: 1.2031654119491577
LOSS: 1.1964552402496338
LOSS: 1.1901239156723022
LOSS: 1.184188723564148
LOSS: 1.1786390542984009
LOSS: 1.1734237670898438
LOSS: 1.1684622764587402
LOSS: 1.1636826992034912
LOSS: 1.1590452194213867
LOSS: 1.154537558555603
LOSS: 1.1501612663269043
LOSS: 1.145922303199768
LOSS: 1.1418278217315674
LOSS: 1.1378836631774902
LOSS: 1.1340950727462769
LOSS: 1.130466103553772
LOSS: 1.1269999742507935
LOSS: 1.1236984729766846
LOSS: 1.1205637454986572
LOSS: 1.1175963878631592
LOSS: 1.1147966384887695
LOSS: 1.1121641397476196
LOSS: 1.1096972227096558
LOSS: 1.1073942184448242
LOSS: 1.105251431465149
LOSS: 1.1032648086547852
LOSS: 1.1014292240142822
LOSS: 1.0997381210327148
LOSS: 1.0981853008270264
LOSS: 1.096763014793396
LOSS: 1.095463514328003
LOSS: 1.0942796468734741
LOSS: 1.0932035446166992
LOSS: 1.0922285318374634
LOSS: 1.0913479328155518
LOSS: 1.0905563831329346
LOSS: 1.0898476839065552
LOSS: 1.0892174243927002
LOSS: 1.0886597633361816
LOSS: 1.0881718397140503
LOSS: 1.0877478122711182
LOSS: 1.0873831510543823
LOSS: 1.087072491645813
LOSS: 1.0868111848831177
LOSS: 1.0865941047668457
LOSS: 1.0864157676696777
LOSS: 1.086271047592163
LOSS: 1.0861557722091675
LOSS: 1.086065649986267
LOSS: 1.085996389389038
LOSS: 1.0859447717666626
LOSS: 1.08590829372406
LOSS: 1.0858840942382812
LOSS: 1.085870385169983
LOSS: 1.0858650207519531
LOSS: 1.0858654975891113
LOSS: 1.0858718156814575
LOSS: 1.0858818292617798
LOSS: 1.0858943462371826
LOSS: 1.0859076976776123
LOSS: 1.0859218835830688
LOSS: 1.085935354232788
LOSS: 1.0859475135803223
LOSS: 1.0859582424163818
LOSS: 1.085966944694519
LOSS: 1.0859737396240234
LOSS: 1.0859787464141846
LOSS: 1.0859814882278442
LOSS: 1.0859830379486084
LOSS: 1.0859824419021606
LOSS: 1.085981011390686
LOSS: 1.085977554321289
LOSS: 1.0859736204147339
LOSS: 1.0859686136245728
LOSS: 1.0859627723693848
LOSS: 1.0859568119049072
LOSS: 1.0859498977661133
LOSS: 1.0859428644180298
LOSS: 1.0859354734420776
LOSS: 1.085928201675415
LOSS: 1.085921049118042
LOSS: 1.0859137773513794
LOSS: 1.0859074592590332
LOSS: 1.0859017372131348
LOSS: 1.0858955383300781
LOSS: 1.0858910083770752
LOSS: 1.085886001586914
LOSS: 1.0858821868896484
LOSS: 1.0858783721923828
LOSS: 1.085875391960144
LOSS: 1.0858720541000366
LOSS: 1.0858700275421143
LOSS: 1.085868239402771
LOSS: 1.0858662128448486
LOSS: 1.0858650207519531
LOSS: 1.0858641862869263
LOSS: 1.0858629941940308
LOSS: 1.0858628749847412
LOSS: 1.085862398147583
LOSS: 1.2102243900299072
LOSS: 1.2031654119491577
LOSS: 1.1964552402496338
LOSS: 1.1901239156723022
LOSS: 1.184188723564148
LOSS: 1.1786390542984009
LOSS: 1.1734237670898438
LOSS: 1.1684622764587402
LOSS: 1.1636826992034912
LOSS: 1.1590452194213867
LOSS: 1.154537558555603
LOSS: 1.1501612663269043
LOSS: 1.145922303199768
LOSS: 1.1418278217315674
LOSS: 1.1378836631774902
LOSS: 1.1340950727462769
LOSS: 1.130466103553772
LOSS: 1.1269999742507935
LOSS: 1.1236984729766846
LOSS: 1.1205637454986572
LOSS: 1.1175963878631592
LOSS: 1.1147966384887695
LOSS: 1.1121641397476196
LOSS: 1.1096972227096558
LOSS: 1.1073942184448242
LOSS: 1.105251431465149
LOSS: 1.1032648086547852
LOSS: 1.1014292240142822
LOSS: 1.0997381210327148
LOSS: 1.0981853008270264
LOSS: 1.096763014793396
LOSS: 1.095463514328003
LOSS: 1.0942796468734741
LOSS: 1.0932035446166992
LOSS: 1.0922285318374634
LOSS: 1.0913479328155518
LOSS: 1.0905563831329346
LOSS: 1.0898476839065552
LOSS: 1.0892174243927002
LOSS: 1.0886597633361816
LOSS: 1.0881718397140503
LOSS: 1.0877478122711182
LOSS: 1.0873831510543823
LOSS: 1.087072491645813
LOSS: 1.0868111848831177
LOSS: 1.0865941047668457
LOSS: 1.0864157676696777
LOSS: 1.086271047592163
LOSS: 1.0861557722091675
LOSS: 1.086065649986267
LOSS: 1.085996389389038
LOSS: 1.0859447717666626
LOSS: 1.08590829372406
LOSS: 1.0858840942382812
LOSS: 1.085870385169983
LOSS: 1.0858650207519531
LOSS: 1.0858654975891113
LOSS: 1.0858718156814575
LOSS: 1.0858818292617798
LOSS: 1.0858943462371826
LOSS: 1.0859076976776123
LOSS: 1.0859218835830688
LOSS: 1.085935354232788
LOSS: 1.0859475135803223
LOSS: 1.0859582424163818
LOSS: 1.085966944694519
LOSS: 1.0859737396240234
LOSS: 1.0859787464141846
LOSS: 1.0859814882278442
LOSS: 1.0859830379486084
LOSS: 1.0859824419021606
LOSS: 1.085981011390686
LOSS: 1.085977554321289
LOSS: 1.0859736204147339
LOSS: 1.0859686136245728
LOSS: 1.0859627723693848
LOSS: 1.0859568119049072
LOSS: 1.0859498977661133
LOSS: 1.0859428644180298
LOSS: 1.0859354734420776
LOSS: 1.085928201675415
LOSS: 1.085921049118042
LOSS: 1.0859137773513794
LOSS: 1.0859074592590332
LOSS: 1.0859017372131348
LOSS: 1.0858955383300781
LOSS: 1.0858910083770752
LOSS: 1.085886001586914
LOSS: 1.0858821868896484
LOSS: 1.0858783721923828
LOSS: 1.085875391960144
LOSS: 1.0858720541000366
LOSS: 1.0858700275421143
LOSS: 1.085868239402771
LOSS: 1.0858662128448486
LOSS: 1.0858650207519531
LOSS: 1.0858641862869263
LOSS: 1.0858629941940308
LOSS: 1.0858628749847412
LOSS: 1.085862398147583
LOSS: 1.2092952728271484
LOSS: 1.2021939754486084
LOSS: 1.195439338684082
LOSS: 1.1890618801116943
LOSS: 1.1830803155899048
LOSS: 1.1774874925613403
LOSS: 1.172237753868103
LOSS: 1.167253017425537
LOSS: 1.162458062171936
LOSS: 1.1578067541122437
LOSS: 1.1532822847366333
LOSS: 1.1488828659057617
LOSS: 1.1446131467819214
LOSS: 1.1404790878295898
LOSS: 1.1364864110946655
LOSS: 1.132639765739441
LOSS: 1.1289433240890503
LOSS: 1.1254000663757324
LOSS: 1.122012734413147
LOSS: 1.1187833547592163
LOSS: 1.115713119506836
LOSS: 1.1128032207489014
LOSS: 1.1100538969039917
LOSS: 1.1074647903442383
LOSS: 1.1050348281860352
LOSS: 1.1027618646621704
LOSS: 1.1006425619125366
LOSS: 1.0986729860305786
LOSS: 1.0968481302261353
LOSS: 1.0951619148254395
LOSS: 1.0936079025268555
LOSS: 1.0921790599822998
LOSS: 1.0908682346343994
LOSS: 1.0896681547164917
LOSS: 1.0885717868804932
LOSS: 1.0875728130340576
LOSS: 1.086665153503418
LOSS: 1.0858434438705444
LOSS: 1.0851025581359863
LOSS: 1.0844380855560303
LOSS: 1.0838449001312256
LOSS: 1.0833197832107544
LOSS: 1.0828583240509033
LOSS: 1.0824558734893799
LOSS: 1.0821069478988647
LOSS: 1.08180832862854
LOSS: 1.081554651260376
LOSS: 1.0813407897949219
LOSS: 1.0811625719070435
LOSS: 1.08101487159729
LOSS: 1.0808945894241333
LOSS: 1.0807976722717285
LOSS: 1.0807204246520996
LOSS: 1.0806599855422974
LOSS: 1.0806142091751099
LOSS: 1.0805813074111938
LOSS: 1.0805584192276
LOSS: 1.0805445909500122
LOSS: 1.0805377960205078
LOSS: 1.080536961555481
LOSS: 1.080540418624878
LOSS: 1.0805472135543823
LOSS: 1.0805561542510986
LOSS: 1.080566167831421
LOSS: 1.0805764198303223
LOSS: 1.080586314201355
LOSS: 1.0805959701538086
LOSS: 1.0806045532226562
LOSS: 1.0806111097335815
LOSS: 1.08061683177948
LOSS: 1.0806210041046143
LOSS: 1.0806244611740112
LOSS: 1.0806257724761963
LOSS: 1.0806266069412231
LOSS: 1.080626130104065
LOSS: 1.0806248188018799
LOSS: 1.0806224346160889
LOSS: 1.08061945438385
LOSS: 1.080615758895874
LOSS: 1.0806114673614502
LOSS: 1.0806068181991577
LOSS: 1.080601692199707
LOSS: 1.0805962085723877
LOSS: 1.080590844154358
LOSS: 1.0805858373641968
LOSS: 1.0805809497833252
LOSS: 1.0805754661560059
LOSS: 1.0805704593658447
LOSS: 1.0805662870407104
LOSS: 1.0805621147155762
LOSS: 1.0805584192276
LOSS: 1.0805548429489136
LOSS: 1.0805518627166748
LOSS: 1.0805493593215942
LOSS: 1.0854257345199585
LOSS: 1.0854244232177734
LOSS: 1.0854233503341675
LOSS: 1.085422158241272
LOSS: 1.0854207277297974
LOSS: 1.085418462753296
LOSS: 1.0854170322418213
LOSS: 1.0854158401489258
LOSS: 1.0854142904281616
LOSS: 1.2092952728271484
LOSS: 1.2021939754486084
LOSS: 1.195439338684082
LOSS: 1.1890618801116943
LOSS: 1.1830803155899048
LOSS: 1.1774874925613403
LOSS: 1.172237753868103
LOSS: 1.167253017425537
LOSS: 1.162458062171936
LOSS: 1.1578067541122437
LOSS: 1.1532822847366333
LOSS: 1.1488828659057617
LOSS: 1.1446131467819214
LOSS: 1.1404790878295898
LOSS: 1.1364864110946655
LOSS: 1.132639765739441
LOSS: 1.1289433240890503
LOSS: 1.1254000663757324
LOSS: 1.122012734413147
LOSS: 1.1187833547592163
LOSS: 1.115713119506836
LOSS: 1.1128032207489014
LOSS: 1.1100538969039917
LOSS: 1.1074647903442383
LOSS: 1.1050348281860352
LOSS: 1.1027618646621704
LOSS: 1.1006425619125366
LOSS: 1.0986729860305786
LOSS: 1.0968481302261353
LOSS: 1.0951619148254395
LOSS: 1.0936079025268555
LOSS: 1.0921790599822998
LOSS: 1.0908682346343994
LOSS: 1.0896681547164917
LOSS: 1.0885717868804932
LOSS: 1.0875728130340576
LOSS: 1.086665153503418
LOSS: 1.0858434438705444
LOSS: 1.0851025581359863
LOSS: 1.0844380855560303
LOSS: 1.0838449001312256
LOSS: 1.0833197832107544
LOSS: 1.0828583240509033
LOSS: 1.0824558734893799
LOSS: 1.0821069478988647
LOSS: 1.08180832862854
LOSS: 1.081554651260376
LOSS: 1.0813407897949219
LOSS: 1.0811625719070435
LOSS: 1.08101487159729
LOSS: 1.0808945894241333
LOSS: 1.0807976722717285
LOSS: 1.0807204246520996
LOSS: 1.0806599855422974
LOSS: 1.0806142091751099
LOSS: 1.0805813074111938
LOSS: 1.0805584192276
LOSS: 1.0805445909500122
LOSS: 1.0805377960205078
LOSS: 1.080536961555481
LOSS: 1.080540418624878
LOSS: 1.0805472135543823
LOSS: 1.0805561542510986
LOSS: 1.080566167831421
LOSS: 1.0805764198303223
LOSS: 1.080586314201355
LOSS: 1.0805959701538086
LOSS: 1.0806045532226562
LOSS: 1.0806111097335815
LOSS: 1.08061683177948
LOSS: 1.0806210041046143
LOSS: 1.0806244611740112
LOSS: 1.0806257724761963
LOSS: 1.0806266069412231
LOSS: 1.080626130104065
LOSS: 1.0806248188018799
LOSS: 1.0806224346160889
LOSS: 1.08061945438385
LOSS: 1.080615758895874
LOSS: 1.0806114673614502
LOSS: 1.0806068181991577
LOSS: 1.080601692199707
LOSS: 1.0805962085723877
LOSS: 1.080590844154358
LOSS: 1.0805858373641968
LOSS: 1.0805809497833252
LOSS: 1.0805754661560059
LOSS: 1.0805704593658447
LOSS: 1.0805662870407104
LOSS: 1.0805621147155762
LOSS: 1.0805584192276
LOSS: 1.0805548429489136
LOSS: 1.0805518627166748
LOSS: 1.0805493593215942
LOSS: 1.0805468559265137
LOSS: 1.0805447101593018
LOSS: 1.0805429220199585
LOSS: 1.0805408954620361
LOSS: 1.0805399417877197
LOSS: 1.0805391073226929
LOSS: 1.205596685409546
LOSS: 1.1983304023742676
LOSS: 1.1914024353027344
LOSS: 1.184841275215149
LOSS: 1.178659439086914
LOSS: 1.1728378534317017
LOSS: 1.1673141717910767
LOSS: 1.1620051860809326
LOSS: 1.1568479537963867
LOSS: 1.15181565284729
LOSS: 1.1469051837921143
LOSS: 1.1421245336532593
LOSS: 1.1374839544296265
LOSS: 1.132993459701538
LOSS: 1.1286622285842896
LOSS: 1.124497413635254
LOSS: 1.120505690574646
LOSS: 1.1166915893554688
LOSS: 1.1130588054656982
LOSS: 1.109609842300415
LOSS: 1.1063462495803833
LOSS: 1.103267788887024
LOSS: 1.1003732681274414
LOSS: 1.0976601839065552
LOSS: 1.09512460231781
LOSS: 1.0927618741989136
LOSS: 1.090565800666809
LOSS: 1.0885292291641235
LOSS: 1.0866446495056152
LOSS: 1.084904670715332
LOSS: 1.0833017826080322
LOSS: 1.0818283557891846
LOSS: 1.0804775953292847
LOSS: 1.0792431831359863
LOSS: 1.0781192779541016
LOSS: 1.077100396156311
LOSS: 1.0761812925338745
LOSS: 1.07535719871521
LOSS: 1.074622392654419
LOSS: 1.0739713907241821
LOSS: 1.073398470878601
LOSS: 1.0728973150253296
LOSS: 1.0724620819091797
LOSS: 1.072086215019226
LOSS: 1.0717636346817017
LOSS: 1.0714889764785767
LOSS: 1.0712569952011108
LOSS: 1.0710625648498535
LOSS: 1.07090163230896
LOSS: 1.0707708597183228
LOSS: 1.070665955543518
LOSS: 1.0705840587615967
LOSS: 1.070521593093872
LOSS: 1.07047700881958
LOSS: 1.0704467296600342
LOSS: 1.0704278945922852
LOSS: 1.0704184770584106
LOSS: 1.0704165697097778
LOSS: 1.0704199075698853
LOSS: 1.0704270601272583
LOSS: 1.0704363584518433
LOSS: 1.0704469680786133
LOSS: 1.0704584121704102
LOSS: 1.0704699754714966
LOSS: 1.0704816579818726
LOSS: 1.0704914331436157
LOSS: 1.0705010890960693
LOSS: 1.0705089569091797
LOSS: 1.070515513420105
LOSS: 1.0705201625823975
LOSS: 1.0705235004425049
LOSS: 1.0705251693725586
LOSS: 1.070525050163269
LOSS: 1.0705230236053467
LOSS: 1.0705206394195557
LOSS: 1.0705169439315796
LOSS: 1.070512056350708
LOSS: 1.0705068111419678
LOSS: 1.0705013275146484
LOSS: 1.0704947710037231
LOSS: 1.0704883337020874
LOSS: 1.0704818964004517
LOSS: 1.070475459098816
LOSS: 1.070469617843628
LOSS: 1.0704632997512817
LOSS: 1.0704575777053833
LOSS: 1.0704516172409058
LOSS: 1.070446491241455
LOSS: 1.0704420804977417
LOSS: 1.0704374313354492
LOSS: 1.070433497428894
LOSS: 1.0704305171966553
LOSS: 1.0704272985458374
LOSS: 1.0704245567321777
LOSS: 1.0704225301742554
LOSS: 1.070420503616333
LOSS: 1.070419192314148
LOSS: 1.070417881011963
LOSS: 1.0704169273376465
LOSS: 1.0704162120819092
LOSS: 1.2088813781738281
LOSS: 1.2017759084701538
LOSS: 1.1950167417526245
LOSS: 1.188631534576416
LOSS: 1.1826289892196655
LOSS: 1.1769797801971436
LOSS: 1.1716102361679077
LOSS: 1.1664342880249023
LOSS: 1.1613965034484863
LOSS: 1.1564770936965942
LOSS: 1.1516766548156738
LOSS: 1.1470019817352295
LOSS: 1.1424620151519775
LOSS: 1.1380642652511597
LOSS: 1.1338154077529907
LOSS: 1.1297205686569214
LOSS: 1.1257843971252441
LOSS: 1.122010350227356
LOSS: 1.1184008121490479
LOSS: 1.1149580478668213
LOSS: 1.1116831302642822
LOSS: 1.108576774597168
LOSS: 1.1056386232376099
LOSS: 1.102867603302002
LOSS: 1.1002614498138428
LOSS: 1.0978169441223145
LOSS: 1.0955301523208618
LOSS: 1.0933960676193237
LOSS: 1.0914087295532227
LOSS: 1.089561939239502
LOSS: 1.087849497795105
LOSS: 1.0862648487091064
LOSS: 1.0848013162612915
LOSS: 1.0834534168243408
LOSS: 1.0822155475616455
LOSS: 1.0810824632644653
LOSS: 1.0800498723983765
LOSS: 1.0791130065917969
LOSS: 1.078266978263855
LOSS: 1.0775079727172852
LOSS: 1.0768312215805054
LOSS: 1.0762310028076172
LOSS: 1.0757029056549072
LOSS: 1.0752402544021606
LOSS: 1.074838638305664
LOSS: 1.0744913816452026
LOSS: 1.0741937160491943
LOSS: 1.0739405155181885
LOSS: 1.0737260580062866
LOSS: 1.0735479593276978
LOSS: 1.0733999013900757
LOSS: 1.0732797384262085
LOSS: 1.0731838941574097
LOSS: 1.0731096267700195
LOSS: 1.0730537176132202
LOSS: 1.073013424873352
LOSS: 1.0729867219924927
LOSS: 1.0729707479476929
LOSS: 1.0729634761810303
LOSS: 1.072962760925293
LOSS: 1.0729669332504272
LOSS: 1.072974681854248
LOSS: 1.072984218597412
LOSS: 1.072994589805603
LOSS: 1.0730063915252686
LOSS: 1.0730174779891968
LOSS: 1.0730279684066772
LOSS: 1.0730373859405518
LOSS: 1.073046088218689
LOSS: 1.0730531215667725
LOSS: 1.0730587244033813
LOSS: 1.0730630159378052
LOSS: 1.0730657577514648
LOSS: 1.0730663537979126
LOSS: 1.073066234588623
LOSS: 1.0730645656585693
LOSS: 1.0730615854263306
LOSS: 1.0730574131011963
LOSS: 1.0730527639389038
LOSS: 1.0730470418930054
LOSS: 1.073041319847107
LOSS: 1.073035478591919
LOSS: 1.0730292797088623
LOSS: 1.0730234384536743
LOSS: 1.0730167627334595
LOSS: 1.0730104446411133
LOSS: 1.073004961013794
LOSS: 1.0729995965957642
LOSS: 1.072994351387024
LOSS: 1.072989583015442
LOSS: 1.072985291481018
LOSS: 1.0729812383651733
LOSS: 1.0729780197143555
LOSS: 1.072974443435669
LOSS: 1.0729718208312988
LOSS: 1.0729691982269287
LOSS: 1.0729676485061646
LOSS: 1.0729660987854004
LOSS: 1.0729645490646362
LOSS: 1.0729634761810303
LOSS: 1.2105740308761597
LOSS: 1.2035728693008423
LOSS: 1.1969233751296997
LOSS: 1.1906533241271973
LOSS: 1.1847724914550781
LOSS: 1.179253101348877
LOSS: 1.1740217208862305
LOSS: 1.1689916849136353
LOSS: 1.164104700088501
LOSS: 1.1593384742736816
LOSS: 1.154690146446228
LOSS: 1.15016508102417
LOSS: 1.1457687616348267
LOSS: 1.141506552696228
LOSS: 1.1373827457427979
LOSS: 1.1333999633789062
LOSS: 1.1295608282089233
LOSS: 1.1258662939071655
LOSS: 1.1223175525665283
LOSS: 1.1189154386520386
LOSS: 1.1156598329544067
LOSS: 1.112550973892212
LOSS: 1.028436303138733
LOSS: 1.028432846069336
LOSS: 1.028429627418518
LOSS: 1.0284266471862793
LOSS: 1.0284239053726196
LOSS: 1.028421401977539
LOSS: 1.0284185409545898
LOSS: 1.0284167528152466
LOSS: 1.205596685409546
LOSS: 1.1983304023742676
LOSS: 1.1914024353027344
LOSS: 1.184841275215149
LOSS: 1.178659439086914
LOSS: 1.1728378534317017
LOSS: 1.1673141717910767
LOSS: 1.1620051860809326
LOSS: 1.1568479537963867
LOSS: 1.15181565284729
LOSS: 1.1469051837921143
LOSS: 1.1421245336532593
LOSS: 1.1374839544296265
LOSS: 1.132993459701538
LOSS: 1.1286622285842896
LOSS: 1.124497413635254
LOSS: 1.120505690574646
LOSS: 1.1166915893554688
LOSS: 1.1130588054656982
LOSS: 1.109609842300415
LOSS: 1.1063462495803833
LOSS: 1.103267788887024
LOSS: 1.1003732681274414
LOSS: 1.0976601839065552
LOSS: 1.09512460231781
LOSS: 1.0927618741989136
LOSS: 1.090565800666809
LOSS: 1.0885292291641235
LOSS: 1.0866446495056152
LOSS: 1.084904670715332
LOSS: 1.0833017826080322
LOSS: 1.0818283557891846
LOSS: 1.0804775953292847
LOSS: 1.0792431831359863
LOSS: 1.0781192779541016
LOSS: 1.077100396156311
LOSS: 1.0761812925338745
LOSS: 1.07535719871521
LOSS: 1.074622392654419
LOSS: 1.0739713907241821
LOSS: 1.073398470878601
LOSS: 1.0728973150253296
LOSS: 1.0724620819091797
LOSS: 1.072086215019226
LOSS: 1.0717636346817017
LOSS: 1.0714889764785767
LOSS: 1.0712569952011108
LOSS: 1.0710625648498535
LOSS: 1.07090163230896
LOSS: 1.0707708597183228
LOSS: 1.070665955543518
LOSS: 1.0705840587615967
LOSS: 1.070521593093872
LOSS: 1.07047700881958
LOSS: 1.0704467296600342
LOSS: 1.0704278945922852
LOSS: 1.0704184770584106
LOSS: 1.0704165697097778
LOSS: 1.0704199075698853
LOSS: 1.0704270601272583
LOSS: 1.0704363584518433
LOSS: 1.0704469680786133
LOSS: 1.0704584121704102
LOSS: 1.0704699754714966
LOSS: 1.0704816579818726
LOSS: 1.0704914331436157
LOSS: 1.0705010890960693
LOSS: 1.0705089569091797
LOSS: 1.070515513420105
LOSS: 1.0705201625823975
LOSS: 1.0705235004425049
LOSS: 1.0705251693725586
LOSS: 1.070525050163269
LOSS: 1.0705230236053467
LOSS: 1.0705206394195557
LOSS: 1.0705169439315796
LOSS: 1.070512056350708
LOSS: 1.0705068111419678
LOSS: 1.0705013275146484
LOSS: 1.0704947710037231
LOSS: 1.0704883337020874
LOSS: 1.0704818964004517
LOSS: 1.070475459098816
LOSS: 1.070469617843628
LOSS: 1.0704632997512817
LOSS: 1.0704575777053833
LOSS: 1.0704516172409058
LOSS: 1.070446491241455
LOSS: 1.0704420804977417
LOSS: 1.0704374313354492
LOSS: 1.070433497428894
LOSS: 1.0704305171966553
LOSS: 1.0704272985458374
LOSS: 1.0704245567321777
LOSS: 1.0704225301742554
LOSS: 1.070420503616333
LOSS: 1.070419192314148
LOSS: 1.070417881011963
LOSS: 1.0704169273376465
LOSS: 1.0704162120819092
LOSS: 1.2230162620544434
LOSS: 1.2164080142974854
LOSS: 1.2101693153381348
LOSS: 1.2043133974075317
LOSS: 1.1987979412078857
LOSS: 1.1935181617736816
LOSS: 1.1883808374404907
LOSS: 1.1833522319793701
LOSS: 1.1784312725067139
LOSS: 1.1736271381378174
LOSS: 1.1689485311508179
LOSS: 1.1644033193588257
LOSS: 1.1599974632263184
LOSS: 1.1557352542877197
LOSS: 1.151620626449585
LOSS: 1.1476553678512573
LOSS: 1.1438418626785278
LOSS: 1.1401811838150024
LOSS: 1.1366736888885498
LOSS: 1.1333198547363281
LOSS: 1.1301188468933105
LOSS: 1.1270694732666016
LOSS: 1.1241700649261475
LOSS: 1.1214179992675781
LOSS: 1.1188104152679443
LOSS: 1.1163434982299805
LOSS: 1.1140127182006836
LOSS: 1.1118135452270508
LOSS: 1.1097412109375
LOSS: 1.1077903509140015
LOSS: 1.1059561967849731
LOSS: 1.104233741760254
LOSS: 1.1026188135147095
LOSS: 1.101107120513916
LOSS: 1.0996949672698975
LOSS: 1.09837806224823
LOSS: 1.0971537828445435
LOSS: 1.096017837524414
LOSS: 1.0949671268463135
LOSS: 1.0939968824386597
LOSS: 1.093104600906372
LOSS: 1.0922845602035522
LOSS: 1.091533899307251
LOSS: 1.0908472537994385
LOSS: 1.0902215242385864
LOSS: 1.0896520614624023
LOSS: 1.089134931564331
LOSS: 1.088667392730713
LOSS: 1.0882458686828613
LOSS: 1.0878663063049316
LOSS: 1.0875271558761597
LOSS: 1.0872244834899902
LOSS: 1.0869561433792114
LOSS: 1.0867195129394531
LOSS: 1.0865113735198975
LOSS: 1.086329460144043
LOSS: 1.0861713886260986
LOSS: 1.0860341787338257
LOSS: 1.0859159231185913
LOSS: 1.0858155488967896
LOSS: 1.0857301950454712
LOSS: 1.0856579542160034
LOSS: 1.085598111152649
LOSS: 1.0855488777160645
LOSS: 1.0855088233947754
LOSS: 1.0854774713516235
LOSS: 1.0854521989822388
LOSS: 1.0854336023330688
LOSS: 1.0854194164276123
LOSS: 1.0854096412658691
LOSS: 1.0854034423828125
LOSS: 1.0853996276855469
LOSS: 1.085398554801941
LOSS: 1.0853979587554932
LOSS: 1.0853996276855469
LOSS: 1.0854014158248901
LOSS: 1.0854042768478394
LOSS: 1.0854074954986572
LOSS: 1.0854101181030273
LOSS: 1.0854138135910034
LOSS: 1.0854169130325317
LOSS: 1.0854191780090332
LOSS: 1.0854213237762451
LOSS: 1.0854238271713257
LOSS: 1.0854253768920898
LOSS: 1.0854265689849854
LOSS: 1.0854270458221436
LOSS: 1.0854274034500122
LOSS: 1.0854272842407227
LOSS: 1.085426926612854
LOSS: 1.0854265689849854
LOSS: 1.0854257345199585
LOSS: 1.0854244232177734
LOSS: 1.0854233503341675
LOSS: 1.085422158241272
LOSS: 1.0854207277297974
LOSS: 1.085418462753296
LOSS: 1.0854170322418213
LOSS: 1.0854158401489258
LOSS: 1.0854142904281616
LOSS: 1.191198706626892
LOSS: 1.1833912134170532
LOSS: 1.175895094871521
LOSS: 1.1687383651733398
LOSS: 1.1619364023208618
LOSS: 1.1554787158966064
LOSS: 1.1493163108825684
LOSS: 1.1433733701705933
LOSS: 1.1375813484191895
LOSS: 1.1319047212600708
LOSS: 1.1263362169265747
LOSS: 1.1208834648132324
LOSS: 1.1155592203140259
LOSS: 1.1103781461715698
LOSS: 1.1053534746170044
LOSS: 1.100497841835022
LOSS: 1.0958218574523926
LOSS: 1.0913351774215698
LOSS: 1.0870448350906372
LOSS: 1.0829572677612305
LOSS: 1.0790759325027466
LOSS: 1.0754032135009766
LOSS: 1.0719389915466309
LOSS: 1.0686806440353394
LOSS: 1.065624475479126
LOSS: 1.0627652406692505
LOSS: 1.0600955486297607
LOSS: 1.0576082468032837
LOSS: 1.055295705795288
LOSS: 1.0531501770019531
LOSS: 1.051165223121643
LOSS: 1.0493346452713013
LOSS: 1.0476523637771606
LOSS: 1.0461134910583496
LOSS: 1.044713020324707
LOSS: 1.043445348739624
LOSS: 1.042304515838623
LOSS: 1.0412839651107788
LOSS: 1.04037606716156
LOSS: 1.0395729541778564
LOSS: 1.0388654470443726
LOSS: 1.0382457971572876
LOSS: 1.037705659866333
LOSS: 1.037237286567688
LOSS: 1.0368342399597168
LOSS: 1.036489486694336
LOSS: 1.036197543144226
LOSS: 1.0359541177749634
LOSS: 1.0357537269592285
LOSS: 1.0355918407440186
LOSS: 1.0354639291763306
LOSS: 1.0353652238845825
LOSS: 1.035292148590088
LOSS: 1.035239815711975
LOSS: 1.0352044105529785
LOSS: 1.0351828336715698
LOSS: 1.035171389579773
LOSS: 1.0351685285568237
LOSS: 1.0351721048355103
LOSS: 1.0351805686950684
LOSS: 1.035191535949707
LOSS: 1.0352058410644531
LOSS: 1.0352206230163574
LOSS: 1.0352355241775513
LOSS: 1.0352506637573242
LOSS: 1.0352643728256226
LOSS: 1.0352764129638672
LOSS: 1.0352861881256104
LOSS: 1.0352939367294312
LOSS: 1.0352994203567505
LOSS: 1.0353021621704102
LOSS: 1.0353037118911743
LOSS: 1.035302996635437
LOSS: 1.0353007316589355
LOSS: 1.03529691696167
LOSS: 1.035292625427246
LOSS: 1.035286545753479
LOSS: 1.0352799892425537
LOSS: 1.0352727174758911
LOSS: 1.0352649688720703
LOSS: 1.0352569818496704
LOSS: 1.0352487564086914
LOSS: 1.0352400541305542
LOSS: 1.0352321863174438
LOSS: 1.035224437713623
LOSS: 1.0352171659469604
LOSS: 1.0352104902267456
LOSS: 1.0352040529251099
LOSS: 1.0351979732513428
LOSS: 1.035192608833313
LOSS: 1.0351879596710205
LOSS: 1.035183310508728
LOSS: 1.0351797342300415
LOSS: 1.0351766347885132
LOSS: 1.0351738929748535
LOSS: 1.0351717472076416
LOSS: 1.0351698398590088
LOSS: 1.0351685285568237
LOSS: 1.0351672172546387
LOSS: 1.0351663827896118
LOSS: 1.2109993696212769
LOSS: 1.2039083242416382
LOSS: 1.1971642971038818
LOSS: 1.1907968521118164
LOSS: 1.1848201751708984
LOSS: 1.1792174577713013
LOSS: 1.1739283800125122
LOSS: 1.168867826461792
LOSS: 1.1639662981033325
LOSS: 1.1591898202896118
LOSS: 1.1545300483703613
LOSS: 1.1499903202056885
LOSS: 1.145577073097229
LOSS: 1.1412975788116455
LOSS: 1.137157678604126
LOSS: 1.1331627368927002
LOSS: 1.1293163299560547
LOSS: 1.1256217956542969
LOSS: 1.1220815181732178
LOSS: 1.1186974048614502
LOSS: 1.1154699325561523
LOSS: 1.1124000549316406
LOSS: 1.1094869375228882
LOSS: 1.082393765449524
LOSS: 1.0823907852172852
LOSS: 1.0823875665664673
LOSS: 1.0823848247528076
LOSS: 1.082382082939148
LOSS: 1.0823804140090942
LOSS: 1.0823783874511719
LOSS: 1.0823774337768555
LOSS: 1.1880083084106445
LOSS: 1.1801010370254517
LOSS: 1.1725003719329834
LOSS: 1.1652356386184692
LOSS: 1.1583278179168701
LOSS: 1.1517802476882935
LOSS: 1.1455659866333008
LOSS: 1.1396262645721436
LOSS: 1.1338893175125122
LOSS: 1.1282998323440552
LOSS: 1.1228315830230713
LOSS: 1.1174802780151367
LOSS: 1.1122533082962036
LOSS: 1.1071621179580688
LOSS: 1.1022189855575562
LOSS: 1.097435474395752
LOSS: 1.0928218364715576
LOSS: 1.0883866548538208
LOSS: 1.0841370820999146
LOSS: 1.0800789594650269
LOSS: 1.0762155055999756
LOSS: 1.0725494623184204
LOSS: 1.0690808296203613
LOSS: 1.0658085346221924
LOSS: 1.0627291202545166
LOSS: 1.0598382949829102
LOSS: 1.057129979133606
LOSS: 1.0545967817306519
LOSS: 1.0522311925888062
LOSS: 1.0500253438949585
LOSS: 1.0479705333709717
LOSS: 1.0460600852966309
LOSS: 1.0442866086959839
LOSS: 1.0426442623138428
LOSS: 1.0411275625228882
LOSS: 1.0397312641143799
LOSS: 1.038450837135315
LOSS: 1.0372811555862427
LOSS: 1.0362170934677124
LOSS: 1.035253882408142
LOSS: 1.0343847274780273
LOSS: 1.0336036682128906
LOSS: 1.0329041481018066
LOSS: 1.0322794914245605
LOSS: 1.0317232608795166
LOSS: 1.0312297344207764
LOSS: 1.0307931900024414
LOSS: 1.0304089784622192
LOSS: 1.0300720930099487
LOSS: 1.0297788381576538
LOSS: 1.0295251607894897
LOSS: 1.0293083190917969
LOSS: 1.02912437915802
LOSS: 1.0289702415466309
LOSS: 1.0288423299789429
LOSS: 1.0287370681762695
LOSS: 1.0286521911621094
LOSS: 1.0285840034484863
LOSS: 1.0285303592681885
LOSS: 1.028489112854004
LOSS: 1.0284578800201416
LOSS: 1.0284357070922852
LOSS: 1.028420090675354
LOSS: 1.028410792350769
LOSS: 1.028406023979187
LOSS: 1.028405785560608
LOSS: 1.0284080505371094
LOSS: 1.028412938117981
LOSS: 1.0284186601638794
LOSS: 1.0284254550933838
LOSS: 1.028432846069336
LOSS: 1.0284397602081299
LOSS: 1.0284461975097656
LOSS: 1.0284515619277954
LOSS: 1.0284563302993774
LOSS: 1.028460144996643
LOSS: 1.028463363647461
LOSS: 1.0284662246704102
LOSS: 1.0284680128097534
LOSS: 1.0284686088562012
LOSS: 1.0284686088562012
LOSS: 1.0284678936004639
LOSS: 1.0284667015075684
LOSS: 1.0284651517868042
LOSS: 1.0284626483917236
LOSS: 1.0284600257873535
LOSS: 1.0284565687179565
LOSS: 1.0284533500671387
LOSS: 1.0284497737884521
LOSS: 1.028445839881897
LOSS: 1.0284429788589478
LOSS: 1.0284395217895508
LOSS: 1.028436303138733
LOSS: 1.028432846069336
LOSS: 1.028429627418518
LOSS: 1.0284266471862793
LOSS: 1.0284239053726196
LOSS: 1.028421401977539
LOSS: 1.0284185409545898
LOSS: 1.0284167528152466
LOSS: 1.2109993696212769
LOSS: 1.2039083242416382
LOSS: 1.1971642971038818
LOSS: 1.1907968521118164
LOSS: 1.1848201751708984
LOSS: 1.1792174577713013
LOSS: 1.1739283800125122
LOSS: 1.168867826461792
LOSS: 1.1639662981033325
LOSS: 1.1591898202896118
LOSS: 1.1545300483703613
LOSS: 1.1499903202056885
LOSS: 1.145577073097229
LOSS: 1.1412975788116455
LOSS: 1.137157678604126
LOSS: 1.1331627368927002
LOSS: 1.1293163299560547
LOSS: 1.1256217956542969
LOSS: 1.1220815181732178
LOSS: 1.1186974048614502
LOSS: 1.1154699325561523
LOSS: 1.1124000549316406
LOSS: 1.1094869375228882
LOSS: 1.106729507446289
LOSS: 1.1041256189346313
LOSS: 1.1016721725463867
LOSS: 1.099365234375
LOSS: 1.0971996784210205
LOSS: 1.095170497894287
LOSS: 1.093271017074585
LOSS: 1.0914946794509888
LOSS: 1.089835286140442
LOSS: 1.0882861614227295
LOSS: 1.0868407487869263
LOSS: 1.085493803024292
LOSS: 1.084240198135376
LOSS: 1.0830751657485962
LOSS: 1.081994891166687
LOSS: 1.080994963645935
LOSS: 1.0800727605819702
LOSS: 1.079224705696106
LOSS: 1.0784465074539185
LOSS: 1.0777349472045898
LOSS: 1.0770865678787231
LOSS: 1.0764966011047363
LOSS: 1.0759612321853638
LOSS: 1.0754765272140503
LOSS: 1.0750383138656616
LOSS: 1.0746421813964844
LOSS: 1.0742857456207275
LOSS: 1.0739645957946777
LOSS: 1.0736762285232544
LOSS: 1.0734190940856934
LOSS: 1.0731894969940186
LOSS: 1.0729857683181763
LOSS: 1.0728063583374023
LOSS: 1.0726479291915894
LOSS: 1.072509765625
LOSS: 1.0723899602890015
LOSS: 1.0722863674163818
LOSS: 1.0721967220306396
LOSS: 1.0721197128295898
LOSS: 1.0720546245574951
LOSS: 1.071999192237854
LOSS: 1.0719517469406128
LOSS: 1.0719130039215088
LOSS: 1.0718803405761719
LOSS: 1.071853518486023
LOSS: 1.071831464767456
LOSS: 1.071814775466919
LOSS: 1.0718013048171997
LOSS: 1.071791172027588
LOSS: 1.0717835426330566
LOSS: 1.0717787742614746
LOSS: 1.0717757940292358
LOSS: 1.0717743635177612
LOSS: 1.071772813796997
LOSS: 1.0717735290527344
LOSS: 1.0717737674713135
LOSS: 1.071775197982788
LOSS: 1.0717765092849731
LOSS: 1.0717780590057373
LOSS: 1.071779489517212
LOSS: 1.0717812776565552
LOSS: 1.0717825889587402
LOSS: 1.071784496307373
LOSS: 1.0717856884002686
LOSS: 1.0717865228652954
LOSS: 1.071787714958191
LOSS: 1.0717883110046387
LOSS: 1.071789026260376
LOSS: 1.071789026260376
LOSS: 1.071789026260376
LOSS: 1.0717891454696655
LOSS: 1.0717884302139282
LOSS: 1.0717878341674805
LOSS: 1.0717875957489014
LOSS: 1.0717874765396118
LOSS: 1.0717864036560059
LOSS: 1.071785569190979
LOSS: 1.2227482795715332
LOSS: 1.2160446643829346
LOSS: 1.2097066640853882
LOSS: 1.2037566900253296
LOSS: 1.1981807947158813
LOSS: 1.1929073333740234
LOSS: 1.187833547592163
LOSS: 1.1828919649124146
LOSS: 1.1780610084533691
LOSS: 1.1733429431915283
LOSS: 1.1687469482421875
LOSS: 1.1642820835113525
LOSS: 1.1599571704864502
LOSS: 1.1557788848876953
LOSS: 1.1517531871795654
LOSS: 1.147884726524353
LOSS: 1.1441779136657715
LOSS: 1.1406348943710327
LOSS: 1.137258529663086
LOSS: 1.134049892425537
LOSS: 1.1310096979141235
LOSS: 1.128137230873108
LOSS: 1.1254312992095947
LOSS: 1.1228893995285034
LOSS: 1.1205074787139893
LOSS: 1.1182817220687866
LOSS: 1.116206169128418
LOSS: 1.1142749786376953
LOSS: 1.1124823093414307
LOSS: 1.1108208894729614
LOSS: 1.1092848777770996
LOSS: 1.1078684329986572
LOSS: 1.1065653562545776
LOSS: 1.105371117591858
LOSS: 1.1042805910110474
LOSS: 1.103289008140564
LOSS: 1.1023924350738525
LOSS: 1.101585865020752
LOSS: 1.100864291191101
LOSS: 1.1002223491668701
LOSS: 1.0996546745300293
LOSS: 1.0991555452346802
LOSS: 1.098719596862793
LOSS: 1.0983411073684692
LOSS: 1.0980138778686523
LOSS: 1.0977336168289185
LOSS: 1.0974950790405273
LOSS: 1.0972936153411865
LOSS: 1.0971262454986572
LOSS: 1.0969889163970947
LOSS: 1.0968775749206543
LOSS: 1.0967903137207031
LOSS: 1.096722960472107
LOSS: 1.0966730117797852
LOSS: 1.0966382026672363
LOSS: 1.0966150760650635
LOSS: 1.0966017246246338
LOSS: 1.0965962409973145
LOSS: 1.0965968370437622
LOSS: 1.096601128578186
LOSS: 1.0966086387634277
LOSS: 1.0966178178787231
LOSS: 1.0966283082962036
LOSS: 1.0966392755508423
LOSS: 1.09665048122406
LOSS: 1.096660852432251
LOSS: 1.0966700315475464
LOSS: 1.0966784954071045
LOSS: 1.0966854095458984
LOSS: 1.0966904163360596
LOSS: 1.0966945886611938
LOSS: 1.096696376800537
LOSS: 1.0966967344284058
LOSS: 1.0966957807540894
LOSS: 1.0966938734054565
LOSS: 1.0966904163360596
LOSS: 1.0966864824295044
LOSS: 1.0966825485229492
LOSS: 1.0966770648956299
LOSS: 1.096671462059021
LOSS: 1.0966651439666748
LOSS: 1.0966589450836182
LOSS: 1.0966533422470093
LOSS: 1.0966476202011108
LOSS: 1.0966413021087646
LOSS: 1.0966362953186035
LOSS: 1.0966304540634155
LOSS: 1.096625804901123
LOSS: 1.0966205596923828
LOSS: 1.096616506576538
LOSS: 1.0966131687164307
LOSS: 1.0966098308563232
LOSS: 1.0966064929962158
LOSS: 1.0966039896011353
LOSS: 1.096601963043213
LOSS: 1.09660005569458
LOSS: 1.0965982675552368
LOSS: 1.0965970754623413
LOSS: 1.096596360206604
LOSS: 1.096595287322998
LOSS: 1.2088813781738281
LOSS: 1.2017759084701538
LOSS: 1.1950167417526245
LOSS: 1.188631534576416
LOSS: 1.1826289892196655
LOSS: 1.1769797801971436
LOSS: 1.1716102361679077
LOSS: 1.1664342880249023
LOSS: 1.1613965034484863
LOSS: 1.1564770936965942
LOSS: 1.1516766548156738
LOSS: 1.1470019817352295
LOSS: 1.1424620151519775
LOSS: 1.1380642652511597
LOSS: 1.1338154077529907
LOSS: 1.1297205686569214
LOSS: 1.1257843971252441
LOSS: 1.122010350227356
LOSS: 1.1184008121490479
LOSS: 1.1149580478668213
LOSS: 1.1116831302642822
LOSS: 1.108576774597168
LOSS: 1.1056386232376099
LOSS: 1.0805468559265137
LOSS: 1.0805447101593018
LOSS: 1.0805429220199585
LOSS: 1.0805408954620361
LOSS: 1.0805399417877197
LOSS: 1.0805391073226929
LOSS: 1.217702865600586
LOSS: 1.2108701467514038
LOSS: 1.2043966054916382
LOSS: 1.1983040571212769
LOSS: 1.1925780773162842
LOSS: 1.1871466636657715
LOSS: 1.1819078922271729
LOSS: 1.1767951250076294
LOSS: 1.171788215637207
LOSS: 1.1668893098831177
LOSS: 1.1621074676513672
LOSS: 1.1574522256851196
LOSS: 1.152931571006775
LOSS: 1.1485527753829956
LOSS: 1.1443216800689697
LOSS: 1.1402432918548584
LOSS: 1.1363214254379272
LOSS: 1.1325596570968628
LOSS: 1.1289607286453247
LOSS: 1.1255263090133667
LOSS: 1.1222578287124634
LOSS: 1.1191552877426147
LOSS: 1.116218090057373
LOSS: 1.113444209098816
LOSS: 1.1108310222625732
LOSS: 1.1083743572235107
LOSS: 1.106069564819336
LOSS: 1.1039111614227295
LOSS: 1.1018937826156616
LOSS: 1.1000111103057861
LOSS: 1.0982575416564941
LOSS: 1.096627950668335
LOSS: 1.0951169729232788
LOSS: 1.0937203168869019
LOSS: 1.0924338102340698
LOSS: 1.0912531614303589
LOSS: 1.0901747941970825
LOSS: 1.0891942977905273
LOSS: 1.0883071422576904
LOSS: 1.087507963180542
LOSS: 1.0867916345596313
LOSS: 1.0861523151397705
LOSS: 1.0855846405029297
LOSS: 1.0850826501846313
LOSS: 1.0846407413482666
LOSS: 1.0842534303665161
LOSS: 1.083916425704956
LOSS: 1.0836248397827148
LOSS: 1.083375096321106
LOSS: 1.083162784576416
LOSS: 1.0829850435256958
LOSS: 1.0828375816345215
LOSS: 1.0827175378799438
LOSS: 1.0826212167739868
LOSS: 1.0825456380844116
LOSS: 1.0824881792068481
LOSS: 1.0824445486068726
LOSS: 1.082414150238037
LOSS: 1.0823936462402344
LOSS: 1.082381010055542
LOSS: 1.082375168800354
LOSS: 1.0823743343353271
LOSS: 1.082377552986145
LOSS: 1.0823839902877808
LOSS: 1.0823920965194702
LOSS: 1.0824017524719238
LOSS: 1.082411527633667
LOSS: 1.0824214220046997
LOSS: 1.0824304819107056
LOSS: 1.0824389457702637
LOSS: 1.0824464559555054
LOSS: 1.0824518203735352
LOSS: 1.082456111907959
LOSS: 1.082459568977356
LOSS: 1.0824611186981201
LOSS: 1.082461953163147
LOSS: 1.0824613571166992
LOSS: 1.0824600458145142
LOSS: 1.0824573040008545
LOSS: 1.0824543237686157
LOSS: 1.0824503898620605
LOSS: 1.0824458599090576
LOSS: 1.0824412107467651
LOSS: 1.0824360847473145
LOSS: 1.0824310779571533
LOSS: 1.082425594329834
LOSS: 1.0824202299118042
LOSS: 1.0824155807495117
LOSS: 1.0824106931686401
LOSS: 1.082405686378479
LOSS: 1.0824018716812134
LOSS: 1.0823975801467896
LOSS: 1.082393765449524
LOSS: 1.0823907852172852
LOSS: 1.0823875665664673
LOSS: 1.0823848247528076
LOSS: 1.082382082939148
LOSS: 1.0823804140090942
LOSS: 1.0823783874511719
LOSS: 1.0823774337768555
LOSS: 1.2105740308761597
LOSS: 1.2035728693008423
LOSS: 1.1969233751296997
LOSS: 1.1906533241271973
LOSS: 1.1847724914550781
LOSS: 1.179253101348877
LOSS: 1.1740217208862305
LOSS: 1.1689916849136353
LOSS: 1.164104700088501
LOSS: 1.1593384742736816
LOSS: 1.154690146446228
LOSS: 1.15016508102417
LOSS: 1.1457687616348267
LOSS: 1.141506552696228
LOSS: 1.1373827457427979
LOSS: 1.1333999633789062
LOSS: 1.1295608282089233
LOSS: 1.1258662939071655
LOSS: 1.1223175525665283
LOSS: 1.1189154386520386
LOSS: 1.1156598329544067
LOSS: 1.112550973892212
LOSS: 1.1095887422561646
LOSS: 1.1067718267440796
LOSS: 1.1040993928909302
LOSS: 1.1015695333480835
LOSS: 1.0991793870925903
LOSS: 1.096925973892212
LOSS: 1.094805359840393
LOSS: 1.0928131341934204
LOSS: 1.0909442901611328
LOSS: 1.0891934633255005
LOSS: 1.0875558853149414
LOSS: 1.0860252380371094
LOSS: 1.0845980644226074
LOSS: 1.0832687616348267
LOSS: 1.0820335149765015
LOSS: 1.0808888673782349
LOSS: 1.0798313617706299
LOSS: 1.0788582563400269
LOSS: 1.0779662132263184
LOSS: 1.077151894569397
LOSS: 1.0764129161834717
LOSS: 1.0757445096969604
LOSS: 1.0751436948776245
LOSS: 1.0746068954467773
LOSS: 1.0741292238235474
LOSS: 1.0737056732177734
LOSS: 1.0733327865600586
LOSS: 1.0730057954788208
LOSS: 1.0727207660675049
LOSS: 1.0724737644195557
LOSS: 1.0722620487213135
LOSS: 1.0720809698104858
LOSS: 1.0719287395477295
LOSS: 1.071801781654358
LOSS: 1.07169771194458
LOSS: 1.071614146232605
LOSS: 1.0715481042861938
LOSS: 1.0714972019195557
LOSS: 1.071460247039795
LOSS: 1.0714335441589355
LOSS: 1.0714163780212402
LOSS: 1.0714060068130493
LOSS: 1.071401596069336
LOSS: 1.0714012384414673
LOSS: 1.0714046955108643
LOSS: 1.0714101791381836
LOSS: 1.0714168548583984
LOSS: 1.0714247226715088
LOSS: 1.0714327096939087
LOSS: 1.0714410543441772
LOSS: 1.0714490413665771
LOSS: 1.0714558362960815
LOSS: 1.0714621543884277
LOSS: 1.0714671611785889
LOSS: 1.0714712142944336
LOSS: 1.0714738368988037
LOSS: 1.0714755058288574
LOSS: 1.0714759826660156
LOSS: 1.0714751482009888
LOSS: 1.0714738368988037
LOSS: 1.071470856666565
LOSS: 1.0714681148529053
LOSS: 1.07146418094635
LOSS: 1.0714603662490845
LOSS: 1.0714561939239502
LOSS: 1.0714519023895264
LOSS: 1.071447730064392
LOSS: 1.071442723274231
LOSS: 1.0714387893676758
LOSS: 1.0714341402053833
LOSS: 1.0714300870895386
LOSS: 1.071426272392273
LOSS: 1.0714223384857178
LOSS: 1.0714188814163208
LOSS: 1.0714160203933716
LOSS: 1.0714129209518433
LOSS: 1.0714105367660522
LOSS: 1.0714082717895508
LOSS: 1.191198706626892
LOSS: 1.1833912134170532
LOSS: 1.175895094871521
LOSS: 1.1687383651733398
LOSS: 1.1619364023208618
LOSS: 1.1554787158966064
LOSS: 1.1493163108825684
LOSS: 1.1433733701705933
LOSS: 1.1375813484191895
LOSS: 1.1319047212600708
LOSS: 1.1263362169265747
LOSS: 1.1208834648132324
LOSS: 1.1155592203140259
LOSS: 1.1103781461715698
LOSS: 1.1053534746170044
LOSS: 1.100497841835022
LOSS: 1.0958218574523926
LOSS: 1.0913351774215698
LOSS: 1.0870448350906372
LOSS: 1.0829572677612305
LOSS: 1.0790759325027466
LOSS: 1.0754032135009766
LOSS: 1.0719389915466309
LOSS: 1.0686806440353394
LOSS: 1.065624475479126
LOSS: 1.0627652406692505
LOSS: 1.0600955486297607
LOSS: 1.0576082468032837
LOSS: 1.055295705795288
LOSS: 1.0531501770019531
LOSS: 1.051165223121643
LOSS: 1.0493346452713013
LOSS: 1.0476523637771606
LOSS: 1.0461134910583496
LOSS: 1.044713020324707
LOSS: 1.043445348739624
LOSS: 1.042304515838623
LOSS: 1.0412839651107788
LOSS: 1.04037606716156
LOSS: 1.0395729541778564
LOSS: 1.0388654470443726
LOSS: 1.0382457971572876
LOSS: 1.037705659866333
LOSS: 1.037237286567688
LOSS: 1.0368342399597168
LOSS: 1.036489486694336
LOSS: 1.036197543144226
LOSS: 1.0359541177749634
LOSS: 1.0357537269592285
LOSS: 1.0355918407440186
LOSS: 1.0354639291763306
LOSS: 1.0353652238845825
LOSS: 1.035292148590088
LOSS: 1.035239815711975
LOSS: 1.0352044105529785
LOSS: 1.0351828336715698
LOSS: 1.035171389579773
LOSS: 1.0351685285568237
LOSS: 1.0351721048355103
LOSS: 1.0351805686950684
LOSS: 1.035191535949707
LOSS: 1.0352058410644531
LOSS: 1.0352206230163574
LOSS: 1.0352355241775513
LOSS: 1.0352506637573242
LOSS: 1.0352643728256226
LOSS: 1.0352764129638672
LOSS: 1.0352861881256104
LOSS: 1.0352939367294312
LOSS: 1.0352994203567505
LOSS: 1.0353021621704102
LOSS: 1.0353037118911743
LOSS: 1.035302996635437
LOSS: 1.0353007316589355
LOSS: 1.03529691696167
LOSS: 1.035292625427246
LOSS: 1.035286545753479
LOSS: 1.0352799892425537
LOSS: 1.0352727174758911
LOSS: 1.0352649688720703
LOSS: 1.0352569818496704
LOSS: 1.0352487564086914
LOSS: 1.0352400541305542
LOSS: 1.0352321863174438
LOSS: 1.035224437713623
LOSS: 1.0352171659469604
LOSS: 1.0352104902267456
LOSS: 1.0352040529251099
LOSS: 1.0351979732513428
LOSS: 1.035192608833313
LOSS: 1.0351879596710205
LOSS: 1.035183310508728
LOSS: 1.0351797342300415
LOSS: 1.0351766347885132
LOSS: 1.0351738929748535
LOSS: 1.0351717472076416
LOSS: 1.0351698398590088
LOSS: 1.0351685285568237
LOSS: 1.0351672172546387
LOSS: 1.0351663827896118
LOSS: 1.2092939615249634
LOSS: 1.2021524906158447
LOSS: 1.1953554153442383
LOSS: 1.188932180404663
LOSS: 1.1828961372375488
LOSS: 1.1772286891937256
LOSS: 1.1718684434890747
LOSS: 1.166729211807251
LOSS: 1.1617423295974731
LOSS: 1.156875729560852
LOSS: 1.152121663093567
LOSS: 1.1474840641021729
LOSS: 1.142970323562622
LOSS: 1.1385875940322876
LOSS: 1.1343426704406738
LOSS: 1.1302413940429688
LOSS: 1.1262885332107544
LOSS: 1.1224877834320068
LOSS: 1.1188430786132812
LOSS: 1.115356683731079
LOSS: 1.1120307445526123
LOSS: 1.1088664531707764
LOSS: 1.1058639287948608
LOSS: 1.1030232906341553
LOSS: 1.1095887422561646
LOSS: 1.1067718267440796
LOSS: 1.1040993928909302
LOSS: 1.1015695333480835
LOSS: 1.0991793870925903
LOSS: 1.096925973892212
LOSS: 1.094805359840393
LOSS: 1.0928131341934204
LOSS: 1.0909442901611328
LOSS: 1.0891934633255005
LOSS: 1.0875558853149414
LOSS: 1.0860252380371094
LOSS: 1.0845980644226074
LOSS: 1.0832687616348267
LOSS: 1.0820335149765015
LOSS: 1.0808888673782349
LOSS: 1.0798313617706299
LOSS: 1.0788582563400269
LOSS: 1.0779662132263184
LOSS: 1.077151894569397
LOSS: 1.0764129161834717
LOSS: 1.0757445096969604
LOSS: 1.0751436948776245
LOSS: 1.0746068954467773
LOSS: 1.0741292238235474
LOSS: 1.0737056732177734
LOSS: 1.0733327865600586
LOSS: 1.0730057954788208
LOSS: 1.0727207660675049
LOSS: 1.0724737644195557
LOSS: 1.0722620487213135
LOSS: 1.0720809698104858
LOSS: 1.0719287395477295
LOSS: 1.071801781654358
LOSS: 1.07169771194458
LOSS: 1.071614146232605
LOSS: 1.0715481042861938
LOSS: 1.0714972019195557
LOSS: 1.071460247039795
LOSS: 1.0714335441589355
LOSS: 1.0714163780212402
LOSS: 1.0714060068130493
LOSS: 1.071401596069336
LOSS: 1.0714012384414673
LOSS: 1.0714046955108643
LOSS: 1.0714101791381836
LOSS: 1.0714168548583984
LOSS: 1.0714247226715088
LOSS: 1.0714327096939087
LOSS: 1.0714410543441772
LOSS: 1.0714490413665771
LOSS: 1.0714558362960815
LOSS: 1.0714621543884277
LOSS: 1.0714671611785889
LOSS: 1.0714712142944336
LOSS: 1.0714738368988037
LOSS: 1.0714755058288574
LOSS: 1.0714759826660156
LOSS: 1.0714751482009888
LOSS: 1.0714738368988037
LOSS: 1.071470856666565
LOSS: 1.0714681148529053
LOSS: 1.07146418094635
LOSS: 1.0714603662490845
LOSS: 1.0714561939239502
LOSS: 1.0714519023895264
LOSS: 1.071447730064392
LOSS: 1.071442723274231
LOSS: 1.0714387893676758
LOSS: 1.0714341402053833
LOSS: 1.0714300870895386
LOSS: 1.071426272392273
LOSS: 1.0714223384857178
LOSS: 1.0714188814163208
LOSS: 1.0714160203933716
LOSS: 1.0714129209518433
LOSS: 1.0714105367660522
LOSS: 1.0714082717895508
LOSS: 1.2227482795715332
LOSS: 1.2160446643829346
LOSS: 1.2097066640853882
LOSS: 1.2037566900253296
LOSS: 1.1981807947158813
LOSS: 1.1929073333740234
LOSS: 1.187833547592163
LOSS: 1.1828919649124146
LOSS: 1.1780610084533691
LOSS: 1.1733429431915283
LOSS: 1.1687469482421875
LOSS: 1.1642820835113525
LOSS: 1.1599571704864502
LOSS: 1.1557788848876953
LOSS: 1.1517531871795654
LOSS: 1.147884726524353
LOSS: 1.1441779136657715
LOSS: 1.1406348943710327
LOSS: 1.137258529663086
LOSS: 1.134049892425537
LOSS: 1.1310096979141235
LOSS: 1.128137230873108
LOSS: 1.1254312992095947
LOSS: 1.1228893995285034
LOSS: 1.1205074787139893
LOSS: 1.1182817220687866
LOSS: 1.116206169128418
LOSS: 1.1142749786376953
LOSS: 1.1124823093414307
LOSS: 1.1108208894729614
LOSS: 1.1092848777770996
LOSS: 1.1078684329986572
LOSS: 1.1065653562545776
LOSS: 1.105371117591858
LOSS: 1.1042805910110474
LOSS: 1.103289008140564
LOSS: 1.1023924350738525
LOSS: 1.101585865020752
LOSS: 1.100864291191101
LOSS: 1.1002223491668701
LOSS: 1.0996546745300293
LOSS: 1.0991555452346802
LOSS: 1.098719596862793
LOSS: 1.0983411073684692
LOSS: 1.0980138778686523
LOSS: 1.0977336168289185
LOSS: 1.0974950790405273
LOSS: 1.0972936153411865
LOSS: 1.0971262454986572
LOSS: 1.0969889163970947
LOSS: 1.0968775749206543
LOSS: 1.0967903137207031
LOSS: 1.096722960472107
LOSS: 1.0966730117797852
LOSS: 1.0966382026672363
LOSS: 1.0966150760650635
LOSS: 1.0966017246246338
LOSS: 1.0965962409973145
LOSS: 1.0965968370437622
LOSS: 1.096601128578186
LOSS: 1.0966086387634277
LOSS: 1.0966178178787231
LOSS: 1.0966283082962036
LOSS: 1.0966392755508423
LOSS: 1.09665048122406
LOSS: 1.096660852432251
LOSS: 1.0966700315475464
LOSS: 1.0966784954071045
LOSS: 1.0966854095458984
LOSS: 1.0966904163360596
LOSS: 1.0966945886611938
LOSS: 1.096696376800537
LOSS: 1.0966967344284058
LOSS: 1.0966957807540894
LOSS: 1.0966938734054565
LOSS: 1.0966904163360596
LOSS: 1.0966864824295044
LOSS: 1.0966825485229492
LOSS: 1.0966770648956299
LOSS: 1.096671462059021
LOSS: 1.0966651439666748
LOSS: 1.0966589450836182
LOSS: 1.0966533422470093
LOSS: 1.0966476202011108
LOSS: 1.0966413021087646
LOSS: 1.0966362953186035
LOSS: 1.0966304540634155
LOSS: 1.096625804901123
LOSS: 1.0966205596923828
LOSS: 1.096616506576538
LOSS: 1.0966131687164307
LOSS: 1.0966098308563232
LOSS: 1.0966064929962158
LOSS: 1.0966039896011353
LOSS: 1.096601963043213
LOSS: 1.09660005569458
LOSS: 1.0965982675552368
LOSS: 1.0965970754623413
LOSS: 1.096596360206604
LOSS: 1.096595287322998
LOSS: 1.2109816074371338
LOSS: 1.2039144039154053
LOSS: 1.1971935033798218
LOSS: 1.1908265352249146
LOSS: 1.1847598552703857
LOSS: 1.1788818836212158
LOSS: 1.173111081123352
LOSS: 1.1674262285232544
LOSS: 1.1618343591690063
LOSS: 1.1563483476638794
LOSS: 1.150981068611145
LOSS: 1.145743489265442
LOSS: 1.1406455039978027
LOSS: 1.1356947422027588
LOSS: 1.1308982372283936
LOSS: 1.1262619495391846
LOSS: 1.1217902898788452
LOSS: 1.1174869537353516
LOSS: 1.1133545637130737
LOSS: 1.1093943119049072
LOSS: 1.1056060791015625
LOSS: 1.1019890308380127
LOSS: 1.0985409021377563
LOSS: 1.095258355140686
LOSS: 1.0921376943588257
LOSS: 1.0891751050949097
LOSS: 1.0863662958145142
LOSS: 1.0837074518203735
LOSS: 1.081195592880249
LOSS: 1.0788275003433228
LOSS: 1.076600432395935
LOSS: 1.0745118856430054
LOSS: 1.0725581645965576
LOSS: 1.0707365274429321
LOSS: 1.069042444229126
LOSS: 1.0674713850021362
LOSS: 1.066017746925354
LOSS: 1.064676284790039
LOSS: 1.063441276550293
LOSS: 1.0623066425323486
LOSS: 1.0612674951553345
LOSS: 1.060318946838379
LOSS: 1.059456467628479
LOSS: 1.058674693107605
LOSS: 1.0579700469970703
LOSS: 1.0573369264602661
LOSS: 1.0567717552185059
LOSS: 1.0562689304351807
LOSS: 1.0558240413665771
LOSS: 1.055431842803955
LOSS: 1.0550882816314697
LOSS: 1.0547888278961182
LOSS: 1.0545297861099243
LOSS: 1.054306983947754
LOSS: 1.0541174411773682
LOSS: 1.0539579391479492
LOSS: 1.053824543952942
LOSS: 1.0537148714065552
LOSS: 1.0536267757415771
LOSS: 1.0535560846328735
LOSS: 1.053501009941101
LOSS: 1.0534591674804688
LOSS: 1.0534281730651855
LOSS: 1.0534073114395142
LOSS: 1.053392767906189
LOSS: 1.053385615348816
LOSS: 1.0533825159072876
LOSS: 1.0533840656280518
LOSS: 1.0533877611160278
LOSS: 1.0533937215805054
LOSS: 1.053400993347168
LOSS: 1.0534088611602783
LOSS: 1.0534167289733887
LOSS: 1.0534236431121826
LOSS: 1.0534309148788452
LOSS: 1.0534368753433228
LOSS: 1.053442120552063
LOSS: 1.0534465312957764
LOSS: 1.0534499883651733
LOSS: 1.0534523725509644
LOSS: 1.0534535646438599
LOSS: 1.053453803062439
LOSS: 1.0534533262252808
LOSS: 1.0534518957138062
LOSS: 1.0534497499465942
LOSS: 1.053446888923645
LOSS: 1.0534440279006958
LOSS: 1.0534404516220093
LOSS: 1.053436517715454
LOSS: 1.0534323453903198
LOSS: 1.053428292274475
LOSS: 1.0534242391586304
LOSS: 1.0534203052520752
LOSS: 1.05341637134552
LOSS: 1.053412675857544
LOSS: 1.0534087419509888
LOSS: 1.0534054040908813
LOSS: 1.053402066230774
LOSS: 1.0533994436264038
LOSS: 1.0533969402313232
LOSS: 1.2126344442367554
LOSS: 1.2056185007095337
LOSS: 1.1989530324935913
LOSS: 1.1926604509353638
LOSS: 1.1867324113845825
LOSS: 1.181106686592102
LOSS: 1.175686240196228
LOSS: 1.1704012155532837
LOSS: 1.1652274131774902
LOSS: 1.1601667404174805
LOSS: 1.155230164527893
LOSS: 1.1504297256469727
LOSS: 1.1457762718200684
LOSS: 1.1412798166275024
LOSS: 1.1369482278823853
LOSS: 1.1327886581420898
LOSS: 1.1288068294525146
LOSS: 1.1250073909759521
LOSS: 1.1213935613632202
LOSS: 1.1179676055908203
LOSS: 1.1147302389144897
LOSS: 1.1116808652877808
LOSS: 1.10881769657135
LOSS: 1.1061365604400635
LOSS: 1.1036336421966553
LOSS: 1.1013027429580688
LOSS: 1.0991370677947998
LOSS: 1.097130298614502
LOSS: 1.095274567604065
LOSS: 1.0935635566711426
LOSS: 1.0919904708862305
LOSS: 1.090549349784851
LOSS: 1.0892343521118164
LOSS: 1.0880401134490967
LOSS: 1.086961030960083
LOSS: 1.0859909057617188
LOSS: 1.0851244926452637
LOSS: 1.0843546390533447
LOSS: 1.0836747884750366
LOSS: 1.0830779075622559
LOSS: 1.0825566053390503
LOSS: 1.0821040868759155
LOSS: 1.0817134380340576
LOSS: 1.0813777446746826
LOSS: 1.0810924768447876
LOSS: 1.0808523893356323
LOSS: 1.0806515216827393
LOSS: 1.0804872512817383
LOSS: 1.0803546905517578
LOSS: 1.080249309539795
LOSS: 1.0801688432693481
LOSS: 1.080108880996704
LOSS: 1.102867603302002
LOSS: 1.1002614498138428
LOSS: 1.0978169441223145
LOSS: 1.0955301523208618
LOSS: 1.0933960676193237
LOSS: 1.0914087295532227
LOSS: 1.089561939239502
LOSS: 1.087849497795105
LOSS: 1.0862648487091064
LOSS: 1.0848013162612915
LOSS: 1.0834534168243408
LOSS: 1.0822155475616455
LOSS: 1.0810824632644653
LOSS: 1.0800498723983765
LOSS: 1.0791130065917969
LOSS: 1.078266978263855
LOSS: 1.0775079727172852
LOSS: 1.0768312215805054
LOSS: 1.0762310028076172
LOSS: 1.0757029056549072
LOSS: 1.0752402544021606
LOSS: 1.074838638305664
LOSS: 1.0744913816452026
LOSS: 1.0741937160491943
LOSS: 1.0739405155181885
LOSS: 1.0737260580062866
LOSS: 1.0735479593276978
LOSS: 1.0733999013900757
LOSS: 1.0732797384262085
LOSS: 1.0731838941574097
LOSS: 1.0731096267700195
LOSS: 1.0730537176132202
LOSS: 1.073013424873352
LOSS: 1.0729867219924927
LOSS: 1.0729707479476929
LOSS: 1.0729634761810303
LOSS: 1.072962760925293
LOSS: 1.0729669332504272
LOSS: 1.072974681854248
LOSS: 1.072984218597412
LOSS: 1.072994589805603
LOSS: 1.0730063915252686
LOSS: 1.0730174779891968
LOSS: 1.0730279684066772
LOSS: 1.0730373859405518
LOSS: 1.073046088218689
LOSS: 1.0730531215667725
LOSS: 1.0730587244033813
LOSS: 1.0730630159378052
LOSS: 1.0730657577514648
LOSS: 1.0730663537979126
LOSS: 1.073066234588623
LOSS: 1.0730645656585693
LOSS: 1.0730615854263306
LOSS: 1.0730574131011963
LOSS: 1.0730527639389038
LOSS: 1.0730470418930054
LOSS: 1.073041319847107
LOSS: 1.073035478591919
LOSS: 1.0730292797088623
LOSS: 1.0730234384536743
LOSS: 1.0730167627334595
LOSS: 1.0730104446411133
LOSS: 1.073004961013794
LOSS: 1.0729995965957642
LOSS: 1.072994351387024
LOSS: 1.072989583015442
LOSS: 1.072985291481018
LOSS: 1.0729812383651733
LOSS: 1.0729780197143555
LOSS: 1.072974443435669
LOSS: 1.0729718208312988
LOSS: 1.0729691982269287
LOSS: 1.0729676485061646
LOSS: 1.0729660987854004
LOSS: 1.0729645490646362
LOSS: 1.0729634761810303
LOSS: 1.2126344442367554
LOSS: 1.2056185007095337
LOSS: 1.1989530324935913
LOSS: 1.1926604509353638
LOSS: 1.1867324113845825
LOSS: 1.181106686592102
LOSS: 1.175686240196228
LOSS: 1.1704012155532837
LOSS: 1.1652274131774902
LOSS: 1.1601667404174805
LOSS: 1.155230164527893
LOSS: 1.1504297256469727
LOSS: 1.1457762718200684
LOSS: 1.1412798166275024
LOSS: 1.1369482278823853
LOSS: 1.1327886581420898
LOSS: 1.1288068294525146
LOSS: 1.1250073909759521
LOSS: 1.1213935613632202
LOSS: 1.1179676055908203
LOSS: 1.1147302389144897
LOSS: 1.1116808652877808
LOSS: 1.10881769657135
LOSS: 1.1061365604400635
LOSS: 1.1036336421966553
LOSS: 1.1013027429580688
LOSS: 1.0991370677947998
LOSS: 1.097130298614502
LOSS: 1.095274567604065
LOSS: 1.0935635566711426
LOSS: 1.0919904708862305
LOSS: 1.090549349784851
LOSS: 1.0892343521118164
LOSS: 1.0880401134490967
LOSS: 1.086961030960083
LOSS: 1.0859909057617188
LOSS: 1.0851244926452637
LOSS: 1.0843546390533447
LOSS: 1.0836747884750366
LOSS: 1.0830779075622559
LOSS: 1.0825566053390503
LOSS: 1.0821040868759155
LOSS: 1.0817134380340576
LOSS: 1.0813777446746826
LOSS: 1.0810924768447876
LOSS: 1.0808523893356323
LOSS: 1.0806515216827393
LOSS: 1.0804872512817383
LOSS: 1.0803546905517578
LOSS: 1.080249309539795
LOSS: 1.0801688432693481
LOSS: 1.080108880996704
LOSS: 1.0800660848617554
LOSS: 1.0800378322601318
LOSS: 1.0800210237503052
LOSS: 1.0800131559371948
LOSS: 1.0800118446350098
LOSS: 1.0800154209136963
LOSS: 1.0800235271453857
LOSS: 1.0800334215164185
LOSS: 1.0800453424453735
LOSS: 1.080057978630066
LOSS: 1.0800708532333374
LOSS: 1.080082893371582
LOSS: 1.0800940990447998
LOSS: 1.0801037549972534
LOSS: 1.080112099647522
LOSS: 1.0801188945770264
LOSS: 1.0801231861114502
LOSS: 1.080126166343689
LOSS: 1.0801270008087158
LOSS: 1.0801265239715576
LOSS: 1.0801243782043457
LOSS: 1.0801211595535278
LOSS: 1.0801175832748413
LOSS: 1.0801125764846802
LOSS: 1.0801069736480713
LOSS: 1.0801002979278564
LOSS: 1.080094337463379
LOSS: 1.080087423324585
LOSS: 1.0800807476043701
LOSS: 1.0800732374191284
LOSS: 1.0800668001174927
LOSS: 1.0800602436065674
LOSS: 1.0800542831420898
LOSS: 1.0800483226776123
LOSS: 1.080042839050293
LOSS: 1.0800378322601318
LOSS: 1.0800334215164185
LOSS: 1.0800291299819946
LOSS: 1.0800254344940186
LOSS: 1.0800223350524902
LOSS: 1.0800197124481201
LOSS: 1.080017328262329
LOSS: 1.0800154209136963
LOSS: 1.080013632774353
LOSS: 1.0800120830535889
LOSS: 1.0800113677978516
LOSS: 1.0800102949142456
LOSS: 1.0800098180770874
LOSS: 1.2150603532791138
LOSS: 1.2080916166305542
LOSS: 1.2014763355255127
LOSS: 1.1952465772628784
LOSS: 1.1894252300262451
LOSS: 1.1840153932571411
LOSS: 1.1789861917495728
LOSS: 1.1742719411849976
LOSS: 1.1697933673858643
LOSS: 1.165490746498108
LOSS: 1.161332607269287
LOSS: 1.1573089361190796
LOSS: 1.1534197330474854
LOSS: 1.14966881275177
LOSS: 1.146060585975647
LOSS: 1.1425986289978027
LOSS: 1.1392854452133179
LOSS: 1.136123538017273
LOSS: 1.133114218711853
LOSS: 1.1302582025527954
LOSS: 1.1275559663772583
LOSS: 1.125007152557373
LOSS: 1.1226115226745605
LOSS: 1.1203675270080566
LOSS: 1.1182732582092285
LOSS: 1.1163265705108643
LOSS: 1.1145232915878296
LOSS: 1.1128594875335693
LOSS: 1.1113293170928955
LOSS: 1.1099269390106201
LOSS: 1.1086450815200806
LOSS: 1.1074769496917725
LOSS: 1.1064141988754272
LOSS: 1.105449914932251
LOSS: 1.1045756340026855
LOSS: 1.1037856340408325
LOSS: 1.1030725240707397
LOSS: 1.102431058883667
LOSS: 1.101855754852295
LOSS: 1.1013423204421997
LOSS: 1.1008867025375366
LOSS: 1.10048508644104
LOSS: 1.1001336574554443
LOSS: 1.0998295545578003
LOSS: 1.0995683670043945
LOSS: 1.0993471145629883
LOSS: 1.099161148071289
LOSS: 1.0990071296691895
LOSS: 1.0988807678222656
LOSS: 1.0987790822982788
LOSS: 1.0986977815628052
LOSS: 1.0986343622207642
LOSS: 1.098584771156311
LOSS: 1.0985480546951294
LOSS: 1.0985208749771118
LOSS: 1.098502516746521
LOSS: 1.0984904766082764
LOSS: 1.0984842777252197
LOSS: 1.0984828472137451
LOSS: 1.0984848737716675
LOSS: 1.0984898805618286
LOSS: 1.098496913909912
LOSS: 1.0985052585601807
LOSS: 1.0985137224197388
LOSS: 1.0985227823257446
LOSS: 1.0985312461853027
LOSS: 1.0985386371612549
LOSS: 1.0985453128814697
LOSS: 1.0985503196716309
LOSS: 1.098554253578186
LOSS: 1.098556637763977
LOSS: 1.0985581874847412
LOSS: 1.0985586643218994
LOSS: 1.098557949066162
LOSS: 1.098556637763977
LOSS: 1.0985544919967651
LOSS: 1.098551630973816
LOSS: 1.0985482931137085
LOSS: 1.098544716835022
LOSS: 1.0985411405563354
LOSS: 1.098536729812622
LOSS: 1.0985320806503296
LOSS: 1.0985280275344849
LOSS: 1.0985236167907715
LOSS: 1.0985190868377686
LOSS: 1.0985146760940552
LOSS: 1.098510503768921
LOSS: 1.0985064506530762
LOSS: 1.0985032320022583
LOSS: 1.0984997749328613
LOSS: 1.098496675491333
LOSS: 1.0984941720962524
LOSS: 1.0984913110733032
LOSS: 1.09848952293396
LOSS: 1.0984879732131958
LOSS: 1.098486065864563
LOSS: 1.098484992980957
LOSS: 1.0984835624694824
LOSS: 1.0984824895858765
LOSS: 1.0984816551208496
LOSS: 1.2109816074371338
LOSS: 1.2039144039154053
LOSS: 1.1971935033798218
LOSS: 1.1908265352249146
LOSS: 1.1847598552703857
LOSS: 1.1788818836212158
LOSS: 1.173111081123352
LOSS: 1.1674262285232544
LOSS: 1.1618343591690063
LOSS: 1.1563483476638794
LOSS: 1.150981068611145
LOSS: 1.145743489265442
LOSS: 1.1406455039978027
LOSS: 1.1356947422027588
LOSS: 1.1308982372283936
LOSS: 1.1262619495391846
LOSS: 1.1217902898788452
LOSS: 1.1174869537353516
LOSS: 1.1133545637130737
LOSS: 1.1093943119049072
LOSS: 1.1056060791015625
LOSS: 1.1019890308380127
LOSS: 1.0985409021377563
LOSS: 1.095258355140686
LOSS: 1.0921376943588257
LOSS: 1.0891751050949097
LOSS: 1.0863662958145142
LOSS: 1.0837074518203735
LOSS: 1.081195592880249
LOSS: 1.0788275003433228
LOSS: 1.076600432395935
LOSS: 1.0745118856430054
LOSS: 1.0725581645965576
LOSS: 1.0707365274429321
LOSS: 1.069042444229126
LOSS: 1.0674713850021362
LOSS: 1.066017746925354
LOSS: 1.064676284790039
LOSS: 1.063441276550293
LOSS: 1.0623066425323486
LOSS: 1.0612674951553345
LOSS: 1.060318946838379
LOSS: 1.059456467628479
LOSS: 1.058674693107605
LOSS: 1.0579700469970703
LOSS: 1.0573369264602661
LOSS: 1.0567717552185059
LOSS: 1.0562689304351807
LOSS: 1.0558240413665771
LOSS: 1.055431842803955
LOSS: 1.0550882816314697
LOSS: 1.0547888278961182
LOSS: 1.0545297861099243
LOSS: 1.054306983947754
LOSS: 1.106729507446289
LOSS: 1.1041256189346313
LOSS: 1.1016721725463867
LOSS: 1.099365234375
LOSS: 1.0971996784210205
LOSS: 1.095170497894287
LOSS: 1.093271017074585
LOSS: 1.0914946794509888
LOSS: 1.089835286140442
LOSS: 1.0882861614227295
LOSS: 1.0868407487869263
LOSS: 1.085493803024292
LOSS: 1.084240198135376
LOSS: 1.0830751657485962
LOSS: 1.081994891166687
LOSS: 1.080994963645935
LOSS: 1.0800727605819702
LOSS: 1.079224705696106
LOSS: 1.0784465074539185
LOSS: 1.0777349472045898
LOSS: 1.0770865678787231
LOSS: 1.0764966011047363
LOSS: 1.0759612321853638
LOSS: 1.0754765272140503
LOSS: 1.0750383138656616
LOSS: 1.0746421813964844
LOSS: 1.0742857456207275
LOSS: 1.0739645957946777
LOSS: 1.0736762285232544
LOSS: 1.0734190940856934
LOSS: 1.0731894969940186
LOSS: 1.0729857683181763
LOSS: 1.0728063583374023
LOSS: 1.0726479291915894
LOSS: 1.072509765625
LOSS: 1.0723899602890015
LOSS: 1.0722863674163818
LOSS: 1.0721967220306396
LOSS: 1.0721197128295898
LOSS: 1.0720546245574951
LOSS: 1.071999192237854
LOSS: 1.0719517469406128
LOSS: 1.0719130039215088
LOSS: 1.0718803405761719
LOSS: 1.071853518486023
LOSS: 1.071831464767456
LOSS: 1.071814775466919
LOSS: 1.0718013048171997
LOSS: 1.071791172027588
LOSS: 1.0717835426330566
LOSS: 1.0717787742614746
LOSS: 1.0717757940292358
LOSS: 1.0717743635177612
LOSS: 1.071772813796997
LOSS: 1.0717735290527344
LOSS: 1.0717737674713135
LOSS: 1.071775197982788
LOSS: 1.0717765092849731
LOSS: 1.0717780590057373
LOSS: 1.071779489517212
LOSS: 1.0717812776565552
LOSS: 1.0717825889587402
LOSS: 1.071784496307373
LOSS: 1.0717856884002686
LOSS: 1.0717865228652954
LOSS: 1.071787714958191
LOSS: 1.0717883110046387
LOSS: 1.071789026260376
LOSS: 1.071789026260376
LOSS: 1.071789026260376
LOSS: 1.0717891454696655
LOSS: 1.0717884302139282
LOSS: 1.0717878341674805
LOSS: 1.0717875957489014
LOSS: 1.0717874765396118
LOSS: 1.0717864036560059
LOSS: 1.071785569190979
LOSS: 1.1987136602401733
LOSS: 1.1912860870361328
LOSS: 1.184188723564148
LOSS: 1.1774519681930542
LOSS: 1.1710941791534424
LOSS: 1.1651113033294678
LOSS: 1.159462809562683
LOSS: 1.1540768146514893
LOSS: 1.1488807201385498
LOSS: 1.1438297033309937
LOSS: 1.1389070749282837
LOSS: 1.1341129541397095
LOSS: 1.1294543743133545
LOSS: 1.1249393224716187
LOSS: 1.12057626247406
LOSS: 1.1163716316223145
LOSS: 1.112330675125122
LOSS: 1.1084582805633545
LOSS: 1.1047570705413818
LOSS: 1.1012297868728638
LOSS: 1.0978776216506958
LOSS: 1.0947012901306152
LOSS: 1.0917000770568848
LOSS: 1.088873028755188
LOSS: 1.0862176418304443
LOSS: 1.0837305784225464
LOSS: 1.0814075469970703
LOSS: 1.0792431831359863
LOSS: 1.0772316455841064
LOSS: 1.0753660202026367
LOSS: 1.073639154434204
LOSS: 1.072043538093567
LOSS: 1.0705724954605103
LOSS: 1.0692188739776611
LOSS: 1.067976713180542
LOSS: 1.066839575767517
LOSS: 1.0658023357391357
LOSS: 1.064860224723816
LOSS: 1.0640090703964233
LOSS: 1.0632429122924805
LOSS: 1.0625582933425903
LOSS: 1.0619498491287231
LOSS: 1.061413288116455
LOSS: 1.0609426498413086
LOSS: 1.0605324506759644
LOSS: 1.060177206993103
LOSS: 1.0598725080490112
LOSS: 1.0596121549606323
LOSS: 1.0593911409378052
LOSS: 1.0592058897018433
LOSS: 1.0590513944625854
LOSS: 1.0589247941970825
LOSS: 1.0588220357894897
LOSS: 1.0587409734725952
LOSS: 1.0586777925491333
LOSS: 1.0586313009262085
LOSS: 1.058598279953003
LOSS: 1.0585767030715942
LOSS: 1.05856454372406
LOSS: 1.0585596561431885
LOSS: 1.0585602521896362
LOSS: 1.0585650205612183
LOSS: 1.058572769165039
LOSS: 1.0585819482803345
LOSS: 1.0585920810699463
LOSS: 1.058601975440979
LOSS: 1.0586121082305908
LOSS: 1.058620810508728
LOSS: 1.0586296319961548
LOSS: 1.058637022972107
LOSS: 1.0586429834365845
LOSS: 1.0586479902267456
LOSS: 1.0586512088775635
LOSS: 1.0586535930633545
LOSS: 1.0586544275283813
LOSS: 1.0586533546447754
LOSS: 1.0586518049240112
LOSS: 1.0586493015289307
LOSS: 1.0586457252502441
LOSS: 1.0586411952972412
LOSS: 1.0586365461349487
LOSS: 1.0586313009262085
LOSS: 1.058625340461731
LOSS: 1.0586203336715698
LOSS: 1.0586143732070923
LOSS: 1.058609127998352
LOSS: 1.0586038827896118
LOSS: 1.0585986375808716
LOSS: 1.058593511581421
LOSS: 1.058589220046997
LOSS: 1.0585845708847046
LOSS: 1.058580994606018
LOSS: 1.058577060699463
LOSS: 1.0585739612579346
LOSS: 1.058571457862854
LOSS: 1.0585685968399048
LOSS: 1.0585665702819824
LOSS: 1.0585646629333496
LOSS: 1.0585631132125854
LOSS: 1.0585620403289795
LOSS: 1.2092939615249634
LOSS: 1.2021524906158447
LOSS: 1.1953554153442383
LOSS: 1.188932180404663
LOSS: 1.1828961372375488
LOSS: 1.1772286891937256
LOSS: 1.1718684434890747
LOSS: 1.166729211807251
LOSS: 1.1617423295974731
LOSS: 1.156875729560852
LOSS: 1.152121663093567
LOSS: 1.1474840641021729
LOSS: 1.142970323562622
LOSS: 1.1385875940322876
LOSS: 1.1343426704406738
LOSS: 1.1302413940429688
LOSS: 1.1262885332107544
LOSS: 1.1224877834320068
LOSS: 1.1188430786132812
LOSS: 1.115356683731079
LOSS: 1.1120307445526123
LOSS: 1.1088664531707764
LOSS: 1.1058639287948608
LOSS: 1.1030232906341553
LOSS: 1.1003425121307373
LOSS: 1.0978190898895264
LOSS: 1.0954498052597046
LOSS: 1.0932292938232422
LOSS: 1.0911529064178467
LOSS: 1.0892142057418823
LOSS: 1.0874066352844238
LOSS: 1.0857234001159668
LOSS: 1.0841585397720337
LOSS: 1.082706093788147
LOSS: 1.0813592672348022
LOSS: 1.0801142454147339
LOSS: 1.0789659023284912
LOSS: 1.0779106616973877
LOSS: 1.0769439935684204
LOSS: 1.0760622024536133
LOSS: 1.0752618312835693
LOSS: 1.0745387077331543
LOSS: 1.0738885402679443
LOSS: 1.0733067989349365
LOSS: 1.072788119316101
LOSS: 1.072327971458435
LOSS: 1.0719213485717773
LOSS: 1.0715630054473877
LOSS: 1.0712491273880005
LOSS: 1.0709747076034546
LOSS: 1.0707368850708008
LOSS: 1.070530891418457
LOSS: 1.0703554153442383
LOSS: 1.0702064037322998
LOSS: 1.0700812339782715
LOSS: 1.06997811794281
LOSS: 1.069893479347229
LOSS: 1.0698261260986328
LOSS: 1.0697733163833618
LOSS: 1.0697332620620728
LOSS: 1.0697029829025269
LOSS: 1.0696814060211182
LOSS: 1.069667100906372
LOSS: 1.0696580410003662
LOSS: 1.0696537494659424
LOSS: 1.0696525573730469
LOSS: 1.0696539878845215
LOSS: 1.0696574449539185
LOSS: 1.0696622133255005
LOSS: 1.0696678161621094
LOSS: 1.0696743726730347
LOSS: 1.069680094718933
LOSS: 1.0696865320205688
LOSS: 1.0696920156478882
LOSS: 1.0696974992752075
LOSS: 1.0697015523910522
LOSS: 1.0697050094604492
LOSS: 1.0697073936462402
LOSS: 1.0697089433670044
LOSS: 1.0697095394134521
LOSS: 1.0697094202041626
LOSS: 1.0697091817855835
LOSS: 1.0697076320648193
LOSS: 1.0697052478790283
LOSS: 1.0697031021118164
LOSS: 1.069701075553894
LOSS: 1.0696983337402344
LOSS: 1.0696946382522583
LOSS: 1.0696920156478882
LOSS: 1.069688320159912
LOSS: 1.0696851015090942
LOSS: 1.0696816444396973
LOSS: 1.0696783065795898
LOSS: 1.0696752071380615
LOSS: 1.0696725845336914
LOSS: 1.0696697235107422
LOSS: 1.0696672201156616
LOSS: 1.0696648359298706
LOSS: 1.06966233253479
LOSS: 1.0696606636047363
LOSS: 1.2150603532791138
LOSS: 1.2080916166305542
LOSS: 1.2014763355255127
LOSS: 1.1952465772628784
LOSS: 1.1894252300262451
LOSS: 1.1840153932571411
LOSS: 1.1789861917495728
LOSS: 1.1742719411849976
LOSS: 1.1697933673858643
LOSS: 1.165490746498108
LOSS: 1.161332607269287
LOSS: 1.1573089361190796
LOSS: 1.1534197330474854
LOSS: 1.14966881275177
LOSS: 1.146060585975647
LOSS: 1.1425986289978027
LOSS: 1.1392854452133179
LOSS: 1.136123538017273
LOSS: 1.133114218711853
LOSS: 1.1302582025527954
LOSS: 1.1275559663772583
LOSS: 1.125007152557373
LOSS: 1.1226115226745605
LOSS: 1.1203675270080566
LOSS: 1.1182732582092285
LOSS: 1.1163265705108643
LOSS: 1.1145232915878296
LOSS: 1.1128594875335693
LOSS: 1.1113293170928955
LOSS: 1.1099269390106201
LOSS: 1.1086450815200806
LOSS: 1.1074769496917725
LOSS: 1.1064141988754272
LOSS: 1.105449914932251
LOSS: 1.1045756340026855
LOSS: 1.1037856340408325
LOSS: 1.1030725240707397
LOSS: 1.102431058883667
LOSS: 1.101855754852295
LOSS: 1.1013423204421997
LOSS: 1.1008867025375366
LOSS: 1.10048508644104
LOSS: 1.1001336574554443
LOSS: 1.0998295545578003
LOSS: 1.0995683670043945
LOSS: 1.0993471145629883
LOSS: 1.099161148071289
LOSS: 1.0990071296691895
LOSS: 1.0988807678222656
LOSS: 1.0987790822982788
LOSS: 1.0986977815628052
LOSS: 1.0986343622207642
LOSS: 1.098584771156311
LOSS: 1.0985480546951294
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.1003425121307373
LOSS: 1.0978190898895264
LOSS: 1.0954498052597046
LOSS: 1.0932292938232422
LOSS: 1.0911529064178467
LOSS: 1.0892142057418823
LOSS: 1.0874066352844238
LOSS: 1.0857234001159668
LOSS: 1.0841585397720337
LOSS: 1.082706093788147
LOSS: 1.0813592672348022
LOSS: 1.0801142454147339
LOSS: 1.0789659023284912
LOSS: 1.0779106616973877
LOSS: 1.0769439935684204
LOSS: 1.0760622024536133
LOSS: 1.0752618312835693
LOSS: 1.0745387077331543
LOSS: 1.0738885402679443
LOSS: 1.0733067989349365
LOSS: 1.072788119316101
LOSS: 1.072327971458435
LOSS: 1.0719213485717773
LOSS: 1.0715630054473877
LOSS: 1.0712491273880005
LOSS: 1.0709747076034546
LOSS: 1.0707368850708008
LOSS: 1.070530891418457
LOSS: 1.0703554153442383
LOSS: 1.0702064037322998
LOSS: 1.0700812339782715
LOSS: 1.06997811794281
LOSS: 1.069893479347229
LOSS: 1.0698261260986328
LOSS: 1.0697733163833618
LOSS: 1.0697332620620728
LOSS: 1.0697029829025269
LOSS: 1.0696814060211182
LOSS: 1.069667100906372
LOSS: 1.0696580410003662
LOSS: 1.0696537494659424
LOSS: 1.0696525573730469
LOSS: 1.0696539878845215
LOSS: 1.0696574449539185
LOSS: 1.0696622133255005
LOSS: 1.0696678161621094
LOSS: 1.0696743726730347
LOSS: 1.069680094718933
LOSS: 1.0696865320205688
LOSS: 1.0696920156478882
LOSS: 1.0696974992752075
LOSS: 1.0697015523910522
LOSS: 1.0697050094604492
LOSS: 1.0697073936462402
LOSS: 1.0697089433670044
LOSS: 1.0697095394134521
LOSS: 1.0697094202041626
LOSS: 1.0697091817855835
LOSS: 1.0697076320648193
LOSS: 1.0697052478790283
LOSS: 1.0697031021118164
LOSS: 1.069701075553894
LOSS: 1.0696983337402344
LOSS: 1.0696946382522583
LOSS: 1.0696920156478882
LOSS: 1.069688320159912
LOSS: 1.0696851015090942
LOSS: 1.0696816444396973
LOSS: 1.0696783065795898
LOSS: 1.0696752071380615
LOSS: 1.0696725845336914
LOSS: 1.0696697235107422
LOSS: 1.0696672201156616
LOSS: 1.0696648359298706
LOSS: 1.06966233253479
LOSS: 1.0696606636047363
LOSS: 1.1987136602401733
LOSS: 1.1912860870361328
LOSS: 1.184188723564148
LOSS: 1.1774519681930542
LOSS: 1.1710941791534424
LOSS: 1.1651113033294678
LOSS: 1.159462809562683
LOSS: 1.1540768146514893
LOSS: 1.1488807201385498
LOSS: 1.1438297033309937
LOSS: 1.1389070749282837
LOSS: 1.1341129541397095
LOSS: 1.1294543743133545
LOSS: 1.1249393224716187
LOSS: 1.12057626247406
LOSS: 1.1163716316223145
LOSS: 1.112330675125122
LOSS: 1.1084582805633545
LOSS: 1.1047570705413818
LOSS: 1.1012297868728638
LOSS: 1.0978776216506958
LOSS: 1.0947012901306152
LOSS: 1.0917000770568848
LOSS: 1.088873028755188
LOSS: 1.0862176418304443
LOSS: 1.0837305784225464
LOSS: 1.0814075469970703
LOSS: 1.0792431831359863
LOSS: 1.0772316455841064
LOSS: 1.0753660202026367
LOSS: 1.073639154434204
LOSS: 1.072043538093567
LOSS: 1.0705724954605103
LOSS: 1.0692188739776611
LOSS: 1.067976713180542
LOSS: 1.066839575767517
LOSS: 1.0658023357391357
LOSS: 1.064860224723816
LOSS: 1.0640090703964233
LOSS: 1.0632429122924805
LOSS: 1.0625582933425903
LOSS: 1.0619498491287231
LOSS: 1.061413288116455
LOSS: 1.0609426498413086
LOSS: 1.0605324506759644
LOSS: 1.060177206993103
LOSS: 1.0598725080490112
LOSS: 1.0596121549606323
LOSS: 1.0593911409378052
LOSS: 1.0592058897018433
LOSS: 1.0590513944625854
LOSS: 1.0589247941970825
LOSS: 1.0588220357894897
LOSS: 1.0587409734725952
LOSS: 1.0586777925491333
LOSS: 1.0586313009262085
LOSS: 1.058598279953003
LOSS: 1.0585767030715942
LOSS: 1.05856454372406
LOSS: 1.0585596561431885
LOSS: 1.0585602521896362
LOSS: 1.0585650205612183
LOSS: 1.058572769165039
LOSS: 1.0585819482803345
LOSS: 1.0585920810699463
LOSS: 1.058601975440979
LOSS: 1.0586121082305908
LOSS: 1.058620810508728
LOSS: 1.0586296319961548
LOSS: 1.058637022972107
LOSS: 1.0586429834365845
LOSS: 1.0586479902267456
LOSS: 1.0586512088775635
LOSS: 1.0586535930633545
LOSS: 1.0586544275283813
LOSS: 1.0586533546447754
LOSS: 1.0586518049240112
LOSS: 1.0586493015289307
LOSS: 1.0586457252502441
LOSS: 1.0586411952972412
LOSS: 1.0586365461349487
LOSS: 1.0586313009262085
LOSS: 1.058625340461731
LOSS: 1.0586203336715698
LOSS: 1.0586143732070923
LOSS: 1.058609127998352
LOSS: 1.0586038827896118
LOSS: 1.0585986375808716
LOSS: 1.058593511581421
LOSS: 1.058589220046997
LOSS: 1.0585845708847046
LOSS: 1.058580994606018
LOSS: 1.058577060699463
LOSS: 1.0585739612579346
LOSS: 1.058571457862854
LOSS: 1.0585685968399048
LOSS: 1.0585665702819824
LOSS: 1.0585646629333496
LOSS: 1.0585631132125854
LOSS: 1.0585620403289795
LOSS: 1.3963652849197388
LOSS: 1.390665888786316
LOSS: 1.3853849172592163
LOSS: 1.3805444240570068
LOSS: 1.376206636428833
LOSS: 1.3723933696746826
LOSS: 1.3691387176513672
LOSS: 1.3664405345916748
LOSS: 1.3642923831939697
LOSS: 1.3626784086227417
LOSS: 1.3614455461502075
LOSS: 1.360551357269287
LOSS: 1.3598363399505615
LOSS: 1.359184741973877
LOSS: 1.358471393585205
LOSS: 1.3576412200927734
LOSS: 1.356663703918457
LOSS: 1.3554760217666626
LOSS: 1.3541679382324219
LOSS: 1.3527131080627441
LOSS: 1.3511217832565308
LOSS: 1.3494967222213745
LOSS: 1.3478401899337769
LOSS: 1.3462415933609009
LOSS: 1.3446711301803589
LOSS: 1.343073844909668
LOSS: 1.3416109085083008
LOSS: 1.3401985168457031
LOSS: 1.3388561010360718
LOSS: 1.3375695943832397
LOSS: 1.336380124092102
LOSS: 1.335240364074707
LOSS: 1.3341436386108398
LOSS: 1.3330581188201904
LOSS: 1.332022786140442
LOSS: 1.3310141563415527
LOSS: 1.3300095796585083
LOSS: 1.3290201425552368
LOSS: 1.3279898166656494
LOSS: 1.3270232677459717
LOSS: 1.3260682821273804
LOSS: 1.3252102136611938
LOSS: 1.3242762088775635
LOSS: 1.3235236406326294
LOSS: 1.3227139711380005
LOSS: 1.3220233917236328
LOSS: 1.3214433193206787
LOSS: 1.320912480354309
LOSS: 1.3203983306884766
LOSS: 1.3199623823165894
LOSS: 1.319631814956665
LOSS: 1.3193310499191284
LOSS: 1.319062352180481
LOSS: 1.3188843727111816
LOSS: 1.3187065124511719
LOSS: 1.318588376045227
LOSS: 1.3184157609939575
LOSS: 1.3183494806289673
LOSS: 1.3182955980300903
LOSS: 1.318248987197876
LOSS: 1.3182252645492554
LOSS: 1.3182153701782227
LOSS: 1.3183046579360962
LOSS: 1.3182363510131836
LOSS: 1.31827974319458
LOSS: 1.3183029890060425
LOSS: 1.3183249235153198
LOSS: 1.3183720111846924
LOSS: 1.318386197090149
LOSS: 1.3183985948562622
LOSS: 1.3183956146240234
LOSS: 1.3183974027633667
LOSS: 1.3183815479278564
LOSS: 1.3183749914169312
LOSS: 1.318358063697815
LOSS: 1.318365454673767
LOSS: 1.318310260772705
LOSS: 1.3183354139328003
LOSS: 1.318263053894043
LOSS: 1.3182666301727295
LOSS: 1.3182445764541626
LOSS: 1.3182079792022705
LOSS: 1.3182157278060913
LOSS: 1.3181618452072144
LOSS: 1.3182121515274048
LOSS: 1.3181543350219727
LOSS: 1.3181394338607788
LOSS: 1.3181463479995728
LOSS: 1.3181039094924927
LOSS: 1.3181040287017822
LOSS: 1.318081021308899
LOSS: 1.318107008934021
LOSS: 1.3180983066558838
LOSS: 1.3180862665176392
LOSS: 1.318081021308899
LOSS: 1.3180696964263916
LOSS: 1.3180726766586304
LOSS: 1.3181239366531372
LOSS: 1.3180965185165405
LOSS: 1.3180865049362183
LOSS: 1.4131693840026855
LOSS: 1.407907247543335
LOSS: 1.403067946434021
LOSS: 1.3987314701080322
LOSS: 1.3949158191680908
LOSS: 1.3916172981262207
LOSS: 1.388924479484558
LOSS: 1.386788010597229
LOSS: 1.385202169418335
LOSS: 1.3841251134872437
LOSS: 1.3834747076034546
LOSS: 1.3830599784851074
LOSS: 1.3828153610229492
LOSS: 1.3825252056121826
LOSS: 1.3821616172790527
LOSS: 1.3816654682159424
LOSS: 1.3809373378753662
LOSS: 1.380030632019043
LOSS: 1.3789702653884888
LOSS: 1.3777915239334106
LOSS: 1.3765344619750977
LOSS: 1.3752219676971436
LOSS: 1.3739278316497803
LOSS: 1.3726521730422974
LOSS: 1.3714238405227661
LOSS: 1.3701987266540527
LOSS: 1.3690741062164307
LOSS: 1.3679516315460205
LOSS: 1.3669633865356445
LOSS: 1.3659659624099731
LOSS: 1.3649461269378662
LOSS: 1.3640049695968628
LOSS: 1.3630638122558594
LOSS: 1.3620786666870117
LOSS: 1.361096739768982
LOSS: 1.3601138591766357
LOSS: 1.3591065406799316
LOSS: 1.3581154346466064
LOSS: 1.3571207523345947
LOSS: 1.3560664653778076
LOSS: 1.3550491333007812
LOSS: 1.354058861732483
LOSS: 1.3531115055084229
LOSS: 1.352170467376709
LOSS: 1.351271152496338
LOSS: 1.3504570722579956
LOSS: 1.349684476852417
LOSS: 1.3489038944244385
LOSS: 1.348239779472351
LOSS: 1.3475710153579712
LOSS: 1.3469634056091309
LOSS: 1.3464505672454834
LOSS: 1.345932126045227
LOSS: 1.345460295677185
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.0800660848617554
LOSS: 1.0800378322601318
LOSS: 1.0800210237503052
LOSS: 1.0800131559371948
LOSS: 1.0800118446350098
LOSS: 1.0800154209136963
LOSS: 1.0800235271453857
LOSS: 1.0800334215164185
LOSS: 1.0800453424453735
LOSS: 1.080057978630066
LOSS: 1.0800708532333374
LOSS: 1.080082893371582
LOSS: 1.0800940990447998
LOSS: 1.0801037549972534
LOSS: 1.080112099647522
LOSS: 1.0801188945770264
LOSS: 1.0801231861114502
LOSS: 1.080126166343689
LOSS: 1.0801270008087158
LOSS: 1.0801265239715576
LOSS: 1.0801243782043457
LOSS: 1.0801211595535278
LOSS: 1.0801175832748413
LOSS: 1.0801125764846802
LOSS: 1.0801069736480713
LOSS: 1.0801002979278564
LOSS: 1.080094337463379
LOSS: 1.080087423324585
LOSS: 1.0800807476043701
LOSS: 1.0800732374191284
LOSS: 1.0800668001174927
LOSS: 1.0800602436065674
LOSS: 1.0800542831420898
LOSS: 1.0800483226776123
LOSS: 1.080042839050293
LOSS: 1.0800378322601318
LOSS: 1.0800334215164185
LOSS: 1.0800291299819946
LOSS: 1.0800254344940186
LOSS: 1.0800223350524902
LOSS: 1.0800197124481201
LOSS: 1.080017328262329
LOSS: 1.0800154209136963
LOSS: 1.080013632774353
LOSS: 1.0800120830535889
LOSS: 1.0800113677978516
LOSS: 1.0800102949142456
LOSS: 1.0800098180770874
LOSS: 1.4131693840026855
LOSS: 1.407907247543335
LOSS: 1.403067946434021
LOSS: 1.3987314701080322
LOSS: 1.3949158191680908
LOSS: 1.3916172981262207
LOSS: 1.388924479484558
LOSS: 1.386788010597229
LOSS: 1.385202169418335
LOSS: 1.3841251134872437
LOSS: 1.3834747076034546
LOSS: 1.3830599784851074
LOSS: 1.3828153610229492
LOSS: 1.3825252056121826
LOSS: 1.3821616172790527
LOSS: 1.3816654682159424
LOSS: 1.3809373378753662
LOSS: 1.380030632019043
LOSS: 1.3789702653884888
LOSS: 1.3777915239334106
LOSS: 1.3765344619750977
LOSS: 1.3752219676971436
LOSS: 1.3739278316497803
LOSS: 1.3726521730422974
LOSS: 1.3714238405227661
LOSS: 1.3701987266540527
LOSS: 1.3690741062164307
LOSS: 1.3679516315460205
LOSS: 1.3669633865356445
LOSS: 1.3659659624099731
LOSS: 1.3649461269378662
LOSS: 1.3640049695968628
LOSS: 1.3630638122558594
LOSS: 1.3620786666870117
LOSS: 1.361096739768982
LOSS: 1.3601138591766357
LOSS: 1.3591065406799316
LOSS: 1.3581154346466064
LOSS: 1.3571207523345947
LOSS: 1.3560664653778076
LOSS: 1.3550491333007812
LOSS: 1.354058861732483
LOSS: 1.3531115055084229
LOSS: 1.352170467376709
LOSS: 1.351271152496338
LOSS: 1.3504570722579956
LOSS: 1.349684476852417
LOSS: 1.3489038944244385
LOSS: 1.348239779472351
LOSS: 1.3475710153579712
LOSS: 1.3469634056091309
LOSS: 1.3464505672454834
LOSS: 1.345932126045227
LOSS: 1.345460295677185
LOSS: 1.3450205326080322
LOSS: 1.3446704149246216
LOSS: 1.3443504571914673
LOSS: 1.3440275192260742
LOSS: 1.3438054323196411
LOSS: 1.3436270952224731
LOSS: 1.343404769897461
LOSS: 1.3432925939559937
LOSS: 1.3431763648986816
LOSS: 1.3431518077850342
LOSS: 1.3430839776992798
LOSS: 1.3430649042129517
LOSS: 1.3430397510528564
LOSS: 1.3430132865905762
LOSS: 1.3430615663528442
LOSS: 1.343050479888916
LOSS: 1.3430747985839844
LOSS: 1.3430832624435425
LOSS: 1.3431079387664795
LOSS: 1.3431612253189087
LOSS: 1.3431427478790283
LOSS: 1.3431395292282104
LOSS: 1.343138575553894
LOSS: 1.3431886434555054
LOSS: 1.3431907892227173
LOSS: 1.343184232711792
LOSS: 1.343138575553894
LOSS: 1.3431259393692017
LOSS: 1.3431223630905151
LOSS: 1.3431185483932495
LOSS: 1.3430712223052979
LOSS: 1.343098759651184
LOSS: 1.3430745601654053
LOSS: 1.3430814743041992
LOSS: 1.343044638633728
LOSS: 1.343042016029358
LOSS: 1.3430414199829102
LOSS: 1.343021273612976
LOSS: 1.3430509567260742
LOSS: 1.34297513961792
LOSS: 1.3430238962173462
LOSS: 1.3429601192474365
LOSS: 1.3429670333862305
LOSS: 1.342966079711914
LOSS: 1.342957854270935
LOSS: 1.3429194688796997
LOSS: 1.400840401649475
LOSS: 1.3952711820602417
LOSS: 1.3901392221450806
LOSS: 1.3854743242263794
LOSS: 1.381301760673523
LOSS: 1.377686619758606
LOSS: 1.3746010065078735
LOSS: 1.3721009492874146
LOSS: 1.3701518774032593
LOSS: 1.3686834573745728
LOSS: 1.3676220178604126
LOSS: 1.3668606281280518
LOSS: 1.3661836385726929
LOSS: 1.3655529022216797
LOSS: 1.3648653030395508
LOSS: 1.3640098571777344
LOSS: 1.3630218505859375
LOSS: 1.3618521690368652
LOSS: 1.3605221509933472
LOSS: 1.3590424060821533
LOSS: 1.3575029373168945
LOSS: 1.3559229373931885
LOSS: 1.3542912006378174
LOSS: 1.3527032136917114
LOSS: 1.3511855602264404
LOSS: 1.349736213684082
LOSS: 1.3482654094696045
LOSS: 1.346906304359436
LOSS: 1.345618486404419
LOSS: 1.3443996906280518
LOSS: 1.3432447910308838
LOSS: 1.3421353101730347
LOSS: 1.3410298824310303
LOSS: 1.3399863243103027
LOSS: 1.338927149772644
LOSS: 1.3379029035568237
LOSS: 1.336865782737732
LOSS: 1.3359003067016602
LOSS: 1.3348363637924194
LOSS: 1.3338638544082642
LOSS: 1.3329449892044067
LOSS: 1.332022786140442
LOSS: 1.3311859369277954
LOSS: 1.3303271532058716
LOSS: 1.3295996189117432
LOSS: 1.3289167881011963
LOSS: 1.328275203704834
LOSS: 1.327785611152649
LOSS: 1.3273156881332397
LOSS: 1.3268753290176392
LOSS: 1.326514720916748
LOSS: 1.3262712955474854
LOSS: 1.3260172605514526
LOSS: 1.3257380723953247
LOSS: 1.3256107568740845
LOSS: 1.3253867626190186
LOSS: 1.3253601789474487
LOSS: 1.3252733945846558
LOSS: 1.3251832723617554
LOSS: 1.325156569480896
LOSS: 1.3252137899398804
LOSS: 1.3252466917037964
LOSS: 1.325231671333313
LOSS: 1.3252960443496704
LOSS: 1.3252105712890625
LOSS: 1.3253370523452759
LOSS: 1.3252954483032227
LOSS: 1.325318694114685
LOSS: 1.3254061937332153
LOSS: 1.3253874778747559
LOSS: 1.3253991603851318
LOSS: 1.3254575729370117
LOSS: 1.3254104852676392
LOSS: 1.3254088163375854
LOSS: 1.3253834247589111
LOSS: 1.3253848552703857
LOSS: 1.3254003524780273
LOSS: 1.3253083229064941
LOSS: 1.3253004550933838
LOSS: 1.3253405094146729
LOSS: 1.3252661228179932
LOSS: 1.3252414464950562
LOSS: 1.3252449035644531
LOSS: 1.3252168893814087
LOSS: 1.3252297639846802
LOSS: 1.3252439498901367
LOSS: 1.3251615762710571
LOSS: 1.3251968622207642
LOSS: 1.3251889944076538
LOSS: 1.3251464366912842
LOSS: 1.3252177238464355
LOSS: 1.3251677751541138
LOSS: 1.3251606225967407
LOSS: 1.3252075910568237
LOSS: 1.3251999616622925
LOSS: 1.325168251991272
LOSS: 1.3251311779022217
LOSS: 1.3251737356185913
LOSS: 1.3251689672470093
LOSS: 1.3251804113388062
LOSS: 1.4005968570709229
LOSS: 1.3949611186981201
LOSS: 1.3897650241851807
LOSS: 1.3850295543670654
LOSS: 1.380782961845398
LOSS: 1.377070426940918
LOSS: 1.3739181756973267
LOSS: 1.371328353881836
LOSS: 1.3693175315856934
LOSS: 1.3677946329116821
LOSS: 1.3666917085647583
LOSS: 1.365920066833496
LOSS: 1.3653870820999146
LOSS: 1.3648502826690674
LOSS: 1.3643003702163696
LOSS: 1.363610029220581
LOSS: 1.3627597093582153
LOSS: 1.3617430925369263
LOSS: 1.3605637550354004
LOSS: 1.3592666387557983
LOSS: 1.3578696250915527
LOSS: 1.3564035892486572
LOSS: 1.3549250364303589
LOSS: 1.3534711599349976
LOSS: 1.3520455360412598
LOSS: 1.3506462574005127
LOSS: 1.3493646383285522
LOSS: 1.3481509685516357
LOSS: 1.3470063209533691
LOSS: 1.3459675312042236
LOSS: 1.3449152708053589
LOSS: 1.3439693450927734
LOSS: 1.3430427312850952
LOSS: 1.342166781425476
LOSS: 1.3413004875183105
LOSS: 1.3403977155685425
LOSS: 1.339561104774475
LOSS: 1.3387529850006104
LOSS: 1.3378617763519287
LOSS: 1.3370208740234375
LOSS: 1.3362417221069336
LOSS: 1.335464358329773
LOSS: 1.3347941637039185
LOSS: 1.334096074104309
LOSS: 1.3334696292877197
LOSS: 1.3329286575317383
LOSS: 1.3324567079544067
LOSS: 1.3319858312606812
LOSS: 1.3316280841827393
LOSS: 1.331300973892212
LOSS: 1.331034779548645
LOSS: 1.3308498859405518
LOSS: 1.3306218385696411
LOSS: 1.3304705619812012
LOSS: 1.3303577899932861
LOSS: 1.330235481262207
LOSS: 1.3301836252212524
LOSS: 1.3301128149032593
LOSS: 1.3300868272781372
LOSS: 1.3300725221633911
LOSS: 1.3300750255584717
LOSS: 1.3300609588623047
LOSS: 1.3301353454589844
LOSS: 1.33010733127594
LOSS: 1.3301196098327637
LOSS: 1.3301267623901367
LOSS: 1.330222725868225
LOSS: 1.3301780223846436
LOSS: 1.3301750421524048
LOSS: 1.3301907777786255
LOSS: 1.3302080631256104
LOSS: 1.3301928043365479
LOSS: 1.3301748037338257
LOSS: 1.3301327228546143
LOSS: 1.330147624015808
LOSS: 1.3301591873168945
LOSS: 1.3301231861114502
LOSS: 1.3301379680633545
LOSS: 1.3300909996032715
LOSS: 1.3301012516021729
LOSS: 1.3300700187683105
LOSS: 1.3299970626831055
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.0541174411773682
LOSS: 1.0539579391479492
LOSS: 1.053824543952942
LOSS: 1.0537148714065552
LOSS: 1.0536267757415771
LOSS: 1.0535560846328735
LOSS: 1.053501009941101
LOSS: 1.0534591674804688
LOSS: 1.0534281730651855
LOSS: 1.0534073114395142
LOSS: 1.053392767906189
LOSS: 1.053385615348816
LOSS: 1.0533825159072876
LOSS: 1.0533840656280518
LOSS: 1.0533877611160278
LOSS: 1.0533937215805054
LOSS: 1.053400993347168
LOSS: 1.0534088611602783
LOSS: 1.0534167289733887
LOSS: 1.0534236431121826
LOSS: 1.0534309148788452
LOSS: 1.0534368753433228
LOSS: 1.053442120552063
LOSS: 1.0534465312957764
LOSS: 1.0534499883651733
LOSS: 1.0534523725509644
LOSS: 1.0534535646438599
LOSS: 1.053453803062439
LOSS: 1.0534533262252808
LOSS: 1.0534518957138062
LOSS: 1.0534497499465942
LOSS: 1.053446888923645
LOSS: 1.0534440279006958
LOSS: 1.0534404516220093
LOSS: 1.053436517715454
LOSS: 1.0534323453903198
LOSS: 1.053428292274475
LOSS: 1.0534242391586304
LOSS: 1.0534203052520752
LOSS: 1.05341637134552
LOSS: 1.053412675857544
LOSS: 1.0534087419509888
LOSS: 1.0534054040908813
LOSS: 1.053402066230774
LOSS: 1.0533994436264038
LOSS: 1.0533969402313232
LOSS: 1.397369623184204
LOSS: 1.3916854858398438
LOSS: 1.3864246606826782
LOSS: 1.3816090822219849
LOSS: 1.3772841691970825
LOSS: 1.373509407043457
LOSS: 1.370247483253479
LOSS: 1.3675562143325806
LOSS: 1.3654130697250366
LOSS: 1.363752841949463
LOSS: 1.3625085353851318
LOSS: 1.3614919185638428
LOSS: 1.3606258630752563
LOSS: 1.3597872257232666
LOSS: 1.3588435649871826
LOSS: 1.3578113317489624
LOSS: 1.3565131425857544
LOSS: 1.355093240737915
LOSS: 1.35346257686615
LOSS: 1.351747751235962
LOSS: 1.3498625755310059
LOSS: 1.347938895225525
LOSS: 1.345982313156128
LOSS: 1.3440507650375366
LOSS: 1.3420917987823486
LOSS: 1.3401705026626587
LOSS: 1.3383159637451172
LOSS: 1.3364843130111694
LOSS: 1.3347771167755127
LOSS: 1.3330469131469727
LOSS: 1.3313905000686646
LOSS: 1.3297630548477173
LOSS: 1.3281872272491455
LOSS: 1.3266229629516602
LOSS: 1.3250341415405273
LOSS: 1.3234745264053345
LOSS: 1.3219298124313354
LOSS: 1.320344090461731
LOSS: 1.3187901973724365
LOSS: 1.3172017335891724
LOSS: 1.315663456916809
LOSS: 1.314123272895813
LOSS: 1.3126277923583984
LOSS: 1.3112190961837769
LOSS: 1.3097785711288452
LOSS: 1.3084182739257812
LOSS: 1.307144284248352
LOSS: 1.305936336517334
LOSS: 1.304826021194458
LOSS: 1.3037406206130981
LOSS: 1.3027554750442505
LOSS: 1.3018406629562378
LOSS: 1.3009653091430664
LOSS: 1.3001868724822998
LOSS: 1.299491047859192
LOSS: 1.298825979232788
LOSS: 1.2982176542282104
LOSS: 1.2976776361465454
LOSS: 1.2972149848937988
LOSS: 1.2967933416366577
LOSS: 1.2965081930160522
LOSS: 1.2961872816085815
LOSS: 1.2959182262420654
LOSS: 1.2957769632339478
LOSS: 1.2956479787826538
LOSS: 1.295524001121521
LOSS: 1.295414924621582
LOSS: 1.2954033613204956
LOSS: 1.295357346534729
LOSS: 1.2953879833221436
LOSS: 1.2953637838363647
LOSS: 1.2953664064407349
LOSS: 1.2954344749450684
LOSS: 1.2954012155532837
LOSS: 1.2954261302947998
LOSS: 1.2955026626586914
LOSS: 1.2954953908920288
LOSS: 1.295507550239563
LOSS: 1.2955341339111328
LOSS: 1.295583963394165
LOSS: 1.2955610752105713
LOSS: 1.2955472469329834
LOSS: 1.2955330610275269
LOSS: 1.2955111265182495
LOSS: 1.2955125570297241
LOSS: 1.295506477355957
LOSS: 1.295533299446106
LOSS: 1.2954720258712769
LOSS: 1.2954256534576416
LOSS: 1.2954012155532837
LOSS: 1.2953814268112183
LOSS: 1.295375108718872
LOSS: 1.295392394065857
LOSS: 1.2953704595565796
LOSS: 1.295420527458191
LOSS: 1.2952929735183716
LOSS: 1.2952886819839478
LOSS: 1.2953308820724487
LOSS: 1.2953176498413086
LOSS: 1.2953119277954102
LOSS: 1.3963652849197388
LOSS: 1.390665888786316
LOSS: 1.3853849172592163
LOSS: 1.3805444240570068
LOSS: 1.376206636428833
LOSS: 1.3723933696746826
LOSS: 1.3691387176513672
LOSS: 1.3664405345916748
LOSS: 1.3642923831939697
LOSS: 1.3626784086227417
LOSS: 1.3614455461502075
LOSS: 1.360551357269287
LOSS: 1.3598363399505615
LOSS: 1.359184741973877
LOSS: 1.358471393585205
LOSS: 1.3576412200927734
LOSS: 1.356663703918457
LOSS: 1.3554760217666626
LOSS: 1.3541679382324219
LOSS: 1.3527131080627441
LOSS: 1.3511217832565308
LOSS: 1.3494967222213745
LOSS: 1.3478401899337769
LOSS: 1.3462415933609009
LOSS: 1.3446711301803589
LOSS: 1.343073844909668
LOSS: 1.3416109085083008
LOSS: 1.3401985168457031
LOSS: 1.3388561010360718
LOSS: 1.3375695943832397
LOSS: 1.336380124092102
LOSS: 1.335240364074707
LOSS: 1.3341436386108398
LOSS: 1.3330581188201904
LOSS: 1.332022786140442
LOSS: 1.3310141563415527
LOSS: 1.3300095796585083
LOSS: 1.3290201425552368
LOSS: 1.3279898166656494
LOSS: 1.3270232677459717
LOSS: 1.3260682821273804
LOSS: 1.3252102136611938
LOSS: 1.3242762088775635
LOSS: 1.3235236406326294
LOSS: 1.3227139711380005
LOSS: 1.3220233917236328
LOSS: 1.3214433193206787
LOSS: 1.320912480354309
LOSS: 1.3203983306884766
LOSS: 1.3199623823165894
LOSS: 1.319631814956665
LOSS: 1.3193310499191284
LOSS: 1.319062352180481
LOSS: 1.3188843727111816
LOSS: 1.3187065124511719
LOSS: 1.318588376045227
LOSS: 1.3184157609939575
LOSS: 1.3183494806289673
LOSS: 1.3182955980300903
LOSS: 1.318248987197876
LOSS: 1.3182252645492554
LOSS: 1.3182153701782227
LOSS: 1.3183046579360962
LOSS: 1.3182363510131836
LOSS: 1.31827974319458
LOSS: 1.3183029890060425
LOSS: 1.3183249235153198
LOSS: 1.3183720111846924
LOSS: 1.318386197090149
LOSS: 1.3183985948562622
LOSS: 1.3183956146240234
LOSS: 1.3183974027633667
LOSS: 1.3183815479278564
LOSS: 1.3183749914169312
LOSS: 1.318358063697815
LOSS: 1.318365454673767
LOSS: 1.318310260772705
LOSS: 1.3183354139328003
LOSS: 1.318263053894043
LOSS: 1.3182666301727295
LOSS: 1.3182445764541626
LOSS: 1.3182079792022705
LOSS: 1.3182157278060913
LOSS: 1.3181618452072144
LOSS: 1.3182121515274048
LOSS: 1.3181543350219727
LOSS: 1.3181394338607788
LOSS: 1.3181463479995728
LOSS: 1.3181039094924927
LOSS: 1.3181040287017822
LOSS: 1.318081021308899
LOSS: 1.318107008934021
LOSS: 1.3180983066558838
LOSS: 1.3180862665176392
LOSS: 1.318081021308899
LOSS: 1.3180696964263916
LOSS: 1.3180726766586304
LOSS: 1.3181239366531372
LOSS: 1.3180965185165405
LOSS: 1.3180865049362183
LOSS: 1.403220772743225
LOSS: 1.3975948095321655
LOSS: 1.392404317855835
LOSS: 1.3876984119415283
LOSS: 1.3834545612335205
LOSS: 1.3797457218170166
LOSS: 1.3766170740127563
LOSS: 1.3740400075912476
LOSS: 1.3720216751098633
LOSS: 1.3705296516418457
LOSS: 1.3695210218429565
LOSS: 1.3687858581542969
LOSS: 1.3682502508163452
LOSS: 1.367829442024231
LOSS: 1.3673774003982544
LOSS: 1.3667429685592651
LOSS: 1.3659157752990723
LOSS: 1.3649519681930542
LOSS: 1.3638362884521484
LOSS: 1.3625277280807495
LOSS: 1.361161470413208
LOSS: 1.3597686290740967
LOSS: 1.358257532119751
LOSS: 1.356819748878479
LOSS: 1.3554189205169678
LOSS: 1.3540467023849487
LOSS: 1.3527246713638306
LOSS: 1.351508617401123
LOSS: 1.3503739833831787
LOSS: 1.3493244647979736
LOSS: 1.3482195138931274
LOSS: 1.347254753112793
LOSS: 1.3462488651275635
LOSS: 1.3453316688537598
LOSS: 1.3443583250045776
LOSS: 1.343408465385437
LOSS: 1.3425105810165405
LOSS: 1.3415521383285522
LOSS: 1.3406094312667847
LOSS: 1.339755892753601
LOSS: 1.338773488998413
LOSS: 1.337951421737671
LOSS: 1.3370325565338135
LOSS: 1.336299180984497
LOSS: 1.3354891538619995
LOSS: 1.334838628768921
LOSS: 1.3341400623321533
LOSS: 1.333602786064148
LOSS: 1.3330057859420776
LOSS: 1.3325973749160767
LOSS: 1.332126259803772
LOSS: 1.331761360168457
LOSS: 1.3314311504364014
LOSS: 1.3311326503753662
LOSS: 1.3308945894241333
LOSS: 1.3306524753570557
LOSS: 1.3304892778396606
LOSS: 1.3303333520889282
LOSS: 1.3301907777786255
LOSS: 1.3300976753234863
LOSS: 1.330000400543213
LOSS: 1.3300014734268188
LOSS: 1.3299211263656616
LOSS: 1.3299431800842285
LOSS: 1.3299485445022583
LOSS: 1.329953670501709
LOSS: 1.3300725221633911
LOSS: 1.3300367593765259
LOSS: 1.3300312757492065
LOSS: 1.3300329446792603
LOSS: 1.3300496339797974
LOSS: 1.3300373554229736
LOSS: 1.3300942182540894
LOSS: 1.3300191164016724
LOSS: 1.3300307989120483
LOSS: 1.3301218748092651
LOSS: 1.330043911933899
LOSS: 1.330068826675415
LOSS: 1.330045223236084
LOSS: 1.3300824165344238
LOSS: 1.3300029039382935
LOSS: 1.329944372177124
LOSS: 1.3299341201782227
LOSS: 1.329964280128479
LOSS: 1.3299033641815186
LOSS: 1.0985208749771118
LOSS: 1.098502516746521
LOSS: 1.0984904766082764
LOSS: 1.0984842777252197
LOSS: 1.0984828472137451
LOSS: 1.0984848737716675
LOSS: 1.0984898805618286
LOSS: 1.098496913909912
LOSS: 1.0985052585601807
LOSS: 1.0985137224197388
LOSS: 1.0985227823257446
LOSS: 1.0985312461853027
LOSS: 1.0985386371612549
LOSS: 1.0985453128814697
LOSS: 1.0985503196716309
LOSS: 1.098554253578186
LOSS: 1.098556637763977
LOSS: 1.0985581874847412
LOSS: 1.0985586643218994
LOSS: 1.098557949066162
LOSS: 1.098556637763977
LOSS: 1.0985544919967651
LOSS: 1.098551630973816
LOSS: 1.0985482931137085
LOSS: 1.098544716835022
LOSS: 1.0985411405563354
LOSS: 1.098536729812622
LOSS: 1.0985320806503296
LOSS: 1.0985280275344849
LOSS: 1.0985236167907715
LOSS: 1.0985190868377686
LOSS: 1.0985146760940552
LOSS: 1.098510503768921
LOSS: 1.0985064506530762
LOSS: 1.0985032320022583
LOSS: 1.0984997749328613
LOSS: 1.098496675491333
LOSS: 1.0984941720962524
LOSS: 1.0984913110733032
LOSS: 1.09848952293396
LOSS: 1.0984879732131958
LOSS: 1.098486065864563
LOSS: 1.098484992980957
LOSS: 1.0984835624694824
LOSS: 1.0984824895858765
LOSS: 1.0984816551208496
LOSS: 1.4005968570709229
LOSS: 1.3949611186981201
LOSS: 1.3897650241851807
LOSS: 1.3850295543670654
LOSS: 1.380782961845398
LOSS: 1.377070426940918
LOSS: 1.3739181756973267
LOSS: 1.371328353881836
LOSS: 1.3693175315856934
LOSS: 1.3677946329116821
LOSS: 1.3666917085647583
LOSS: 1.365920066833496
LOSS: 1.3653870820999146
LOSS: 1.3648502826690674
LOSS: 1.3643003702163696
LOSS: 1.363610029220581
LOSS: 1.3627597093582153
LOSS: 1.3617430925369263
LOSS: 1.3605637550354004
LOSS: 1.3592666387557983
LOSS: 1.3578696250915527
LOSS: 1.3564035892486572
LOSS: 1.3549250364303589
LOSS: 1.3534711599349976
LOSS: 1.3520455360412598
LOSS: 1.3506462574005127
LOSS: 1.3493646383285522
LOSS: 1.3481509685516357
LOSS: 1.3470063209533691
LOSS: 1.3459675312042236
LOSS: 1.3449152708053589
LOSS: 1.3439693450927734
LOSS: 1.3430427312850952
LOSS: 1.342166781425476
LOSS: 1.3413004875183105
LOSS: 1.3403977155685425
LOSS: 1.339561104774475
LOSS: 1.3387529850006104
LOSS: 1.3378617763519287
LOSS: 1.3370208740234375
LOSS: 1.3362417221069336
LOSS: 1.335464358329773
LOSS: 1.3347941637039185
LOSS: 1.334096074104309
LOSS: 1.3334696292877197
LOSS: 1.3329286575317383
LOSS: 1.3324567079544067
LOSS: 1.3319858312606812
LOSS: 1.3316280841827393
LOSS: 1.331300973892212
LOSS: 1.331034779548645
LOSS: 1.3308498859405518
LOSS: 1.3306218385696411
LOSS: 1.3304705619812012
LOSS: 1.3303577899932861
LOSS: 1.330235481262207
LOSS: 1.3301836252212524
LOSS: 1.3301128149032593
LOSS: 1.3300868272781372
LOSS: 1.3300725221633911
LOSS: 1.3300750255584717
LOSS: 1.3300609588623047
LOSS: 1.3301353454589844
LOSS: 1.33010733127594
LOSS: 1.3301196098327637
LOSS: 1.3301267623901367
LOSS: 1.330222725868225
LOSS: 1.3301780223846436
LOSS: 1.3301750421524048
LOSS: 1.3301907777786255
LOSS: 1.3302080631256104
LOSS: 1.3301928043365479
LOSS: 1.3301748037338257
LOSS: 1.3301327228546143
LOSS: 1.330147624015808
LOSS: 1.3301591873168945
LOSS: 1.3301231861114502
LOSS: 1.3301379680633545
LOSS: 1.3300909996032715
LOSS: 1.3301012516021729
LOSS: 1.3300700187683105
LOSS: 1.3299970626831055
LOSS: 1.3300398588180542
LOSS: 1.3300191164016724
LOSS: 1.3299981355667114
LOSS: 1.329968810081482
LOSS: 1.3299347162246704
LOSS: 1.3299272060394287
LOSS: 1.330010175704956
LOSS: 1.3299670219421387
LOSS: 1.3299338817596436
LOSS: 1.329986333847046
LOSS: 1.3299604654312134
LOSS: 1.3299227952957153
LOSS: 1.3299577236175537
LOSS: 1.3299052715301514
LOSS: 1.3299514055252075
LOSS: 1.3299107551574707
LOSS: 1.3299107551574707
LOSS: 1.3299689292907715
LOSS: 1.397369623184204
LOSS: 1.3916854858398438
LOSS: 1.3864246606826782
LOSS: 1.3816090822219849
LOSS: 1.3772841691970825
LOSS: 1.373509407043457
LOSS: 1.370247483253479
LOSS: 1.3675562143325806
LOSS: 1.3654130697250366
LOSS: 1.363752841949463
LOSS: 1.3625085353851318
LOSS: 1.3614919185638428
LOSS: 1.3606258630752563
LOSS: 1.3597872257232666
LOSS: 1.3588435649871826
LOSS: 1.3578113317489624
LOSS: 1.3565131425857544
LOSS: 1.355093240737915
LOSS: 1.35346257686615
LOSS: 1.351747751235962
LOSS: 1.3498625755310059
LOSS: 1.347938895225525
LOSS: 1.345982313156128
LOSS: 1.3440507650375366
LOSS: 1.3420917987823486
LOSS: 1.3401705026626587
LOSS: 1.3383159637451172
LOSS: 1.3364843130111694
LOSS: 1.3347771167755127
LOSS: 1.3330469131469727
LOSS: 1.3313905000686646
LOSS: 1.3297630548477173
LOSS: 1.3281872272491455
LOSS: 1.3266229629516602
LOSS: 1.3250341415405273
LOSS: 1.3234745264053345
LOSS: 1.3219298124313354
LOSS: 1.320344090461731
LOSS: 1.3187901973724365
LOSS: 1.3172017335891724
LOSS: 1.315663456916809
LOSS: 1.314123272895813
LOSS: 1.3126277923583984
LOSS: 1.3112190961837769
LOSS: 1.3097785711288452
LOSS: 1.3084182739257812
LOSS: 1.307144284248352
LOSS: 1.305936336517334
LOSS: 1.304826021194458
LOSS: 1.3037406206130981
LOSS: 1.3027554750442505
LOSS: 1.3018406629562378
LOSS: 1.3009653091430664
LOSS: 1.3001868724822998
LOSS: 1.299491047859192
LOSS: 1.298825979232788
LOSS: 1.2982176542282104
LOSS: 1.2976776361465454
LOSS: 1.2972149848937988
LOSS: 1.2967933416366577
LOSS: 1.2965081930160522
LOSS: 1.2961872816085815
LOSS: 1.2959182262420654
LOSS: 1.2957769632339478
LOSS: 1.2956479787826538
LOSS: 1.295524001121521
LOSS: 1.295414924621582
LOSS: 1.2954033613204956
LOSS: 1.295357346534729
LOSS: 1.2953879833221436
LOSS: 1.2953637838363647
LOSS: 1.2953664064407349
LOSS: 1.2954344749450684
LOSS: 1.2954012155532837
LOSS: 1.2954261302947998
LOSS: 1.2955026626586914
LOSS: 1.2954953908920288
LOSS: 1.295507550239563
LOSS: 1.2955341339111328
LOSS: 1.295583963394165
LOSS: 1.2955610752105713
LOSS: 1.2955472469329834
LOSS: 1.2955330610275269
LOSS: 1.2955111265182495
LOSS: 1.2955125570297241
LOSS: 1.295506477355957
LOSS: 1.295533299446106
LOSS: 1.2954720258712769
LOSS: 1.2954256534576416
LOSS: 1.2954012155532837
LOSS: 1.2953814268112183
LOSS: 1.295375108718872
LOSS: 1.295392394065857
LOSS: 1.2953704595565796
LOSS: 1.295420527458191
LOSS: 1.2952929735183716
LOSS: 1.2952886819839478
LOSS: 1.2953308820724487
LOSS: 1.2953176498413086
LOSS: 1.2953119277954102
LOSS: 1.4018166065216064
LOSS: 1.3962650299072266
LOSS: 1.3910976648330688
LOSS: 1.386411190032959
LOSS: 1.382225513458252
LOSS: 1.3785685300827026
LOSS: 1.3754817247390747
LOSS: 1.3729603290557861
LOSS: 1.3709909915924072
LOSS: 1.3695306777954102
LOSS: 1.3684875965118408
LOSS: 1.3677685260772705
LOSS: 1.3672118186950684
LOSS: 1.3667066097259521
LOSS: 1.3661127090454102
LOSS: 1.3654019832611084
LOSS: 1.3645211458206177
LOSS: 1.3634374141693115
LOSS: 1.3622335195541382
LOSS: 1.360867977142334
LOSS: 1.359453558921814
LOSS: 1.3578894138336182
LOSS: 1.3563604354858398
LOSS: 1.3548152446746826
LOSS: 1.3533250093460083
LOSS: 1.351858377456665
LOSS: 1.3504594564437866
LOSS: 1.349139928817749
LOSS: 1.3478511571884155
LOSS: 1.3466349840164185
LOSS: 1.3455229997634888
LOSS: 1.3443366289138794
LOSS: 1.343234896659851
LOSS: 1.3421489000320435
LOSS: 1.3410652875900269
LOSS: 1.3399800062179565
LOSS: 1.3389266729354858
LOSS: 1.3378044366836548
LOSS: 1.3367180824279785
LOSS: 1.3356307744979858
LOSS: 1.334560513496399
LOSS: 1.3335210084915161
LOSS: 1.3325282335281372
LOSS: 1.3315482139587402
LOSS: 1.3305903673171997
LOSS: 1.3297662734985352
LOSS: 1.3289352655410767
LOSS: 1.328218936920166
LOSS: 1.3275091648101807
LOSS: 1.3269050121307373
LOSS: 1.326330304145813
LOSS: 1.3258150815963745
LOSS: 1.3253521919250488
LOSS: 1.3249990940093994
LOSS: 1.3245614767074585
LOSS: 1.3242852687835693
LOSS: 1.3239973783493042
LOSS: 1.3237370252609253
LOSS: 1.3235020637512207
LOSS: 1.3233659267425537
LOSS: 1.3232293128967285
LOSS: 1.3231412172317505
LOSS: 1.3231236934661865
LOSS: 1.3230875730514526
LOSS: 1.3230478763580322
LOSS: 1.3230538368225098
LOSS: 1.3230445384979248
LOSS: 1.3230630159378052
LOSS: 1.3230968713760376
LOSS: 1.3231329917907715
LOSS: 1.3231523036956787
LOSS: 1.3231735229492188
LOSS: 1.3232113122940063
LOSS: 1.3232145309448242
LOSS: 1.3232154846191406
LOSS: 1.323241114616394
LOSS: 1.3232096433639526
LOSS: 1.3232178688049316
LOSS: 1.3232300281524658
LOSS: 1.3232651948928833
LOSS: 1.3232109546661377
LOSS: 1.3231515884399414
LOSS: 1.3232014179229736
LOSS: 1.3231877088546753
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3450205326080322
LOSS: 1.3446704149246216
LOSS: 1.3443504571914673
LOSS: 1.3440275192260742
LOSS: 1.3438054323196411
LOSS: 1.3436270952224731
LOSS: 1.343404769897461
LOSS: 1.3432925939559937
LOSS: 1.3431763648986816
LOSS: 1.3431518077850342
LOSS: 1.3430839776992798
LOSS: 1.3430649042129517
LOSS: 1.3430397510528564
LOSS: 1.3430132865905762
LOSS: 1.3430615663528442
LOSS: 1.343050479888916
LOSS: 1.3430747985839844
LOSS: 1.3430832624435425
LOSS: 1.3431079387664795
LOSS: 1.3431612253189087
LOSS: 1.3431427478790283
LOSS: 1.3431395292282104
LOSS: 1.343138575553894
LOSS: 1.3431886434555054
LOSS: 1.3431907892227173
LOSS: 1.343184232711792
LOSS: 1.343138575553894
LOSS: 1.3431259393692017
LOSS: 1.3431223630905151
LOSS: 1.3431185483932495
LOSS: 1.3430712223052979
LOSS: 1.343098759651184
LOSS: 1.3430745601654053
LOSS: 1.3430814743041992
LOSS: 1.343044638633728
LOSS: 1.343042016029358
LOSS: 1.3430414199829102
LOSS: 1.343021273612976
LOSS: 1.3430509567260742
LOSS: 1.34297513961792
LOSS: 1.3430238962173462
LOSS: 1.3429601192474365
LOSS: 1.3429670333862305
LOSS: 1.342966079711914
LOSS: 1.342957854270935
LOSS: 1.3429194688796997
LOSS: 1.400840401649475
LOSS: 1.3952711820602417
LOSS: 1.3901392221450806
LOSS: 1.3854743242263794
LOSS: 1.381301760673523
LOSS: 1.377686619758606
LOSS: 1.3746010065078735
LOSS: 1.3721009492874146
LOSS: 1.3701518774032593
LOSS: 1.3686834573745728
LOSS: 1.3676220178604126
LOSS: 1.3668606281280518
LOSS: 1.3661836385726929
LOSS: 1.3655529022216797
LOSS: 1.3648653030395508
LOSS: 1.3640098571777344
LOSS: 1.3630218505859375
LOSS: 1.3618521690368652
LOSS: 1.3605221509933472
LOSS: 1.3590424060821533
LOSS: 1.3575029373168945
LOSS: 1.3559229373931885
LOSS: 1.3542912006378174
LOSS: 1.3527032136917114
LOSS: 1.3511855602264404
LOSS: 1.349736213684082
LOSS: 1.3482654094696045
LOSS: 1.346906304359436
LOSS: 1.345618486404419
LOSS: 1.3443996906280518
LOSS: 1.3432447910308838
LOSS: 1.3421353101730347
LOSS: 1.3410298824310303
LOSS: 1.3399863243103027
LOSS: 1.338927149772644
LOSS: 1.3379029035568237
LOSS: 1.336865782737732
LOSS: 1.3359003067016602
LOSS: 1.3348363637924194
LOSS: 1.3338638544082642
LOSS: 1.3329449892044067
LOSS: 1.332022786140442
LOSS: 1.3311859369277954
LOSS: 1.3303271532058716
LOSS: 1.3295996189117432
LOSS: 1.3289167881011963
LOSS: 1.328275203704834
LOSS: 1.327785611152649
LOSS: 1.3273156881332397
LOSS: 1.3268753290176392
LOSS: 1.326514720916748
LOSS: 1.3262712955474854
LOSS: 1.3260172605514526
LOSS: 1.3257380723953247
LOSS: 1.3256107568740845
LOSS: 1.3253867626190186
LOSS: 1.3253601789474487
LOSS: 1.3252733945846558
LOSS: 1.3251832723617554
LOSS: 1.325156569480896
LOSS: 1.3252137899398804
LOSS: 1.3252466917037964
LOSS: 1.325231671333313
LOSS: 1.3252960443496704
LOSS: 1.3252105712890625
LOSS: 1.3253370523452759
LOSS: 1.3252954483032227
LOSS: 1.325318694114685
LOSS: 1.3254061937332153
LOSS: 1.3253874778747559
LOSS: 1.3253991603851318
LOSS: 1.3254575729370117
LOSS: 1.3254104852676392
LOSS: 1.3254088163375854
LOSS: 1.3253834247589111
LOSS: 1.3253848552703857
LOSS: 1.3254003524780273
LOSS: 1.3253083229064941
LOSS: 1.3253004550933838
LOSS: 1.3253405094146729
LOSS: 1.3252661228179932
LOSS: 1.3252414464950562
LOSS: 1.3252449035644531
LOSS: 1.3252168893814087
LOSS: 1.3252297639846802
LOSS: 1.3252439498901367
LOSS: 1.3251615762710571
LOSS: 1.3251968622207642
LOSS: 1.3251889944076538
LOSS: 1.3251464366912842
LOSS: 1.3252177238464355
LOSS: 1.3251677751541138
LOSS: 1.3251606225967407
LOSS: 1.3252075910568237
LOSS: 1.3251999616622925
LOSS: 1.325168251991272
LOSS: 1.3251311779022217
LOSS: 1.3251737356185913
LOSS: 1.3251689672470093
LOSS: 1.3251804113388062
LOSS: 1.4024505615234375
LOSS: 1.3969216346740723
LOSS: 1.391855001449585
LOSS: 1.3871853351593018
LOSS: 1.3830604553222656
LOSS: 1.3794786930084229
LOSS: 1.3764402866363525
LOSS: 1.373986840248108
LOSS: 1.3720563650131226
LOSS: 1.3706551790237427
LOSS: 1.3696191310882568
LOSS: 1.368923306465149
LOSS: 1.3683563470840454
LOSS: 1.3678336143493652
LOSS: 1.3671488761901855
LOSS: 1.366387128829956
LOSS: 1.3654544353485107
LOSS: 1.3643760681152344
LOSS: 1.3630331754684448
LOSS: 1.3616228103637695
LOSS: 1.3601336479187012
LOSS: 1.3585864305496216
LOSS: 1.35706627368927
LOSS: 1.355489730834961
LOSS: 1.3539599180221558
LOSS: 1.3524659872055054
LOSS: 1.351089596748352
LOSS: 1.3497625589370728
LOSS: 1.348474144935608
LOSS: 1.347233772277832
LOSS: 1.3460710048675537
LOSS: 1.3449238538742065
LOSS: 1.343868613243103
LOSS: 1.3427989482879639
LOSS: 1.3417052030563354
LOSS: 1.3406211137771606
LOSS: 1.3396118879318237
LOSS: 1.3385416269302368
LOSS: 1.3375011682510376
LOSS: 1.3364568948745728
LOSS: 1.3354014158248901
LOSS: 1.3344546556472778
LOSS: 1.3334800004959106
LOSS: 1.332659125328064
LOSS: 1.3317666053771973
LOSS: 1.331014633178711
LOSS: 1.330303430557251
LOSS: 1.329685926437378
LOSS: 1.329192876815796
LOSS: 1.3286538124084473
LOSS: 1.3281688690185547
LOSS: 1.3277541399002075
LOSS: 1.3273961544036865
LOSS: 1.3271305561065674
LOSS: 1.326856255531311
LOSS: 1.3266178369522095
LOSS: 1.3264890909194946
LOSS: 1.326283574104309
LOSS: 1.3261990547180176
LOSS: 1.3261784315109253
LOSS: 1.32606041431427
LOSS: 1.3261052370071411
LOSS: 1.3260244131088257
LOSS: 1.3260642290115356
LOSS: 1.3260912895202637
LOSS: 1.3260656595230103
LOSS: 1.3260949850082397
LOSS: 1.326134204864502
LOSS: 1.3261523246765137
LOSS: 1.3261754512786865
LOSS: 1.3262196779251099
LOSS: 1.3261518478393555
LOSS: 1.3262276649475098
LOSS: 1.326181411743164
LOSS: 1.326135516166687
LOSS: 1.3261746168136597
LOSS: 1.3261712789535522
LOSS: 1.3262068033218384
LOSS: 1.3261703252792358
LOSS: 1.3261077404022217
LOSS: 1.3261669874191284
LOSS: 1.3261090517044067
LOSS: 1.326067328453064
LOSS: 1.3260139226913452
LOSS: 1.326047658920288
LOSS: 1.326004981994629
LOSS: 1.3260470628738403
LOSS: 1.3259587287902832
LOSS: 1.3259531259536743
LOSS: 1.3259313106536865
LOSS: 1.3259283304214478
LOSS: 1.3259023427963257
LOSS: 1.3259556293487549
LOSS: 1.325990080833435
LOSS: 1.3259562253952026
LOSS: 1.3259353637695312
LOSS: 1.3259121179580688
LOSS: 1.3258503675460815
LOSS: 1.325909972190857
LOSS: 1.3259156942367554
LOSS: 1.3983412981033325
LOSS: 1.3927257061004639
LOSS: 1.3875036239624023
LOSS: 1.3827762603759766
LOSS: 1.3785463571548462
LOSS: 1.3748055696487427
LOSS: 1.3716390132904053
LOSS: 1.3690420389175415
LOSS: 1.366979956626892
LOSS: 1.3654314279556274
LOSS: 1.3642719984054565
LOSS: 1.3633992671966553
LOSS: 1.362671971321106
LOSS: 1.3619894981384277
LOSS: 1.361215353012085
LOSS: 1.3603110313415527
LOSS: 1.359235167503357
LOSS: 1.3579844236373901
LOSS: 1.3565922975540161
LOSS: 1.355014681816101
LOSS: 1.3534315824508667
LOSS: 1.3517472743988037
LOSS: 1.3500151634216309
LOSS: 1.3483167886734009
LOSS: 1.346634030342102
LOSS: 1.3450253009796143
LOSS: 1.3434529304504395
LOSS: 1.341964840888977
LOSS: 1.340543270111084
LOSS: 1.339176058769226
LOSS: 1.3378875255584717
LOSS: 1.336676001548767
LOSS: 1.3354175090789795
LOSS: 1.3342504501342773
LOSS: 1.333101511001587
LOSS: 1.3319246768951416
LOSS: 1.3307708501815796
LOSS: 1.3296250104904175
LOSS: 1.3285590410232544
LOSS: 1.3274458646774292
LOSS: 1.3263448476791382
LOSS: 1.3253017663955688
LOSS: 1.3243030309677124
LOSS: 1.323373556137085
LOSS: 1.322487235069275
LOSS: 1.3217277526855469
LOSS: 1.3209619522094727
LOSS: 1.3203554153442383
LOSS: 1.319729208946228
LOSS: 1.3192120790481567
LOSS: 1.318795084953308
LOSS: 1.3183740377426147
LOSS: 1.31801438331604
LOSS: 1.3178073167800903
LOSS: 1.3175064325332642
LOSS: 1.3173117637634277
LOSS: 1.317163348197937
LOSS: 1.3170565366744995
LOSS: 1.316980242729187
LOSS: 1.3169169425964355
LOSS: 1.3168777227401733
LOSS: 1.3169173002243042
LOSS: 1.3168432712554932
LOSS: 1.3168416023254395
LOSS: 1.3168890476226807
LOSS: 1.3168957233428955
LOSS: 1.3169580698013306
LOSS: 1.3169656991958618
LOSS: 1.317010521888733
LOSS: 1.317025065422058
LOSS: 1.3170857429504395
LOSS: 1.3170380592346191
LOSS: 1.317028522491455
LOSS: 1.3170440196990967
LOSS: 1.3170193433761597
LOSS: 1.31704843044281
LOSS: 1.3170145750045776
LOSS: 1.3169933557510376
LOSS: 1.3169364929199219
LOSS: 1.3169081211090088
LOSS: 1.316932201385498
LOSS: 1.316868543624878
LOSS: 1.3168576955795288
LOSS: 1.3168706893920898
LOSS: 1.3168224096298218
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
LOSS: 1.3300398588180542
LOSS: 1.3300191164016724
LOSS: 1.3299981355667114
LOSS: 1.329968810081482
LOSS: 1.3299347162246704
LOSS: 1.3299272060394287
LOSS: 1.330010175704956
LOSS: 1.3299670219421387
LOSS: 1.3299338817596436
LOSS: 1.329986333847046
LOSS: 1.3299604654312134
LOSS: 1.3299227952957153
LOSS: 1.3299577236175537
LOSS: 1.3299052715301514
LOSS: 1.3299514055252075
LOSS: 1.3299107551574707
LOSS: 1.3299107551574707
LOSS: 1.3299689292907715
LOSS: 1.3983412981033325
LOSS: 1.3927257061004639
LOSS: 1.3875036239624023
LOSS: 1.3827762603759766
LOSS: 1.3785463571548462
LOSS: 1.3748055696487427
LOSS: 1.3716390132904053
LOSS: 1.3690420389175415
LOSS: 1.366979956626892
LOSS: 1.3654314279556274
LOSS: 1.3642719984054565
LOSS: 1.3633992671966553
LOSS: 1.362671971321106
LOSS: 1.3619894981384277
LOSS: 1.361215353012085
LOSS: 1.3603110313415527
LOSS: 1.359235167503357
LOSS: 1.3579844236373901
LOSS: 1.3565922975540161
LOSS: 1.355014681816101
LOSS: 1.3534315824508667
LOSS: 1.3517472743988037
LOSS: 1.3500151634216309
LOSS: 1.3483167886734009
LOSS: 1.346634030342102
LOSS: 1.3450253009796143
LOSS: 1.3434529304504395
LOSS: 1.341964840888977
LOSS: 1.340543270111084
LOSS: 1.339176058769226
LOSS: 1.3378875255584717
LOSS: 1.336676001548767
LOSS: 1.3354175090789795
LOSS: 1.3342504501342773
LOSS: 1.333101511001587
LOSS: 1.3319246768951416
LOSS: 1.3307708501815796
LOSS: 1.3296250104904175
LOSS: 1.3285590410232544
LOSS: 1.3274458646774292
LOSS: 1.3263448476791382
LOSS: 1.3253017663955688
LOSS: 1.3243030309677124
LOSS: 1.323373556137085
LOSS: 1.322487235069275
LOSS: 1.3217277526855469
LOSS: 1.3209619522094727
LOSS: 1.3203554153442383
LOSS: 1.319729208946228
LOSS: 1.3192120790481567
LOSS: 1.318795084953308
LOSS: 1.3183740377426147
LOSS: 1.31801438331604
LOSS: 1.3178073167800903
LOSS: 1.3175064325332642
LOSS: 1.3173117637634277
LOSS: 1.317163348197937
LOSS: 1.3170565366744995
LOSS: 1.316980242729187
LOSS: 1.3169169425964355
LOSS: 1.3168777227401733
LOSS: 1.3169173002243042
LOSS: 1.3168432712554932
LOSS: 1.3168416023254395
LOSS: 1.3168890476226807
LOSS: 1.3168957233428955
LOSS: 1.3169580698013306
LOSS: 1.3169656991958618
LOSS: 1.317010521888733
LOSS: 1.317025065422058
LOSS: 1.3170857429504395
LOSS: 1.3170380592346191
LOSS: 1.317028522491455
LOSS: 1.3170440196990967
LOSS: 1.3170193433761597
LOSS: 1.31704843044281
LOSS: 1.3170145750045776
LOSS: 1.3169933557510376
LOSS: 1.3169364929199219
LOSS: 1.3169081211090088
LOSS: 1.316932201385498
LOSS: 1.316868543624878
LOSS: 1.3168576955795288
LOSS: 1.3168706893920898
LOSS: 1.3168224096298218
LOSS: 1.3168220520019531
LOSS: 1.3168843984603882
LOSS: 1.316828727722168
LOSS: 1.3167732954025269
LOSS: 1.316788911819458
LOSS: 1.3167706727981567
LOSS: 1.3167363405227661
LOSS: 1.3167190551757812
LOSS: 1.3167293071746826
LOSS: 1.316755771636963
LOSS: 1.3167699575424194
LOSS: 1.316738247871399
LOSS: 1.316720962524414
LOSS: 1.3167567253112793
LOSS: 1.316727876663208
LOSS: 1.4018166065216064
LOSS: 1.3962650299072266
LOSS: 1.3910976648330688
LOSS: 1.386411190032959
LOSS: 1.382225513458252
LOSS: 1.3785685300827026
LOSS: 1.3754817247390747
LOSS: 1.3729603290557861
LOSS: 1.3709909915924072
LOSS: 1.3695306777954102
LOSS: 1.3684875965118408
LOSS: 1.3677685260772705
LOSS: 1.3672118186950684
LOSS: 1.3667066097259521
LOSS: 1.3661127090454102
LOSS: 1.3654019832611084
LOSS: 1.3645211458206177
LOSS: 1.3634374141693115
LOSS: 1.3622335195541382
LOSS: 1.360867977142334
LOSS: 1.359453558921814
LOSS: 1.3578894138336182
LOSS: 1.3563604354858398
LOSS: 1.3548152446746826
LOSS: 1.3533250093460083
LOSS: 1.351858377456665
LOSS: 1.3504594564437866
LOSS: 1.349139928817749
LOSS: 1.3478511571884155
LOSS: 1.3466349840164185
LOSS: 1.3455229997634888
LOSS: 1.3443366289138794
LOSS: 1.343234896659851
LOSS: 1.3421489000320435
LOSS: 1.3410652875900269
LOSS: 1.3399800062179565
LOSS: 1.3389266729354858
LOSS: 1.3378044366836548
LOSS: 1.3367180824279785
LOSS: 1.3356307744979858
LOSS: 1.334560513496399
LOSS: 1.3335210084915161
LOSS: 1.3325282335281372
LOSS: 1.3315482139587402
LOSS: 1.3305903673171997
LOSS: 1.3297662734985352
LOSS: 1.3289352655410767
LOSS: 1.328218936920166
LOSS: 1.3275091648101807
LOSS: 1.3269050121307373
LOSS: 1.326330304145813
LOSS: 1.3258150815963745
LOSS: 1.3253521919250488
LOSS: 1.3249990940093994
LOSS: 1.3245614767074585
LOSS: 1.3242852687835693
LOSS: 1.3239973783493042
LOSS: 1.3237370252609253
LOSS: 1.3235020637512207
LOSS: 1.3233659267425537
LOSS: 1.3232293128967285
LOSS: 1.3231412172317505
LOSS: 1.3231236934661865
LOSS: 1.3230875730514526
LOSS: 1.3230478763580322
LOSS: 1.3230538368225098
LOSS: 1.3230445384979248
LOSS: 1.3230630159378052
LOSS: 1.3230968713760376
LOSS: 1.3231329917907715
LOSS: 1.3231523036956787
LOSS: 1.3231735229492188
LOSS: 1.3232113122940063
LOSS: 1.3232145309448242
LOSS: 1.3232154846191406
LOSS: 1.323241114616394
LOSS: 1.3232096433639526
LOSS: 1.3232178688049316
LOSS: 1.3232300281524658
LOSS: 1.3232651948928833
LOSS: 1.3232109546661377
LOSS: 1.3231515884399414
LOSS: 1.3232014179229736
LOSS: 1.3231877088546753
LOSS: 1.323173999786377
LOSS: 1.3231314420700073
LOSS: 1.3231359720230103
LOSS: 1.323149561882019
LOSS: 1.3230996131896973
LOSS: 1.323080062866211
LOSS: 1.3230834007263184
LOSS: 1.3230698108673096
LOSS: 1.3231078386306763
LOSS: 1.3230637311935425
LOSS: 1.3230371475219727
LOSS: 1.3230717182159424
LOSS: 1.3230417966842651
LOSS: 1.3230270147323608
LOSS: 1.323046088218689
LOSS: 1.3230763673782349
LOSS: 1.398823857307434
LOSS: 1.3931738138198853
LOSS: 1.3879563808441162
LOSS: 1.3831923007965088
LOSS: 1.3789387941360474
LOSS: 1.375191569328308
LOSS: 1.3720227479934692
LOSS: 1.3693950176239014
LOSS: 1.367319941520691
LOSS: 1.3657317161560059
LOSS: 1.3645374774932861
LOSS: 1.3636177778244019
LOSS: 1.362838864326477
LOSS: 1.3621032238006592
LOSS: 1.3612546920776367
LOSS: 1.3602617979049683
LOSS: 1.3591158390045166
LOSS: 1.3577743768692017
LOSS: 1.3562778234481812
LOSS: 1.3546854257583618
LOSS: 1.3529472351074219
LOSS: 1.3511685132980347
LOSS: 1.3494027853012085
LOSS: 1.34757661819458
LOSS: 1.345790982246399
LOSS: 1.3440873622894287
LOSS: 1.3424241542816162
LOSS: 1.3408286571502686
LOSS: 1.3392479419708252
LOSS: 1.337783694267273
LOSS: 1.336342692375183
LOSS: 1.3349642753601074
LOSS: 1.3335860967636108
LOSS: 1.3322398662567139
LOSS: 1.3309246301651
LOSS: 1.3295763731002808
LOSS: 1.328264594078064
LOSS: 1.3269414901733398
LOSS: 1.3256690502166748
LOSS: 1.324310541152954
LOSS: 1.3231042623519897
LOSS: 1.3217885494232178
LOSS: 1.3206290006637573
LOSS: 1.3194050788879395
LOSS: 1.3183118104934692
LOSS: 1.3173173666000366
LOSS: 1.3163284063339233
LOSS: 1.315408706665039
LOSS: 1.3145755529403687
LOSS: 1.313813328742981
LOSS: 1.31315279006958
LOSS: 1.3125460147857666
LOSS: 1.3119490146636963
LOSS: 1.311506986618042
LOSS: 1.311036467552185
LOSS: 1.3106272220611572
LOSS: 1.310342788696289
LOSS: 1.3100537061691284
LOSS: 1.3097870349884033
LOSS: 1.3096073865890503
LOSS: 1.3094557523727417
LOSS: 1.3093677759170532
LOSS: 1.3092799186706543
LOSS: 1.3092390298843384
LOSS: 1.3092126846313477
LOSS: 1.3092520236968994
LOSS: 1.3092188835144043
LOSS: 1.3092552423477173
LOSS: 1.3093018531799316
LOSS: 1.309364676475525
LOSS: 1.3093327283859253
LOSS: 1.3094087839126587
LOSS: 1.309402346611023
LOSS: 1.3094271421432495
LOSS: 1.3094600439071655
LOSS: 1.3094220161437988
LOSS: 1.3094688653945923
LOSS: 1.3094482421875
LOSS: 1.3094732761383057
LOSS: 1.3094416856765747
LOSS: 1.3094276189804077
LOSS: 1.309464931488037
LOSS: 1.309416651725769
LOSS: 1.3094148635864258
LOSS: 1.3093652725219727
LOSS: 1.3093308210372925
LOSS: 1.3093836307525635
LOSS: 1.309300422668457
LOSS: 1.309289574623108
LOSS: 1.3092761039733887
LOSS: 1.3093174695968628
LOSS: 1.309282660484314
LOSS: 1.309240698814392
LOSS: 1.309217095375061
LOSS: 1.3092246055603027
LOSS: 1.3092254400253296
LOSS: 1.3092138767242432
LOSS: 1.309213399887085
LOSS: 1.3092681169509888
LOSS: 1.309228777885437
LOSS: 1.4032834768295288
LOSS: 1.3977738618850708
LOSS: 1.3926808834075928
LOSS: 1.3880435228347778
LOSS: 1.3839198350906372
LOSS: 1.380344033241272
LOSS: 1.377307415008545
LOSS: 1.3748897314071655
LOSS: 1.372947096824646
LOSS: 1.3715276718139648
LOSS: 1.3705530166625977
LOSS: 1.3698976039886475
LOSS: 1.3693009614944458
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3299652338027954
LOSS: 1.329870581626892
LOSS: 1.3298735618591309
LOSS: 1.3298736810684204
LOSS: 1.3298476934432983
LOSS: 1.3298403024673462
LOSS: 1.3298815488815308
LOSS: 1.3298271894454956
LOSS: 1.3298231363296509
LOSS: 1.3298629522323608
LOSS: 1.3297913074493408
LOSS: 1.3298252820968628
LOSS: 1.3297926187515259
LOSS: 1.3297597169876099
LOSS: 1.3298730850219727
LOSS: 1.401581883430481
LOSS: 1.396032691001892
LOSS: 1.3909525871276855
LOSS: 1.3863017559051514
LOSS: 1.3821592330932617
LOSS: 1.3785473108291626
LOSS: 1.3755241632461548
LOSS: 1.3730381727218628
LOSS: 1.3710577487945557
LOSS: 1.369594693183899
LOSS: 1.3685402870178223
LOSS: 1.3676040172576904
LOSS: 1.3668957948684692
LOSS: 1.3661303520202637
LOSS: 1.3652950525283813
LOSS: 1.364304542541504
LOSS: 1.363094687461853
LOSS: 1.3617161512374878
LOSS: 1.3602604866027832
LOSS: 1.3586103916168213
LOSS: 1.3569109439849854
LOSS: 1.3552024364471436
LOSS: 1.3533812761306763
LOSS: 1.3516279458999634
LOSS: 1.349954605102539
LOSS: 1.3482633829116821
LOSS: 1.346660852432251
LOSS: 1.3451602458953857
LOSS: 1.34364652633667
LOSS: 1.342292070388794
LOSS: 1.3409112691879272
LOSS: 1.3396192789077759
LOSS: 1.3383731842041016
LOSS: 1.3370651006698608
LOSS: 1.3358683586120605
LOSS: 1.3346283435821533
LOSS: 1.3334236145019531
LOSS: 1.3322083950042725
LOSS: 1.3310083150863647
LOSS: 1.3298968076705933
LOSS: 1.3288284540176392
LOSS: 1.3276453018188477
LOSS: 1.3266429901123047
LOSS: 1.3256640434265137
LOSS: 1.324734091758728
LOSS: 1.3239905834197998
LOSS: 1.3231652975082397
LOSS: 1.3224955797195435
LOSS: 1.3218662738800049
LOSS: 1.3212920427322388
LOSS: 1.320847749710083
LOSS: 1.3204398155212402
LOSS: 1.320061206817627
LOSS: 1.3197990655899048
LOSS: 1.3195804357528687
LOSS: 1.3193295001983643
LOSS: 1.3191732168197632
LOSS: 1.3190715312957764
LOSS: 1.318962574005127
LOSS: 1.3188436031341553
LOSS: 1.3188214302062988
LOSS: 1.3188902139663696
LOSS: 1.3189367055892944
LOSS: 1.3188507556915283
LOSS: 1.3189527988433838
LOSS: 1.318939447402954
LOSS: 1.3189959526062012
LOSS: 1.3189424276351929
LOSS: 1.318976640701294
LOSS: 1.3190093040466309
LOSS: 1.3190104961395264
LOSS: 1.319122076034546
LOSS: 1.3190398216247559
LOSS: 1.3190137147903442
LOSS: 1.3190422058105469
LOSS: 1.3190163373947144
LOSS: 1.3190369606018066
LOSS: 1.3189489841461182
LOSS: 1.3189888000488281
LOSS: 1.3189141750335693
LOSS: 1.318855881690979
LOSS: 1.3188421726226807
LOSS: 1.318884253501892
LOSS: 1.3188453912734985
LOSS: 1.3188045024871826
LOSS: 1.318747878074646
LOSS: 1.3187344074249268
LOSS: 1.3187172412872314
LOSS: 1.3186793327331543
LOSS: 1.3187332153320312
LOSS: 1.3187388181686401
LOSS: 1.3187696933746338
LOSS: 1.3187226057052612
LOSS: 1.3186982870101929
LOSS: 1.3186676502227783
LOSS: 1.3186975717544556
LOSS: 1.3186705112457275
LOSS: 1.3186707496643066
LOSS: 1.3186553716659546
LOSS: 1.3186691999435425
LOSS: 1.4024505615234375
LOSS: 1.3969216346740723
LOSS: 1.391855001449585
LOSS: 1.3871853351593018
LOSS: 1.3830604553222656
LOSS: 1.3794786930084229
LOSS: 1.3764402866363525
LOSS: 1.373986840248108
LOSS: 1.3720563650131226
LOSS: 1.3706551790237427
LOSS: 1.3696191310882568
LOSS: 1.368923306465149
LOSS: 1.3683563470840454
LOSS: 1.3678336143493652
LOSS: 1.3671488761901855
LOSS: 1.366387128829956
LOSS: 1.3654544353485107
LOSS: 1.3643760681152344
LOSS: 1.3630331754684448
LOSS: 1.3616228103637695
LOSS: 1.3601336479187012
LOSS: 1.3585864305496216
LOSS: 1.35706627368927
LOSS: 1.355489730834961
LOSS: 1.3539599180221558
LOSS: 1.3524659872055054
LOSS: 1.351089596748352
LOSS: 1.3497625589370728
LOSS: 1.348474144935608
LOSS: 1.347233772277832
LOSS: 1.3460710048675537
LOSS: 1.3449238538742065
LOSS: 1.343868613243103
LOSS: 1.3427989482879639
LOSS: 1.3417052030563354
LOSS: 1.3406211137771606
LOSS: 1.3396118879318237
LOSS: 1.3385416269302368
LOSS: 1.3375011682510376
LOSS: 1.3364568948745728
LOSS: 1.3354014158248901
LOSS: 1.3344546556472778
LOSS: 1.3334800004959106
LOSS: 1.332659125328064
LOSS: 1.3317666053771973
LOSS: 1.331014633178711
LOSS: 1.330303430557251
LOSS: 1.329685926437378
LOSS: 1.329192876815796
LOSS: 1.3286538124084473
LOSS: 1.3281688690185547
LOSS: 1.3277541399002075
LOSS: 1.3273961544036865
LOSS: 1.3271305561065674
LOSS: 1.326856255531311
LOSS: 1.3266178369522095
LOSS: 1.3264890909194946
LOSS: 1.326283574104309
LOSS: 1.3261990547180176
LOSS: 1.3261784315109253
LOSS: 1.32606041431427
LOSS: 1.3261052370071411
LOSS: 1.3260244131088257
LOSS: 1.3260642290115356
LOSS: 1.3260912895202637
LOSS: 1.3260656595230103
LOSS: 1.3260949850082397
LOSS: 1.326134204864502
LOSS: 1.3261523246765137
LOSS: 1.3261754512786865
LOSS: 1.3262196779251099
LOSS: 1.3261518478393555
LOSS: 1.3262276649475098
LOSS: 1.326181411743164
LOSS: 1.326135516166687
LOSS: 1.3261746168136597
LOSS: 1.3261712789535522
LOSS: 1.3262068033218384
LOSS: 1.3261703252792358
LOSS: 1.3261077404022217
LOSS: 1.3261669874191284
LOSS: 1.3261090517044067
LOSS: 1.326067328453064
LOSS: 1.3260139226913452
LOSS: 1.326047658920288
LOSS: 1.326004981994629
LOSS: 1.3260470628738403
LOSS: 1.3259587287902832
LOSS: 1.3259531259536743
LOSS: 1.3259313106536865
LOSS: 1.3259283304214478
LOSS: 1.3259023427963257
LOSS: 1.3259556293487549
LOSS: 1.325990080833435
LOSS: 1.3259562253952026
LOSS: 1.3259353637695312
LOSS: 1.3259121179580688
LOSS: 1.3258503675460815
LOSS: 1.325909972190857
LOSS: 1.3259156942367554
LOSS: 1.3918763399124146
LOSS: 1.386068344116211
LOSS: 1.3806825876235962
LOSS: 1.3757206201553345
LOSS: 1.37124764919281
LOSS: 1.3673012256622314
LOSS: 1.3638932704925537
LOSS: 1.3610607385635376
LOSS: 1.3587566614151
LOSS: 1.3569278717041016
LOSS: 1.355503797531128
LOSS: 1.3544068336486816
LOSS: 1.3534585237503052
LOSS: 1.3525251150131226
LOSS: 1.3515673875808716
LOSS: 1.350446343421936
LOSS: 1.3491783142089844
LOSS: 1.3477109670639038
LOSS: 1.3460924625396729
LOSS: 1.3443734645843506
LOSS: 1.3425257205963135
LOSS: 1.3406425714492798
LOSS: 1.3387112617492676
LOSS: 1.336760401725769
LOSS: 1.3348692655563354
LOSS: 1.3330169916152954
LOSS: 1.3312405347824097
LOSS: 1.3295063972473145
LOSS: 1.3278696537017822
LOSS: 1.326316475868225
LOSS: 1.3248003721237183
LOSS: 1.3233720064163208
LOSS: 1.3219751119613647
LOSS: 1.3206290006637573
LOSS: 1.3192896842956543
LOSS: 1.3179728984832764
LOSS: 1.3166639804840088
LOSS: 1.3154007196426392
LOSS: 1.3141053915023804
LOSS: 1.3128801584243774
LOSS: 1.3116363286972046
LOSS: 1.310450553894043
LOSS: 1.3093411922454834
LOSS: 1.3082941770553589
LOSS: 1.3072917461395264
LOSS: 1.30632746219635
LOSS: 1.3054910898208618
LOSS: 1.3047040700912476
LOSS: 1.304012656211853
LOSS: 1.3033859729766846
LOSS: 1.3028299808502197
LOSS: 1.3023920059204102
LOSS: 1.3019671440124512
LOSS: 1.3016095161437988
LOSS: 1.301282286643982
LOSS: 1.3010436296463013
LOSS: 1.3008220195770264
LOSS: 1.3006510734558105
LOSS: 1.3004827499389648
LOSS: 1.300439715385437
LOSS: 1.3002978563308716
LOSS: 1.3003021478652954
LOSS: 1.3002781867980957
LOSS: 1.3003016710281372
LOSS: 1.3003077507019043
LOSS: 1.3003389835357666
LOSS: 1.3003677129745483
LOSS: 1.300402045249939
LOSS: 1.3003801107406616
LOSS: 1.300407886505127
LOSS: 1.3004498481750488
LOSS: 1.3004882335662842
LOSS: 1.3004623651504517
LOSS: 1.3005425930023193
LOSS: 1.3005026578903198
LOSS: 1.3004735708236694
LOSS: 1.3004722595214844
LOSS: 1.300457239151001
LOSS: 1.3004525899887085
LOSS: 1.3004084825515747
LOSS: 1.3003339767456055
LOSS: 1.3003180027008057
LOSS: 1.3003050088882446
LOSS: 1.300305724143982
LOSS: 1.3002806901931763
LOSS: 1.3002673387527466
LOSS: 1.300243854522705
LOSS: 1.3002448081970215
LOSS: 1.3002265691757202
LOSS: 1.3001837730407715
LOSS: 1.3001928329467773
LOSS: 1.3002004623413086
LOSS: 1.3001773357391357
LOSS: 1.3001662492752075
LOSS: 1.3001712560653687
LOSS: 1.3001420497894287
LOSS: 1.3001623153686523
LOSS: 1.3001563549041748
LOSS: 1.3001370429992676
LOSS: 1.3001468181610107
LOSS: 1.4072647094726562
LOSS: 1.4017877578735352
LOSS: 1.3967208862304688
LOSS: 1.3921233415603638
LOSS: 1.3880387544631958
LOSS: 1.3845207691192627
LOSS: 1.3815009593963623
LOSS: 1.3791136741638184
LOSS: 1.377251148223877
LOSS: 1.3759320974349976
LOSS: 1.3750287294387817
LOSS: 1.374450922012329
LOSS: 1.3740551471710205
LOSS: 1.3737291097640991
LOSS: 1.3732842206954956
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.323173999786377
LOSS: 1.3231314420700073
LOSS: 1.3231359720230103
LOSS: 1.323149561882019
LOSS: 1.3230996131896973
LOSS: 1.323080062866211
LOSS: 1.3230834007263184
LOSS: 1.3230698108673096
LOSS: 1.3231078386306763
LOSS: 1.3230637311935425
LOSS: 1.3230371475219727
LOSS: 1.3230717182159424
LOSS: 1.3230417966842651
LOSS: 1.3230270147323608
LOSS: 1.323046088218689
LOSS: 1.3230763673782349
LOSS: 1.403220772743225
LOSS: 1.3975948095321655
LOSS: 1.392404317855835
LOSS: 1.3876984119415283
LOSS: 1.3834545612335205
LOSS: 1.3797457218170166
LOSS: 1.3766170740127563
LOSS: 1.3740400075912476
LOSS: 1.3720216751098633
LOSS: 1.3705296516418457
LOSS: 1.3695210218429565
LOSS: 1.3687858581542969
LOSS: 1.3682502508163452
LOSS: 1.367829442024231
LOSS: 1.3673774003982544
LOSS: 1.3667429685592651
LOSS: 1.3659157752990723
LOSS: 1.3649519681930542
LOSS: 1.3638362884521484
LOSS: 1.3625277280807495
LOSS: 1.361161470413208
LOSS: 1.3597686290740967
LOSS: 1.358257532119751
LOSS: 1.356819748878479
LOSS: 1.3554189205169678
LOSS: 1.3540467023849487
LOSS: 1.3527246713638306
LOSS: 1.351508617401123
LOSS: 1.3503739833831787
LOSS: 1.3493244647979736
LOSS: 1.3482195138931274
LOSS: 1.347254753112793
LOSS: 1.3462488651275635
LOSS: 1.3453316688537598
LOSS: 1.3443583250045776
LOSS: 1.343408465385437
LOSS: 1.3425105810165405
LOSS: 1.3415521383285522
LOSS: 1.3406094312667847
LOSS: 1.339755892753601
LOSS: 1.338773488998413
LOSS: 1.337951421737671
LOSS: 1.3370325565338135
LOSS: 1.336299180984497
LOSS: 1.3354891538619995
LOSS: 1.334838628768921
LOSS: 1.3341400623321533
LOSS: 1.333602786064148
LOSS: 1.3330057859420776
LOSS: 1.3325973749160767
LOSS: 1.332126259803772
LOSS: 1.331761360168457
LOSS: 1.3314311504364014
LOSS: 1.3311326503753662
LOSS: 1.3308945894241333
LOSS: 1.3306524753570557
LOSS: 1.3304892778396606
LOSS: 1.3303333520889282
LOSS: 1.3301907777786255
LOSS: 1.3300976753234863
LOSS: 1.330000400543213
LOSS: 1.3300014734268188
LOSS: 1.3299211263656616
LOSS: 1.3299431800842285
LOSS: 1.3299485445022583
LOSS: 1.329953670501709
LOSS: 1.3300725221633911
LOSS: 1.3300367593765259
LOSS: 1.3300312757492065
LOSS: 1.3300329446792603
LOSS: 1.3300496339797974
LOSS: 1.3300373554229736
LOSS: 1.3300942182540894
LOSS: 1.3300191164016724
LOSS: 1.3300307989120483
LOSS: 1.3301218748092651
LOSS: 1.330043911933899
LOSS: 1.330068826675415
LOSS: 1.330045223236084
LOSS: 1.3300824165344238
LOSS: 1.3300029039382935
LOSS: 1.329944372177124
LOSS: 1.3299341201782227
LOSS: 1.329964280128479
LOSS: 1.3299033641815186
LOSS: 1.3299652338027954
LOSS: 1.329870581626892
LOSS: 1.3298735618591309
LOSS: 1.3298736810684204
LOSS: 1.3298476934432983
LOSS: 1.3298403024673462
LOSS: 1.3298815488815308
LOSS: 1.3298271894454956
LOSS: 1.3298231363296509
LOSS: 1.3298629522323608
LOSS: 1.3297913074493408
LOSS: 1.3298252820968628
LOSS: 1.3297926187515259
LOSS: 1.3297597169876099
LOSS: 1.3298730850219727
LOSS: 1.401581883430481
LOSS: 1.396032691001892
LOSS: 1.3909525871276855
LOSS: 1.3863017559051514
LOSS: 1.3821592330932617
LOSS: 1.3785473108291626
LOSS: 1.3755241632461548
LOSS: 1.3730381727218628
LOSS: 1.3710577487945557
LOSS: 1.369594693183899
LOSS: 1.3685402870178223
LOSS: 1.3676040172576904
LOSS: 1.3668957948684692
LOSS: 1.3661303520202637
LOSS: 1.3652950525283813
LOSS: 1.364304542541504
LOSS: 1.363094687461853
LOSS: 1.3617161512374878
LOSS: 1.3602604866027832
LOSS: 1.3586103916168213
LOSS: 1.3569109439849854
LOSS: 1.3552024364471436
LOSS: 1.3533812761306763
LOSS: 1.3516279458999634
LOSS: 1.349954605102539
LOSS: 1.3482633829116821
LOSS: 1.346660852432251
LOSS: 1.3451602458953857
LOSS: 1.34364652633667
LOSS: 1.342292070388794
LOSS: 1.3409112691879272
LOSS: 1.3396192789077759
LOSS: 1.3383731842041016
LOSS: 1.3370651006698608
LOSS: 1.3358683586120605
LOSS: 1.3346283435821533
LOSS: 1.3334236145019531
LOSS: 1.3322083950042725
LOSS: 1.3310083150863647
LOSS: 1.3298968076705933
LOSS: 1.3288284540176392
LOSS: 1.3276453018188477
LOSS: 1.3266429901123047
LOSS: 1.3256640434265137
LOSS: 1.324734091758728
LOSS: 1.3239905834197998
LOSS: 1.3231652975082397
LOSS: 1.3224955797195435
LOSS: 1.3218662738800049
LOSS: 1.3212920427322388
LOSS: 1.320847749710083
LOSS: 1.3204398155212402
LOSS: 1.320061206817627
LOSS: 1.3197990655899048
LOSS: 1.3195804357528687
LOSS: 1.3193295001983643
LOSS: 1.3191732168197632
LOSS: 1.3190715312957764
LOSS: 1.318962574005127
LOSS: 1.3188436031341553
LOSS: 1.3188214302062988
LOSS: 1.3188902139663696
LOSS: 1.3189367055892944
LOSS: 1.3188507556915283
LOSS: 1.3189527988433838
LOSS: 1.318939447402954
LOSS: 1.3189959526062012
LOSS: 1.3189424276351929
LOSS: 1.318976640701294
LOSS: 1.3190093040466309
LOSS: 1.3190104961395264
LOSS: 1.319122076034546
LOSS: 1.3190398216247559
LOSS: 1.3190137147903442
LOSS: 1.3190422058105469
LOSS: 1.3190163373947144
LOSS: 1.3190369606018066
LOSS: 1.3189489841461182
LOSS: 1.3189888000488281
LOSS: 1.3189141750335693
LOSS: 1.318855881690979
LOSS: 1.3188421726226807
LOSS: 1.318884253501892
LOSS: 1.3188453912734985
LOSS: 1.3188045024871826
LOSS: 1.318747878074646
LOSS: 1.3187344074249268
LOSS: 1.3187172412872314
LOSS: 1.3186793327331543
LOSS: 1.3187332153320312
LOSS: 1.3187388181686401
LOSS: 1.3187696933746338
LOSS: 1.3187226057052612
LOSS: 1.3186982870101929
LOSS: 1.3186676502227783
LOSS: 1.3186975717544556
LOSS: 1.3186705112457275
LOSS: 1.3186707496643066
LOSS: 1.3186553716659546
LOSS: 1.3186691999435425
LOSS: 1.403432846069336
LOSS: 1.3979369401931763
LOSS: 1.3928996324539185
LOSS: 1.3882988691329956
LOSS: 1.3841944932937622
LOSS: 1.380660891532898
LOSS: 1.3776838779449463
LOSS: 1.3752485513687134
LOSS: 1.3733986616134644
LOSS: 1.3719980716705322
LOSS: 1.3710709810256958
LOSS: 1.3703851699829102
LOSS: 1.3698577880859375
LOSS: 1.3694045543670654
LOSS: 1.3688071966171265
LOSS: 1.3680806159973145
LOSS: 1.3671623468399048
LOSS: 1.3660798072814941
LOSS: 1.3649086952209473
LOSS: 1.3634893894195557
LOSS: 1.3620078563690186
LOSS: 1.3604977130889893
LOSS: 1.3590000867843628
LOSS: 1.3574724197387695
LOSS: 1.3560492992401123
LOSS: 1.354599118232727
LOSS: 1.3532103300094604
LOSS: 1.3519221544265747
LOSS: 1.3507078886032104
LOSS: 1.3495242595672607
LOSS: 1.3483988046646118
LOSS: 1.3473162651062012
LOSS: 1.3462555408477783
LOSS: 1.3452122211456299
LOSS: 1.34415864944458
LOSS: 1.3431603908538818
LOSS: 1.3420926332473755
LOSS: 1.3410344123840332
LOSS: 1.3400152921676636
LOSS: 1.3390166759490967
LOSS: 1.33805251121521
LOSS: 1.33706533908844
LOSS: 1.3361903429031372
LOSS: 1.3353203535079956
LOSS: 1.334498643875122
LOSS: 1.3337591886520386
LOSS: 1.3331184387207031
LOSS: 1.33245849609375
LOSS: 1.3319132328033447
LOSS: 1.3314127922058105
LOSS: 1.3309738636016846
LOSS: 1.330631136894226
LOSS: 1.3302496671676636
LOSS: 1.3299336433410645
LOSS: 1.3297226428985596
LOSS: 1.3294999599456787
LOSS: 1.3293342590332031
LOSS: 1.3292173147201538
LOSS: 1.3290929794311523
LOSS: 1.3290454149246216
LOSS: 1.3289682865142822
LOSS: 1.3289496898651123
LOSS: 1.3290356397628784
LOSS: 1.328965425491333
LOSS: 1.328976035118103
LOSS: 1.3290497064590454
LOSS: 1.3290197849273682
LOSS: 1.3290666341781616
LOSS: 1.3290849924087524
LOSS: 1.3291127681732178
LOSS: 1.3291045427322388
LOSS: 1.3292192220687866
LOSS: 1.3291138410568237
LOSS: 1.329100489616394
LOSS: 1.3291248083114624
LOSS: 1.3291218280792236
LOSS: 1.3290899991989136
LOSS: 1.3291223049163818
LOSS: 1.329082727432251
LOSS: 1.329046368598938
LOSS: 1.3290338516235352
LOSS: 1.329025387763977
LOSS: 1.3289849758148193
LOSS: 1.3289484977722168
LOSS: 1.3289175033569336
LOSS: 1.3289202451705933
LOSS: 1.3289145231246948
LOSS: 1.3288865089416504
LOSS: 1.3288882970809937
LOSS: 1.3288899660110474
LOSS: 1.3288426399230957
LOSS: 1.3288565874099731
LOSS: 1.3288379907608032
LOSS: 1.328829288482666
LOSS: 1.3288865089416504
LOSS: 1.328855276107788
LOSS: 1.3288909196853638
LOSS: 1.3289304971694946
LOSS: 1.328912615776062
LOSS: 1.3288416862487793
LOSS: 1.3918763399124146
LOSS: 1.386068344116211
LOSS: 1.3806825876235962
LOSS: 1.3757206201553345
LOSS: 1.37124764919281
LOSS: 1.3673012256622314
LOSS: 1.3638932704925537
LOSS: 1.3610607385635376
LOSS: 1.3587566614151
LOSS: 1.3569278717041016
LOSS: 1.355503797531128
LOSS: 1.3544068336486816
LOSS: 1.3534585237503052
LOSS: 1.3525251150131226
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3168220520019531
LOSS: 1.3168843984603882
LOSS: 1.316828727722168
LOSS: 1.3167732954025269
LOSS: 1.316788911819458
LOSS: 1.3167706727981567
LOSS: 1.3167363405227661
LOSS: 1.3167190551757812
LOSS: 1.3167293071746826
LOSS: 1.316755771636963
LOSS: 1.3167699575424194
LOSS: 1.316738247871399
LOSS: 1.316720962524414
LOSS: 1.3167567253112793
LOSS: 1.316727876663208
LOSS: 1.4072647094726562
LOSS: 1.4017877578735352
LOSS: 1.3967208862304688
LOSS: 1.3921233415603638
LOSS: 1.3880387544631958
LOSS: 1.3845207691192627
LOSS: 1.3815009593963623
LOSS: 1.3791136741638184
LOSS: 1.377251148223877
LOSS: 1.3759320974349976
LOSS: 1.3750287294387817
LOSS: 1.374450922012329
LOSS: 1.3740551471710205
LOSS: 1.3737291097640991
LOSS: 1.3732842206954956
LOSS: 1.372732400894165
LOSS: 1.3720029592514038
LOSS: 1.3710975646972656
LOSS: 1.37008535861969
LOSS: 1.3688280582427979
LOSS: 1.3675001859664917
LOSS: 1.3661566972732544
LOSS: 1.3647778034210205
LOSS: 1.363370418548584
LOSS: 1.3620257377624512
LOSS: 1.3608028888702393
LOSS: 1.3595155477523804
LOSS: 1.358314871788025
LOSS: 1.3572134971618652
LOSS: 1.3561328649520874
LOSS: 1.3551077842712402
LOSS: 1.3541321754455566
LOSS: 1.353147268295288
LOSS: 1.352163553237915
LOSS: 1.3511713743209839
LOSS: 1.35019052028656
LOSS: 1.3492120504379272
LOSS: 1.3481673002243042
LOSS: 1.3471394777297974
LOSS: 1.346112608909607
LOSS: 1.3451178073883057
LOSS: 1.3441355228424072
LOSS: 1.3431273698806763
LOSS: 1.3421803712844849
LOSS: 1.3412929773330688
LOSS: 1.3404059410095215
LOSS: 1.3396310806274414
LOSS: 1.3388655185699463
LOSS: 1.3381366729736328
LOSS: 1.3375219106674194
LOSS: 1.3368741273880005
LOSS: 1.3363382816314697
LOSS: 1.3358083963394165
LOSS: 1.3353222608566284
LOSS: 1.334969401359558
LOSS: 1.334473967552185
LOSS: 1.3341444730758667
LOSS: 1.3338384628295898
LOSS: 1.3336101770401
LOSS: 1.3333693742752075
LOSS: 1.3331454992294312
LOSS: 1.3329753875732422
LOSS: 1.3328555822372437
LOSS: 1.3327949047088623
LOSS: 1.332720398902893
LOSS: 1.3326520919799805
LOSS: 1.3326523303985596
LOSS: 1.3326914310455322
LOSS: 1.3326472043991089
LOSS: 1.3326865434646606
LOSS: 1.3326301574707031
LOSS: 1.332646369934082
LOSS: 1.3326594829559326
LOSS: 1.332728624343872
LOSS: 1.3327056169509888
LOSS: 1.332680106163025
LOSS: 1.3327172994613647
LOSS: 1.3327549695968628
LOSS: 1.3327795267105103
LOSS: 1.3328592777252197
LOSS: 1.332761526107788
LOSS: 1.3327381610870361
LOSS: 1.3328412771224976
LOSS: 1.3327349424362183
LOSS: 1.3327221870422363
LOSS: 1.3327488899230957
LOSS: 1.332757830619812
LOSS: 1.3327460289001465
LOSS: 1.332764744758606
LOSS: 1.332746148109436
LOSS: 1.3326612710952759
LOSS: 1.3326503038406372
LOSS: 1.3327217102050781
LOSS: 1.3326590061187744
LOSS: 1.3326255083084106
LOSS: 1.3325823545455933
LOSS: 1.3326371908187866
LOSS: 1.3326271772384644
LOSS: 1.3326410055160522
LOSS: 1.3326233625411987
LOSS: 1.398823857307434
LOSS: 1.3931738138198853
LOSS: 1.3879563808441162
LOSS: 1.3831923007965088
LOSS: 1.3789387941360474
LOSS: 1.375191569328308
LOSS: 1.3720227479934692
LOSS: 1.3693950176239014
LOSS: 1.367319941520691
LOSS: 1.3657317161560059
LOSS: 1.3645374774932861
LOSS: 1.3636177778244019
LOSS: 1.362838864326477
LOSS: 1.3621032238006592
LOSS: 1.3612546920776367
LOSS: 1.3602617979049683
LOSS: 1.3591158390045166
LOSS: 1.3577743768692017
LOSS: 1.3562778234481812
LOSS: 1.3546854257583618
LOSS: 1.3529472351074219
LOSS: 1.3511685132980347
LOSS: 1.3494027853012085
LOSS: 1.34757661819458
LOSS: 1.345790982246399
LOSS: 1.3440873622894287
LOSS: 1.3424241542816162
LOSS: 1.3408286571502686
LOSS: 1.3392479419708252
LOSS: 1.337783694267273
LOSS: 1.336342692375183
LOSS: 1.3349642753601074
LOSS: 1.3335860967636108
LOSS: 1.3322398662567139
LOSS: 1.3309246301651
LOSS: 1.3295763731002808
LOSS: 1.328264594078064
LOSS: 1.3269414901733398
LOSS: 1.3256690502166748
LOSS: 1.324310541152954
LOSS: 1.3231042623519897
LOSS: 1.3217885494232178
LOSS: 1.3206290006637573
LOSS: 1.3194050788879395
LOSS: 1.3183118104934692
LOSS: 1.3173173666000366
LOSS: 1.3163284063339233
LOSS: 1.315408706665039
LOSS: 1.3145755529403687
LOSS: 1.313813328742981
LOSS: 1.31315279006958
LOSS: 1.3125460147857666
LOSS: 1.3119490146636963
LOSS: 1.311506986618042
LOSS: 1.311036467552185
LOSS: 1.3106272220611572
LOSS: 1.310342788696289
LOSS: 1.3100537061691284
LOSS: 1.3097870349884033
LOSS: 1.3096073865890503
LOSS: 1.3094557523727417
LOSS: 1.3093677759170532
LOSS: 1.3092799186706543
LOSS: 1.3092390298843384
LOSS: 1.3092126846313477
LOSS: 1.3092520236968994
LOSS: 1.3092188835144043
LOSS: 1.3092552423477173
LOSS: 1.3093018531799316
LOSS: 1.309364676475525
LOSS: 1.3093327283859253
LOSS: 1.3094087839126587
LOSS: 1.309402346611023
LOSS: 1.3094271421432495
LOSS: 1.3094600439071655
LOSS: 1.3094220161437988
LOSS: 1.3094688653945923
LOSS: 1.3094482421875
LOSS: 1.3094732761383057
LOSS: 1.3094416856765747
LOSS: 1.3094276189804077
LOSS: 1.309464931488037
LOSS: 1.309416651725769
LOSS: 1.3094148635864258
LOSS: 1.3093652725219727
LOSS: 1.3093308210372925
LOSS: 1.3093836307525635
LOSS: 1.309300422668457
LOSS: 1.309289574623108
LOSS: 1.3092761039733887
LOSS: 1.3093174695968628
LOSS: 1.309282660484314
LOSS: 1.309240698814392
LOSS: 1.309217095375061
LOSS: 1.3092246055603027
LOSS: 1.3092254400253296
LOSS: 1.3092138767242432
LOSS: 1.309213399887085
LOSS: 1.3092681169509888
LOSS: 1.309228777885437
LOSS: 1.4032834768295288
LOSS: 1.3977738618850708
LOSS: 1.3926808834075928
LOSS: 1.3880435228347778
LOSS: 1.3839198350906372
LOSS: 1.380344033241272
LOSS: 1.377307415008545
LOSS: 1.3748897314071655
LOSS: 1.372947096824646
LOSS: 1.3715276718139648
LOSS: 1.3705530166625977
LOSS: 1.3698976039886475
LOSS: 1.3693009614944458
LOSS: 1.3687807321548462
LOSS: 1.3681130409240723
LOSS: 1.367389440536499
LOSS: 1.3664743900299072
LOSS: 1.3654288053512573
LOSS: 1.3641209602355957
LOSS: 1.3627506494522095
LOSS: 1.3613654375076294
LOSS: 1.3598387241363525
LOSS: 1.358296275138855
LOSS: 1.3568158149719238
LOSS: 1.3553588390350342
LOSS: 1.3539738655090332
LOSS: 1.3526670932769775
LOSS: 1.3514084815979004
LOSS: 1.3502017259597778
LOSS: 1.3491061925888062
LOSS: 1.348055362701416
LOSS: 1.3470349311828613
LOSS: 1.3460570573806763
LOSS: 1.3450924158096313
LOSS: 1.3441623449325562
LOSS: 1.3432365655899048
LOSS: 1.3423386812210083
LOSS: 1.3413995504379272
LOSS: 1.3405200242996216
LOSS: 1.3396251201629639
LOSS: 1.3387808799743652
LOSS: 1.3379937410354614
LOSS: 1.337193250656128
LOSS: 1.3365179300308228
LOSS: 1.335861086845398
LOSS: 1.3352866172790527
LOSS: 1.3347692489624023
LOSS: 1.3343011140823364
LOSS: 1.3339122533798218
LOSS: 1.333592176437378
LOSS: 1.3332700729370117
LOSS: 1.3330198526382446
LOSS: 1.3328258991241455
LOSS: 1.3326977491378784
LOSS: 1.3325555324554443
LOSS: 1.3324376344680786
LOSS: 1.332380771636963
LOSS: 1.3323256969451904
LOSS: 1.332295298576355
LOSS: 1.332329273223877
LOSS: 1.332313060760498
LOSS: 1.332282543182373
LOSS: 1.33231520652771
LOSS: 1.3323155641555786
LOSS: 1.3323466777801514
LOSS: 1.332382082939148
LOSS: 1.332430124282837
LOSS: 1.3324165344238281
LOSS: 1.3324605226516724
LOSS: 1.3324012756347656
LOSS: 1.3324087858200073
LOSS: 1.3324509859085083
LOSS: 1.3324127197265625
LOSS: 1.3324230909347534
LOSS: 1.3323655128479004
LOSS: 1.3323347568511963
LOSS: 1.3323562145233154
LOSS: 1.3323436975479126
LOSS: 1.3322844505310059
LOSS: 1.3322652578353882
LOSS: 1.3322476148605347
LOSS: 1.3322386741638184
LOSS: 1.3322197198867798
LOSS: 1.3321902751922607
LOSS: 1.3322160243988037
LOSS: 1.3321608304977417
LOSS: 1.3321545124053955
LOSS: 1.3321586847305298
LOSS: 1.3322267532348633
LOSS: 1.3321527242660522
LOSS: 1.3321585655212402
LOSS: 1.3321292400360107
LOSS: 1.33213210105896
LOSS: 1.3321516513824463
LOSS: 1.3321353197097778
LOSS: 1.3321154117584229
LOSS: 1.3321130275726318
LOSS: 1.3321150541305542
LOSS: 1.3321194648742676
LOSS: 1.3321415185928345
LOSS: 1.4094009399414062
LOSS: 1.4040732383728027
LOSS: 1.3991901874542236
LOSS: 1.3947534561157227
LOSS: 1.3908509016036987
LOSS: 1.3874949216842651
LOSS: 1.3847076892852783
LOSS: 1.3824896812438965
LOSS: 1.3808670043945312
LOSS: 1.3796783685684204
LOSS: 1.3789594173431396
LOSS: 1.3784867525100708
LOSS: 1.378212332725525
LOSS: 1.377980351448059
LOSS: 1.3775159120559692
LOSS: 1.3770133256912231
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3687807321548462
LOSS: 1.3681130409240723
LOSS: 1.367389440536499
LOSS: 1.3664743900299072
LOSS: 1.3654288053512573
LOSS: 1.3641209602355957
LOSS: 1.3627506494522095
LOSS: 1.3613654375076294
LOSS: 1.3598387241363525
LOSS: 1.358296275138855
LOSS: 1.3568158149719238
LOSS: 1.3553588390350342
LOSS: 1.3539738655090332
LOSS: 1.3526670932769775
LOSS: 1.3514084815979004
LOSS: 1.3502017259597778
LOSS: 1.3491061925888062
LOSS: 1.348055362701416
LOSS: 1.3470349311828613
LOSS: 1.3460570573806763
LOSS: 1.3450924158096313
LOSS: 1.3441623449325562
LOSS: 1.3432365655899048
LOSS: 1.3423386812210083
LOSS: 1.3413995504379272
LOSS: 1.3405200242996216
LOSS: 1.3396251201629639
LOSS: 1.3387808799743652
LOSS: 1.3379937410354614
LOSS: 1.337193250656128
LOSS: 1.3365179300308228
LOSS: 1.335861086845398
LOSS: 1.3352866172790527
LOSS: 1.3347692489624023
LOSS: 1.3343011140823364
LOSS: 1.3339122533798218
LOSS: 1.333592176437378
LOSS: 1.3332700729370117
LOSS: 1.3330198526382446
LOSS: 1.3328258991241455
LOSS: 1.3326977491378784
LOSS: 1.3325555324554443
LOSS: 1.3324376344680786
LOSS: 1.332380771636963
LOSS: 1.3323256969451904
LOSS: 1.332295298576355
LOSS: 1.332329273223877
LOSS: 1.332313060760498
LOSS: 1.332282543182373
LOSS: 1.33231520652771
LOSS: 1.3323155641555786
LOSS: 1.3323466777801514
LOSS: 1.332382082939148
LOSS: 1.332430124282837
LOSS: 1.3324165344238281
LOSS: 1.3324605226516724
LOSS: 1.3324012756347656
LOSS: 1.3324087858200073
LOSS: 1.3324509859085083
LOSS: 1.3324127197265625
LOSS: 1.3324230909347534
LOSS: 1.3323655128479004
LOSS: 1.3323347568511963
LOSS: 1.3323562145233154
LOSS: 1.3323436975479126
LOSS: 1.3322844505310059
LOSS: 1.3322652578353882
LOSS: 1.3322476148605347
LOSS: 1.3322386741638184
LOSS: 1.3322197198867798
LOSS: 1.3321902751922607
LOSS: 1.3322160243988037
LOSS: 1.3321608304977417
LOSS: 1.3321545124053955
LOSS: 1.3321586847305298
LOSS: 1.3322267532348633
LOSS: 1.3321527242660522
LOSS: 1.3321585655212402
LOSS: 1.3321292400360107
LOSS: 1.33213210105896
LOSS: 1.3321516513824463
LOSS: 1.3321353197097778
LOSS: 1.3321154117584229
LOSS: 1.3321130275726318
LOSS: 1.3321150541305542
LOSS: 1.3321194648742676
LOSS: 1.3321415185928345
LOSS: 1.403432846069336
LOSS: 1.3979369401931763
LOSS: 1.3928996324539185
LOSS: 1.3882988691329956
LOSS: 1.3841944932937622
LOSS: 1.380660891532898
LOSS: 1.3776838779449463
LOSS: 1.3752485513687134
LOSS: 1.3733986616134644
LOSS: 1.3719980716705322
LOSS: 1.3710709810256958
LOSS: 1.3703851699829102
LOSS: 1.3698577880859375
LOSS: 1.3694045543670654
LOSS: 1.3688071966171265
LOSS: 1.3680806159973145
LOSS: 1.3671623468399048
LOSS: 1.3660798072814941
LOSS: 1.3649086952209473
LOSS: 1.3634893894195557
LOSS: 1.3620078563690186
LOSS: 1.3604977130889893
LOSS: 1.3590000867843628
LOSS: 1.3574724197387695
LOSS: 1.3560492992401123
LOSS: 1.354599118232727
LOSS: 1.3532103300094604
LOSS: 1.3519221544265747
LOSS: 1.3507078886032104
LOSS: 1.3495242595672607
LOSS: 1.3483988046646118
LOSS: 1.3473162651062012
LOSS: 1.3462555408477783
LOSS: 1.3452122211456299
LOSS: 1.34415864944458
LOSS: 1.3431603908538818
LOSS: 1.3420926332473755
LOSS: 1.3410344123840332
LOSS: 1.3400152921676636
LOSS: 1.3390166759490967
LOSS: 1.33805251121521
LOSS: 1.33706533908844
LOSS: 1.3361903429031372
LOSS: 1.3353203535079956
LOSS: 1.334498643875122
LOSS: 1.3337591886520386
LOSS: 1.3331184387207031
LOSS: 1.33245849609375
LOSS: 1.3319132328033447
LOSS: 1.3314127922058105
LOSS: 1.3309738636016846
LOSS: 1.330631136894226
LOSS: 1.3302496671676636
LOSS: 1.3299336433410645
LOSS: 1.3297226428985596
LOSS: 1.3294999599456787
LOSS: 1.3293342590332031
LOSS: 1.3292173147201538
LOSS: 1.3290929794311523
LOSS: 1.3290454149246216
LOSS: 1.3289682865142822
LOSS: 1.3289496898651123
LOSS: 1.3290356397628784
LOSS: 1.328965425491333
LOSS: 1.328976035118103
LOSS: 1.3290497064590454
LOSS: 1.3290197849273682
LOSS: 1.3290666341781616
LOSS: 1.3290849924087524
LOSS: 1.3291127681732178
LOSS: 1.3291045427322388
LOSS: 1.3292192220687866
LOSS: 1.3291138410568237
LOSS: 1.329100489616394
LOSS: 1.3291248083114624
LOSS: 1.3291218280792236
LOSS: 1.3290899991989136
LOSS: 1.3291223049163818
LOSS: 1.329082727432251
LOSS: 1.329046368598938
LOSS: 1.3290338516235352
LOSS: 1.329025387763977
LOSS: 1.3289849758148193
LOSS: 1.3289484977722168
LOSS: 1.3289175033569336
LOSS: 1.3289202451705933
LOSS: 1.3289145231246948
LOSS: 1.3288865089416504
LOSS: 1.3288882970809937
LOSS: 1.3288899660110474
LOSS: 1.3288426399230957
LOSS: 1.3288565874099731
LOSS: 1.3288379907608032
LOSS: 1.328829288482666
LOSS: 1.3288865089416504
LOSS: 1.328855276107788
LOSS: 1.3288909196853638
LOSS: 1.3289304971694946
LOSS: 1.328912615776062
LOSS: 1.3288416862487793
LOSS: 1.4002398252487183
LOSS: 1.3945422172546387
LOSS: 1.3892707824707031
LOSS: 1.384446620941162
LOSS: 1.3801127672195435
LOSS: 1.3763277530670166
LOSS: 1.3730543851852417
LOSS: 1.3703663349151611
LOSS: 1.3682217597961426
LOSS: 1.3665839433670044
LOSS: 1.3653830289840698
LOSS: 1.364475965499878
LOSS: 1.3637497425079346
LOSS: 1.3630584478378296
LOSS: 1.362304449081421
LOSS: 1.3614286184310913
LOSS: 1.3603442907333374
LOSS: 1.359118938446045
LOSS: 1.3577133417129517
LOSS: 1.3561075925827026
LOSS: 1.3544257879257202
LOSS: 1.352670431137085
LOSS: 1.3508892059326172
LOSS: 1.3490970134735107
LOSS: 1.3472981452941895
LOSS: 1.3455662727355957
LOSS: 1.343866229057312
LOSS: 1.3422353267669678
LOSS: 1.3406392335891724
LOSS: 1.3390984535217285
LOSS: 1.3376046419143677
LOSS: 1.336156964302063
LOSS: 1.33475661277771
LOSS: 1.3333358764648438
LOSS: 1.331896424293518
LOSS: 1.3304786682128906
LOSS: 1.3290480375289917
LOSS: 1.3275794982910156
LOSS: 1.326137900352478
LOSS: 1.3246976137161255
LOSS: 1.3232104778289795
LOSS: 1.3218151330947876
LOSS: 1.3203837871551514
LOSS: 1.3189458847045898
LOSS: 1.3176206350326538
LOSS: 1.3162723779678345
LOSS: 1.3150242567062378
LOSS: 1.313822627067566
LOSS: 1.312711238861084
LOSS: 1.311622142791748
LOSS: 1.3105946779251099
LOSS: 1.3096277713775635
LOSS: 1.308713674545288
LOSS: 1.3078663349151611
LOSS: 1.307061791419983
LOSS: 1.3063321113586426
LOSS: 1.3056496381759644
LOSS: 1.304976224899292
LOSS: 1.3044203519821167
LOSS: 1.3038816452026367
LOSS: 1.303436040878296
LOSS: 1.3030099868774414
LOSS: 1.3026944398880005
LOSS: 1.3023656606674194
LOSS: 1.3021098375320435
LOSS: 1.3018763065338135
LOSS: 1.3017646074295044
LOSS: 1.3016608953475952
LOSS: 1.3015153408050537
LOSS: 1.3014649152755737
LOSS: 1.3013867139816284
LOSS: 1.3013368844985962
LOSS: 1.301311731338501
LOSS: 1.3012751340866089
LOSS: 1.3013358116149902
LOSS: 1.3013461828231812
LOSS: 1.3013486862182617
LOSS: 1.3013739585876465
LOSS: 1.3014171123504639
LOSS: 1.301428198814392
LOSS: 1.3014551401138306
LOSS: 1.3014200925827026
LOSS: 1.301457405090332
LOSS: 1.30144464969635
LOSS: 1.301470160484314
LOSS: 1.3014729022979736
LOSS: 1.3014507293701172
LOSS: 1.3014403581619263
LOSS: 1.301386833190918
LOSS: 1.3014159202575684
LOSS: 1.301469326019287
LOSS: 1.301410436630249
LOSS: 1.3014214038848877
LOSS: 1.301370620727539
LOSS: 1.3013395071029663
LOSS: 1.3013111352920532
LOSS: 1.3013397455215454
LOSS: 1.3013397455215454
LOSS: 1.3013238906860352
LOSS: 1.3013204336166382
LOSS: 1.398431420326233
LOSS: 1.3928049802780151
LOSS: 1.38755464553833
LOSS: 1.382811188697815
LOSS: 1.3785512447357178
LOSS: 1.3747847080230713
LOSS: 1.3715957403182983
LOSS: 1.3690040111541748
LOSS: 1.3669240474700928
LOSS: 1.3653799295425415
LOSS: 1.3642539978027344
LOSS: 1.3634283542633057
LOSS: 1.3628222942352295
LOSS: 1.3622301816940308
LOSS: 1.3616552352905273
LOSS: 1.3608882427215576
LOSS: 1.359983205795288
LOSS: 1.3588826656341553
LOSS: 1.3576364517211914
LOSS: 1.3562942743301392
LOSS: 1.3548085689544678
LOSS: 1.3532824516296387
LOSS: 1.3517369031906128
LOSS: 1.3502308130264282
LOSS: 1.348673701286316
LOSS: 1.3472596406936646
LOSS: 1.34584379196167
LOSS: 1.3445327281951904
LOSS: 1.343288540840149
LOSS: 1.3420724868774414
LOSS: 1.3409556150436401
LOSS: 1.3399244546890259
LOSS: 1.3388701677322388
LOSS: 1.3378397226333618
LOSS: 1.3368431329727173
LOSS: 1.335880994796753
LOSS: 1.3348894119262695
LOSS: 1.3339197635650635
LOSS: 1.3329644203186035
LOSS: 1.3320143222808838
LOSS: 1.3310860395431519
LOSS: 1.3301944732666016
LOSS: 1.3293428421020508
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.372732400894165
LOSS: 1.3720029592514038
LOSS: 1.3710975646972656
LOSS: 1.37008535861969
LOSS: 1.3688280582427979
LOSS: 1.3675001859664917
LOSS: 1.3661566972732544
LOSS: 1.3647778034210205
LOSS: 1.363370418548584
LOSS: 1.3620257377624512
LOSS: 1.3608028888702393
LOSS: 1.3595155477523804
LOSS: 1.358314871788025
LOSS: 1.3572134971618652
LOSS: 1.3561328649520874
LOSS: 1.3551077842712402
LOSS: 1.3541321754455566
LOSS: 1.353147268295288
LOSS: 1.352163553237915
LOSS: 1.3511713743209839
LOSS: 1.35019052028656
LOSS: 1.3492120504379272
LOSS: 1.3481673002243042
LOSS: 1.3471394777297974
LOSS: 1.346112608909607
LOSS: 1.3451178073883057
LOSS: 1.3441355228424072
LOSS: 1.3431273698806763
LOSS: 1.3421803712844849
LOSS: 1.3412929773330688
LOSS: 1.3404059410095215
LOSS: 1.3396310806274414
LOSS: 1.3388655185699463
LOSS: 1.3381366729736328
LOSS: 1.3375219106674194
LOSS: 1.3368741273880005
LOSS: 1.3363382816314697
LOSS: 1.3358083963394165
LOSS: 1.3353222608566284
LOSS: 1.334969401359558
LOSS: 1.334473967552185
LOSS: 1.3341444730758667
LOSS: 1.3338384628295898
LOSS: 1.3336101770401
LOSS: 1.3333693742752075
LOSS: 1.3331454992294312
LOSS: 1.3329753875732422
LOSS: 1.3328555822372437
LOSS: 1.3327949047088623
LOSS: 1.332720398902893
LOSS: 1.3326520919799805
LOSS: 1.3326523303985596
LOSS: 1.3326914310455322
LOSS: 1.3326472043991089
LOSS: 1.3326865434646606
LOSS: 1.3326301574707031
LOSS: 1.332646369934082
LOSS: 1.3326594829559326
LOSS: 1.332728624343872
LOSS: 1.3327056169509888
LOSS: 1.332680106163025
LOSS: 1.3327172994613647
LOSS: 1.3327549695968628
LOSS: 1.3327795267105103
LOSS: 1.3328592777252197
LOSS: 1.332761526107788
LOSS: 1.3327381610870361
LOSS: 1.3328412771224976
LOSS: 1.3327349424362183
LOSS: 1.3327221870422363
LOSS: 1.3327488899230957
LOSS: 1.332757830619812
LOSS: 1.3327460289001465
LOSS: 1.332764744758606
LOSS: 1.332746148109436
LOSS: 1.3326612710952759
LOSS: 1.3326503038406372
LOSS: 1.3327217102050781
LOSS: 1.3326590061187744
LOSS: 1.3326255083084106
LOSS: 1.3325823545455933
LOSS: 1.3326371908187866
LOSS: 1.3326271772384644
LOSS: 1.3326410055160522
LOSS: 1.3326233625411987
LOSS: 1.4036140441894531
LOSS: 1.3981636762619019
LOSS: 1.3931162357330322
LOSS: 1.3885585069656372
LOSS: 1.3845113515853882
LOSS: 1.3810044527053833
LOSS: 1.378043293952942
LOSS: 1.375677227973938
LOSS: 1.373842716217041
LOSS: 1.3725262880325317
LOSS: 1.3716156482696533
LOSS: 1.37102472782135
LOSS: 1.3705953359603882
LOSS: 1.3701571226119995
LOSS: 1.3696929216384888
LOSS: 1.369053840637207
LOSS: 1.3682916164398193
LOSS: 1.3673136234283447
LOSS: 1.3661590814590454
LOSS: 1.3649405241012573
LOSS: 1.363603949546814
LOSS: 1.362242341041565
LOSS: 1.3608543872833252
LOSS: 1.3594768047332764
LOSS: 1.3581660985946655
LOSS: 1.35688054561615
LOSS: 1.3557184934616089
LOSS: 1.3545663356781006
LOSS: 1.3534789085388184
LOSS: 1.3524818420410156
LOSS: 1.3515316247940063
LOSS: 1.3506091833114624
LOSS: 1.3497288227081299
LOSS: 1.3488436937332153
LOSS: 1.3479983806610107
LOSS: 1.3471472263336182
LOSS: 1.3462868928909302
LOSS: 1.3454315662384033
LOSS: 1.3446071147918701
LOSS: 1.343781590461731
LOSS: 1.3429824113845825
LOSS: 1.3421944379806519
LOSS: 1.3414640426635742
LOSS: 1.340783953666687
LOSS: 1.3401776552200317
LOSS: 1.3396103382110596
LOSS: 1.3390847444534302
LOSS: 1.338657259941101
LOSS: 1.3383069038391113
LOSS: 1.3379676342010498
LOSS: 1.3376511335372925
LOSS: 1.3374261856079102
LOSS: 1.3372914791107178
LOSS: 1.3370460271835327
LOSS: 1.3369109630584717
LOSS: 1.3367801904678345
LOSS: 1.3367007970809937
LOSS: 1.3367047309875488
LOSS: 1.3366570472717285
LOSS: 1.3366144895553589
LOSS: 1.3366129398345947
LOSS: 1.3366135358810425
LOSS: 1.336645245552063
LOSS: 1.3366553783416748
LOSS: 1.3366875648498535
LOSS: 1.3367446660995483
LOSS: 1.3367929458618164
LOSS: 1.3367621898651123
LOSS: 1.3368409872055054
LOSS: 1.3368042707443237
LOSS: 1.3368524312973022
LOSS: 1.3367851972579956
LOSS: 1.3368102312088013
LOSS: 1.3367875814437866
LOSS: 1.3367705345153809
LOSS: 1.3367953300476074
LOSS: 1.3367643356323242
LOSS: 1.336740255355835
LOSS: 1.3367228507995605
LOSS: 1.3367109298706055
LOSS: 1.3367081880569458
LOSS: 1.3366626501083374
LOSS: 1.3367023468017578
LOSS: 1.3366786241531372
LOSS: 1.3366236686706543
LOSS: 1.3366247415542603
LOSS: 1.3366248607635498
LOSS: 1.3366025686264038
LOSS: 1.336605429649353
LOSS: 1.3366025686264038
LOSS: 1.3366097211837769
LOSS: 1.3366100788116455
LOSS: 1.3366165161132812
LOSS: 1.3365916013717651
LOSS: 1.3365848064422607
LOSS: 1.3366039991378784
LOSS: 1.336627721786499
LOSS: 1.3366057872772217
LOSS: 1.3366161584854126
LOSS: 1.3365925550460815
LOSS: 1.3948910236358643
LOSS: 1.3891527652740479
LOSS: 1.383847951889038
LOSS: 1.3789559602737427
LOSS: 1.3745830059051514
LOSS: 1.3707228899002075
LOSS: 1.3674336671829224
LOSS: 1.3646875619888306
LOSS: 1.3624496459960938
LOSS: 1.3607077598571777
LOSS: 1.3593990802764893
LOSS: 1.3583065271377563
LOSS: 1.3573905229568481
LOSS: 1.3564752340316772
LOSS: 1.3555023670196533
LOSS: 1.3543689250946045
LOSS: 1.3530601263046265
LOSS: 1.3516147136688232
LOSS: 1.3499757051467896
LOSS: 1.3482178449630737
LOSS: 1.346359133720398
LOSS: 1.3444883823394775
LOSS: 1.342538833618164
LOSS: 1.3406434059143066
LOSS: 1.3387386798858643
LOSS: 1.3369308710098267
LOSS: 1.3351835012435913
LOSS: 1.3335438966751099
LOSS: 1.3318898677825928
LOSS: 1.3304040431976318
LOSS: 1.3288971185684204
LOSS: 1.327450156211853
LOSS: 1.3261041641235352
LOSS: 1.3248026371002197
LOSS: 1.3234602212905884
LOSS: 1.3221484422683716
LOSS: 1.3208526372909546
LOSS: 1.3196007013320923
LOSS: 1.318367838859558
LOSS: 1.3171563148498535
LOSS: 1.3159643411636353
LOSS: 1.3147910833358765
LOSS: 1.3137156963348389
LOSS: 1.312716007232666
LOSS: 1.3117481470108032
LOSS: 1.3108444213867188
LOSS: 1.3101006746292114
LOSS: 1.3093476295471191
LOSS: 1.3087384700775146
LOSS: 1.3081501722335815
LOSS: 1.3076773881912231
LOSS: 1.3072274923324585
LOSS: 1.3068795204162598
LOSS: 1.3065581321716309
LOSS: 1.306322455406189
LOSS: 1.3060743808746338
LOSS: 1.3059049844741821
LOSS: 1.3057547807693481
LOSS: 1.3057150840759277
LOSS: 1.3057256937026978
LOSS: 1.3055483102798462
LOSS: 1.3055418729782104
LOSS: 1.3056291341781616
LOSS: 1.3056061267852783
LOSS: 1.305607795715332
LOSS: 1.3056418895721436
LOSS: 1.305721402168274
LOSS: 1.3057054281234741
LOSS: 1.3057137727737427
LOSS: 1.305749535560608
LOSS: 1.3057761192321777
LOSS: 1.3058191537857056
LOSS: 1.3057156801223755
LOSS: 1.3058151006698608
LOSS: 1.3056968450546265
LOSS: 1.3057491779327393
LOSS: 1.305794358253479
LOSS: 1.3057010173797607
LOSS: 1.3056966066360474
LOSS: 1.3056190013885498
LOSS: 1.3056315183639526
LOSS: 1.3056174516677856
LOSS: 1.3055684566497803
LOSS: 1.3055580854415894
LOSS: 1.3056375980377197
LOSS: 1.305547833442688
LOSS: 1.3054640293121338
LOSS: 1.3055237531661987
LOSS: 1.3054853677749634
LOSS: 1.3054859638214111
LOSS: 1.3054397106170654
LOSS: 1.305477261543274
LOSS: 1.3054280281066895
LOSS: 1.305472731590271
LOSS: 1.3054559230804443
LOSS: 1.305494785308838
LOSS: 1.3054581880569458
LOSS: 1.3054931163787842
LOSS: 1.3053783178329468
LOSS: 1.3054609298706055
LOSS: 1.4094009399414062
LOSS: 1.4040732383728027
LOSS: 1.3991901874542236
LOSS: 1.3947534561157227
LOSS: 1.3908509016036987
LOSS: 1.3874949216842651
LOSS: 1.3847076892852783
LOSS: 1.3824896812438965
LOSS: 1.3808670043945312
LOSS: 1.3796783685684204
LOSS: 1.3789594173431396
LOSS: 1.3784867525100708
LOSS: 1.378212332725525
LOSS: 1.377980351448059
LOSS: 1.3775159120559692
LOSS: 1.3770133256912231
LOSS: 1.3762595653533936
LOSS: 1.3754138946533203
LOSS: 1.3743964433670044
LOSS: 1.3732056617736816
LOSS: 1.3719757795333862
LOSS: 1.3707220554351807
LOSS: 1.3694132566452026
LOSS: 1.3681648969650269
LOSS: 1.366966724395752
LOSS: 1.365820050239563
LOSS: 1.364745020866394
LOSS: 1.363722562789917
LOSS: 1.3628114461898804
LOSS: 1.3618862628936768
LOSS: 1.3610566854476929
LOSS: 1.360276460647583
LOSS: 1.3594708442687988
LOSS: 1.3586716651916504
LOSS: 1.3579046726226807
LOSS: 1.3571170568466187
LOSS: 1.3563014268875122
LOSS: 1.3554818630218506
LOSS: 1.354699730873108
LOSS: 1.3539725542068481
LOSS: 1.3532177209854126
LOSS: 1.3525909185409546
LOSS: 1.3518880605697632
LOSS: 1.3512252569198608
LOSS: 1.3506678342819214
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
LOSS: 1.3515673875808716
LOSS: 1.350446343421936
LOSS: 1.3491783142089844
LOSS: 1.3477109670639038
LOSS: 1.3460924625396729
LOSS: 1.3443734645843506
LOSS: 1.3425257205963135
LOSS: 1.3406425714492798
LOSS: 1.3387112617492676
LOSS: 1.336760401725769
LOSS: 1.3348692655563354
LOSS: 1.3330169916152954
LOSS: 1.3312405347824097
LOSS: 1.3295063972473145
LOSS: 1.3278696537017822
LOSS: 1.326316475868225
LOSS: 1.3248003721237183
LOSS: 1.3233720064163208
LOSS: 1.3219751119613647
LOSS: 1.3206290006637573
LOSS: 1.3192896842956543
LOSS: 1.3179728984832764
LOSS: 1.3166639804840088
LOSS: 1.3154007196426392
LOSS: 1.3141053915023804
LOSS: 1.3128801584243774
LOSS: 1.3116363286972046
LOSS: 1.310450553894043
LOSS: 1.3093411922454834
LOSS: 1.3082941770553589
LOSS: 1.3072917461395264
LOSS: 1.30632746219635
LOSS: 1.3054910898208618
LOSS: 1.3047040700912476
LOSS: 1.304012656211853
LOSS: 1.3033859729766846
LOSS: 1.3028299808502197
LOSS: 1.3023920059204102
LOSS: 1.3019671440124512
LOSS: 1.3016095161437988
LOSS: 1.301282286643982
LOSS: 1.3010436296463013
LOSS: 1.3008220195770264
LOSS: 1.3006510734558105
LOSS: 1.3004827499389648
LOSS: 1.300439715385437
LOSS: 1.3002978563308716
LOSS: 1.3003021478652954
LOSS: 1.3002781867980957
LOSS: 1.3003016710281372
LOSS: 1.3003077507019043
LOSS: 1.3003389835357666
LOSS: 1.3003677129745483
LOSS: 1.300402045249939
LOSS: 1.3003801107406616
LOSS: 1.300407886505127
LOSS: 1.3004498481750488
LOSS: 1.3004882335662842
LOSS: 1.3004623651504517
LOSS: 1.3005425930023193
LOSS: 1.3005026578903198
LOSS: 1.3004735708236694
LOSS: 1.3004722595214844
LOSS: 1.300457239151001
LOSS: 1.3004525899887085
LOSS: 1.3004084825515747
LOSS: 1.3003339767456055
LOSS: 1.3003180027008057
LOSS: 1.3003050088882446
LOSS: 1.300305724143982
LOSS: 1.3002806901931763
LOSS: 1.3002673387527466
LOSS: 1.300243854522705
LOSS: 1.3002448081970215
LOSS: 1.3002265691757202
LOSS: 1.3001837730407715
LOSS: 1.3001928329467773
LOSS: 1.3002004623413086
LOSS: 1.3001773357391357
LOSS: 1.3001662492752075
LOSS: 1.3001712560653687
LOSS: 1.3001420497894287
LOSS: 1.3001623153686523
LOSS: 1.3001563549041748
LOSS: 1.3001370429992676
LOSS: 1.3001468181610107
LOSS: 1.398431420326233
LOSS: 1.3928049802780151
LOSS: 1.38755464553833
LOSS: 1.382811188697815
LOSS: 1.3785512447357178
LOSS: 1.3747847080230713
LOSS: 1.3715957403182983
LOSS: 1.3690040111541748
LOSS: 1.3669240474700928
LOSS: 1.3653799295425415
LOSS: 1.3642539978027344
LOSS: 1.3634283542633057
LOSS: 1.3628222942352295
LOSS: 1.3622301816940308
LOSS: 1.3616552352905273
LOSS: 1.3608882427215576
LOSS: 1.359983205795288
LOSS: 1.3588826656341553
LOSS: 1.3576364517211914
LOSS: 1.3562942743301392
LOSS: 1.3548085689544678
LOSS: 1.3532824516296387
LOSS: 1.3517369031906128
LOSS: 1.3502308130264282
LOSS: 1.348673701286316
LOSS: 1.3472596406936646
LOSS: 1.34584379196167
LOSS: 1.3445327281951904
LOSS: 1.343288540840149
LOSS: 1.3420724868774414
LOSS: 1.3409556150436401
LOSS: 1.3399244546890259
LOSS: 1.3388701677322388
LOSS: 1.3378397226333618
LOSS: 1.3368431329727173
LOSS: 1.335880994796753
LOSS: 1.3348894119262695
LOSS: 1.3339197635650635
LOSS: 1.3329644203186035
LOSS: 1.3320143222808838
LOSS: 1.3310860395431519
LOSS: 1.3301944732666016
LOSS: 1.3293428421020508
LOSS: 1.328575611114502
LOSS: 1.327799677848816
LOSS: 1.3271454572677612
LOSS: 1.3264832496643066
LOSS: 1.3259435892105103
LOSS: 1.3254929780960083
LOSS: 1.3250563144683838
LOSS: 1.3246968984603882
LOSS: 1.3244316577911377
LOSS: 1.3241136074066162
LOSS: 1.3238805532455444
LOSS: 1.3237007856369019
LOSS: 1.3235245943069458
LOSS: 1.3234009742736816
LOSS: 1.3233122825622559
LOSS: 1.3233298063278198
LOSS: 1.3231979608535767
LOSS: 1.3232485055923462
LOSS: 1.3231911659240723
LOSS: 1.3232282400131226
LOSS: 1.323227047920227
LOSS: 1.3232651948928833
LOSS: 1.3232580423355103
LOSS: 1.3233749866485596
LOSS: 1.3233481645584106
LOSS: 1.323379635810852
LOSS: 1.3233697414398193
LOSS: 1.323407530784607
LOSS: 1.3234500885009766
LOSS: 1.3233875036239624
LOSS: 1.3233685493469238
LOSS: 1.323376178741455
LOSS: 1.3233577013015747
LOSS: 1.3233680725097656
LOSS: 1.3233466148376465
LOSS: 1.3233245611190796
LOSS: 1.3233473300933838
LOSS: 1.3233026266098022
LOSS: 1.323360562324524
LOSS: 1.3232924938201904
LOSS: 1.323238492012024
LOSS: 1.3232240676879883
LOSS: 1.323227047920227
LOSS: 1.323196530342102
LOSS: 1.3231931924819946
LOSS: 1.3232046365737915
LOSS: 1.3231669664382935
LOSS: 1.323194146156311
LOSS: 1.323150873184204
LOSS: 1.3231781721115112
LOSS: 1.3231918811798096
LOSS: 1.3232011795043945
LOSS: 1.3231861591339111
LOSS: 1.3231499195098877
LOSS: 1.323203444480896
LOSS: 1.3231679201126099
LOSS: 1.3232191801071167
LOSS: 1.4036140441894531
LOSS: 1.3981636762619019
LOSS: 1.3931162357330322
LOSS: 1.3885585069656372
LOSS: 1.3845113515853882
LOSS: 1.3810044527053833
LOSS: 1.378043293952942
LOSS: 1.375677227973938
LOSS: 1.373842716217041
LOSS: 1.3725262880325317
LOSS: 1.3716156482696533
LOSS: 1.37102472782135
LOSS: 1.3705953359603882
LOSS: 1.3701571226119995
LOSS: 1.3696929216384888
LOSS: 1.369053840637207
LOSS: 1.3682916164398193
LOSS: 1.3673136234283447
LOSS: 1.3661590814590454
LOSS: 1.3649405241012573
LOSS: 1.363603949546814
LOSS: 1.362242341041565
LOSS: 1.3608543872833252
LOSS: 1.3594768047332764
LOSS: 1.3581660985946655
LOSS: 1.35688054561615
LOSS: 1.3557184934616089
LOSS: 1.3545663356781006
LOSS: 1.3534789085388184
LOSS: 1.3524818420410156
LOSS: 1.3515316247940063
LOSS: 1.3506091833114624
LOSS: 1.3497288227081299
LOSS: 1.3488436937332153
LOSS: 1.3479983806610107
LOSS: 1.3471472263336182
LOSS: 1.3462868928909302
LOSS: 1.3454315662384033
LOSS: 1.3446071147918701
LOSS: 1.343781590461731
LOSS: 1.3429824113845825
LOSS: 1.3421944379806519
LOSS: 1.3414640426635742
LOSS: 1.340783953666687
LOSS: 1.3401776552200317
LOSS: 1.3396103382110596
LOSS: 1.3390847444534302
LOSS: 1.338657259941101
LOSS: 1.3383069038391113
LOSS: 1.3379676342010498
LOSS: 1.3376511335372925
LOSS: 1.3374261856079102
LOSS: 1.3372914791107178
LOSS: 1.3370460271835327
LOSS: 1.3369109630584717
LOSS: 1.3367801904678345
LOSS: 1.3367007970809937
LOSS: 1.3367047309875488
LOSS: 1.3366570472717285
LOSS: 1.3366144895553589
LOSS: 1.3366129398345947
LOSS: 1.3366135358810425
LOSS: 1.336645245552063
LOSS: 1.3366553783416748
LOSS: 1.3366875648498535
LOSS: 1.3367446660995483
LOSS: 1.3367929458618164
LOSS: 1.3367621898651123
LOSS: 1.3368409872055054
LOSS: 1.3368042707443237
LOSS: 1.3368524312973022
LOSS: 1.3367851972579956
LOSS: 1.3368102312088013
LOSS: 1.3367875814437866
LOSS: 1.3367705345153809
LOSS: 1.3367953300476074
LOSS: 1.3367643356323242
LOSS: 1.336740255355835
LOSS: 1.3367228507995605
LOSS: 1.3367109298706055
LOSS: 1.3367081880569458
LOSS: 1.3366626501083374
LOSS: 1.3367023468017578
LOSS: 1.3366786241531372
LOSS: 1.3366236686706543
LOSS: 1.3366247415542603
LOSS: 1.3366248607635498
LOSS: 1.3366025686264038
LOSS: 1.336605429649353
LOSS: 1.3366025686264038
LOSS: 1.3366097211837769
LOSS: 1.3366100788116455
LOSS: 1.3366165161132812
LOSS: 1.3365916013717651
LOSS: 1.3365848064422607
LOSS: 1.3366039991378784
LOSS: 1.336627721786499
LOSS: 1.3366057872772217
LOSS: 1.3366161584854126
LOSS: 1.3365925550460815
LOSS: 1.3948910236358643
LOSS: 1.3891527652740479
LOSS: 1.383847951889038
LOSS: 1.3789559602737427
LOSS: 1.3745830059051514
LOSS: 1.3707228899002075
LOSS: 1.3674336671829224
LOSS: 1.3646875619888306
LOSS: 1.3624496459960938
LOSS: 1.3607077598571777
LOSS: 1.3593990802764893
LOSS: 1.3583065271377563
LOSS: 1.3573905229568481
LOSS: 1.3564752340316772
LOSS: 1.3555023670196533
LOSS: 1.3543689250946045
LOSS: 1.3530601263046265
LOSS: 1.3516147136688232
LOSS: 1.3499757051467896
LOSS: 1.3482178449630737
LOSS: 1.346359133720398
LOSS: 1.3444883823394775
LOSS: 1.342538833618164
LOSS: 1.3406434059143066
LOSS: 1.3387386798858643
LOSS: 1.3369308710098267
LOSS: 1.3351835012435913
LOSS: 1.3335438966751099
LOSS: 1.3318898677825928
LOSS: 1.3304040431976318
LOSS: 1.3288971185684204
LOSS: 1.327450156211853
LOSS: 1.3261041641235352
LOSS: 1.3248026371002197
LOSS: 1.3234602212905884
LOSS: 1.3221484422683716
LOSS: 1.3208526372909546
LOSS: 1.3196007013320923
LOSS: 1.318367838859558
LOSS: 1.3171563148498535
LOSS: 1.3159643411636353
LOSS: 1.3147910833358765
LOSS: 1.3137156963348389
LOSS: 1.312716007232666
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3762595653533936
LOSS: 1.3754138946533203
LOSS: 1.3743964433670044
LOSS: 1.3732056617736816
LOSS: 1.3719757795333862
LOSS: 1.3707220554351807
LOSS: 1.3694132566452026
LOSS: 1.3681648969650269
LOSS: 1.366966724395752
LOSS: 1.365820050239563
LOSS: 1.364745020866394
LOSS: 1.363722562789917
LOSS: 1.3628114461898804
LOSS: 1.3618862628936768
LOSS: 1.3610566854476929
LOSS: 1.360276460647583
LOSS: 1.3594708442687988
LOSS: 1.3586716651916504
LOSS: 1.3579046726226807
LOSS: 1.3571170568466187
LOSS: 1.3563014268875122
LOSS: 1.3554818630218506
LOSS: 1.354699730873108
LOSS: 1.3539725542068481
LOSS: 1.3532177209854126
LOSS: 1.3525909185409546
LOSS: 1.3518880605697632
LOSS: 1.3512252569198608
LOSS: 1.3506678342819214
LOSS: 1.350162386894226
LOSS: 1.3497447967529297
LOSS: 1.3493057489395142
LOSS: 1.3489408493041992
LOSS: 1.3486453294754028
LOSS: 1.3484338521957397
LOSS: 1.348161220550537
LOSS: 1.348090410232544
LOSS: 1.3478515148162842
LOSS: 1.3476988077163696
LOSS: 1.347619891166687
LOSS: 1.3475514650344849
LOSS: 1.3474828004837036
LOSS: 1.347477912902832
LOSS: 1.3474873304367065
LOSS: 1.3474862575531006
LOSS: 1.347486972808838
LOSS: 1.3475016355514526
LOSS: 1.3475115299224854
LOSS: 1.3475172519683838
LOSS: 1.3475372791290283
LOSS: 1.347522258758545
LOSS: 1.3475228548049927
LOSS: 1.3475345373153687
LOSS: 1.3476637601852417
LOSS: 1.3475470542907715
LOSS: 1.3475614786148071
LOSS: 1.3476217985153198
LOSS: 1.3475303649902344
LOSS: 1.3475505113601685
LOSS: 1.3475664854049683
LOSS: 1.3474831581115723
LOSS: 1.3474769592285156
LOSS: 1.3474653959274292
LOSS: 1.347428321838379
LOSS: 1.347468614578247
LOSS: 1.3474233150482178
LOSS: 1.3473836183547974
LOSS: 1.3473628759384155
LOSS: 1.3473774194717407
LOSS: 1.347347378730774
LOSS: 1.3473632335662842
LOSS: 1.3473238945007324
LOSS: 1.3473023176193237
LOSS: 1.3472900390625
LOSS: 1.3473021984100342
LOSS: 1.3473836183547974
LOSS: 1.347301959991455
LOSS: 1.3473310470581055
LOSS: 1.3473066091537476
LOSS: 1.3473010063171387
LOSS: 1.3473081588745117
LOSS: 1.3473222255706787
LOSS: 1.3473503589630127
LOSS: 1.3473056554794312
LOSS: 1.4002398252487183
LOSS: 1.3945422172546387
LOSS: 1.3892707824707031
LOSS: 1.384446620941162
LOSS: 1.3801127672195435
LOSS: 1.3763277530670166
LOSS: 1.3730543851852417
LOSS: 1.3703663349151611
LOSS: 1.3682217597961426
LOSS: 1.3665839433670044
LOSS: 1.3653830289840698
LOSS: 1.364475965499878
LOSS: 1.3637497425079346
LOSS: 1.3630584478378296
LOSS: 1.362304449081421
LOSS: 1.3614286184310913
LOSS: 1.3603442907333374
LOSS: 1.359118938446045
LOSS: 1.3577133417129517
LOSS: 1.3561075925827026
LOSS: 1.3544257879257202
LOSS: 1.352670431137085
LOSS: 1.3508892059326172
LOSS: 1.3490970134735107
LOSS: 1.3472981452941895
LOSS: 1.3455662727355957
LOSS: 1.343866229057312
LOSS: 1.3422353267669678
LOSS: 1.3406392335891724
LOSS: 1.3390984535217285
LOSS: 1.3376046419143677
LOSS: 1.336156964302063
LOSS: 1.33475661277771
LOSS: 1.3333358764648438
LOSS: 1.331896424293518
LOSS: 1.3304786682128906
LOSS: 1.3290480375289917
LOSS: 1.3275794982910156
LOSS: 1.326137900352478
LOSS: 1.3246976137161255
LOSS: 1.3232104778289795
LOSS: 1.3218151330947876
LOSS: 1.3203837871551514
LOSS: 1.3189458847045898
LOSS: 1.3176206350326538
LOSS: 1.3162723779678345
LOSS: 1.3150242567062378
LOSS: 1.313822627067566
LOSS: 1.312711238861084
LOSS: 1.311622142791748
LOSS: 1.3105946779251099
LOSS: 1.3096277713775635
LOSS: 1.308713674545288
LOSS: 1.3078663349151611
LOSS: 1.307061791419983
LOSS: 1.3063321113586426
LOSS: 1.3056496381759644
LOSS: 1.304976224899292
LOSS: 1.3044203519821167
LOSS: 1.3038816452026367
LOSS: 1.303436040878296
LOSS: 1.3030099868774414
LOSS: 1.3026944398880005
LOSS: 1.3023656606674194
LOSS: 1.3021098375320435
LOSS: 1.3018763065338135
LOSS: 1.3017646074295044
LOSS: 1.3016608953475952
LOSS: 1.3015153408050537
LOSS: 1.3014649152755737
LOSS: 1.3013867139816284
LOSS: 1.3013368844985962
LOSS: 1.301311731338501
LOSS: 1.3012751340866089
LOSS: 1.3013358116149902
LOSS: 1.3013461828231812
LOSS: 1.3013486862182617
LOSS: 1.3013739585876465
LOSS: 1.3014171123504639
LOSS: 1.301428198814392
LOSS: 1.3014551401138306
LOSS: 1.3014200925827026
LOSS: 1.301457405090332
LOSS: 1.30144464969635
LOSS: 1.301470160484314
LOSS: 1.3014729022979736
LOSS: 1.3014507293701172
LOSS: 1.3014403581619263
LOSS: 1.301386833190918
LOSS: 1.3014159202575684
LOSS: 1.301469326019287
LOSS: 1.301410436630249
LOSS: 1.3014214038848877
LOSS: 1.301370620727539
LOSS: 1.3013395071029663
LOSS: 1.3013111352920532
LOSS: 1.3013397455215454
LOSS: 1.3013397455215454
LOSS: 1.3013238906860352
LOSS: 1.3013204336166382
LOSS: 1.3993587493896484
LOSS: 1.393721342086792
LOSS: 1.3885191679000854
LOSS: 1.3837552070617676
LOSS: 1.379541277885437
LOSS: 1.3757797479629517
LOSS: 1.3726152181625366
LOSS: 1.3700120449066162
LOSS: 1.3679560422897339
LOSS: 1.3663990497589111
LOSS: 1.3652762174606323
LOSS: 1.364441990852356
LOSS: 1.363778829574585
LOSS: 1.3631576299667358
LOSS: 1.36244535446167
LOSS: 1.3616487979888916
LOSS: 1.3606034517288208
LOSS: 1.359451174736023
LOSS: 1.3581047058105469
LOSS: 1.3566539287567139
LOSS: 1.3551403284072876
LOSS: 1.3534753322601318
LOSS: 1.351861596107483
LOSS: 1.3502135276794434
LOSS: 1.3486526012420654
LOSS: 1.347083568572998
LOSS: 1.3455784320831299
LOSS: 1.3441635370254517
LOSS: 1.3428643941879272
LOSS: 1.3415383100509644
LOSS: 1.3402986526489258
LOSS: 1.3391132354736328
LOSS: 1.3379579782485962
LOSS: 1.336781620979309
LOSS: 1.335701823234558
LOSS: 1.3345900774002075
LOSS: 1.3334481716156006
LOSS: 1.3323577642440796
LOSS: 1.331249475479126
LOSS: 1.330157995223999
LOSS: 1.3290979862213135
LOSS: 1.3280484676361084
LOSS: 1.3270649909973145
LOSS: 1.3261220455169678
LOSS: 1.3252054452896118
LOSS: 1.324397325515747
LOSS: 1.3236244916915894
LOSS: 1.3229970932006836
LOSS: 1.3222851753234863
LOSS: 1.3217194080352783
LOSS: 1.3212145566940308
LOSS: 1.3207920789718628
LOSS: 1.3204326629638672
LOSS: 1.3201103210449219
LOSS: 1.3197417259216309
LOSS: 1.319494605064392
LOSS: 1.3192787170410156
LOSS: 1.3190861940383911
LOSS: 1.318947434425354
LOSS: 1.3189163208007812
LOSS: 1.3187823295593262
LOSS: 1.3187470436096191
LOSS: 1.318698525428772
LOSS: 1.318691611289978
LOSS: 1.318788766860962
LOSS: 1.318745732307434
LOSS: 1.3187766075134277
LOSS: 1.3187967538833618
LOSS: 1.3188254833221436
LOSS: 1.318869948387146
LOSS: 1.3188670873641968
LOSS: 1.3189048767089844
LOSS: 1.3189557790756226
LOSS: 1.3189221620559692
LOSS: 1.3189622163772583
LOSS: 1.3189536333084106
LOSS: 1.318926215171814
LOSS: 1.3189456462860107
LOSS: 1.318915843963623
LOSS: 1.318926453590393
LOSS: 1.3188918828964233
LOSS: 1.318885326385498
LOSS: 1.3188531398773193
LOSS: 1.3188025951385498
LOSS: 1.3188318014144897
LOSS: 1.3188012838363647
LOSS: 1.3187764883041382
LOSS: 1.318739652633667
LOSS: 1.318788766860962
LOSS: 1.318725347518921
LOSS: 1.3187206983566284
LOSS: 1.3187905550003052
LOSS: 1.3187347650527954
LOSS: 1.3187118768692017
LOSS: 1.3187652826309204
LOSS: 1.3186970949172974
LOSS: 1.3187041282653809
LOSS: 1.3187365531921387
LOSS: 1.3187053203582764
LOSS: 1.3186821937561035
LOSS: 1.4028469324111938
LOSS: 1.3973537683486938
LOSS: 1.3922791481018066
LOSS: 1.3876724243164062
LOSS: 1.3835729360580444
LOSS: 1.3800029754638672
LOSS: 1.3770108222961426
LOSS: 1.374538540840149
LOSS: 1.372633695602417
LOSS: 1.371208667755127
LOSS: 1.3701574802398682
LOSS: 1.3693804740905762
LOSS: 1.3687162399291992
LOSS: 1.368054986000061
LOSS: 1.3672899007797241
LOSS: 1.3663707971572876
LOSS: 1.365293264389038
LOSS: 1.3639904260635376
LOSS: 1.3625826835632324
LOSS: 1.3610336780548096
LOSS: 1.3594284057617188
LOSS: 1.3577569723129272
LOSS: 1.356080174446106
LOSS: 1.3544161319732666
LOSS: 1.3527801036834717
LOSS: 1.3512176275253296
LOSS: 1.3497035503387451
LOSS: 1.3482896089553833
LOSS: 1.3468714952468872
LOSS: 1.345564365386963
LOSS: 1.344288945198059
LOSS: 1.343079924583435
LOSS: 1.341884732246399
LOSS: 1.3407286405563354
LOSS: 1.3395837545394897
LOSS: 1.3384323120117188
LOSS: 1.3373095989227295
LOSS: 1.3361927270889282
LOSS: 1.3350661993026733
LOSS: 1.334003210067749
LOSS: 1.3329089879989624
LOSS: 1.3319085836410522
LOSS: 1.3309170007705688
LOSS: 1.3300343751907349
LOSS: 1.3291923999786377
LOSS: 1.3284175395965576
LOSS: 1.3277801275253296
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.328575611114502
LOSS: 1.327799677848816
LOSS: 1.3271454572677612
LOSS: 1.3264832496643066
LOSS: 1.3259435892105103
LOSS: 1.3254929780960083
LOSS: 1.3250563144683838
LOSS: 1.3246968984603882
LOSS: 1.3244316577911377
LOSS: 1.3241136074066162
LOSS: 1.3238805532455444
LOSS: 1.3237007856369019
LOSS: 1.3235245943069458
LOSS: 1.3234009742736816
LOSS: 1.3233122825622559
LOSS: 1.3233298063278198
LOSS: 1.3231979608535767
LOSS: 1.3232485055923462
LOSS: 1.3231911659240723
LOSS: 1.3232282400131226
LOSS: 1.323227047920227
LOSS: 1.3232651948928833
LOSS: 1.3232580423355103
LOSS: 1.3233749866485596
LOSS: 1.3233481645584106
LOSS: 1.323379635810852
LOSS: 1.3233697414398193
LOSS: 1.323407530784607
LOSS: 1.3234500885009766
LOSS: 1.3233875036239624
LOSS: 1.3233685493469238
LOSS: 1.323376178741455
LOSS: 1.3233577013015747
LOSS: 1.3233680725097656
LOSS: 1.3233466148376465
LOSS: 1.3233245611190796
LOSS: 1.3233473300933838
LOSS: 1.3233026266098022
LOSS: 1.323360562324524
LOSS: 1.3232924938201904
LOSS: 1.323238492012024
LOSS: 1.3232240676879883
LOSS: 1.323227047920227
LOSS: 1.323196530342102
LOSS: 1.3231931924819946
LOSS: 1.3232046365737915
LOSS: 1.3231669664382935
LOSS: 1.323194146156311
LOSS: 1.323150873184204
LOSS: 1.3231781721115112
LOSS: 1.3231918811798096
LOSS: 1.3232011795043945
LOSS: 1.3231861591339111
LOSS: 1.3231499195098877
LOSS: 1.323203444480896
LOSS: 1.3231679201126099
LOSS: 1.3232191801071167
LOSS: 1.4028469324111938
LOSS: 1.3973537683486938
LOSS: 1.3922791481018066
LOSS: 1.3876724243164062
LOSS: 1.3835729360580444
LOSS: 1.3800029754638672
LOSS: 1.3770108222961426
LOSS: 1.374538540840149
LOSS: 1.372633695602417
LOSS: 1.371208667755127
LOSS: 1.3701574802398682
LOSS: 1.3693804740905762
LOSS: 1.3687162399291992
LOSS: 1.368054986000061
LOSS: 1.3672899007797241
LOSS: 1.3663707971572876
LOSS: 1.365293264389038
LOSS: 1.3639904260635376
LOSS: 1.3625826835632324
LOSS: 1.3610336780548096
LOSS: 1.3594284057617188
LOSS: 1.3577569723129272
LOSS: 1.356080174446106
LOSS: 1.3544161319732666
LOSS: 1.3527801036834717
LOSS: 1.3512176275253296
LOSS: 1.3497035503387451
LOSS: 1.3482896089553833
LOSS: 1.3468714952468872
LOSS: 1.345564365386963
LOSS: 1.344288945198059
LOSS: 1.343079924583435
LOSS: 1.341884732246399
LOSS: 1.3407286405563354
LOSS: 1.3395837545394897
LOSS: 1.3384323120117188
LOSS: 1.3373095989227295
LOSS: 1.3361927270889282
LOSS: 1.3350661993026733
LOSS: 1.334003210067749
LOSS: 1.3329089879989624
LOSS: 1.3319085836410522
LOSS: 1.3309170007705688
LOSS: 1.3300343751907349
LOSS: 1.3291923999786377
LOSS: 1.3284175395965576
LOSS: 1.3277801275253296
LOSS: 1.3271269798278809
LOSS: 1.3265419006347656
LOSS: 1.3260418176651
LOSS: 1.3256351947784424
LOSS: 1.3252559900283813
LOSS: 1.3249530792236328
LOSS: 1.324690341949463
LOSS: 1.3244578838348389
LOSS: 1.3242806196212769
LOSS: 1.3241474628448486
LOSS: 1.3240280151367188
LOSS: 1.3239142894744873
LOSS: 1.3238962888717651
LOSS: 1.3238797187805176
LOSS: 1.323870301246643
LOSS: 1.3238978385925293
LOSS: 1.3238826990127563
LOSS: 1.323911428451538
LOSS: 1.3239365816116333
LOSS: 1.3239936828613281
LOSS: 1.3240128755569458
LOSS: 1.3240411281585693
LOSS: 1.3241108655929565
LOSS: 1.324045181274414
LOSS: 1.3239964246749878
LOSS: 1.324093222618103
LOSS: 1.324061393737793
LOSS: 1.3240140676498413
LOSS: 1.3239943981170654
LOSS: 1.3240145444869995
LOSS: 1.3239998817443848
LOSS: 1.3239638805389404
LOSS: 1.323938250541687
LOSS: 1.3239238262176514
LOSS: 1.323892593383789
LOSS: 1.3238693475723267
LOSS: 1.3238179683685303
LOSS: 1.3238364458084106
LOSS: 1.3237924575805664
LOSS: 1.323790192604065
LOSS: 1.3237700462341309
LOSS: 1.3237881660461426
LOSS: 1.3237478733062744
LOSS: 1.3237334489822388
LOSS: 1.3237767219543457
LOSS: 1.3237465620040894
LOSS: 1.323772668838501
LOSS: 1.323778510093689
LOSS: 1.323830485343933
LOSS: 1.3237953186035156
LOSS: 1.3237310647964478
LOSS: 1.3237658739089966
LOSS: 1.3237465620040894
LOSS: 1.4033112525939941
LOSS: 1.3978228569030762
LOSS: 1.3927379846572876
LOSS: 1.388137698173523
LOSS: 1.3840279579162598
LOSS: 1.3804442882537842
LOSS: 1.3774499893188477
LOSS: 1.3749972581863403
LOSS: 1.373138666152954
LOSS: 1.3717385530471802
LOSS: 1.3707793951034546
LOSS: 1.3701503276824951
LOSS: 1.3697021007537842
LOSS: 1.3692373037338257
LOSS: 1.368678331375122
LOSS: 1.3680627346038818
LOSS: 1.3672118186950684
LOSS: 1.3662521839141846
LOSS: 1.365041732788086
LOSS: 1.3637659549713135
LOSS: 1.3623855113983154
LOSS: 1.3609939813613892
LOSS: 1.3595402240753174
LOSS: 1.3581039905548096
LOSS: 1.3567482233047485
LOSS: 1.3553985357284546
LOSS: 1.3541488647460938
LOSS: 1.3529691696166992
LOSS: 1.351874589920044
LOSS: 1.3507870435714722
LOSS: 1.349778413772583
LOSS: 1.3488463163375854
LOSS: 1.3478766679763794
LOSS: 1.3469822406768799
LOSS: 1.3461415767669678
LOSS: 1.3451944589614868
LOSS: 1.344283938407898
LOSS: 1.3434040546417236
LOSS: 1.3425453901290894
LOSS: 1.3417103290557861
LOSS: 1.3408836126327515
LOSS: 1.3400646448135376
LOSS: 1.3392975330352783
LOSS: 1.3385952711105347
LOSS: 1.3378660678863525
LOSS: 1.3372912406921387
LOSS: 1.3367507457733154
LOSS: 1.3363027572631836
LOSS: 1.335868000984192
LOSS: 1.3355194330215454
LOSS: 1.3351787328720093
LOSS: 1.334903597831726
LOSS: 1.3346643447875977
LOSS: 1.3344919681549072
LOSS: 1.3343044519424438
LOSS: 1.334200382232666
LOSS: 1.3340636491775513
LOSS: 1.334002137184143
LOSS: 1.3339334726333618
LOSS: 1.3339426517486572
LOSS: 1.3339016437530518
LOSS: 1.3339678049087524
LOSS: 1.3339054584503174
LOSS: 1.3339595794677734
LOSS: 1.333963394165039
LOSS: 1.3339787721633911
LOSS: 1.333983063697815
LOSS: 1.3340914249420166
LOSS: 1.3340198993682861
LOSS: 1.334015965461731
LOSS: 1.3340442180633545
LOSS: 1.3340870141983032
LOSS: 1.334052562713623
LOSS: 1.3340363502502441
LOSS: 1.3339993953704834
LOSS: 1.334033489227295
LOSS: 1.3339807987213135
LOSS: 1.3339306116104126
LOSS: 1.3339213132858276
LOSS: 1.3339170217514038
LOSS: 1.333871603012085
LOSS: 1.3338773250579834
LOSS: 1.333823800086975
LOSS: 1.333854079246521
LOSS: 1.3337968587875366
LOSS: 1.3338056802749634
LOSS: 1.333818793296814
LOSS: 1.333778977394104
LOSS: 1.3337560892105103
LOSS: 1.3337981700897217
LOSS: 1.333774209022522
LOSS: 1.3337552547454834
LOSS: 1.3337675333023071
LOSS: 1.3338537216186523
LOSS: 1.3337849378585815
LOSS: 1.3337637186050415
LOSS: 1.3337887525558472
LOSS: 1.3337205648422241
LOSS: 1.3337639570236206
LOSS: 1.3337922096252441
LOSS: 1.3980880975723267
LOSS: 1.392386555671692
LOSS: 1.3871022462844849
LOSS: 1.3822764158248901
LOSS: 1.3779467344284058
LOSS: 1.374124526977539
LOSS: 1.370870590209961
LOSS: 1.3681687116622925
LOSS: 1.3660070896148682
LOSS: 1.3643481731414795
LOSS: 1.3630878925323486
LOSS: 1.362133502960205
LOSS: 1.3613357543945312
LOSS: 1.3605570793151855
LOSS: 1.3597242832183838
LOSS: 1.358738899230957
LOSS: 1.3575620651245117
LOSS: 1.3562142848968506
LOSS: 1.3547581434249878
LOSS: 1.3531538248062134
LOSS: 1.3513669967651367
LOSS: 1.3495869636535645
LOSS: 1.34772789478302
LOSS: 1.3458994626998901
LOSS: 1.3441060781478882
LOSS: 1.3423131704330444
LOSS: 1.3405818939208984
LOSS: 1.3389394283294678
LOSS: 1.3373315334320068
LOSS: 1.3358004093170166
LOSS: 1.3342925310134888
LOSS: 1.3328698873519897
LOSS: 1.3314640522003174
LOSS: 1.3300449848175049
LOSS: 1.3286433219909668
LOSS: 1.327269434928894
LOSS: 1.3259116411209106
LOSS: 1.3245264291763306
LOSS: 1.3230979442596436
LOSS: 1.3217061758041382
LOSS: 1.3203481435775757
LOSS: 1.3190113306045532
LOSS: 1.3177000284194946
LOSS: 1.316433072090149
LOSS: 1.3152397871017456
LOSS: 1.314095377922058
LOSS: 1.3129910230636597
LOSS: 1.3119442462921143
LOSS: 1.311017632484436
LOSS: 1.3101400136947632
LOSS: 1.3093092441558838
LOSS: 1.3085639476776123
LOSS: 1.3079320192337036
LOSS: 1.307312250137329
LOSS: 1.306716799736023
LOSS: 1.306204915046692
LOSS: 1.305720567703247
LOSS: 1.3053295612335205
LOSS: 1.304992437362671
LOSS: 1.3046995401382446
LOSS: 1.3044979572296143
LOSS: 1.3042831420898438
LOSS: 1.3041728734970093
LOSS: 1.304019808769226
LOSS: 1.3039299249649048
LOSS: 1.3038803339004517
LOSS: 1.3038407564163208
LOSS: 1.3038448095321655
LOSS: 1.3038556575775146
LOSS: 1.3038948774337769
LOSS: 1.3039112091064453
LOSS: 1.3039438724517822
LOSS: 1.3039190769195557
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.350162386894226
LOSS: 1.3497447967529297
LOSS: 1.3493057489395142
LOSS: 1.3489408493041992
LOSS: 1.3486453294754028
LOSS: 1.3484338521957397
LOSS: 1.348161220550537
LOSS: 1.348090410232544
LOSS: 1.3478515148162842
LOSS: 1.3476988077163696
LOSS: 1.347619891166687
LOSS: 1.3475514650344849
LOSS: 1.3474828004837036
LOSS: 1.347477912902832
LOSS: 1.3474873304367065
LOSS: 1.3474862575531006
LOSS: 1.347486972808838
LOSS: 1.3475016355514526
LOSS: 1.3475115299224854
LOSS: 1.3475172519683838
LOSS: 1.3475372791290283
LOSS: 1.347522258758545
LOSS: 1.3475228548049927
LOSS: 1.3475345373153687
LOSS: 1.3476637601852417
LOSS: 1.3475470542907715
LOSS: 1.3475614786148071
LOSS: 1.3476217985153198
LOSS: 1.3475303649902344
LOSS: 1.3475505113601685
LOSS: 1.3475664854049683
LOSS: 1.3474831581115723
LOSS: 1.3474769592285156
LOSS: 1.3474653959274292
LOSS: 1.347428321838379
LOSS: 1.347468614578247
LOSS: 1.3474233150482178
LOSS: 1.3473836183547974
LOSS: 1.3473628759384155
LOSS: 1.3473774194717407
LOSS: 1.347347378730774
LOSS: 1.3473632335662842
LOSS: 1.3473238945007324
LOSS: 1.3473023176193237
LOSS: 1.3472900390625
LOSS: 1.3473021984100342
LOSS: 1.3473836183547974
LOSS: 1.347301959991455
LOSS: 1.3473310470581055
LOSS: 1.3473066091537476
LOSS: 1.3473010063171387
LOSS: 1.3473081588745117
LOSS: 1.3473222255706787
LOSS: 1.3473503589630127
LOSS: 1.3473056554794312
LOSS: 1.402662992477417
LOSS: 1.397119402885437
LOSS: 1.391978144645691
LOSS: 1.3873398303985596
LOSS: 1.3831732273101807
LOSS: 1.3795602321624756
LOSS: 1.376491904258728
LOSS: 1.3739947080612183
LOSS: 1.3720648288726807
LOSS: 1.3706474304199219
LOSS: 1.3696236610412598
LOSS: 1.368926763534546
LOSS: 1.3684087991714478
LOSS: 1.3679330348968506
LOSS: 1.367418646812439
LOSS: 1.3667241334915161
LOSS: 1.3658487796783447
LOSS: 1.3648111820220947
LOSS: 1.3636279106140137
LOSS: 1.3622887134552002
LOSS: 1.360882043838501
LOSS: 1.3593980073928833
LOSS: 1.3579659461975098
LOSS: 1.356427550315857
LOSS: 1.3549443483352661
LOSS: 1.3535573482513428
LOSS: 1.3522311449050903
LOSS: 1.350956678390503
LOSS: 1.349753737449646
LOSS: 1.3486014604568481
LOSS: 1.3475526571273804
LOSS: 1.3464659452438354
LOSS: 1.3454513549804688
LOSS: 1.3444528579711914
LOSS: 1.3434473276138306
LOSS: 1.3424683809280396
LOSS: 1.3414705991744995
LOSS: 1.3404566049575806
LOSS: 1.3394867181777954
LOSS: 1.3385246992111206
LOSS: 1.3375385999679565
LOSS: 1.3366450071334839
LOSS: 1.3357398509979248
LOSS: 1.3348807096481323
LOSS: 1.3340970277786255
LOSS: 1.3333512544631958
LOSS: 1.3327140808105469
LOSS: 1.3320848941802979
LOSS: 1.3315080404281616
LOSS: 1.3310381174087524
LOSS: 1.3305809497833252
LOSS: 1.3302215337753296
LOSS: 1.3298559188842773
LOSS: 1.3295758962631226
LOSS: 1.3293253183364868
LOSS: 1.3290876150131226
LOSS: 1.3289334774017334
LOSS: 1.328749179840088
LOSS: 1.3286548852920532
LOSS: 1.3286365270614624
LOSS: 1.328527569770813
LOSS: 1.3284575939178467
LOSS: 1.3284807205200195
LOSS: 1.3284399509429932
LOSS: 1.328440546989441
LOSS: 1.3284683227539062
LOSS: 1.3285051584243774
LOSS: 1.3284709453582764
LOSS: 1.3285653591156006
LOSS: 1.3285807371139526
LOSS: 1.3285596370697021
LOSS: 1.3285624980926514
LOSS: 1.3285949230194092
LOSS: 1.328544020652771
LOSS: 1.3286045789718628
LOSS: 1.3285754919052124
LOSS: 1.3285852670669556
LOSS: 1.3285460472106934
LOSS: 1.3285413980484009
LOSS: 1.3285070657730103
LOSS: 1.3285670280456543
LOSS: 1.3285046815872192
LOSS: 1.3285361528396606
LOSS: 1.3284446001052856
LOSS: 1.3284192085266113
LOSS: 1.3284246921539307
LOSS: 1.328426480293274
LOSS: 1.3283746242523193
LOSS: 1.328428030014038
LOSS: 1.328368902206421
LOSS: 1.3284032344818115
LOSS: 1.3283768892288208
LOSS: 1.328325867652893
LOSS: 1.3283343315124512
LOSS: 1.3284395933151245
LOSS: 1.328339695930481
LOSS: 1.3283604383468628
LOSS: 1.3283565044403076
LOSS: 1.3283822536468506
LOSS: 1.328343152999878
LOSS: 1.3993587493896484
LOSS: 1.393721342086792
LOSS: 1.3885191679000854
LOSS: 1.3837552070617676
LOSS: 1.379541277885437
LOSS: 1.3757797479629517
LOSS: 1.3726152181625366
LOSS: 1.3700120449066162
LOSS: 1.3679560422897339
LOSS: 1.3663990497589111
LOSS: 1.3652762174606323
LOSS: 1.364441990852356
LOSS: 1.363778829574585
LOSS: 1.3631576299667358
LOSS: 1.36244535446167
LOSS: 1.3616487979888916
LOSS: 1.3606034517288208
LOSS: 1.359451174736023
LOSS: 1.3581047058105469
LOSS: 1.3566539287567139
LOSS: 1.3551403284072876
LOSS: 1.3534753322601318
LOSS: 1.351861596107483
LOSS: 1.3502135276794434
LOSS: 1.3486526012420654
LOSS: 1.347083568572998
LOSS: 1.3455784320831299
LOSS: 1.3441635370254517
LOSS: 1.3428643941879272
LOSS: 1.3415383100509644
LOSS: 1.3402986526489258
LOSS: 1.3391132354736328
LOSS: 1.3379579782485962
LOSS: 1.336781620979309
LOSS: 1.335701823234558
LOSS: 1.3345900774002075
LOSS: 1.3334481716156006
LOSS: 1.3323577642440796
LOSS: 1.331249475479126
LOSS: 1.330157995223999
LOSS: 1.3290979862213135
LOSS: 1.3280484676361084
LOSS: 1.3270649909973145
LOSS: 1.3261220455169678
LOSS: 1.3252054452896118
LOSS: 1.324397325515747
LOSS: 1.3236244916915894
LOSS: 1.3229970932006836
LOSS: 1.3222851753234863
LOSS: 1.3217194080352783
LOSS: 1.3212145566940308
LOSS: 1.3207920789718628
LOSS: 1.3204326629638672
LOSS: 1.3201103210449219
LOSS: 1.3197417259216309
LOSS: 1.319494605064392
LOSS: 1.3192787170410156
LOSS: 1.3190861940383911
LOSS: 1.318947434425354
LOSS: 1.3189163208007812
LOSS: 1.3187823295593262
LOSS: 1.3187470436096191
LOSS: 1.318698525428772
LOSS: 1.318691611289978
LOSS: 1.318788766860962
LOSS: 1.318745732307434
LOSS: 1.3187766075134277
LOSS: 1.3187967538833618
LOSS: 1.3188254833221436
LOSS: 1.318869948387146
LOSS: 1.3188670873641968
LOSS: 1.3189048767089844
LOSS: 1.3189557790756226
LOSS: 1.3189221620559692
LOSS: 1.3189622163772583
LOSS: 1.3189536333084106
LOSS: 1.318926215171814
LOSS: 1.3189456462860107
LOSS: 1.318915843963623
LOSS: 1.318926453590393
LOSS: 1.3188918828964233
LOSS: 1.318885326385498
LOSS: 1.3188531398773193
LOSS: 1.3188025951385498
LOSS: 1.3188318014144897
LOSS: 1.3188012838363647
LOSS: 1.3187764883041382
LOSS: 1.318739652633667
LOSS: 1.318788766860962
LOSS: 1.318725347518921
LOSS: 1.3187206983566284
LOSS: 1.3187905550003052
LOSS: 1.3187347650527954
LOSS: 1.3187118768692017
LOSS: 1.3187652826309204
LOSS: 1.3186970949172974
LOSS: 1.3187041282653809
LOSS: 1.3187365531921387
LOSS: 1.3187053203582764
LOSS: 1.3186821937561035
LOSS: 1.3980443477630615
LOSS: 1.3923143148422241
LOSS: 1.3869823217391968
LOSS: 1.382090449333191
LOSS: 1.3777337074279785
LOSS: 1.3738749027252197
LOSS: 1.370565414428711
LOSS: 1.3678244352340698
LOSS: 1.3656466007232666
LOSS: 1.363962173461914
LOSS: 1.3627121448516846
LOSS: 1.3617814779281616
LOSS: 1.3610481023788452
LOSS: 1.3603750467300415
LOSS: 1.3596562147140503
LOSS: 1.3588083982467651
LOSS: 1.3578109741210938
LOSS: 1.356618046760559
LOSS: 1.3552744388580322
LOSS: 1.3537710905075073
LOSS: 1.3521345853805542
LOSS: 1.3504835367202759
LOSS: 1.3487474918365479
LOSS: 1.3470079898834229
LOSS: 1.345310091972351
LOSS: 1.343645453453064
LOSS: 1.3420759439468384
LOSS: 1.3405084609985352
LOSS: 1.3390157222747803
LOSS: 1.3375226259231567
LOSS: 1.3361505270004272
LOSS: 1.3348044157028198
LOSS: 1.333503246307373
LOSS: 1.3322153091430664
LOSS: 1.3309060335159302
LOSS: 1.3296236991882324
LOSS: 1.3283404111862183
LOSS: 1.3270217180252075
LOSS: 1.3257213830947876
LOSS: 1.3244142532348633
LOSS: 1.3231028318405151
LOSS: 1.3218181133270264
LOSS: 1.3205678462982178
LOSS: 1.319317102432251
LOSS: 1.3181698322296143
LOSS: 1.3170814514160156
LOSS: 1.3159642219543457
LOSS: 1.314955234527588
LOSS: 1.3140124082565308
LOSS: 1.3131694793701172
LOSS: 1.312299132347107
LOSS: 1.3115079402923584
LOSS: 1.310835599899292
LOSS: 1.3101378679275513
LOSS: 1.3095424175262451
LOSS: 1.3089888095855713
LOSS: 1.3084944486618042
LOSS: 1.308045506477356
LOSS: 1.3075830936431885
LOSS: 1.3072189092636108
LOSS: 1.3069809675216675
LOSS: 1.3066855669021606
LOSS: 1.306511402130127
LOSS: 1.3063688278198242
LOSS: 1.3061788082122803
LOSS: 1.306146502494812
LOSS: 1.3060266971588135
LOSS: 1.3060047626495361
LOSS: 1.3059601783752441
LOSS: 1.305959701538086
LOSS: 1.3058955669403076
LOSS: 1.305997610092163
LOSS: 1.3059710264205933
LOSS: 1.3060154914855957
LOSS: 1.3060299158096313
LOSS: 1.3117481470108032
LOSS: 1.3108444213867188
LOSS: 1.3101006746292114
LOSS: 1.3093476295471191
LOSS: 1.3087384700775146
LOSS: 1.3081501722335815
LOSS: 1.3076773881912231
LOSS: 1.3072274923324585
LOSS: 1.3068795204162598
LOSS: 1.3065581321716309
LOSS: 1.306322455406189
LOSS: 1.3060743808746338
LOSS: 1.3059049844741821
LOSS: 1.3057547807693481
LOSS: 1.3057150840759277
LOSS: 1.3057256937026978
LOSS: 1.3055483102798462
LOSS: 1.3055418729782104
LOSS: 1.3056291341781616
LOSS: 1.3056061267852783
LOSS: 1.305607795715332
LOSS: 1.3056418895721436
LOSS: 1.305721402168274
LOSS: 1.3057054281234741
LOSS: 1.3057137727737427
LOSS: 1.305749535560608
LOSS: 1.3057761192321777
LOSS: 1.3058191537857056
LOSS: 1.3057156801223755
LOSS: 1.3058151006698608
LOSS: 1.3056968450546265
LOSS: 1.3057491779327393
LOSS: 1.305794358253479
LOSS: 1.3057010173797607
LOSS: 1.3056966066360474
LOSS: 1.3056190013885498
LOSS: 1.3056315183639526
LOSS: 1.3056174516677856
LOSS: 1.3055684566497803
LOSS: 1.3055580854415894
LOSS: 1.3056375980377197
LOSS: 1.305547833442688
LOSS: 1.3054640293121338
LOSS: 1.3055237531661987
LOSS: 1.3054853677749634
LOSS: 1.3054859638214111
LOSS: 1.3054397106170654
LOSS: 1.305477261543274
LOSS: 1.3054280281066895
LOSS: 1.305472731590271
LOSS: 1.3054559230804443
LOSS: 1.305494785308838
LOSS: 1.3054581880569458
LOSS: 1.3054931163787842
LOSS: 1.3053783178329468
LOSS: 1.3054609298706055
LOSS: 1.3980880975723267
LOSS: 1.392386555671692
LOSS: 1.3871022462844849
LOSS: 1.3822764158248901
LOSS: 1.3779467344284058
LOSS: 1.374124526977539
LOSS: 1.370870590209961
LOSS: 1.3681687116622925
LOSS: 1.3660070896148682
LOSS: 1.3643481731414795
LOSS: 1.3630878925323486
LOSS: 1.362133502960205
LOSS: 1.3613357543945312
LOSS: 1.3605570793151855
LOSS: 1.3597242832183838
LOSS: 1.358738899230957
LOSS: 1.3575620651245117
LOSS: 1.3562142848968506
LOSS: 1.3547581434249878
LOSS: 1.3531538248062134
LOSS: 1.3513669967651367
LOSS: 1.3495869636535645
LOSS: 1.34772789478302
LOSS: 1.3458994626998901
LOSS: 1.3441060781478882
LOSS: 1.3423131704330444
LOSS: 1.3405818939208984
LOSS: 1.3389394283294678
LOSS: 1.3373315334320068
LOSS: 1.3358004093170166
LOSS: 1.3342925310134888
LOSS: 1.3328698873519897
LOSS: 1.3314640522003174
LOSS: 1.3300449848175049
LOSS: 1.3286433219909668
LOSS: 1.327269434928894
LOSS: 1.3259116411209106
LOSS: 1.3245264291763306
LOSS: 1.3230979442596436
LOSS: 1.3217061758041382
LOSS: 1.3203481435775757
LOSS: 1.3190113306045532
LOSS: 1.3177000284194946
LOSS: 1.316433072090149
LOSS: 1.3152397871017456
LOSS: 1.314095377922058
LOSS: 1.3129910230636597
LOSS: 1.3119442462921143
LOSS: 1.311017632484436
LOSS: 1.3101400136947632
LOSS: 1.3093092441558838
LOSS: 1.3085639476776123
LOSS: 1.3079320192337036
LOSS: 1.307312250137329
LOSS: 1.306716799736023
LOSS: 1.306204915046692
LOSS: 1.305720567703247
LOSS: 1.3053295612335205
LOSS: 1.304992437362671
LOSS: 1.3046995401382446
LOSS: 1.3044979572296143
LOSS: 1.3042831420898438
LOSS: 1.3041728734970093
LOSS: 1.304019808769226
LOSS: 1.3039299249649048
LOSS: 1.3038803339004517
LOSS: 1.3038407564163208
LOSS: 1.3038448095321655
LOSS: 1.3038556575775146
LOSS: 1.3038948774337769
LOSS: 1.3039112091064453
LOSS: 1.3039438724517822
LOSS: 1.3039190769195557
LOSS: 1.3040188550949097
LOSS: 1.3040401935577393
LOSS: 1.3040225505828857
LOSS: 1.3040615320205688
LOSS: 1.3040536642074585
LOSS: 1.3041540384292603
LOSS: 1.3040800094604492
LOSS: 1.3040847778320312
LOSS: 1.3041118383407593
LOSS: 1.304072618484497
LOSS: 1.3040845394134521
LOSS: 1.304049015045166
LOSS: 1.3040189743041992
LOSS: 1.3040577173233032
LOSS: 1.3040413856506348
LOSS: 1.3039839267730713
LOSS: 1.3039408922195435
LOSS: 1.3039307594299316
LOSS: 1.3039166927337646
LOSS: 1.3039867877960205
LOSS: 1.3039319515228271
LOSS: 1.3038742542266846
LOSS: 1.30389404296875
LOSS: 1.3038605451583862
LOSS: 1.30385422706604
LOSS: 1.3039027452468872
LOSS: 1.3038352727890015
LOSS: 1.402662992477417
LOSS: 1.397119402885437
LOSS: 1.391978144645691
LOSS: 1.3873398303985596
LOSS: 1.3831732273101807
LOSS: 1.3795602321624756
LOSS: 1.376491904258728
LOSS: 1.3739947080612183
LOSS: 1.3720648288726807
LOSS: 1.3706474304199219
LOSS: 1.3696236610412598
LOSS: 1.368926763534546
LOSS: 1.3684087991714478
LOSS: 1.3679330348968506
LOSS: 1.367418646812439
LOSS: 1.3667241334915161
LOSS: 1.3658487796783447
LOSS: 1.3648111820220947
LOSS: 1.3636279106140137
LOSS: 1.3622887134552002
LOSS: 1.360882043838501
LOSS: 1.3593980073928833
LOSS: 1.3579659461975098
LOSS: 1.356427550315857
LOSS: 1.3549443483352661
LOSS: 1.3535573482513428
LOSS: 1.3522311449050903
LOSS: 1.350956678390503
LOSS: 1.349753737449646
LOSS: 1.3486014604568481
LOSS: 1.3475526571273804
LOSS: 1.3464659452438354
LOSS: 1.3454513549804688
LOSS: 1.3444528579711914
LOSS: 1.3434473276138306
LOSS: 1.3424683809280396
LOSS: 1.3414705991744995
LOSS: 1.3404566049575806
LOSS: 1.3394867181777954
LOSS: 1.3385246992111206
LOSS: 1.3375385999679565
LOSS: 1.3366450071334839
LOSS: 1.3357398509979248
LOSS: 1.3348807096481323
LOSS: 1.3340970277786255
LOSS: 1.3333512544631958
LOSS: 1.3327140808105469
LOSS: 1.3320848941802979
LOSS: 1.3315080404281616
LOSS: 1.3310381174087524
LOSS: 1.3305809497833252
LOSS: 1.3302215337753296
LOSS: 1.3298559188842773
LOSS: 1.3295758962631226
LOSS: 1.3293253183364868
LOSS: 1.3290876150131226
LOSS: 1.3289334774017334
LOSS: 1.328749179840088
LOSS: 1.3286548852920532
LOSS: 1.3286365270614624
LOSS: 1.328527569770813
LOSS: 1.3284575939178467
LOSS: 1.3284807205200195
LOSS: 1.3284399509429932
LOSS: 1.328440546989441
LOSS: 1.3284683227539062
LOSS: 1.3285051584243774
LOSS: 1.3284709453582764
LOSS: 1.3285653591156006
LOSS: 1.3285807371139526
LOSS: 1.3285596370697021
LOSS: 1.3285624980926514
LOSS: 1.3285949230194092
LOSS: 1.328544020652771
LOSS: 1.3286045789718628
LOSS: 1.3285754919052124
LOSS: 1.3285852670669556
LOSS: 1.3285460472106934
LOSS: 1.3285413980484009
LOSS: 1.3285070657730103
LOSS: 1.3285670280456543
LOSS: 1.3285046815872192
LOSS: 1.3285361528396606
LOSS: 1.3284446001052856
LOSS: 1.3284192085266113
LOSS: 1.3284246921539307
LOSS: 1.328426480293274
LOSS: 1.3283746242523193
LOSS: 1.328428030014038
LOSS: 1.328368902206421
LOSS: 1.3284032344818115
LOSS: 1.3283768892288208
LOSS: 1.328325867652893
LOSS: 1.3283343315124512
LOSS: 1.3284395933151245
LOSS: 1.328339695930481
LOSS: 1.3283604383468628
LOSS: 1.3283565044403076
LOSS: 1.3283822536468506
LOSS: 1.328343152999878
LOSS: 1.3998916149139404
LOSS: 1.3942548036575317
LOSS: 1.3890289068222046
LOSS: 1.3842655420303345
LOSS: 1.3799983263015747
LOSS: 1.3762669563293457
LOSS: 1.373083233833313
LOSS: 1.370475172996521
LOSS: 1.368416666984558
LOSS: 1.3668568134307861
LOSS: 1.3657389879226685
LOSS: 1.3649653196334839
LOSS: 1.364323377609253
LOSS: 1.3637702465057373
LOSS: 1.363183617591858
LOSS: 1.362375259399414
LOSS: 1.3614685535430908
LOSS: 1.3604120016098022
LOSS: 1.3590964078903198
LOSS: 1.3577214479446411
LOSS: 1.3562124967575073
LOSS: 1.3546934127807617
LOSS: 1.3531299829483032
LOSS: 1.3515228033065796
LOSS: 1.3499873876571655
LOSS: 1.3485227823257446
LOSS: 1.3471001386642456
LOSS: 1.345737338066101
LOSS: 1.344456434249878
LOSS: 1.3432581424713135
LOSS: 1.3420889377593994
LOSS: 1.3409899473190308
LOSS: 1.3399091958999634
LOSS: 1.3388863801956177
LOSS: 1.3378465175628662
LOSS: 1.3368123769760132
LOSS: 1.3357875347137451
LOSS: 1.334765076637268
LOSS: 1.3337609767913818
LOSS: 1.3327676057815552
LOSS: 1.3317601680755615
LOSS: 1.3308318853378296
LOSS: 1.329905390739441
LOSS: 1.3290743827819824
LOSS: 1.3282560110092163
LOSS: 1.3275338411331177
LOSS: 1.3268072605133057
LOSS: 1.3261866569519043
LOSS: 1.3256875276565552
LOSS: 1.3251408338546753
LOSS: 1.3247758150100708
LOSS: 1.3243776559829712
LOSS: 1.3240211009979248
LOSS: 1.3237018585205078
LOSS: 1.3234515190124512
LOSS: 1.3232353925704956
LOSS: 1.3230708837509155
LOSS: 1.3229397535324097
LOSS: 1.3227766752243042
LOSS: 1.3227274417877197
LOSS: 1.3226463794708252
LOSS: 1.3226518630981445
LOSS: 1.3226269483566284
LOSS: 1.3226090669631958
LOSS: 1.322615146636963
LOSS: 1.3226629495620728
LOSS: 1.322659969329834
LOSS: 1.322725772857666
LOSS: 1.322729468345642
LOSS: 1.3227462768554688
LOSS: 1.3227412700653076
LOSS: 1.3227410316467285
LOSS: 1.3227883577346802
LOSS: 1.3227449655532837
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3271269798278809
LOSS: 1.3265419006347656
LOSS: 1.3260418176651
LOSS: 1.3256351947784424
LOSS: 1.3252559900283813
LOSS: 1.3249530792236328
LOSS: 1.324690341949463
LOSS: 1.3244578838348389
LOSS: 1.3242806196212769
LOSS: 1.3241474628448486
LOSS: 1.3240280151367188
LOSS: 1.3239142894744873
LOSS: 1.3238962888717651
LOSS: 1.3238797187805176
LOSS: 1.323870301246643
LOSS: 1.3238978385925293
LOSS: 1.3238826990127563
LOSS: 1.323911428451538
LOSS: 1.3239365816116333
LOSS: 1.3239936828613281
LOSS: 1.3240128755569458
LOSS: 1.3240411281585693
LOSS: 1.3241108655929565
LOSS: 1.324045181274414
LOSS: 1.3239964246749878
LOSS: 1.324093222618103
LOSS: 1.324061393737793
LOSS: 1.3240140676498413
LOSS: 1.3239943981170654
LOSS: 1.3240145444869995
LOSS: 1.3239998817443848
LOSS: 1.3239638805389404
LOSS: 1.323938250541687
LOSS: 1.3239238262176514
LOSS: 1.323892593383789
LOSS: 1.3238693475723267
LOSS: 1.3238179683685303
LOSS: 1.3238364458084106
LOSS: 1.3237924575805664
LOSS: 1.323790192604065
LOSS: 1.3237700462341309
LOSS: 1.3237881660461426
LOSS: 1.3237478733062744
LOSS: 1.3237334489822388
LOSS: 1.3237767219543457
LOSS: 1.3237465620040894
LOSS: 1.323772668838501
LOSS: 1.323778510093689
LOSS: 1.323830485343933
LOSS: 1.3237953186035156
LOSS: 1.3237310647964478
LOSS: 1.3237658739089966
LOSS: 1.3237465620040894
LOSS: 1.4033112525939941
LOSS: 1.3978228569030762
LOSS: 1.3927379846572876
LOSS: 1.388137698173523
LOSS: 1.3840279579162598
LOSS: 1.3804442882537842
LOSS: 1.3774499893188477
LOSS: 1.3749972581863403
LOSS: 1.373138666152954
LOSS: 1.3717385530471802
LOSS: 1.3707793951034546
LOSS: 1.3701503276824951
LOSS: 1.3697021007537842
LOSS: 1.3692373037338257
LOSS: 1.368678331375122
LOSS: 1.3680627346038818
LOSS: 1.3672118186950684
LOSS: 1.3662521839141846
LOSS: 1.365041732788086
LOSS: 1.3637659549713135
LOSS: 1.3623855113983154
LOSS: 1.3609939813613892
LOSS: 1.3595402240753174
LOSS: 1.3581039905548096
LOSS: 1.3567482233047485
LOSS: 1.3553985357284546
LOSS: 1.3541488647460938
LOSS: 1.3529691696166992
LOSS: 1.351874589920044
LOSS: 1.3507870435714722
LOSS: 1.349778413772583
LOSS: 1.3488463163375854
LOSS: 1.3478766679763794
LOSS: 1.3469822406768799
LOSS: 1.3461415767669678
LOSS: 1.3451944589614868
LOSS: 1.344283938407898
LOSS: 1.3434040546417236
LOSS: 1.3425453901290894
LOSS: 1.3417103290557861
LOSS: 1.3408836126327515
LOSS: 1.3400646448135376
LOSS: 1.3392975330352783
LOSS: 1.3385952711105347
LOSS: 1.3378660678863525
LOSS: 1.3372912406921387
LOSS: 1.3367507457733154
LOSS: 1.3363027572631836
LOSS: 1.335868000984192
LOSS: 1.3355194330215454
LOSS: 1.3351787328720093
LOSS: 1.334903597831726
LOSS: 1.3346643447875977
LOSS: 1.3344919681549072
LOSS: 1.3343044519424438
LOSS: 1.334200382232666
LOSS: 1.3340636491775513
LOSS: 1.334002137184143
LOSS: 1.3339334726333618
LOSS: 1.3339426517486572
LOSS: 1.3339016437530518
LOSS: 1.3339678049087524
LOSS: 1.3339054584503174
LOSS: 1.3339595794677734
LOSS: 1.333963394165039
LOSS: 1.3339787721633911
LOSS: 1.333983063697815
LOSS: 1.3340914249420166
LOSS: 1.3340198993682861
LOSS: 1.334015965461731
LOSS: 1.3340442180633545
LOSS: 1.3340870141983032
LOSS: 1.334052562713623
LOSS: 1.3340363502502441
LOSS: 1.3339993953704834
LOSS: 1.334033489227295
LOSS: 1.3339807987213135
LOSS: 1.3339306116104126
LOSS: 1.3339213132858276
LOSS: 1.3339170217514038
LOSS: 1.333871603012085
LOSS: 1.3338773250579834
LOSS: 1.333823800086975
LOSS: 1.333854079246521
LOSS: 1.3337968587875366
LOSS: 1.3338056802749634
LOSS: 1.333818793296814
LOSS: 1.333778977394104
LOSS: 1.3337560892105103
LOSS: 1.3337981700897217
LOSS: 1.333774209022522
LOSS: 1.3337552547454834
LOSS: 1.3337675333023071
LOSS: 1.3338537216186523
LOSS: 1.3337849378585815
LOSS: 1.3337637186050415
LOSS: 1.3337887525558472
LOSS: 1.3337205648422241
LOSS: 1.3337639570236206
LOSS: 1.3337922096252441
LOSS: 1.3965485095977783
LOSS: 1.3908600807189941
LOSS: 1.3855805397033691
LOSS: 1.3807756900787354
LOSS: 1.376424789428711
LOSS: 1.372654676437378
LOSS: 1.369402527809143
LOSS: 1.3667055368423462
LOSS: 1.3645786046981812
LOSS: 1.362910270690918
LOSS: 1.3617063760757446
LOSS: 1.3608227968215942
LOSS: 1.3600181341171265
LOSS: 1.359261393547058
LOSS: 1.3584307432174683
LOSS: 1.3575291633605957
LOSS: 1.356420874595642
LOSS: 1.3551772832870483
LOSS: 1.3536789417266846
LOSS: 1.3521642684936523
LOSS: 1.3504695892333984
LOSS: 1.348779320716858
LOSS: 1.347009301185608
LOSS: 1.345307469367981
LOSS: 1.3435711860656738
LOSS: 1.3419749736785889
LOSS: 1.3403418064117432
LOSS: 1.338809847831726
LOSS: 1.337372064590454
LOSS: 1.3360183238983154
LOSS: 1.3347030878067017
LOSS: 1.333444356918335
LOSS: 1.3322625160217285
LOSS: 1.3310705423355103
LOSS: 1.3299459218978882
LOSS: 1.3287378549575806
LOSS: 1.3276194334030151
LOSS: 1.3265074491500854
LOSS: 1.3254011869430542
LOSS: 1.324332356452942
LOSS: 1.3233284950256348
LOSS: 1.3222768306732178
LOSS: 1.321302890777588
LOSS: 1.3204090595245361
LOSS: 1.31956148147583
LOSS: 1.3188035488128662
LOSS: 1.3181233406066895
LOSS: 1.3174595832824707
LOSS: 1.3169116973876953
LOSS: 1.3164390325546265
LOSS: 1.31602942943573
LOSS: 1.3156541585922241
LOSS: 1.3153846263885498
LOSS: 1.3151017427444458
LOSS: 1.3148952722549438
LOSS: 1.3147451877593994
LOSS: 1.3146053552627563
LOSS: 1.3145064115524292
LOSS: 1.314404845237732
LOSS: 1.3143917322158813
LOSS: 1.3143208026885986
LOSS: 1.3143318891525269
LOSS: 1.3143566846847534
LOSS: 1.3143349885940552
LOSS: 1.3144468069076538
LOSS: 1.314461588859558
LOSS: 1.3144571781158447
LOSS: 1.3144575357437134
LOSS: 1.3144886493682861
LOSS: 1.3145314455032349
LOSS: 1.3145309686660767
LOSS: 1.3145478963851929
LOSS: 1.3145028352737427
LOSS: 1.3145581483840942
LOSS: 1.3144583702087402
LOSS: 1.3144651651382446
LOSS: 1.3144683837890625
LOSS: 1.3144659996032715
LOSS: 1.314426064491272
LOSS: 1.3143329620361328
LOSS: 1.3143411874771118
LOSS: 1.3143645524978638
LOSS: 1.3143372535705566
LOSS: 1.3143389225006104
LOSS: 1.3143365383148193
LOSS: 1.314313530921936
LOSS: 1.3142759799957275
LOSS: 1.314233422279358
LOSS: 1.3142062425613403
LOSS: 1.3142387866973877
LOSS: 1.3142547607421875
LOSS: 1.3142569065093994
LOSS: 1.3142240047454834
LOSS: 1.314205527305603
LOSS: 1.3142167329788208
LOSS: 1.3141950368881226
LOSS: 1.3141851425170898
LOSS: 1.3142204284667969
LOSS: 1.3141772747039795
LOSS: 1.3142364025115967
LOSS: 1.406085729598999
LOSS: 1.4006835222244263
LOSS: 1.3956557512283325
LOSS: 1.3911314010620117
LOSS: 1.387109637260437
LOSS: 1.3836374282836914
LOSS: 1.3807108402252197
LOSS: 1.3783717155456543
LOSS: 1.3765815496444702
LOSS: 1.3752646446228027
LOSS: 1.374351143836975
LOSS: 1.3737258911132812
LOSS: 1.3732337951660156
LOSS: 1.3727813959121704
LOSS: 1.3722559213638306
LOSS: 1.3714922666549683
LOSS: 1.3706092834472656
LOSS: 1.3695532083511353
LOSS: 1.368276596069336
LOSS: 1.3669350147247314
LOSS: 1.365531086921692
LOSS: 1.3640433549880981
LOSS: 1.3625595569610596
LOSS: 1.361085057258606
LOSS: 1.3596829175949097
LOSS: 1.3583308458328247
LOSS: 1.3570271730422974
LOSS: 1.3558143377304077
LOSS: 1.354649305343628
LOSS: 1.3535399436950684
LOSS: 1.3525389432907104
LOSS: 1.351501703262329
LOSS: 1.3505393266677856
LOSS: 1.34954035282135
LOSS: 1.3486754894256592
LOSS: 1.347673773765564
LOSS: 1.3467220067977905
LOSS: 1.3457679748535156
LOSS: 1.344854712486267
LOSS: 1.3439255952835083
LOSS: 1.3430465459823608
LOSS: 1.3422194719314575
LOSS: 1.341418743133545
LOSS: 1.3406916856765747
LOSS: 1.3399772644042969
LOSS: 1.3394198417663574
LOSS: 1.338895559310913
LOSS: 1.3383548259735107
LOSS: 1.3379148244857788
LOSS: 1.3375308513641357
LOSS: 1.337209701538086
LOSS: 1.3369759321212769
LOSS: 1.3367016315460205
LOSS: 1.3365005254745483
LOSS: 1.3363348245620728
LOSS: 1.33625066280365
LOSS: 1.336142897605896
LOSS: 1.3360567092895508
LOSS: 1.3360188007354736
LOSS: 1.3359979391098022
LOSS: 1.3359878063201904
LOSS: 1.335964322090149
LOSS: 1.3360481262207031
LOSS: 1.3360140323638916
LOSS: 1.3360724449157715
LOSS: 1.3360837697982788
LOSS: 1.3360815048217773
LOSS: 1.33612859249115
LOSS: 1.3361282348632812
LOSS: 1.3362032175064087
LOSS: 1.3361427783966064
LOSS: 1.3361860513687134
LOSS: 1.3361684083938599
LOSS: 1.3360555171966553
LOSS: 1.3360787630081177
LOSS: 1.3360819816589355
LOSS: 1.336095929145813
LOSS: 1.33602774143219
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3040188550949097
LOSS: 1.3040401935577393
LOSS: 1.3040225505828857
LOSS: 1.3040615320205688
LOSS: 1.3040536642074585
LOSS: 1.3041540384292603
LOSS: 1.3040800094604492
LOSS: 1.3040847778320312
LOSS: 1.3041118383407593
LOSS: 1.304072618484497
LOSS: 1.3040845394134521
LOSS: 1.304049015045166
LOSS: 1.3040189743041992
LOSS: 1.3040577173233032
LOSS: 1.3040413856506348
LOSS: 1.3039839267730713
LOSS: 1.3039408922195435
LOSS: 1.3039307594299316
LOSS: 1.3039166927337646
LOSS: 1.3039867877960205
LOSS: 1.3039319515228271
LOSS: 1.3038742542266846
LOSS: 1.30389404296875
LOSS: 1.3038605451583862
LOSS: 1.30385422706604
LOSS: 1.3039027452468872
LOSS: 1.3038352727890015
LOSS: 1.406085729598999
LOSS: 1.4006835222244263
LOSS: 1.3956557512283325
LOSS: 1.3911314010620117
LOSS: 1.387109637260437
LOSS: 1.3836374282836914
LOSS: 1.3807108402252197
LOSS: 1.3783717155456543
LOSS: 1.3765815496444702
LOSS: 1.3752646446228027
LOSS: 1.374351143836975
LOSS: 1.3737258911132812
LOSS: 1.3732337951660156
LOSS: 1.3727813959121704
LOSS: 1.3722559213638306
LOSS: 1.3714922666549683
LOSS: 1.3706092834472656
LOSS: 1.3695532083511353
LOSS: 1.368276596069336
LOSS: 1.3669350147247314
LOSS: 1.365531086921692
LOSS: 1.3640433549880981
LOSS: 1.3625595569610596
LOSS: 1.361085057258606
LOSS: 1.3596829175949097
LOSS: 1.3583308458328247
LOSS: 1.3570271730422974
LOSS: 1.3558143377304077
LOSS: 1.354649305343628
LOSS: 1.3535399436950684
LOSS: 1.3525389432907104
LOSS: 1.351501703262329
LOSS: 1.3505393266677856
LOSS: 1.34954035282135
LOSS: 1.3486754894256592
LOSS: 1.347673773765564
LOSS: 1.3467220067977905
LOSS: 1.3457679748535156
LOSS: 1.344854712486267
LOSS: 1.3439255952835083
LOSS: 1.3430465459823608
LOSS: 1.3422194719314575
LOSS: 1.341418743133545
LOSS: 1.3406916856765747
LOSS: 1.3399772644042969
LOSS: 1.3394198417663574
LOSS: 1.338895559310913
LOSS: 1.3383548259735107
LOSS: 1.3379148244857788
LOSS: 1.3375308513641357
LOSS: 1.337209701538086
LOSS: 1.3369759321212769
LOSS: 1.3367016315460205
LOSS: 1.3365005254745483
LOSS: 1.3363348245620728
LOSS: 1.33625066280365
LOSS: 1.336142897605896
LOSS: 1.3360567092895508
LOSS: 1.3360188007354736
LOSS: 1.3359979391098022
LOSS: 1.3359878063201904
LOSS: 1.335964322090149
LOSS: 1.3360481262207031
LOSS: 1.3360140323638916
LOSS: 1.3360724449157715
LOSS: 1.3360837697982788
LOSS: 1.3360815048217773
LOSS: 1.33612859249115
LOSS: 1.3361282348632812
LOSS: 1.3362032175064087
LOSS: 1.3361427783966064
LOSS: 1.3361860513687134
LOSS: 1.3361684083938599
LOSS: 1.3360555171966553
LOSS: 1.3360787630081177
LOSS: 1.3360819816589355
LOSS: 1.336095929145813
LOSS: 1.33602774143219
LOSS: 1.3360072374343872
LOSS: 1.3359981775283813
LOSS: 1.3360127210617065
LOSS: 1.3359531164169312
LOSS: 1.3359402418136597
LOSS: 1.3359013795852661
LOSS: 1.3359193801879883
LOSS: 1.3359193801879883
LOSS: 1.335870623588562
LOSS: 1.335900902748108
LOSS: 1.3358466625213623
LOSS: 1.3358887434005737
LOSS: 1.3358267545700073
LOSS: 1.3358187675476074
LOSS: 1.3359088897705078
LOSS: 1.335844874382019
LOSS: 1.3358078002929688
LOSS: 1.3358241319656372
LOSS: 1.3358509540557861
LOSS: 1.335833191871643
LOSS: 1.3358540534973145
LOSS: 1.3358100652694702
LOSS: 1.3998916149139404
LOSS: 1.3942548036575317
LOSS: 1.3890289068222046
LOSS: 1.3842655420303345
LOSS: 1.3799983263015747
LOSS: 1.3762669563293457
LOSS: 1.373083233833313
LOSS: 1.370475172996521
LOSS: 1.368416666984558
LOSS: 1.3668568134307861
LOSS: 1.3657389879226685
LOSS: 1.3649653196334839
LOSS: 1.364323377609253
LOSS: 1.3637702465057373
LOSS: 1.363183617591858
LOSS: 1.362375259399414
LOSS: 1.3614685535430908
LOSS: 1.3604120016098022
LOSS: 1.3590964078903198
LOSS: 1.3577214479446411
LOSS: 1.3562124967575073
LOSS: 1.3546934127807617
LOSS: 1.3531299829483032
LOSS: 1.3515228033065796
LOSS: 1.3499873876571655
LOSS: 1.3485227823257446
LOSS: 1.3471001386642456
LOSS: 1.345737338066101
LOSS: 1.344456434249878
LOSS: 1.3432581424713135
LOSS: 1.3420889377593994
LOSS: 1.3409899473190308
LOSS: 1.3399091958999634
LOSS: 1.3388863801956177
LOSS: 1.3378465175628662
LOSS: 1.3368123769760132
LOSS: 1.3357875347137451
LOSS: 1.334765076637268
LOSS: 1.3337609767913818
LOSS: 1.3327676057815552
LOSS: 1.3317601680755615
LOSS: 1.3308318853378296
LOSS: 1.329905390739441
LOSS: 1.3290743827819824
LOSS: 1.3282560110092163
LOSS: 1.3275338411331177
LOSS: 1.3268072605133057
LOSS: 1.3261866569519043
LOSS: 1.3256875276565552
LOSS: 1.3251408338546753
LOSS: 1.3247758150100708
LOSS: 1.3243776559829712
LOSS: 1.3240211009979248
LOSS: 1.3237018585205078
LOSS: 1.3234515190124512
LOSS: 1.3232353925704956
LOSS: 1.3230708837509155
LOSS: 1.3229397535324097
LOSS: 1.3227766752243042
LOSS: 1.3227274417877197
LOSS: 1.3226463794708252
LOSS: 1.3226518630981445
LOSS: 1.3226269483566284
LOSS: 1.3226090669631958
LOSS: 1.322615146636963
LOSS: 1.3226629495620728
LOSS: 1.322659969329834
LOSS: 1.322725772857666
LOSS: 1.322729468345642
LOSS: 1.3227462768554688
LOSS: 1.3227412700653076
LOSS: 1.3227410316467285
LOSS: 1.3227883577346802
LOSS: 1.3227449655532837
LOSS: 1.3226916790008545
LOSS: 1.322675347328186
LOSS: 1.3227804899215698
LOSS: 1.3227734565734863
LOSS: 1.322713851928711
LOSS: 1.3226981163024902
LOSS: 1.322657823562622
LOSS: 1.322629690170288
LOSS: 1.322627067565918
LOSS: 1.322622299194336
LOSS: 1.322597861289978
LOSS: 1.322593331336975
LOSS: 1.3225566148757935
LOSS: 1.3225797414779663
LOSS: 1.3224798440933228
LOSS: 1.3225364685058594
LOSS: 1.322526454925537
LOSS: 1.3225034475326538
LOSS: 1.322564721107483
LOSS: 1.3225270509719849
LOSS: 1.3225136995315552
LOSS: 1.3224924802780151
LOSS: 1.322501540184021
LOSS: 1.3225433826446533
LOSS: 1.322501301765442
LOSS: 1.3224736452102661
LOSS: 1.4077287912368774
LOSS: 1.4022928476333618
LOSS: 1.3972852230072021
LOSS: 1.3927453756332397
LOSS: 1.3887227773666382
LOSS: 1.385217547416687
LOSS: 1.3822938203811646
LOSS: 1.3799426555633545
LOSS: 1.3781554698944092
LOSS: 1.376841425895691
LOSS: 1.3759610652923584
LOSS: 1.3754029273986816
LOSS: 1.3749722242355347
LOSS: 1.3745763301849365
LOSS: 1.3741233348846436
LOSS: 1.3736051321029663
LOSS: 1.3727246522903442
LOSS: 1.371729850769043
LOSS: 1.370606541633606
LOSS: 1.3693161010742188
LOSS: 1.3679680824279785
LOSS: 1.3665298223495483
LOSS: 1.3651167154312134
LOSS: 1.3636665344238281
LOSS: 1.3622939586639404
LOSS: 1.3609377145767212
LOSS: 1.3596328496932983
LOSS: 1.3583894968032837
LOSS: 1.3572217226028442
LOSS: 1.3561115264892578
LOSS: 1.3550350666046143
LOSS: 1.3539845943450928
LOSS: 1.352927327156067
LOSS: 1.3518993854522705
LOSS: 1.350870966911316
LOSS: 1.349855661392212
LOSS: 1.3488008975982666
LOSS: 1.347751498222351
LOSS: 1.3466694355010986
LOSS: 1.3456357717514038
LOSS: 1.3446403741836548
LOSS: 1.3435839414596558
LOSS: 1.3425958156585693
LOSS: 1.3416672945022583
LOSS: 1.34079110622406
LOSS: 1.3399184942245483
LOSS: 1.3391320705413818
LOSS: 1.3384202718734741
LOSS: 1.3377327919006348
LOSS: 1.337105393409729
LOSS: 1.3365470170974731
LOSS: 1.3360270261764526
LOSS: 1.3355672359466553
LOSS: 1.3351585865020752
LOSS: 1.3348768949508667
LOSS: 1.3344820737838745
LOSS: 1.3342145681381226
LOSS: 1.3339823484420776
LOSS: 1.3337422609329224
LOSS: 1.3335516452789307
LOSS: 1.333424687385559
LOSS: 1.333313226699829
LOSS: 1.3332664966583252
LOSS: 1.3332493305206299
LOSS: 1.3332664966583252
LOSS: 1.3332316875457764
LOSS: 1.3332092761993408
LOSS: 1.3332411050796509
LOSS: 1.3332350254058838
LOSS: 1.3332414627075195
LOSS: 1.3332552909851074
LOSS: 1.3332918882369995
LOSS: 1.3332945108413696
LOSS: 1.3333184719085693
LOSS: 1.3333317041397095
LOSS: 1.333321213722229
LOSS: 1.3333133459091187
LOSS: 1.3333216905593872
LOSS: 1.3333109617233276
LOSS: 1.3333083391189575
LOSS: 1.3332960605621338
LOSS: 1.3332828283309937
LOSS: 1.3332493305206299
LOSS: 1.3332276344299316
LOSS: 1.3332136869430542
LOSS: 1.3332040309906006
LOSS: 1.3331706523895264
LOSS: 1.333189606666565
LOSS: 1.3331342935562134
LOSS: 1.3331080675125122
LOSS: 1.3331087827682495
LOSS: 1.3331235647201538
LOSS: 1.333119511604309
LOSS: 1.3331021070480347
LOSS: 1.333081603050232
LOSS: 1.3330976963043213
LOSS: 1.333067536354065
LOSS: 1.3330496549606323
LOSS: 1.3330596685409546
LOSS: 1.3330997228622437
LOSS: 1.4021503925323486
LOSS: 1.396639347076416
LOSS: 1.391543984413147
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3059898614883423
LOSS: 1.3060503005981445
LOSS: 1.3061226606369019
LOSS: 1.3061326742172241
LOSS: 1.306154727935791
LOSS: 1.3061162233352661
LOSS: 1.3061800003051758
LOSS: 1.3061550855636597
LOSS: 1.3061517477035522
LOSS: 1.3060932159423828
LOSS: 1.3061254024505615
LOSS: 1.3061453104019165
LOSS: 1.3061163425445557
LOSS: 1.306097388267517
LOSS: 1.3060225248336792
LOSS: 1.3060332536697388
LOSS: 1.30607008934021
LOSS: 1.3060165643692017
LOSS: 1.3060276508331299
LOSS: 1.3060343265533447
LOSS: 1.306004285812378
LOSS: 1.3060017824172974
LOSS: 1.3059651851654053
LOSS: 1.3059617280960083
LOSS: 1.3059413433074951
LOSS: 1.4066249132156372
LOSS: 1.4012631177902222
LOSS: 1.3963103294372559
LOSS: 1.3918507099151611
LOSS: 1.3879003524780273
LOSS: 1.3844892978668213
LOSS: 1.3816461563110352
LOSS: 1.3793727159500122
LOSS: 1.3776580095291138
LOSS: 1.3764318227767944
LOSS: 1.3756024837493896
LOSS: 1.3750693798065186
LOSS: 1.3746562004089355
LOSS: 1.3742094039916992
LOSS: 1.3736659288406372
LOSS: 1.3729687929153442
LOSS: 1.3720959424972534
LOSS: 1.3710315227508545
LOSS: 1.3698551654815674
LOSS: 1.3685342073440552
LOSS: 1.3671993017196655
LOSS: 1.3657517433166504
LOSS: 1.364311695098877
LOSS: 1.362899661064148
LOSS: 1.3615092039108276
LOSS: 1.3602038621902466
LOSS: 1.3589746952056885
LOSS: 1.357752799987793
LOSS: 1.3566337823867798
LOSS: 1.3555521965026855
LOSS: 1.3545135259628296
LOSS: 1.3534854650497437
LOSS: 1.3525199890136719
LOSS: 1.351540207862854
LOSS: 1.350575566291809
LOSS: 1.34959077835083
LOSS: 1.3486309051513672
LOSS: 1.3476696014404297
LOSS: 1.3467015027999878
LOSS: 1.345758318901062
LOSS: 1.3448352813720703
LOSS: 1.3440220355987549
LOSS: 1.3431183099746704
LOSS: 1.342322587966919
LOSS: 1.341617465019226
LOSS: 1.340950608253479
LOSS: 1.3402937650680542
LOSS: 1.3397678136825562
LOSS: 1.3392767906188965
LOSS: 1.3388644456863403
LOSS: 1.338449239730835
LOSS: 1.3381085395812988
LOSS: 1.3378576040267944
LOSS: 1.3376599550247192
LOSS: 1.3374154567718506
LOSS: 1.33724045753479
LOSS: 1.3370922803878784
LOSS: 1.3370158672332764
LOSS: 1.3369462490081787
LOSS: 1.3369197845458984
LOSS: 1.3369113206863403
LOSS: 1.3369570970535278
LOSS: 1.336901068687439
LOSS: 1.3369698524475098
LOSS: 1.3369613885879517
LOSS: 1.3370383977890015
LOSS: 1.3370474576950073
LOSS: 1.3370740413665771
LOSS: 1.337149739265442
LOSS: 1.3370784521102905
LOSS: 1.3371011018753052
LOSS: 1.3371188640594482
LOSS: 1.3371533155441284
LOSS: 1.3372231721878052
LOSS: 1.3371338844299316
LOSS: 1.3370933532714844
LOSS: 1.3371367454528809
LOSS: 1.3370593786239624
LOSS: 1.3370836973190308
LOSS: 1.3369580507278442
LOSS: 1.3370139598846436
LOSS: 1.3370016813278198
LOSS: 1.337011694908142
LOSS: 1.3370105028152466
LOSS: 1.3369446992874146
LOSS: 1.3370221853256226
LOSS: 1.3369150161743164
LOSS: 1.3369487524032593
LOSS: 1.336872935295105
LOSS: 1.3368988037109375
LOSS: 1.3369466066360474
LOSS: 1.3369944095611572
LOSS: 1.336912989616394
LOSS: 1.336940050125122
LOSS: 1.3368775844573975
LOSS: 1.3369516134262085
LOSS: 1.336965560913086
LOSS: 1.3369147777557373
LOSS: 1.3369580507278442
LOSS: 1.3368810415267944
LOSS: 1.3965485095977783
LOSS: 1.3908600807189941
LOSS: 1.3855805397033691
LOSS: 1.3807756900787354
LOSS: 1.376424789428711
LOSS: 1.372654676437378
LOSS: 1.369402527809143
LOSS: 1.3667055368423462
LOSS: 1.3645786046981812
LOSS: 1.362910270690918
LOSS: 1.3617063760757446
LOSS: 1.3608227968215942
LOSS: 1.3600181341171265
LOSS: 1.359261393547058
LOSS: 1.3584307432174683
LOSS: 1.3575291633605957
LOSS: 1.356420874595642
LOSS: 1.3551772832870483
LOSS: 1.3536789417266846
LOSS: 1.3521642684936523
LOSS: 1.3504695892333984
LOSS: 1.348779320716858
LOSS: 1.347009301185608
LOSS: 1.345307469367981
LOSS: 1.3435711860656738
LOSS: 1.3419749736785889
LOSS: 1.3403418064117432
LOSS: 1.338809847831726
LOSS: 1.337372064590454
LOSS: 1.3360183238983154
LOSS: 1.3347030878067017
LOSS: 1.333444356918335
LOSS: 1.3322625160217285
LOSS: 1.3310705423355103
LOSS: 1.3299459218978882
LOSS: 1.3287378549575806
LOSS: 1.3276194334030151
LOSS: 1.3265074491500854
LOSS: 1.3254011869430542
LOSS: 1.324332356452942
LOSS: 1.3233284950256348
LOSS: 1.3222768306732178
LOSS: 1.321302890777588
LOSS: 1.3204090595245361
LOSS: 1.31956148147583
LOSS: 1.3188035488128662
LOSS: 1.3181233406066895
LOSS: 1.3174595832824707
LOSS: 1.3169116973876953
LOSS: 1.3164390325546265
LOSS: 1.31602942943573
LOSS: 1.3156541585922241
LOSS: 1.3153846263885498
LOSS: 1.3151017427444458
LOSS: 1.3148952722549438
LOSS: 1.3147451877593994
LOSS: 1.3146053552627563
LOSS: 1.3145064115524292
LOSS: 1.314404845237732
LOSS: 1.3143917322158813
LOSS: 1.3143208026885986
LOSS: 1.3143318891525269
LOSS: 1.3143566846847534
LOSS: 1.3143349885940552
LOSS: 1.3144468069076538
LOSS: 1.314461588859558
LOSS: 1.3144571781158447
LOSS: 1.3144575357437134
LOSS: 1.3144886493682861
LOSS: 1.3145314455032349
LOSS: 1.3145309686660767
LOSS: 1.3145478963851929
LOSS: 1.3145028352737427
LOSS: 1.3145581483840942
LOSS: 1.3144583702087402
LOSS: 1.3144651651382446
LOSS: 1.3144683837890625
LOSS: 1.3144659996032715
LOSS: 1.314426064491272
LOSS: 1.3143329620361328
LOSS: 1.3143411874771118
LOSS: 1.3143645524978638
LOSS: 1.3143372535705566
LOSS: 1.3143389225006104
LOSS: 1.3143365383148193
LOSS: 1.314313530921936
LOSS: 1.3142759799957275
LOSS: 1.314233422279358
LOSS: 1.3142062425613403
LOSS: 1.3142387866973877
LOSS: 1.3142547607421875
LOSS: 1.3142569065093994
LOSS: 1.3142240047454834
LOSS: 1.314205527305603
LOSS: 1.3142167329788208
LOSS: 1.3141950368881226
LOSS: 1.3141851425170898
LOSS: 1.3142204284667969
LOSS: 1.3141772747039795
LOSS: 1.3142364025115967
LOSS: 1.4025479555130005
LOSS: 1.3970032930374146
LOSS: 1.3918839693069458
LOSS: 1.3872488737106323
LOSS: 1.3830900192260742
LOSS: 1.3794803619384766
LOSS: 1.37642502784729
LOSS: 1.3739525079727173
LOSS: 1.3720086812973022
LOSS: 1.370610237121582
LOSS: 1.3695604801177979
LOSS: 1.3689091205596924
LOSS: 1.3682729005813599
LOSS: 1.3677407503128052
LOSS: 1.3671271800994873
LOSS: 1.366380214691162
LOSS: 1.3654872179031372
LOSS: 1.3643882274627686
LOSS: 1.3631610870361328
LOSS: 1.3617995977401733
LOSS: 1.360347032546997
LOSS: 1.3588320016860962
LOSS: 1.3573213815689087
LOSS: 1.3558220863342285
LOSS: 1.3543827533721924
LOSS: 1.35298490524292
LOSS: 1.3516606092453003
LOSS: 1.3503724336624146
LOSS: 1.3491796255111694
LOSS: 1.3480409383773804
LOSS: 1.3469761610031128
LOSS: 1.3459436893463135
LOSS: 1.344943642616272
LOSS: 1.3439693450927734
LOSS: 1.3430148363113403
LOSS: 1.3420528173446655
LOSS: 1.3411457538604736
LOSS: 1.3401693105697632
LOSS: 1.339231014251709
LOSS: 1.3382858037948608
LOSS: 1.3374128341674805
LOSS: 1.3365176916122437
LOSS: 1.3357034921646118
LOSS: 1.3349300622940063
LOSS: 1.334219217300415
LOSS: 1.3336182832717896
LOSS: 1.3330775499343872
LOSS: 1.3324705362319946
LOSS: 1.3320236206054688
LOSS: 1.3316872119903564
LOSS: 1.3312658071517944
LOSS: 1.3309870958328247
LOSS: 1.3306829929351807
LOSS: 1.330512523651123
LOSS: 1.3303329944610596
LOSS: 1.3301877975463867
LOSS: 1.330068588256836
LOSS: 1.329984188079834
LOSS: 1.329964518547058
LOSS: 1.3299094438552856
LOSS: 1.3298403024673462
LOSS: 1.329891324043274
LOSS: 1.3298845291137695
LOSS: 1.329867959022522
LOSS: 1.330017328262329
LOSS: 1.3299659490585327
LOSS: 1.3299882411956787
LOSS: 1.330052375793457
LOSS: 1.3300509452819824
LOSS: 1.3300460577011108
LOSS: 1.33011794090271
LOSS: 1.330060362815857
LOSS: 1.3301070928573608
LOSS: 1.330108880996704
LOSS: 1.3300650119781494
LOSS: 1.3300641775131226
LOSS: 1.3300479650497437
LOSS: 1.3300241231918335
LOSS: 1.3300278186798096
LOSS: 1.3299875259399414
LOSS: 1.3299779891967773
LOSS: 1.3299754858016968
LOSS: 1.32993483543396
LOSS: 1.3299235105514526
LOSS: 1.329967737197876
LOSS: 1.3298962116241455
LOSS: 1.3298817873001099
LOSS: 1.329844355583191
LOSS: 1.3298743963241577
LOSS: 1.3298450708389282
LOSS: 1.3299254179000854
LOSS: 1.3298654556274414
LOSS: 1.3298499584197998
LOSS: 1.3298271894454956
LOSS: 1.329872727394104
LOSS: 1.3298518657684326
LOSS: 1.3298479318618774
LOSS: 1.329859972000122
LOSS: 1.3298747539520264
LOSS: 1.329835295677185
LOSS: 1.3957171440124512
LOSS: 1.3900203704833984
LOSS: 1.3847575187683105
LOSS: 1.3799482583999634
LOSS: 1.375630497932434
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
LOSS: 1.3226916790008545
LOSS: 1.322675347328186
LOSS: 1.3227804899215698
LOSS: 1.3227734565734863
LOSS: 1.322713851928711
LOSS: 1.3226981163024902
LOSS: 1.322657823562622
LOSS: 1.322629690170288
LOSS: 1.322627067565918
LOSS: 1.322622299194336
LOSS: 1.322597861289978
LOSS: 1.322593331336975
LOSS: 1.3225566148757935
LOSS: 1.3225797414779663
LOSS: 1.3224798440933228
LOSS: 1.3225364685058594
LOSS: 1.322526454925537
LOSS: 1.3225034475326538
LOSS: 1.322564721107483
LOSS: 1.3225270509719849
LOSS: 1.3225136995315552
LOSS: 1.3224924802780151
LOSS: 1.322501540184021
LOSS: 1.3225433826446533
LOSS: 1.322501301765442
LOSS: 1.3224736452102661
LOSS: 1.3980443477630615
LOSS: 1.3923143148422241
LOSS: 1.3869823217391968
LOSS: 1.382090449333191
LOSS: 1.3777337074279785
LOSS: 1.3738749027252197
LOSS: 1.370565414428711
LOSS: 1.3678244352340698
LOSS: 1.3656466007232666
LOSS: 1.363962173461914
LOSS: 1.3627121448516846
LOSS: 1.3617814779281616
LOSS: 1.3610481023788452
LOSS: 1.3603750467300415
LOSS: 1.3596562147140503
LOSS: 1.3588083982467651
LOSS: 1.3578109741210938
LOSS: 1.356618046760559
LOSS: 1.3552744388580322
LOSS: 1.3537710905075073
LOSS: 1.3521345853805542
LOSS: 1.3504835367202759
LOSS: 1.3487474918365479
LOSS: 1.3470079898834229
LOSS: 1.345310091972351
LOSS: 1.343645453453064
LOSS: 1.3420759439468384
LOSS: 1.3405084609985352
LOSS: 1.3390157222747803
LOSS: 1.3375226259231567
LOSS: 1.3361505270004272
LOSS: 1.3348044157028198
LOSS: 1.333503246307373
LOSS: 1.3322153091430664
LOSS: 1.3309060335159302
LOSS: 1.3296236991882324
LOSS: 1.3283404111862183
LOSS: 1.3270217180252075
LOSS: 1.3257213830947876
LOSS: 1.3244142532348633
LOSS: 1.3231028318405151
LOSS: 1.3218181133270264
LOSS: 1.3205678462982178
LOSS: 1.319317102432251
LOSS: 1.3181698322296143
LOSS: 1.3170814514160156
LOSS: 1.3159642219543457
LOSS: 1.314955234527588
LOSS: 1.3140124082565308
LOSS: 1.3131694793701172
LOSS: 1.312299132347107
LOSS: 1.3115079402923584
LOSS: 1.310835599899292
LOSS: 1.3101378679275513
LOSS: 1.3095424175262451
LOSS: 1.3089888095855713
LOSS: 1.3084944486618042
LOSS: 1.308045506477356
LOSS: 1.3075830936431885
LOSS: 1.3072189092636108
LOSS: 1.3069809675216675
LOSS: 1.3066855669021606
LOSS: 1.306511402130127
LOSS: 1.3063688278198242
LOSS: 1.3061788082122803
LOSS: 1.306146502494812
LOSS: 1.3060266971588135
LOSS: 1.3060047626495361
LOSS: 1.3059601783752441
LOSS: 1.305959701538086
LOSS: 1.3058955669403076
LOSS: 1.305997610092163
LOSS: 1.3059710264205933
LOSS: 1.3060154914855957
LOSS: 1.3060299158096313
LOSS: 1.3059898614883423
LOSS: 1.3060503005981445
LOSS: 1.3061226606369019
LOSS: 1.3061326742172241
LOSS: 1.306154727935791
LOSS: 1.3061162233352661
LOSS: 1.3061800003051758
LOSS: 1.3061550855636597
LOSS: 1.3061517477035522
LOSS: 1.3060932159423828
LOSS: 1.3061254024505615
LOSS: 1.3061453104019165
LOSS: 1.3061163425445557
LOSS: 1.306097388267517
LOSS: 1.3060225248336792
LOSS: 1.3060332536697388
LOSS: 1.30607008934021
LOSS: 1.3060165643692017
LOSS: 1.3060276508331299
LOSS: 1.3060343265533447
LOSS: 1.306004285812378
LOSS: 1.3060017824172974
LOSS: 1.3059651851654053
LOSS: 1.3059617280960083
LOSS: 1.3059413433074951
LOSS: 1.4066249132156372
LOSS: 1.4012631177902222
LOSS: 1.3963103294372559
LOSS: 1.3918507099151611
LOSS: 1.3879003524780273
LOSS: 1.3844892978668213
LOSS: 1.3816461563110352
LOSS: 1.3793727159500122
LOSS: 1.3776580095291138
LOSS: 1.3764318227767944
LOSS: 1.3756024837493896
LOSS: 1.3750693798065186
LOSS: 1.3746562004089355
LOSS: 1.3742094039916992
LOSS: 1.3736659288406372
LOSS: 1.3729687929153442
LOSS: 1.3720959424972534
LOSS: 1.3710315227508545
LOSS: 1.3698551654815674
LOSS: 1.3685342073440552
LOSS: 1.3671993017196655
LOSS: 1.3657517433166504
LOSS: 1.364311695098877
LOSS: 1.362899661064148
LOSS: 1.3615092039108276
LOSS: 1.3602038621902466
LOSS: 1.3589746952056885
LOSS: 1.357752799987793
LOSS: 1.3566337823867798
LOSS: 1.3555521965026855
LOSS: 1.3545135259628296
LOSS: 1.3534854650497437
LOSS: 1.3525199890136719
LOSS: 1.351540207862854
LOSS: 1.350575566291809
LOSS: 1.34959077835083
LOSS: 1.3486309051513672
LOSS: 1.3476696014404297
LOSS: 1.3467015027999878
LOSS: 1.345758318901062
LOSS: 1.3448352813720703
LOSS: 1.3440220355987549
LOSS: 1.3431183099746704
LOSS: 1.342322587966919
LOSS: 1.341617465019226
LOSS: 1.340950608253479
LOSS: 1.3402937650680542
LOSS: 1.3397678136825562
LOSS: 1.3392767906188965
LOSS: 1.3388644456863403
LOSS: 1.338449239730835
LOSS: 1.3381085395812988
LOSS: 1.3378576040267944
LOSS: 1.3376599550247192
LOSS: 1.3374154567718506
LOSS: 1.33724045753479
LOSS: 1.3370922803878784
LOSS: 1.3370158672332764
LOSS: 1.3369462490081787
LOSS: 1.3369197845458984
LOSS: 1.3369113206863403
LOSS: 1.3369570970535278
LOSS: 1.336901068687439
LOSS: 1.3369698524475098
LOSS: 1.3369613885879517
LOSS: 1.3370383977890015
LOSS: 1.3370474576950073
LOSS: 1.3370740413665771
LOSS: 1.337149739265442
LOSS: 1.3370784521102905
LOSS: 1.3371011018753052
LOSS: 1.3371188640594482
LOSS: 1.3371533155441284
LOSS: 1.3372231721878052
LOSS: 1.3371338844299316
LOSS: 1.3370933532714844
LOSS: 1.3371367454528809
LOSS: 1.3370593786239624
LOSS: 1.3370836973190308
LOSS: 1.3369580507278442
LOSS: 1.3370139598846436
LOSS: 1.3370016813278198
LOSS: 1.337011694908142
LOSS: 1.3370105028152466
LOSS: 1.3369446992874146
LOSS: 1.3370221853256226
LOSS: 1.3369150161743164
LOSS: 1.3369487524032593
LOSS: 1.336872935295105
LOSS: 1.3368988037109375
LOSS: 1.3369466066360474
LOSS: 1.3369944095611572
LOSS: 1.336912989616394
LOSS: 1.336940050125122
LOSS: 1.3368775844573975
LOSS: 1.3369516134262085
LOSS: 1.336965560913086
LOSS: 1.3369147777557373
LOSS: 1.3369580507278442
LOSS: 1.3368810415267944
LOSS: 1.3985767364501953
LOSS: 1.3929029703140259
LOSS: 1.3876125812530518
LOSS: 1.3828105926513672
LOSS: 1.378493070602417
LOSS: 1.3747048377990723
LOSS: 1.3714815378189087
LOSS: 1.368788719177246
LOSS: 1.3666741847991943
LOSS: 1.3650672435760498
LOSS: 1.363860011100769
LOSS: 1.362979531288147
LOSS: 1.3622902631759644
LOSS: 1.361661672592163
LOSS: 1.3609609603881836
LOSS: 1.3601362705230713
LOSS: 1.3591293096542358
LOSS: 1.3579691648483276
LOSS: 1.356640338897705
LOSS: 1.3551563024520874
LOSS: 1.3535648584365845
LOSS: 1.3519306182861328
LOSS: 1.3502833843231201
LOSS: 1.348647117614746
LOSS: 1.3470067977905273
LOSS: 1.3454569578170776
LOSS: 1.3439226150512695
LOSS: 1.342464566230774
LOSS: 1.3411208391189575
LOSS: 1.3397834300994873
LOSS: 1.3385429382324219
LOSS: 1.3373621702194214
LOSS: 1.3362245559692383
LOSS: 1.3350738286972046
LOSS: 1.3339236974716187
LOSS: 1.3328362703323364
LOSS: 1.3317025899887085
LOSS: 1.330607533454895
LOSS: 1.3294843435287476
LOSS: 1.3284094333648682
LOSS: 1.3273286819458008
LOSS: 1.3262958526611328
LOSS: 1.3252955675125122
LOSS: 1.3243290185928345
LOSS: 1.3234232664108276
LOSS: 1.3226208686828613
LOSS: 1.3218491077423096
LOSS: 1.3211159706115723
LOSS: 1.3204652070999146
LOSS: 1.319909930229187
LOSS: 1.319401741027832
LOSS: 1.3189107179641724
LOSS: 1.3185261487960815
LOSS: 1.3181935548782349
LOSS: 1.3178037405014038
LOSS: 1.3175491094589233
LOSS: 1.3173424005508423
LOSS: 1.3171168565750122
LOSS: 1.316947340965271
LOSS: 1.31685471534729
LOSS: 1.3167047500610352
LOSS: 1.3166719675064087
LOSS: 1.3166614770889282
LOSS: 1.3165969848632812
LOSS: 1.316624641418457
LOSS: 1.3166353702545166
LOSS: 1.316624402999878
LOSS: 1.3166569471359253
LOSS: 1.3166697025299072
LOSS: 1.316762924194336
LOSS: 1.3167238235473633
LOSS: 1.3167216777801514
LOSS: 1.3167394399642944
LOSS: 1.3167561292648315
LOSS: 1.3167388439178467
LOSS: 1.3167388439178467
LOSS: 1.3167327642440796
LOSS: 1.3167426586151123
LOSS: 1.3167065382003784
LOSS: 1.3166851997375488
LOSS: 1.316705584526062
LOSS: 1.3166954517364502
LOSS: 1.3166611194610596
LOSS: 1.3166594505310059
LOSS: 1.3166640996932983
LOSS: 1.3165678977966309
LOSS: 1.3166455030441284
LOSS: 1.3165671825408936
LOSS: 1.3165507316589355
LOSS: 1.3166184425354004
LOSS: 1.316538691520691
LOSS: 1.3165056705474854
LOSS: 1.3165713548660278
LOSS: 1.3165370225906372
LOSS: 1.316509485244751
LOSS: 1.3165158033370972
LOSS: 1.3165361881256104
LOSS: 1.3164676427841187
LOSS: 1.3165146112442017
LOSS: 1.3165149688720703
LOSS: 1.4025479555130005
LOSS: 1.3970032930374146
LOSS: 1.3918839693069458
LOSS: 1.3872488737106323
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
LOSS: 1.3360072374343872
LOSS: 1.3359981775283813
LOSS: 1.3360127210617065
LOSS: 1.3359531164169312
LOSS: 1.3359402418136597
LOSS: 1.3359013795852661
LOSS: 1.3359193801879883
LOSS: 1.3359193801879883
LOSS: 1.335870623588562
LOSS: 1.335900902748108
LOSS: 1.3358466625213623
LOSS: 1.3358887434005737
LOSS: 1.3358267545700073
LOSS: 1.3358187675476074
LOSS: 1.3359088897705078
LOSS: 1.335844874382019
LOSS: 1.3358078002929688
LOSS: 1.3358241319656372
LOSS: 1.3358509540557861
LOSS: 1.335833191871643
LOSS: 1.3358540534973145
LOSS: 1.3358100652694702
LOSS: 1.3957171440124512
LOSS: 1.3900203704833984
LOSS: 1.3847575187683105
LOSS: 1.3799482583999634
LOSS: 1.375630497932434
LOSS: 1.3718409538269043
LOSS: 1.3685858249664307
LOSS: 1.3659135103225708
LOSS: 1.3637617826461792
LOSS: 1.36212158203125
LOSS: 1.3608529567718506
LOSS: 1.3598767518997192
LOSS: 1.3590476512908936
LOSS: 1.3582966327667236
LOSS: 1.3573561906814575
LOSS: 1.3563166856765747
LOSS: 1.355111002922058
LOSS: 1.3537616729736328
LOSS: 1.352224588394165
LOSS: 1.3506282567977905
LOSS: 1.3488738536834717
LOSS: 1.3471137285232544
LOSS: 1.3452969789505005
LOSS: 1.3435159921646118
LOSS: 1.3417425155639648
LOSS: 1.3400628566741943
LOSS: 1.3384088277816772
LOSS: 1.3368546962738037
LOSS: 1.335335373878479
LOSS: 1.3339215517044067
LOSS: 1.3325767517089844
LOSS: 1.3312500715255737
LOSS: 1.3299896717071533
LOSS: 1.3287490606307983
LOSS: 1.3275201320648193
LOSS: 1.326330542564392
LOSS: 1.325173258781433
LOSS: 1.3240137100219727
LOSS: 1.3228538036346436
LOSS: 1.321717619895935
LOSS: 1.3206309080123901
LOSS: 1.3195661306381226
LOSS: 1.3186084032058716
LOSS: 1.3176356554031372
LOSS: 1.3167792558670044
LOSS: 1.3159902095794678
LOSS: 1.3152625560760498
LOSS: 1.3146055936813354
LOSS: 1.3140313625335693
LOSS: 1.3135367631912231
LOSS: 1.3131341934204102
LOSS: 1.3127813339233398
LOSS: 1.3124390840530396
LOSS: 1.3121891021728516
LOSS: 1.3120111227035522
LOSS: 1.3118089437484741
LOSS: 1.3116816282272339
LOSS: 1.3115975856781006
LOSS: 1.3114936351776123
LOSS: 1.3114726543426514
LOSS: 1.311481237411499
LOSS: 1.311518907546997
LOSS: 1.3114949464797974
LOSS: 1.3116008043289185
LOSS: 1.3115595579147339
LOSS: 1.311580777168274
LOSS: 1.311644196510315
LOSS: 1.3116827011108398
LOSS: 1.3117035627365112
LOSS: 1.3117363452911377
LOSS: 1.3117308616638184
LOSS: 1.3117414712905884
LOSS: 1.3117601871490479
LOSS: 1.311732292175293
LOSS: 1.311719536781311
LOSS: 1.31174635887146
LOSS: 1.3116610050201416
LOSS: 1.311687707901001
LOSS: 1.311645269393921
LOSS: 1.3116142749786377
LOSS: 1.3116750717163086
LOSS: 1.3115967512130737
LOSS: 1.3115521669387817
LOSS: 1.3115705251693726
LOSS: 1.3115483522415161
LOSS: 1.3115261793136597
LOSS: 1.3115609884262085
LOSS: 1.3115020990371704
LOSS: 1.3114700317382812
LOSS: 1.3114731311798096
LOSS: 1.311488389968872
LOSS: 1.3115193843841553
LOSS: 1.3114831447601318
LOSS: 1.3115050792694092
LOSS: 1.311472773551941
LOSS: 1.3114805221557617
LOSS: 1.3114733695983887
LOSS: 1.3114856481552124
LOSS: 1.311497688293457
LOSS: 1.3115547895431519
LOSS: 1.4077287912368774
LOSS: 1.4022928476333618
LOSS: 1.3972852230072021
LOSS: 1.3927453756332397
LOSS: 1.3887227773666382
LOSS: 1.385217547416687
LOSS: 1.3822938203811646
LOSS: 1.3799426555633545
LOSS: 1.3781554698944092
LOSS: 1.376841425895691
LOSS: 1.3759610652923584
LOSS: 1.3754029273986816
LOSS: 1.3749722242355347
LOSS: 1.3745763301849365
LOSS: 1.3741233348846436
LOSS: 1.3736051321029663
LOSS: 1.3727246522903442
LOSS: 1.371729850769043
LOSS: 1.370606541633606
LOSS: 1.3693161010742188
LOSS: 1.3679680824279785
LOSS: 1.3665298223495483
LOSS: 1.3651167154312134
LOSS: 1.3636665344238281
LOSS: 1.3622939586639404
LOSS: 1.3609377145767212
LOSS: 1.3596328496932983
LOSS: 1.3583894968032837
LOSS: 1.3572217226028442
LOSS: 1.3561115264892578
LOSS: 1.3550350666046143
LOSS: 1.3539845943450928
LOSS: 1.352927327156067
LOSS: 1.3518993854522705
LOSS: 1.350870966911316
LOSS: 1.349855661392212
LOSS: 1.3488008975982666
LOSS: 1.347751498222351
LOSS: 1.3466694355010986
LOSS: 1.3456357717514038
LOSS: 1.3446403741836548
LOSS: 1.3435839414596558
LOSS: 1.3425958156585693
LOSS: 1.3416672945022583
LOSS: 1.34079110622406
LOSS: 1.3399184942245483
LOSS: 1.3391320705413818
LOSS: 1.3384202718734741
LOSS: 1.3377327919006348
LOSS: 1.337105393409729
LOSS: 1.3365470170974731
LOSS: 1.3360270261764526
LOSS: 1.3355672359466553
LOSS: 1.3351585865020752
LOSS: 1.3348768949508667
LOSS: 1.3344820737838745
LOSS: 1.3342145681381226
LOSS: 1.3339823484420776
LOSS: 1.3337422609329224
LOSS: 1.3335516452789307
LOSS: 1.333424687385559
LOSS: 1.333313226699829
LOSS: 1.3332664966583252
LOSS: 1.3332493305206299
LOSS: 1.3332664966583252
LOSS: 1.3332316875457764
LOSS: 1.3332092761993408
LOSS: 1.3332411050796509
LOSS: 1.3332350254058838
LOSS: 1.3332414627075195
LOSS: 1.3332552909851074
LOSS: 1.3332918882369995
LOSS: 1.3332945108413696
LOSS: 1.3333184719085693
LOSS: 1.3333317041397095
LOSS: 1.333321213722229
LOSS: 1.3333133459091187
LOSS: 1.3333216905593872
LOSS: 1.3333109617233276
LOSS: 1.3333083391189575
LOSS: 1.3332960605621338
LOSS: 1.3332828283309937
LOSS: 1.3332493305206299
LOSS: 1.3332276344299316
LOSS: 1.3332136869430542
LOSS: 1.3332040309906006
LOSS: 1.3331706523895264
LOSS: 1.333189606666565
LOSS: 1.3331342935562134
LOSS: 1.3331080675125122
LOSS: 1.3331087827682495
LOSS: 1.3331235647201538
LOSS: 1.333119511604309
LOSS: 1.3331021070480347
LOSS: 1.333081603050232
LOSS: 1.3330976963043213
LOSS: 1.333067536354065
LOSS: 1.3330496549606323
LOSS: 1.3330596685409546
LOSS: 1.3330997228622437
LOSS: 1.4021503925323486
LOSS: 1.396639347076416
LOSS: 1.391543984413147
LOSS: 1.3869266510009766
LOSS: 1.3827935457229614
LOSS: 1.3792113065719604
LOSS: 1.3762041330337524
LOSS: 1.373746633529663
LOSS: 1.3718111515045166
LOSS: 1.37037992477417
LOSS: 1.3694027662277222
LOSS: 1.3686590194702148
LOSS: 1.3681069612503052
LOSS: 1.3674967288970947
LOSS: 1.3668628931045532
LOSS: 1.3660634756088257
LOSS: 1.365094542503357
LOSS: 1.3639404773712158
LOSS: 1.3626353740692139
LOSS: 1.3611997365951538
LOSS: 1.3597298860549927
LOSS: 1.358128309249878
LOSS: 1.3565484285354614
LOSS: 1.3549814224243164
LOSS: 1.3534955978393555
LOSS: 1.352012276649475
LOSS: 1.3505616188049316
LOSS: 1.3492554426193237
LOSS: 1.3479584455490112
LOSS: 1.3467401266098022
LOSS: 1.3455686569213867
LOSS: 1.3444323539733887
LOSS: 1.3433725833892822
LOSS: 1.342286229133606
LOSS: 1.3412286043167114
LOSS: 1.3401966094970703
LOSS: 1.3391220569610596
LOSS: 1.3381121158599854
LOSS: 1.337101936340332
LOSS: 1.3360567092895508
LOSS: 1.3351330757141113
LOSS: 1.3341833353042603
LOSS: 1.333260178565979
LOSS: 1.332445502281189
LOSS: 1.3316519260406494
LOSS: 1.3309403657913208
LOSS: 1.3303179740905762
LOSS: 1.3297027349472046
LOSS: 1.3292133808135986
LOSS: 1.3286974430084229
LOSS: 1.3283183574676514
LOSS: 1.3279635906219482
LOSS: 1.3276593685150146
LOSS: 1.327492356300354
LOSS: 1.327216625213623
LOSS: 1.3270986080169678
LOSS: 1.3269505500793457
LOSS: 1.3269089460372925
LOSS: 1.3267693519592285
LOSS: 1.3267356157302856
LOSS: 1.3267325162887573
LOSS: 1.3266855478286743
LOSS: 1.3267083168029785
LOSS: 1.326738953590393
LOSS: 1.3267650604248047
LOSS: 1.3268311023712158
LOSS: 1.3268717527389526
LOSS: 1.3268932104110718
LOSS: 1.326887607574463
LOSS: 1.3268872499465942
LOSS: 1.3268959522247314
LOSS: 1.3269622325897217
LOSS: 1.3268705606460571
LOSS: 1.326856255531311
LOSS: 1.3269480466842651
LOSS: 1.3268626928329468
LOSS: 1.3268340826034546
LOSS: 1.3268084526062012
LOSS: 1.3267691135406494
LOSS: 1.326838493347168
LOSS: 1.3267849683761597
LOSS: 1.3267453908920288
LOSS: 1.3266987800598145
LOSS: 1.3266717195510864
LOSS: 1.326734185218811
LOSS: 1.3266773223876953
LOSS: 1.326674461364746
LOSS: 1.3266404867172241
LOSS: 1.3266233205795288
LOSS: 1.3265972137451172
LOSS: 1.326621413230896
LOSS: 1.3266195058822632
LOSS: 1.3266551494598389
LOSS: 1.326612949371338
LOSS: 1.3266620635986328
LOSS: 1.3265960216522217
LOSS: 1.3266385793685913
LOSS: 1.3266106843948364
LOSS: 1.326660394668579
LOSS: 1.3266412019729614
LOSS: 1.343083143234253
LOSS: 1.326780080795288
LOSS: 1.3100228309631348
LOSS: 1.2928398847579956
LOSS: 1.2752338647842407
LOSS: 1.257226824760437
LOSS: 1.2388797998428345
LOSS: 1.2202779054641724
LOSS: 1.3869266510009766
LOSS: 1.3827935457229614
LOSS: 1.3792113065719604
LOSS: 1.3762041330337524
LOSS: 1.373746633529663
LOSS: 1.3718111515045166
LOSS: 1.37037992477417
LOSS: 1.3694027662277222
LOSS: 1.3686590194702148
LOSS: 1.3681069612503052
LOSS: 1.3674967288970947
LOSS: 1.3668628931045532
LOSS: 1.3660634756088257
LOSS: 1.365094542503357
LOSS: 1.3639404773712158
LOSS: 1.3626353740692139
LOSS: 1.3611997365951538
LOSS: 1.3597298860549927
LOSS: 1.358128309249878
LOSS: 1.3565484285354614
LOSS: 1.3549814224243164
LOSS: 1.3534955978393555
LOSS: 1.352012276649475
LOSS: 1.3505616188049316
LOSS: 1.3492554426193237
LOSS: 1.3479584455490112
LOSS: 1.3467401266098022
LOSS: 1.3455686569213867
LOSS: 1.3444323539733887
LOSS: 1.3433725833892822
LOSS: 1.342286229133606
LOSS: 1.3412286043167114
LOSS: 1.3401966094970703
LOSS: 1.3391220569610596
LOSS: 1.3381121158599854
LOSS: 1.337101936340332
LOSS: 1.3360567092895508
LOSS: 1.3351330757141113
LOSS: 1.3341833353042603
LOSS: 1.333260178565979
LOSS: 1.332445502281189
LOSS: 1.3316519260406494
LOSS: 1.3309403657913208
LOSS: 1.3303179740905762
LOSS: 1.3297027349472046
LOSS: 1.3292133808135986
LOSS: 1.3286974430084229
LOSS: 1.3283183574676514
LOSS: 1.3279635906219482
LOSS: 1.3276593685150146
LOSS: 1.327492356300354
LOSS: 1.327216625213623
LOSS: 1.3270986080169678
LOSS: 1.3269505500793457
LOSS: 1.3269089460372925
LOSS: 1.3267693519592285
LOSS: 1.3267356157302856
LOSS: 1.3267325162887573
LOSS: 1.3266855478286743
LOSS: 1.3267083168029785
LOSS: 1.326738953590393
LOSS: 1.3267650604248047
LOSS: 1.3268311023712158
LOSS: 1.3268717527389526
LOSS: 1.3268932104110718
LOSS: 1.326887607574463
LOSS: 1.3268872499465942
LOSS: 1.3268959522247314
LOSS: 1.3269622325897217
LOSS: 1.3268705606460571
LOSS: 1.326856255531311
LOSS: 1.3269480466842651
LOSS: 1.3268626928329468
LOSS: 1.3268340826034546
LOSS: 1.3268084526062012
LOSS: 1.3267691135406494
LOSS: 1.326838493347168
LOSS: 1.3267849683761597
LOSS: 1.3267453908920288
LOSS: 1.3266987800598145
LOSS: 1.3266717195510864
LOSS: 1.326734185218811
LOSS: 1.3266773223876953
LOSS: 1.326674461364746
LOSS: 1.3266404867172241
LOSS: 1.3266233205795288
LOSS: 1.3265972137451172
LOSS: 1.326621413230896
LOSS: 1.3266195058822632
LOSS: 1.3266551494598389
LOSS: 1.326612949371338
LOSS: 1.3266620635986328
LOSS: 1.3265960216522217
LOSS: 1.3266385793685913
LOSS: 1.3266106843948364
LOSS: 1.326660394668579
LOSS: 1.3266412019729614
LOSS: 1.3985767364501953
LOSS: 1.3929029703140259
LOSS: 1.3876125812530518
LOSS: 1.3828105926513672
LOSS: 1.378493070602417
LOSS: 1.3747048377990723
LOSS: 1.3714815378189087
LOSS: 1.368788719177246
LOSS: 1.3666741847991943
LOSS: 1.3650672435760498
LOSS: 1.363860011100769
LOSS: 1.362979531288147
LOSS: 1.3622902631759644
LOSS: 1.361661672592163
LOSS: 1.3609609603881836
LOSS: 1.3601362705230713
LOSS: 1.3591293096542358
LOSS: 1.3579691648483276
LOSS: 1.356640338897705
LOSS: 1.3551563024520874
LOSS: 1.3535648584365845
LOSS: 1.3519306182861328
LOSS: 1.3502833843231201
LOSS: 1.348647117614746
LOSS: 1.3470067977905273
LOSS: 1.3454569578170776
LOSS: 1.3439226150512695
LOSS: 1.342464566230774
LOSS: 1.3411208391189575
LOSS: 1.3397834300994873
LOSS: 1.3385429382324219
LOSS: 1.3373621702194214
LOSS: 1.3362245559692383
LOSS: 1.3350738286972046
LOSS: 1.3339236974716187
LOSS: 1.3328362703323364
LOSS: 1.3317025899887085
LOSS: 1.330607533454895
LOSS: 1.3294843435287476
LOSS: 1.3284094333648682
LOSS: 1.3273286819458008
LOSS: 1.3262958526611328
LOSS: 1.3252955675125122
LOSS: 1.3243290185928345
LOSS: 1.3234232664108276
LOSS: 1.3226208686828613
LOSS: 1.3218491077423096
LOSS: 1.3211159706115723
LOSS: 1.3204652070999146
LOSS: 1.319909930229187
LOSS: 1.319401741027832
LOSS: 1.3189107179641724
LOSS: 1.3185261487960815
LOSS: 1.3181935548782349
LOSS: 1.3178037405014038
LOSS: 1.3175491094589233
LOSS: 1.3173424005508423
LOSS: 1.3171168565750122
LOSS: 1.316947340965271
LOSS: 1.31685471534729
LOSS: 1.3167047500610352
LOSS: 1.3166719675064087
LOSS: 1.3166614770889282
LOSS: 1.3165969848632812
LOSS: 1.316624641418457
LOSS: 1.3166353702545166
LOSS: 1.316624402999878
LOSS: 1.3166569471359253
LOSS: 1.3166697025299072
LOSS: 1.316762924194336
LOSS: 1.3167238235473633
LOSS: 1.3167216777801514
LOSS: 1.3167394399642944
LOSS: 1.3167561292648315
LOSS: 1.3167388439178467
LOSS: 1.3167388439178467
LOSS: 1.3167327642440796
LOSS: 1.3167426586151123
LOSS: 1.3167065382003784
LOSS: 1.3166851997375488
LOSS: 1.316705584526062
LOSS: 1.3166954517364502
LOSS: 1.3166611194610596
LOSS: 1.3166594505310059
LOSS: 1.3166640996932983
LOSS: 1.3165678977966309
LOSS: 1.3166455030441284
LOSS: 1.3165671825408936
LOSS: 1.3165507316589355
LOSS: 1.3166184425354004
LOSS: 1.316538691520691
LOSS: 1.3165056705474854
LOSS: 1.3165713548660278
LOSS: 1.3165370225906372
LOSS: 1.316509485244751
LOSS: 1.3165158033370972
LOSS: 1.3165361881256104
LOSS: 1.3164676427841187
LOSS: 1.3165146112442017
LOSS: 1.3165149688720703
LOSS: 1.350943684577942
LOSS: 1.335446834564209
LOSS: 1.3195103406906128
LOSS: 1.3031998872756958
LOSS: 1.28656804561615
LOSS: 1.2696573734283447
LOSS: 1.2525497674942017
LOSS: 1.235343098640442
LOSS: 1.2181265354156494
LOSS: 1.2009774446487427
LOSS: 1.1839669942855835
LOSS: 1.1671642065048218
LOSS: 1.1506434679031372
LOSS: 1.1344817876815796
LOSS: 1.1187522411346436
LOSS: 1.1035200357437134
LOSS: 1.0888359546661377
LOSS: 1.0747358798980713
LOSS: 1.0612406730651855
LOSS: 1.0483579635620117
LOSS: 1.036082148551941
LOSS: 1.0243970155715942
LOSS: 1.013274908065796
LOSS: 1.0026816129684448
LOSS: 0.9925786256790161
LOSS: 0.9829251170158386
LOSS: 0.973685085773468
LOSS: 0.9648277163505554
LOSS: 0.9563325047492981
LOSS: 0.9481907486915588
LOSS: 0.940402090549469
LOSS: 0.9329699873924255
LOSS: 0.9258962273597717
LOSS: 0.919180154800415
LOSS: 0.9128182530403137
LOSS: 0.90680330991745
LOSS: 0.9011252522468567
LOSS: 0.8957687616348267
LOSS: 0.8907129764556885
LOSS: 0.8859335780143738
LOSS: 0.881403923034668
LOSS: 0.8770973682403564
LOSS: 0.8729913234710693
LOSS: 0.8690614700317383
LOSS: 0.8652926683425903
LOSS: 0.8616706728935242
LOSS: 0.8581843972206116
LOSS: 0.8548274636268616
LOSS: 0.851595401763916
LOSS: 0.8484864234924316
LOSS: 0.8455005288124084
LOSS: 0.842637836933136
LOSS: 0.8398980498313904
LOSS: 0.8372812271118164
LOSS: 0.8347858786582947
LOSS: 0.83240807056427
LOSS: 0.8301411271095276
LOSS: 0.8279796838760376
LOSS: 0.8259167075157166
LOSS: 0.8239473700523376
LOSS: 0.8220681548118591
LOSS: 0.8202767968177795
LOSS: 0.8185736536979675
LOSS: 0.816958487033844
LOSS: 0.8154318332672119
LOSS: 0.8139926195144653
LOSS: 0.8126381635665894
LOSS: 0.8113653659820557
LOSS: 0.8101699352264404
LOSS: 0.8090469837188721
LOSS: 0.807992160320282
LOSS: 0.8070018887519836
LOSS: 0.8060696721076965
LOSS: 0.8051925897598267
LOSS: 0.8043668866157532
LOSS: 0.8035879135131836
LOSS: 0.8028532862663269
LOSS: 0.8021588921546936
LOSS: 0.8015030026435852
LOSS: 0.8008825182914734
LOSS: 0.800295352935791
LOSS: 0.7997404932975769
LOSS: 0.7992149591445923
LOSS: 0.7987175583839417
LOSS: 0.7982459664344788
LOSS: 0.7977979183197021
LOSS: 0.7973712086677551
LOSS: 0.7969640493392944
LOSS: 0.796574056148529
LOSS: 0.7961999177932739
LOSS: 0.7958402633666992
LOSS: 0.7954933047294617
LOSS: 0.795158326625824
LOSS: 0.7948337197303772
LOSS: 0.7945190072059631
LOSS: 0.7942132353782654
LOSS: 0.7939154505729675
LOSS: 0.7936246991157532
LOSS: 0.7933396697044373
LOSS: 0.7930604815483093
LOSS: 1.350943684577942
LOSS: 1.335446834564209
LOSS: 1.3195103406906128
LOSS: 1.3031998872756958
LOSS: 1.28656804561615
LOSS: 1.2696573734283447
LOSS: 1.2525497674942017
LOSS: 1.235343098640442
LOSS: 1.2181265354156494
LOSS: 1.2009774446487427
LOSS: 1.1839669942855835
LOSS: 1.1671642065048218
LOSS: 1.1506434679031372
LOSS: 1.1344817876815796
LOSS: 1.1187522411346436
LOSS: 1.1035200357437134
LOSS: 1.0888359546661377
LOSS: 1.0747358798980713
LOSS: 1.0612406730651855
LOSS: 1.0483579635620117
LOSS: 1.036082148551941
LOSS: 1.0243970155715942
LOSS: 1.013274908065796
LOSS: 1.0026816129684448
LOSS: 0.9925786256790161
LOSS: 0.9829251170158386
LOSS: 0.973685085773468
LOSS: 0.9648277163505554
LOSS: 0.9563325047492981
LOSS: 0.9481907486915588
LOSS: 0.940402090549469
LOSS: 0.9329699873924255
LOSS: 0.9258962273597717
LOSS: 1.2015219926834106
LOSS: 1.1827367544174194
LOSS: 1.1640641689300537
LOSS: 1.1456477642059326
LOSS: 1.1276181936264038
LOSS: 1.1100887060165405
LOSS: 1.0931529998779297
LOSS: 1.0768909454345703
LOSS: 1.0613676309585571
LOSS: 1.0466333627700806
LOSS: 1.032721996307373
LOSS: 1.0196510553359985
LOSS: 1.0074208974838257
LOSS: 0.99601811170578
LOSS: 0.9854169487953186
LOSS: 0.9755792021751404
LOSS: 0.9664583802223206
LOSS: 0.9580025672912598
LOSS: 0.9501582980155945
LOSS: 0.942872166633606
LOSS: 0.9360941648483276
LOSS: 0.9297767877578735
LOSS: 0.9238763451576233
LOSS: 0.918351411819458
LOSS: 0.9131664037704468
LOSS: 0.908290445804596
LOSS: 0.9037031531333923
LOSS: 0.8993895053863525
LOSS: 0.8953394293785095
LOSS: 0.8915455937385559
LOSS: 0.8879966735839844
LOSS: 0.8846839666366577
LOSS: 0.8815969824790955
LOSS: 0.8787262439727783
LOSS: 0.876060426235199
LOSS: 0.8735859394073486
LOSS: 0.8712922930717468
LOSS: 0.869160532951355
LOSS: 0.8671749234199524
LOSS: 0.8653197288513184
LOSS: 0.8635784387588501
LOSS: 0.8619388937950134
LOSS: 0.8603904247283936
LOSS: 0.8589258790016174
LOSS: 0.8575403690338135
LOSS: 0.8562328219413757
LOSS: 0.8550031185150146
LOSS: 0.8538509607315063
LOSS: 0.852776288986206
LOSS: 0.8517772555351257
LOSS: 0.8508508205413818
LOSS: 0.8499922156333923
LOSS: 0.8491953015327454
LOSS: 0.8484530448913574
LOSS: 0.8477581143379211
LOSS: 0.8471014499664307
LOSS: 0.8464784026145935
LOSS: 0.8458817005157471
LOSS: 0.8453079462051392
LOSS: 0.8447540998458862
LOSS: 0.8442193269729614
LOSS: 0.8437031507492065
LOSS: 0.8432057499885559
LOSS: 0.8427273035049438
LOSS: 0.8422686457633972
LOSS: 0.8418283462524414
LOSS: 0.8414056897163391
LOSS: 0.8409993648529053
LOSS: 0.8406081199645996
LOSS: 0.8402302861213684
LOSS: 0.839863657951355
LOSS: 0.8395075798034668
LOSS: 0.8391607999801636
LOSS: 0.8388229012489319
LOSS: 0.8384923934936523
LOSS: 0.8381701111793518
LOSS: 0.8378549218177795
LOSS: 0.8375459313392639
LOSS: 0.8372436761856079
LOSS: 0.8369477987289429
LOSS: 0.8366571664810181
LOSS: 0.8363723158836365
LOSS: 0.8360927700996399
LOSS: 0.8358191251754761
LOSS: 0.83555006980896
LOSS: 0.8352873921394348
LOSS: 0.8350304961204529
LOSS: 0.8347796201705933
LOSS: 0.8345351815223694
LOSS: 0.8342967629432678
LOSS: 0.8340641856193542
LOSS: 0.8338380455970764
LOSS: 1.3463788032531738
LOSS: 1.3299880027770996
LOSS: 1.3131529092788696
LOSS: 1.29599130153656
LOSS: 1.2785632610321045
LOSS: 1.260917067527771
LOSS: 1.243055820465088
LOSS: 1.2249808311462402
LOSS: 1.2067344188690186
LOSS: 1.1883939504623413
LOSS: 1.1700578927993774
LOSS: 1.1518378257751465
LOSS: 1.1338474750518799
LOSS: 1.1161909103393555
LOSS: 1.098961591720581
LOSS: 1.082244873046875
LOSS: 1.066117286682129
LOSS: 1.0506443977355957
LOSS: 1.0358766317367554
LOSS: 1.0218473672866821
LOSS: 1.008571743965149
LOSS: 0.9960487484931946
LOSS: 0.9842633008956909
LOSS: 0.9731897115707397
LOSS: 0.9627935290336609
LOSS: 0.9530345797538757
LOSS: 0.9438700079917908
LOSS: 0.9352566003799438
LOSS: 0.927153468132019
LOSS: 0.9195231795310974
LOSS: 0.912329912185669
LOSS: 0.9055407047271729
LOSS: 0.8991261124610901
LOSS: 0.8930611610412598
LOSS: 0.8873279690742493
LOSS: 0.8819133639335632
LOSS: 0.8768103122711182
LOSS: 0.8720150589942932
LOSS: 0.8675256967544556
LOSS: 0.8633372187614441
LOSS: 0.8594403862953186
LOSS: 0.8558239936828613
LOSS: 0.8524715900421143
LOSS: 0.849361777305603
LOSS: 0.8464710712432861
LOSS: 0.843778133392334
LOSS: 0.841264009475708
LOSS: 0.8389143347740173
LOSS: 0.8367197513580322
LOSS: 0.8346739411354065
LOSS: 0.8327720761299133
LOSS: 0.831008791923523
LOSS: 0.8293792009353638
LOSS: 0.8278775215148926
LOSS: 0.8264927268028259
LOSS: 0.8252160549163818
LOSS: 0.8240340352058411
LOSS: 0.822934091091156
LOSS: 0.8219053745269775
LOSS: 0.820936381816864
LOSS: 0.8200187087059021
LOSS: 0.8191463351249695
LOSS: 0.8183137774467468
LOSS: 0.8175179958343506
LOSS: 0.8167579174041748
LOSS: 0.8160316348075867
LOSS: 0.8153391480445862
LOSS: 0.8146793246269226
LOSS: 0.8140522241592407
LOSS: 0.8134565353393555
LOSS: 0.8128878474235535
LOSS: 0.8123470544815063
LOSS: 0.8118302226066589
LOSS: 0.8113362193107605
LOSS: 0.8108636736869812
LOSS: 0.8104079961776733
LOSS: 0.8099660873413086
LOSS: 0.8095327615737915
LOSS: 0.8091044425964355
LOSS: 0.8086796402931213
LOSS: 0.8082608580589294
LOSS: 0.8078508377075195
LOSS: 0.8074522614479065
LOSS: 0.8070658445358276
LOSS: 0.8066933751106262
LOSS: 0.8063333034515381
LOSS: 0.8059855103492737
LOSS: 0.8056485652923584
LOSS: 0.8053230047225952
LOSS: 0.8050079941749573
LOSS: 0.8047031760215759
LOSS: 0.804408609867096
LOSS: 0.8041236996650696
LOSS: 0.8038486242294312
LOSS: 0.8035832047462463
LOSS: 0.8033259510993958
LOSS: 0.8030766248703003
LOSS: 0.8028360605239868
LOSS: 0.8026029467582703
LOSS: 0.8023777008056641
LOSS: 1.343083143234253
LOSS: 1.326780080795288
LOSS: 1.3100228309631348
LOSS: 1.2928398847579956
LOSS: 1.2752338647842407
LOSS: 1.257226824760437
LOSS: 1.2388797998428345
LOSS: 1.2202779054641724
LOSS: 1.2015219926834106
LOSS: 1.1827367544174194
LOSS: 1.1640641689300537
LOSS: 1.1456477642059326
LOSS: 1.1276181936264038
LOSS: 1.1100887060165405
LOSS: 1.0931529998779297
LOSS: 1.0768909454345703
LOSS: 1.0613676309585571
LOSS: 1.0466333627700806
LOSS: 1.032721996307373
LOSS: 1.0196510553359985
LOSS: 1.0074208974838257
LOSS: 0.99601811170578
LOSS: 0.9854169487953186
LOSS: 0.9755792021751404
LOSS: 0.9664583802223206
LOSS: 0.9580025672912598
LOSS: 0.9501582980155945
LOSS: 0.942872166633606
LOSS: 0.9360941648483276
LOSS: 0.9297767877578735
LOSS: 0.9238763451576233
LOSS: 0.918351411819458
LOSS: 0.9131664037704468
LOSS: 0.908290445804596
LOSS: 0.9037031531333923
LOSS: 0.8993895053863525
LOSS: 0.8953394293785095
LOSS: 0.8915455937385559
LOSS: 0.8879966735839844
LOSS: 0.8846839666366577
LOSS: 0.8815969824790955
LOSS: 0.8787262439727783
LOSS: 0.876060426235199
LOSS: 0.8735859394073486
LOSS: 0.8712922930717468
LOSS: 0.869160532951355
LOSS: 0.8671749234199524
LOSS: 0.8653197288513184
LOSS: 0.8635784387588501
LOSS: 0.8619388937950134
LOSS: 0.8603904247283936
LOSS: 0.8589258790016174
LOSS: 0.8575403690338135
LOSS: 0.8562328219413757
LOSS: 0.8550031185150146
LOSS: 0.8538509607315063
LOSS: 0.852776288986206
LOSS: 0.8517772555351257
LOSS: 0.8508508205413818
LOSS: 0.8499922156333923
LOSS: 0.8491953015327454
LOSS: 0.8484530448913574
LOSS: 0.8477581143379211
LOSS: 0.8471014499664307
LOSS: 0.8464784026145935
LOSS: 0.8458817005157471
LOSS: 0.8453079462051392
LOSS: 0.8447540998458862
LOSS: 0.8442193269729614
LOSS: 0.8437031507492065
LOSS: 0.8432057499885559
LOSS: 0.8427273035049438
LOSS: 0.8422686457633972
LOSS: 0.8418283462524414
LOSS: 0.8414056897163391
LOSS: 0.8409993648529053
LOSS: 0.8406081199645996
LOSS: 0.8402302861213684
LOSS: 0.839863657951355
LOSS: 0.8395075798034668
LOSS: 0.8391607999801636
LOSS: 0.8388229012489319
LOSS: 0.8384923934936523
LOSS: 0.8381701111793518
LOSS: 0.8378549218177795
LOSS: 0.8375459313392639
LOSS: 0.8372436761856079
LOSS: 0.8369477987289429
LOSS: 0.8366571664810181
LOSS: 0.8363723158836365
LOSS: 0.8360927700996399
LOSS: 0.8358191251754761
LOSS: 0.83555006980896
LOSS: 0.8352873921394348
LOSS: 0.8350304961204529
LOSS: 0.8347796201705933
LOSS: 0.8345351815223694
LOSS: 0.8342967629432678
LOSS: 0.8340641856193542
LOSS: 0.8338380455970764
LOSS: 1.3463788032531738
LOSS: 1.3299880027770996
LOSS: 1.3131529092788696
LOSS: 1.29599130153656
LOSS: 1.2785632610321045
LOSS: 1.260917067527771
LOSS: 1.243055820465088
LOSS: 1.2249808311462402
LOSS: 1.2067344188690186
LOSS: 1.1883939504623413
LOSS: 1.1700578927993774
LOSS: 1.1518378257751465
LOSS: 1.1338474750518799
LOSS: 1.1161909103393555
LOSS: 1.098961591720581
LOSS: 1.082244873046875
LOSS: 1.066117286682129
LOSS: 1.0506443977355957
LOSS: 1.0358766317367554
LOSS: 1.0218473672866821
LOSS: 1.008571743965149
LOSS: 0.9960487484931946
LOSS: 0.9842633008956909
LOSS: 0.9731897115707397
LOSS: 0.9627935290336609
LOSS: 0.9530345797538757
LOSS: 0.9438700079917908
LOSS: 0.9352566003799438
LOSS: 0.927153468132019
LOSS: 0.9195231795310974
LOSS: 0.912329912185669
LOSS: 0.9055407047271729
LOSS: 0.8991261124610901
LOSS: 0.8930611610412598
LOSS: 0.8873279690742493
LOSS: 0.8819133639335632
LOSS: 0.8768103122711182
LOSS: 1.3718409538269043
LOSS: 1.3685858249664307
LOSS: 1.3659135103225708
LOSS: 1.3637617826461792
LOSS: 1.36212158203125
LOSS: 1.3608529567718506
LOSS: 1.3598767518997192
LOSS: 1.3590476512908936
LOSS: 1.3582966327667236
LOSS: 1.3573561906814575
LOSS: 1.3563166856765747
LOSS: 1.355111002922058
LOSS: 1.3537616729736328
LOSS: 1.352224588394165
LOSS: 1.3506282567977905
LOSS: 1.3488738536834717
LOSS: 1.3471137285232544
LOSS: 1.3452969789505005
LOSS: 1.3435159921646118
LOSS: 1.3417425155639648
LOSS: 1.3400628566741943
LOSS: 1.3384088277816772
LOSS: 1.3368546962738037
LOSS: 1.335335373878479
LOSS: 1.3339215517044067
LOSS: 1.3325767517089844
LOSS: 1.3312500715255737
LOSS: 1.3299896717071533
LOSS: 1.3287490606307983
LOSS: 1.3275201320648193
LOSS: 1.326330542564392
LOSS: 1.325173258781433
LOSS: 1.3240137100219727
LOSS: 1.3228538036346436
LOSS: 1.321717619895935
LOSS: 1.3206309080123901
LOSS: 1.3195661306381226
LOSS: 1.3186084032058716
LOSS: 1.3176356554031372
LOSS: 1.3167792558670044
LOSS: 1.3159902095794678
LOSS: 1.3152625560760498
LOSS: 1.3146055936813354
LOSS: 1.3140313625335693
LOSS: 1.3135367631912231
LOSS: 1.3131341934204102
LOSS: 1.3127813339233398
LOSS: 1.3124390840530396
LOSS: 1.3121891021728516
LOSS: 1.3120111227035522
LOSS: 1.3118089437484741
LOSS: 1.3116816282272339
LOSS: 1.3115975856781006
LOSS: 1.3114936351776123
LOSS: 1.3114726543426514
LOSS: 1.311481237411499
LOSS: 1.311518907546997
LOSS: 1.3114949464797974
LOSS: 1.3116008043289185
LOSS: 1.3115595579147339
LOSS: 1.311580777168274
LOSS: 1.311644196510315
LOSS: 1.3116827011108398
LOSS: 1.3117035627365112
LOSS: 1.3117363452911377
LOSS: 1.3117308616638184
LOSS: 1.3117414712905884
LOSS: 1.3117601871490479
LOSS: 1.311732292175293
LOSS: 1.311719536781311
LOSS: 1.31174635887146
LOSS: 1.3116610050201416
LOSS: 1.311687707901001
LOSS: 1.311645269393921
LOSS: 1.3116142749786377
LOSS: 1.3116750717163086
LOSS: 1.3115967512130737
LOSS: 1.3115521669387817
LOSS: 1.3115705251693726
LOSS: 1.3115483522415161
LOSS: 1.3115261793136597
LOSS: 1.3115609884262085
LOSS: 1.3115020990371704
LOSS: 1.3114700317382812
LOSS: 1.3114731311798096
LOSS: 1.311488389968872
LOSS: 1.3115193843841553
LOSS: 1.3114831447601318
LOSS: 1.3115050792694092
LOSS: 1.311472773551941
LOSS: 1.3114805221557617
LOSS: 1.3114733695983887
LOSS: 1.3114856481552124
LOSS: 1.311497688293457
LOSS: 1.3115547895431519
LOSS: 1.3578050136566162
LOSS: 1.3412730693817139
LOSS: 1.3241482973098755
LOSS: 1.3065506219863892
LOSS: 1.288591742515564
LOSS: 1.2703349590301514
LOSS: 1.2518179416656494
LOSS: 1.2331068515777588
LOSS: 1.214285969734192
LOSS: 1.1954458951950073
LOSS: 1.1766892671585083
LOSS: 1.1581401824951172
LOSS: 1.1399365663528442
LOSS: 1.1222190856933594
LOSS: 1.1051229238510132
LOSS: 1.0887658596038818
LOSS: 1.0732381343841553
LOSS: 1.0586010217666626
LOSS: 1.0448840856552124
LOSS: 1.0320852994918823
LOSS: 1.0201767683029175
LOSS: 1.0091135501861572
LOSS: 0.9988359808921814
LOSS: 0.989277184009552
LOSS: 0.9803652763366699
LOSS: 0.972027599811554
LOSS: 0.9641937613487244
LOSS: 0.9567972421646118
LOSS: 0.9497771263122559
LOSS: 0.9430829286575317
LOSS: 0.9366740584373474
LOSS: 0.9305221438407898
LOSS: 0.9246107935905457
LOSS: 0.9189328551292419
LOSS: 0.9134889841079712
LOSS: 0.9082821011543274
LOSS: 0.9033175706863403
LOSS: 0.8985966444015503
LOSS: 0.8941186666488647
LOSS: 0.889877438545227
LOSS: 0.8858655691146851
LOSS: 0.8820686936378479
LOSS: 0.878472089767456
LOSS: 0.8750574588775635
LOSS: 0.8718076944351196
LOSS: 0.8687063455581665
LOSS: 0.8657397627830505
LOSS: 0.8628985285758972
LOSS: 0.8601750731468201
LOSS: 0.8575655817985535
LOSS: 0.8550698161125183
LOSS: 0.8526880741119385
LOSS: 0.850419819355011
LOSS: 0.8482676148414612
LOSS: 0.8462294936180115
LOSS: 0.8443049192428589
LOSS: 0.8424903750419617
LOSS: 0.840781569480896
LOSS: 0.8391740322113037
LOSS: 0.8376612663269043
LOSS: 0.8362388610839844
LOSS: 0.8349002003669739
LOSS: 0.8336429595947266
LOSS: 0.8324620723724365
LOSS: 0.8313547372817993
LOSS: 0.8303185701370239
LOSS: 0.8293497562408447
LOSS: 0.8284446001052856
LOSS: 0.82759690284729
LOSS: 0.8268016576766968
LOSS: 0.826052188873291
LOSS: 0.8253417015075684
LOSS: 0.8246628046035767
LOSS: 0.8240113258361816
LOSS: 0.8233832716941833
LOSS: 0.8227764964103699
LOSS: 0.822189211845398
LOSS: 0.8216230273246765
LOSS: 0.8210779428482056
LOSS: 0.8205538988113403
LOSS: 0.8200517892837524
LOSS: 0.8195701837539673
LOSS: 0.8191083073616028
LOSS: 0.818664014339447
LOSS: 0.8182361125946045
LOSS: 0.8178220987319946
LOSS: 0.8174206018447876
LOSS: 0.8170299530029297
LOSS: 0.8166495561599731
LOSS: 0.8162780404090881
LOSS: 0.8159149289131165
LOSS: 0.8155602216720581
LOSS: 0.8152132034301758
LOSS: 0.8148747086524963
LOSS: 0.8145447969436646
LOSS: 0.8142232298851013
LOSS: 0.813910186290741
LOSS: 0.8136060237884521
LOSS: 0.8133102655410767
LOSS: 0.813022792339325
LOSS: 1.3578050136566162
LOSS: 1.3412730693817139
LOSS: 1.3241482973098755
LOSS: 1.3065506219863892
LOSS: 1.288591742515564
LOSS: 1.2703349590301514
LOSS: 1.2518179416656494
LOSS: 1.2331068515777588
LOSS: 1.214285969734192
LOSS: 1.1954458951950073
LOSS: 1.1766892671585083
LOSS: 1.1581401824951172
LOSS: 1.1399365663528442
LOSS: 1.1222190856933594
LOSS: 1.1051229238510132
LOSS: 1.0887658596038818
LOSS: 1.0732381343841553
LOSS: 1.0586010217666626
LOSS: 1.0448840856552124
LOSS: 1.0320852994918823
LOSS: 1.0201767683029175
LOSS: 1.0091135501861572
LOSS: 0.9988359808921814
LOSS: 0.989277184009552
LOSS: 0.9803652763366699
LOSS: 0.972027599811554
LOSS: 0.9641937613487244
LOSS: 0.9567972421646118
LOSS: 0.9497771263122559
LOSS: 0.9430829286575317
LOSS: 0.9366740584373474
LOSS: 0.9305221438407898
LOSS: 0.9246107935905457
LOSS: 0.9189328551292419
LOSS: 0.9134889841079712
LOSS: 0.9082821011543274
LOSS: 0.9033175706863403
LOSS: 0.8985966444015503
LOSS: 0.8941186666488647
LOSS: 0.889877438545227
LOSS: 0.8858655691146851
LOSS: 0.8820686936378479
LOSS: 0.878472089767456
LOSS: 0.8750574588775635
LOSS: 0.8718076944351196
LOSS: 0.8687063455581665
LOSS: 0.8657397627830505
LOSS: 0.8628985285758972
LOSS: 0.8601750731468201
LOSS: 0.8575655817985535
LOSS: 0.8550698161125183
LOSS: 0.8526880741119385
LOSS: 0.850419819355011
LOSS: 0.8482676148414612
LOSS: 0.8462294936180115
LOSS: 0.8443049192428589
LOSS: 0.8424903750419617
LOSS: 0.840781569480896
LOSS: 0.8391740322113037
LOSS: 0.8376612663269043
LOSS: 0.8362388610839844
LOSS: 0.8349002003669739
LOSS: 0.8336429595947266
LOSS: 0.8324620723724365
LOSS: 0.8313547372817993
LOSS: 0.8303185701370239
LOSS: 0.8293497562408447
LOSS: 0.8284446001052856
LOSS: 0.82759690284729
LOSS: 0.8268016576766968
LOSS: 0.826052188873291
LOSS: 0.8253417015075684
LOSS: 0.8246628046035767
LOSS: 0.8240113258361816
LOSS: 0.8233832716941833
LOSS: 0.8227764964103699
LOSS: 0.822189211845398
LOSS: 0.8216230273246765
LOSS: 0.8210779428482056
LOSS: 0.8205538988113403
LOSS: 0.8200517892837524
LOSS: 0.8195701837539673
LOSS: 0.8191083073616028
LOSS: 0.818664014339447
LOSS: 0.8182361125946045
LOSS: 0.8178220987319946
LOSS: 0.8174206018447876
LOSS: 0.8170299530029297
LOSS: 0.8166495561599731
LOSS: 0.8162780404090881
LOSS: 0.8159149289131165
LOSS: 0.8155602216720581
LOSS: 0.8152132034301758
LOSS: 0.8148747086524963
LOSS: 0.8145447969436646
LOSS: 0.8142232298851013
LOSS: 0.813910186290741
LOSS: 0.8136060237884521
LOSS: 0.8133102655410767
LOSS: 0.813022792339325
LOSS: 1.3471133708953857
LOSS: 1.330703854560852
LOSS: 1.3138689994812012
LOSS: 1.296735405921936
LOSS: 1.2793956995010376
LOSS: 1.2619342803955078
LOSS: 1.244388222694397
LOSS: 1.2267637252807617
LOSS: 1.2090774774551392
LOSS: 1.1913800239562988
LOSS: 1.1737561225891113
LOSS: 1.1563160419464111
LOSS: 1.1391801834106445
LOSS: 1.1224671602249146
LOSS: 1.1062837839126587
LOSS: 1.0907212495803833
LOSS: 1.0758475065231323
LOSS: 1.0617061853408813
LOSS: 1.048319697380066
LOSS: 1.0356911420822144
LOSS: 1.023803949356079
LOSS: 1.0126265287399292
LOSS: 1.0021167993545532
LOSS: 0.9922255277633667
LOSS: 0.9828981161117554
LOSS: 0.9740809798240662
LOSS: 0.9657270908355713
LOSS: 0.9577977657318115
LOSS: 0.9502606987953186
LOSS: 0.9430943131446838
LOSS: 0.936284065246582
LOSS: 0.9298242330551147
LOSS: 0.9237107038497925
LOSS: 0.9179408550262451
LOSS: 0.9125117063522339LOSS: 1.3830900192260742
LOSS: 1.3794803619384766
LOSS: 1.37642502784729
LOSS: 1.3739525079727173
LOSS: 1.3720086812973022
LOSS: 1.370610237121582
LOSS: 1.3695604801177979
LOSS: 1.3689091205596924
LOSS: 1.3682729005813599
LOSS: 1.3677407503128052
LOSS: 1.3671271800994873
LOSS: 1.366380214691162
LOSS: 1.3654872179031372
LOSS: 1.3643882274627686
LOSS: 1.3631610870361328
LOSS: 1.3617995977401733
LOSS: 1.360347032546997
LOSS: 1.3588320016860962
LOSS: 1.3573213815689087
LOSS: 1.3558220863342285
LOSS: 1.3543827533721924
LOSS: 1.35298490524292
LOSS: 1.3516606092453003
LOSS: 1.3503724336624146
LOSS: 1.3491796255111694
LOSS: 1.3480409383773804
LOSS: 1.3469761610031128
LOSS: 1.3459436893463135
LOSS: 1.344943642616272
LOSS: 1.3439693450927734
LOSS: 1.3430148363113403
LOSS: 1.3420528173446655
LOSS: 1.3411457538604736
LOSS: 1.3401693105697632
LOSS: 1.339231014251709
LOSS: 1.3382858037948608
LOSS: 1.3374128341674805
LOSS: 1.3365176916122437
LOSS: 1.3357034921646118
LOSS: 1.3349300622940063
LOSS: 1.334219217300415
LOSS: 1.3336182832717896
LOSS: 1.3330775499343872
LOSS: 1.3324705362319946
LOSS: 1.3320236206054688
LOSS: 1.3316872119903564
LOSS: 1.3312658071517944
LOSS: 1.3309870958328247
LOSS: 1.3306829929351807
LOSS: 1.330512523651123
LOSS: 1.3303329944610596
LOSS: 1.3301877975463867
LOSS: 1.330068588256836
LOSS: 1.329984188079834
LOSS: 1.329964518547058
LOSS: 1.3299094438552856
LOSS: 1.3298403024673462
LOSS: 1.329891324043274
LOSS: 1.3298845291137695
LOSS: 1.329867959022522
LOSS: 1.330017328262329
LOSS: 1.3299659490585327
LOSS: 1.3299882411956787
LOSS: 1.330052375793457
LOSS: 1.3300509452819824
LOSS: 1.3300460577011108
LOSS: 1.33011794090271
LOSS: 1.330060362815857
LOSS: 1.3301070928573608
LOSS: 1.330108880996704
LOSS: 1.3300650119781494
LOSS: 1.3300641775131226
LOSS: 1.3300479650497437
LOSS: 1.3300241231918335
LOSS: 1.3300278186798096
LOSS: 1.3299875259399414
LOSS: 1.3299779891967773
LOSS: 1.3299754858016968
LOSS: 1.32993483543396
LOSS: 1.3299235105514526
LOSS: 1.329967737197876
LOSS: 1.3298962116241455
LOSS: 1.3298817873001099
LOSS: 1.329844355583191
LOSS: 1.3298743963241577
LOSS: 1.3298450708389282
LOSS: 1.3299254179000854
LOSS: 1.3298654556274414
LOSS: 1.3298499584197998
LOSS: 1.3298271894454956
LOSS: 1.329872727394104
LOSS: 1.3298518657684326
LOSS: 1.3298479318618774
LOSS: 1.329859972000122
LOSS: 1.3298747539520264
LOSS: 1.329835295677185
LOSS: 1.337420105934143
LOSS: 1.3205845355987549
LOSS: 1.3034180402755737
LOSS: 1.286067008972168
LOSS: 1.2686353921890259
LOSS: 1.2511793375015259
LOSS: 1.2337268590927124
LOSS: 1.2162967920303345
LOSS: 1.1989096403121948
LOSS: 1.1815922260284424
LOSS: 1.164386510848999
LOSS: 1.1473503112792969
LOSS: 1.1305567026138306
LOSS: 1.114085078239441
LOSS: 1.0980114936828613
LOSS: 1.0824007987976074
LOSS: 1.0672993659973145
LOSS: 1.0527360439300537
LOSS: 1.0387252569198608
LOSS: 1.0252729654312134
LOSS: 1.0123802423477173
LOSS: 1.0000468492507935
LOSS: 0.9882690906524658
LOSS: 0.9770411849021912
LOSS: 0.966352105140686
LOSS: 0.9561877250671387
LOSS: 0.9465304613113403
LOSS: 0.9373591542243958
LOSS: 0.9286540150642395
LOSS: 0.920396089553833
LOSS: 0.9125717878341675
LOSS: 0.9051722884178162
LOSS: 0.8981920480728149
LOSS: 0.8916255235671997
LOSS: 0.8854679465293884
LOSS: 0.8797114491462708
LOSS: 0.8743457198143005
LOSS: 0.8693541884422302
LOSS: 0.8647170662879944
LOSS: 0.8604102730751038
LOSS: 0.8564080595970154
LOSS: 0.8526812791824341
LOSS: 0.8492009043693542
LOSS: 0.8459411263465881
LOSS: 0.842879056930542
LOSS: 0.8399986624717712
LOSS: 0.8372877240180969
LOSS: 0.8347373008728027
LOSS: 0.8323429226875305
LOSS: 0.8300958275794983
LOSS: 0.8279904723167419
LOSS: 0.8260202407836914
LOSS: 0.8241807222366333
LOSS: 0.8224670886993408
LOSS: 0.8208755850791931
LOSS: 0.8194026947021484
LOSS: 0.8180426955223083
LOSS: 0.8167909383773804
LOSS: 0.8156408667564392
LOSS: 0.8145853281021118
LOSS: 0.8136151432991028
LOSS: 0.8127238750457764
LOSS: 0.8119005560874939
LOSS: 0.8111347556114197
LOSS: 0.8104178309440613
LOSS: 0.8097408413887024
LOSS: 0.8090968728065491
LOSS: 0.8084796071052551
LOSS: 0.8078849911689758
LOSS: 0.807309627532959
LOSS: 0.8067510724067688
LOSS: 0.8062082529067993
LOSS: 0.8056783676147461
LOSS: 0.805162787437439
LOSS: 0.8046606779098511
LOSS: 0.8041720390319824
LOSS: 0.8036966919898987
LOSS: 0.8032340407371521
LOSS: 0.8027837872505188
LOSS: 0.8023459911346436
LOSS: 0.8019209504127502
LOSS: 0.8015081882476807
LOSS: 0.8011090159416199
LOSS: 0.8007234930992126
LOSS: 0.8003508448600769
LOSS: 0.7999923825263977
LOSS: 0.7996470332145691
LOSS: 0.7993149757385254
LOSS: 0.7989941239356995
LOSS: 0.7986847162246704
LOSS: 0.7983860373497009
LOSS: 0.798096239566803
LOSS: 0.7978150248527527
LOSS: 0.7975414991378784
LOSS: 0.7972750663757324
LOSS: 0.7970143556594849
LOSS: 0.7967592477798462
LOSS: 0.7965097427368164
LOSS: 0.7962648272514343
LOSS: 0.7960255146026611
LOSS: 1.337420105934143
LOSS: 1.3205845355987549
LOSS: 1.3034180402755737
LOSS: 1.286067008972168
LOSS: 1.2686353921890259
LOSS: 1.2511793375015259
LOSS: 1.2337268590927124
LOSS: 1.2162967920303345
LOSS: 1.1989096403121948
LOSS: 1.1815922260284424
LOSS: 1.164386510848999
LOSS: 1.1473503112792969
LOSS: 1.1305567026138306
LOSS: 1.114085078239441
LOSS: 1.0980114936828613
LOSS: 1.0824007987976074
LOSS: 1.0672993659973145
LOSS: 1.0527360439300537
LOSS: 1.0387252569198608
LOSS: 1.0252729654312134
LOSS: 1.0123802423477173
LOSS: 1.0000468492507935
LOSS: 0.9882690906524658
LOSS: 0.9770411849021912
LOSS: 0.966352105140686
LOSS: 0.9561877250671387
LOSS: 0.9465304613113403
LOSS: 0.9373591542243958
LOSS: 0.9286540150642395
LOSS: 0.920396089553833
LOSS: 0.9125717878341675
LOSS: 0.9051722884178162
LOSS: 0.8981920480728149
LOSS: 0.8916255235671997
LOSS: 0.8854679465293884
LOSS: 0.8797114491462708
LOSS: 0.8743457198143005
LOSS: 0.8693541884422302
LOSS: 0.8647170662879944
LOSS: 0.8604102730751038
LOSS: 0.8564080595970154
LOSS: 0.8526812791824341
LOSS: 0.8492009043693542
LOSS: 0.8459411263465881
LOSS: 0.842879056930542
LOSS: 0.8399986624717712
LOSS: 0.8372877240180969
LOSS: 0.8347373008728027
LOSS: 0.8323429226875305
LOSS: 0.8300958275794983
LOSS: 0.8279904723167419
LOSS: 0.8260202407836914
LOSS: 0.8241807222366333
LOSS: 0.8224670886993408
LOSS: 0.8208755850791931
LOSS: 0.8194026947021484
LOSS: 0.8180426955223083
LOSS: 0.8167909383773804
LOSS: 0.8156408667564392
LOSS: 0.8145853281021118
LOSS: 0.8136151432991028
LOSS: 0.8127238750457764
LOSS: 0.8119005560874939
LOSS: 0.8111347556114197
LOSS: 0.8104178309440613
LOSS: 0.8097408413887024
LOSS: 0.8090968728065491
LOSS: 0.8084796071052551
LOSS: 0.8078849911689758
LOSS: 0.807309627532959
LOSS: 0.8067510724067688
LOSS: 0.8062082529067993
LOSS: 0.8056783676147461
LOSS: 0.805162787437439
LOSS: 0.8046606779098511
LOSS: 0.8041720390319824
LOSS: 0.8036966919898987
LOSS: 0.8032340407371521
LOSS: 0.8027837872505188
LOSS: 0.8023459911346436
LOSS: 0.8019209504127502
LOSS: 0.8015081882476807
LOSS: 0.8011090159416199
LOSS: 0.8007234930992126
LOSS: 0.8003508448600769
LOSS: 0.7999923825263977
LOSS: 0.7996470332145691
LOSS: 0.7993149757385254
LOSS: 0.7989941239356995
LOSS: 0.7986847162246704
LOSS: 0.7983860373497009
LOSS: 0.798096239566803
LOSS: 0.7978150248527527
LOSS: 0.7975414991378784
LOSS: 0.7972750663757324
LOSS: 0.7970143556594849
LOSS: 0.7967592477798462
LOSS: 0.7965097427368164
LOSS: 0.7962648272514343
LOSS: 0.7960255146026611
LOSS: 1.3450928926467896
LOSS: 1.328939437866211
LOSS: 1.312210202217102
LOSS: 1.294981837272644
LOSS: 1.277282953262329
LOSS: 1.259178638458252
LOSS: 1.2407801151275635
LOSS: 1.2222015857696533
LOSS: 1.2035595178604126
LOSS: 1.1849724054336548
LOSS: 1.1665573120117188
LOSS: 1.148429274559021
LOSS: 1.130698800086975
LOSS: 1.113469123840332
LOSS: 1.0968292951583862
LOSS: 1.0808544158935547
LOSS: 1.06560218334198
LOSS: 1.051111102104187
LOSS: 1.0373979806900024
LOSS: 1.0244592428207397
LOSS: 1.0122737884521484
LOSS: 1.000808835029602
LOSS: 0.9900248050689697
LOSS: 0.9798768758773804
LOSS: 0.9703185558319092
LOSS: 0.9613020420074463
LOSS: 0.9527810215950012
LOSS: 0.9447112679481506
LOSS: 0.9370516538619995
LOSS: 0.9297668933868408
LOSS: 0.9228296875953674
LOSS: 0.9162213206291199
LOSS: 0.9099327921867371
LOSS: 0.9039605259895325
LOSS: 0.8720150589942932
LOSS: 0.8675256967544556
LOSS: 0.8633372187614441
LOSS: 0.8594403862953186
LOSS: 0.8558239936828613
LOSS: 0.8524715900421143
LOSS: 0.849361777305603
LOSS: 0.8464710712432861
LOSS: 0.843778133392334
LOSS: 0.841264009475708
LOSS: 0.8389143347740173
LOSS: 0.8367197513580322
LOSS: 0.8346739411354065
LOSS: 0.8327720761299133
LOSS: 0.831008791923523
LOSS: 0.8293792009353638
LOSS: 0.8278775215148926
LOSS: 0.8264927268028259
LOSS: 0.8252160549163818
LOSS: 0.8240340352058411
LOSS: 0.822934091091156
LOSS: 0.8219053745269775
LOSS: 0.820936381816864
LOSS: 0.8200187087059021
LOSS: 0.8191463351249695
LOSS: 0.8183137774467468
LOSS: 0.8175179958343506
LOSS: 0.8167579174041748
LOSS: 0.8160316348075867
LOSS: 0.8153391480445862
LOSS: 0.8146793246269226
LOSS: 0.8140522241592407
LOSS: 0.8134565353393555
LOSS: 0.8128878474235535
LOSS: 0.8123470544815063
LOSS: 0.8118302226066589
LOSS: 0.8113362193107605
LOSS: 0.8108636736869812
LOSS: 0.8104079961776733
LOSS: 0.8099660873413086
LOSS: 0.8095327615737915
LOSS: 0.8091044425964355
LOSS: 0.8086796402931213
LOSS: 0.8082608580589294
LOSS: 0.8078508377075195
LOSS: 0.8074522614479065
LOSS: 0.8070658445358276
LOSS: 0.8066933751106262
LOSS: 0.8063333034515381
LOSS: 0.8059855103492737
LOSS: 0.8056485652923584
LOSS: 0.8053230047225952
LOSS: 0.8050079941749573
LOSS: 0.8047031760215759
LOSS: 0.804408609867096
LOSS: 0.8041236996650696
LOSS: 0.8038486242294312
LOSS: 0.8035832047462463
LOSS: 0.8033259510993958
LOSS: 0.8030766248703003
LOSS: 0.8028360605239868
LOSS: 0.8026029467582703
LOSS: 0.8023777008056641
LOSS: 1.3532112836837769
LOSS: 1.3374372720718384
LOSS: 1.3212260007858276
LOSS: 1.3047493696212769
LOSS: 1.2881090641021729
LOSS: 1.2713390588760376
LOSS: 1.2544643878936768
LOSS: 1.2375357151031494
LOSS: 1.2206130027770996
LOSS: 1.2037568092346191
LOSS: 1.187029242515564
LOSS: 1.1704918146133423
LOSS: 1.1542071104049683
LOSS: 1.1382386684417725
LOSS: 1.1226469278335571
LOSS: 1.1074892282485962
LOSS: 1.0928144454956055
LOSS: 1.0786603689193726
LOSS: 1.0650513172149658
LOSS: 1.052002191543579
LOSS: 1.03952157497406
LOSS: 1.027614951133728
LOSS: 1.0162839889526367
LOSS: 1.0055254697799683
LOSS: 0.9953306317329407
LOSS: 0.9856851100921631
LOSS: 0.9765699505805969
LOSS: 0.9679644703865051
LOSS: 0.9598492980003357
LOSS: 0.9522086381912231
LOSS: 0.9450294375419617
LOSS: 0.9382990598678589
LOSS: 0.9320034384727478
LOSS: 0.9261243343353271
LOSS: 0.9206414222717285
LOSS: 0.9155305027961731
LOSS: 0.9107680320739746
LOSS: 0.9063296318054199
LOSS: 0.9021891951560974
LOSS: 0.8983213305473328
LOSS: 0.8946999907493591
LOSS: 0.8912995457649231
LOSS: 0.8880959153175354
LOSS: 0.8850706815719604
LOSS: 0.8822068572044373
LOSS: 0.879492998123169
LOSS: 0.8769211769104004
LOSS: 0.8744864463806152
LOSS: 0.8721842169761658
LOSS: 0.8700107336044312
LOSS: 0.8679617047309875
LOSS: 0.8660324811935425
LOSS: 0.864216685295105
LOSS: 0.8625065088272095
LOSS: 0.8608949780464172
LOSS: 0.859375
LOSS: 0.8579397797584534
LOSS: 0.8565859794616699
LOSS: 0.8553106784820557
LOSS: 0.8541140556335449
LOSS: 0.852994441986084
LOSS: 0.8519526720046997
LOSS: 0.8509846329689026
LOSS: 0.8500896096229553
LOSS: 0.8492603898048401
LOSS: 0.8484925031661987
LOSS: 0.847779393196106
LOSS: 0.8471125364303589
LOSS: 0.8464844226837158
LOSS: 0.8458889126777649
LOSS: 0.8453187346458435
LOSS: 0.8447699546813965
LOSS: 0.8442385792732239
LOSS: 0.8437235355377197
LOSS: 0.8432222008705139
LOSS: 0.8427358269691467
LOSS: 0.8422631621360779
LOSS: 0.8418051600456238
LOSS: 0.8413605093955994
LOSS: 0.840929388999939
LOSS: 0.8405134081840515
LOSS: 0.8401110768318176
LOSS: 0.839722216129303
LOSS: 0.8393468260765076
LOSS: 0.8389845490455627
LOSS: 0.8386343717575073
LOSS: 0.8382954597473145
LOSS: 0.8379672169685364
LOSS: 0.8376492857933044
LOSS: 0.8373407125473022
LOSS: 0.8370412588119507
LOSS: 0.8367502689361572
LOSS: 0.8364684581756592
LOSS: 0.8361940383911133
LOSS: 0.8359277844429016
LOSS: 0.8356684446334839
LOSS: 0.8354161977767944
LOSS: 0.8351691961288452
LOSS: 0.8349270820617676
LOSS: 0.8346899151802063
LOSS: 1.341871976852417
LOSS: 1.3247369527816772
LOSS: 1.3069751262664795
LOSS: 1.2886186838150024
LOSS: 1.269662857055664
LOSS: 1.2501885890960693
LOSS: 1.2303094863891602
LOSS: 1.2101333141326904
LOSS: 1.189762830734253
LOSS: 1.169317364692688
LOSS: 1.1489324569702148
LOSS: 1.1287513971328735
LOSS: 1.1089141368865967
LOSS: 1.089553713798523
LOSS: 1.0707905292510986
LOSS: 1.052729845046997
LOSS: 1.0354547500610352
LOSS: 1.019025206565857
LOSS: 1.003477931022644
LOSS: 0.9888278245925903
LOSS: 0.9750707745552063
LOSS: 0.9621864557266235
LOSS: 0.9501428604125977
LOSS: 0.9388986229896545
LOSS: 0.9284086227416992
LOSS: 0.9186270236968994
LOSS: 0.9095094203948975
LOSS: 0.9010117650032043
LOSS: 0.8930878639221191
LOSS: 0.8856886625289917
LOSS: 0.8787609934806824
LOSS: 0.8722519278526306
LOSS: 0.8661120533943176
LOSS: 0.8602966666221619
LOSS: 0.8547697067260742
LOSS: 0.8495016694068909
LOSS: 0.8444725275039673
LOSS: 0.8396695852279663
LOSS: 0.8350903987884521
LOSS: 0.8307381272315979
LOSS: 0.8266201615333557
LOSS: 0.8227435350418091
LOSS: 0.8191121220588684
LOSS: 0.8157291412353516
LOSS: 0.8125857710838318
LOSS: 0.8096703886985779
LOSS: 0.8069632649421692
LOSS: 0.8044411540031433
LOSS: 0.8020802140235901
LOSS: 0.7998594641685486
LOSS: 0.7977588772773743
LOSS: 0.795765221118927
LOSS: 0.7938716411590576
LOSS: 0.7920749187469482
LOSS: 0.7903780937194824
LOSS: 0.7887846231460571
LOSS: 0.7872980833053589
LOSS: 0.7859194874763489
LOSS: 0.7846478223800659
LOSS: 0.7834785580635071
LOSS: 0.7824029326438904
LOSS: 0.7814110517501831
LOSS: 0.7804916501045227
LOSS: 0.7796334624290466
LOSS: 0.7788241505622864
LOSS: 0.7780550122261047
LOSS: 0.7773171663284302
LOSS: 0.7766062617301941
LOSS: 0.7759188413619995
LOSS: 0.7752544283866882
LOSS: 0.7746155858039856
LOSS: 0.7740028500556946
LOSS: 0.7734186053276062
LOSS: 0.7728642821311951
LOSS: 0.7723402380943298
LOSS: 0.7718454003334045
LOSS: 0.771377444267273
LOSS: 0.7709328532218933
LOSS: 0.7705082893371582
LOSS: 0.7701007127761841
LOSS: 0.7697057723999023
LOSS: 0.7693216800689697
LOSS: 0.7689472436904907
LOSS: 0.7685812711715698
LOSS: 0.7682237029075623
LOSS: 0.7678744196891785
LOSS: 0.7675352096557617
LOSS: 0.7672043442726135
LOSS: 0.766883134841919
LOSS: 0.7665723562240601
LOSS: 0.7662706971168518
LOSS: 0.7659788131713867
LOSS: 0.7656961679458618
LOSS: 0.7654216885566711
LOSS: 0.765155017375946
LOSS: 0.7648966908454895
LOSS: 0.7646452188491821
LOSS: 0.7644007205963135
LOSS: 0.7641628980636597
LOSS: 0.7639312148094177
LOSS: 1.3532112836837769
LOSS: 1.3374372720718384
LOSS: 1.3212260007858276
LOSS: 1.3047493696212769
LOSS: 1.2881090641021729
LOSS: 1.2713390588760376
LOSS: 1.2544643878936768
LOSS: 1.2375357151031494
LOSS: 1.2206130027770996
LOSS: 1.2037568092346191
LOSS: 1.187029242515564
LOSS: 1.1704918146133423
LOSS: 1.1542071104049683
LOSS: 1.1382386684417725
LOSS: 1.1226469278335571
LOSS: 1.1074892282485962
LOSS: 1.0928144454956055
LOSS: 1.0786603689193726
LOSS: 1.0650513172149658
LOSS: 1.052002191543579
LOSS: 1.03952157497406
LOSS: 1.027614951133728
LOSS: 1.0162839889526367
LOSS: 1.0055254697799683
LOSS: 0.9953306317329407
LOSS: 0.9856851100921631
LOSS: 0.9765699505805969
LOSS: 0.9679644703865051
LOSS: 0.9598492980003357
LOSS: 0.9522086381912231
LOSS: 0.9450294375419617
LOSS: 0.9382990598678589
LOSS: 0.9320034384727478
LOSS: 0.9261243343353271
LOSS: 0.9206414222717285
LOSS: 0.9155305027961731
LOSS: 0.9107680320739746
LOSS: 0.9063296318054199
LOSS: 0.9021891951560974
LOSS: 0.8983213305473328
LOSS: 0.8946999907493591
LOSS: 0.8912995457649231
LOSS: 0.8880959153175354
LOSS: 0.8850706815719604
LOSS: 0.8822068572044373
LOSS: 0.879492998123169
LOSS: 0.8769211769104004
LOSS: 0.8744864463806152
LOSS: 0.8721842169761658
LOSS: 0.8700107336044312
LOSS: 0.8679617047309875
LOSS: 0.8660324811935425
LOSS: 0.864216685295105
LOSS: 0.8625065088272095
LOSS: 0.8608949780464172
LOSS: 0.859375
LOSS: 0.8579397797584534
LOSS: 0.8565859794616699
LOSS: 0.8553106784820557
LOSS: 0.8541140556335449
LOSS: 0.852994441986084
LOSS: 0.8519526720046997
LOSS: 0.8509846329689026
LOSS: 0.8500896096229553
LOSS: 0.8492603898048401
LOSS: 0.8484925031661987
LOSS: 0.847779393196106
LOSS: 0.919180154800415
LOSS: 0.9128182530403137
LOSS: 0.90680330991745
LOSS: 0.9011252522468567
LOSS: 0.8957687616348267
LOSS: 0.8907129764556885
LOSS: 0.8859335780143738
LOSS: 0.881403923034668
LOSS: 0.8770973682403564
LOSS: 0.8729913234710693
LOSS: 0.8690614700317383
LOSS: 0.8652926683425903
LOSS: 0.8616706728935242
LOSS: 0.8581843972206116
LOSS: 0.8548274636268616
LOSS: 0.851595401763916
LOSS: 0.8484864234924316
LOSS: 0.8455005288124084
LOSS: 0.842637836933136
LOSS: 0.8398980498313904
LOSS: 0.8372812271118164
LOSS: 0.8347858786582947
LOSS: 0.83240807056427
LOSS: 0.8301411271095276
LOSS: 0.8279796838760376
LOSS: 0.8259167075157166
LOSS: 0.8239473700523376
LOSS: 0.8220681548118591
LOSS: 0.8202767968177795
LOSS: 0.8185736536979675
LOSS: 0.816958487033844
LOSS: 0.8154318332672119
LOSS: 0.8139926195144653
LOSS: 0.8126381635665894
LOSS: 0.8113653659820557
LOSS: 0.8101699352264404
LOSS: 0.8090469837188721
LOSS: 0.807992160320282
LOSS: 0.8070018887519836
LOSS: 0.8060696721076965
LOSS: 0.8051925897598267
LOSS: 0.8043668866157532
LOSS: 0.8035879135131836
LOSS: 0.8028532862663269
LOSS: 0.8021588921546936
LOSS: 0.8015030026435852
LOSS: 0.8008825182914734
LOSS: 0.800295352935791
LOSS: 0.7997404932975769
LOSS: 0.7992149591445923
LOSS: 0.7987175583839417
LOSS: 0.7982459664344788
LOSS: 0.7977979183197021
LOSS: 0.7973712086677551
LOSS: 0.7969640493392944
LOSS: 0.796574056148529
LOSS: 0.7961999177932739
LOSS: 0.7958402633666992
LOSS: 0.7954933047294617
LOSS: 0.795158326625824
LOSS: 0.7948337197303772
LOSS: 0.7945190072059631
LOSS: 0.7942132353782654
LOSS: 0.7939154505729675
LOSS: 0.7936246991157532
LOSS: 0.7933396697044373
LOSS: 0.7930604815483093
LOSS: 1.3490055799484253
LOSS: 1.3332253694534302
LOSS: 1.3170710802078247
LOSS: 1.3007146120071411
LOSS: 1.2842943668365479
LOSS: 1.2679219245910645
LOSS: 1.251673698425293
LOSS: 1.2356102466583252
LOSS: 1.2197785377502441
LOSS: 1.2042042016983032
LOSS: 1.1889021396636963
LOSS: 1.1738895177841187
LOSS: 1.1591899394989014
LOSS: 1.1448256969451904
LOSS: 1.130812644958496
LOSS: 1.117159366607666
LOSS: 1.1038649082183838
LOSS: 1.0909184217453003
LOSS: 1.0783036947250366
LOSS: 1.0660091638565063
LOSS: 1.0540298223495483
LOSS: 1.0423712730407715
LOSS: 1.0310450792312622
LOSS: 1.0200705528259277
LOSS: 1.0094670057296753
LOSS: 0.9992537498474121
LOSS: 0.989449143409729
LOSS: 0.9800687432289124
LOSS: 0.9711264967918396
LOSS: 0.9626320600509644
LOSS: 0.9545891880989075
LOSS: 0.9469954967498779
LOSS: 0.9398423433303833
LOSS: 0.9331128001213074
LOSS: 0.9267854690551758
LOSS: 0.9208351969718933
LOSS: 0.915235698223114
LOSS: 0.9099623560905457
LOSS: 0.9049937725067139
LOSS: 0.9003104567527771
LOSS: 0.8958976864814758
LOSS: 0.8917434811592102
LOSS: 0.8878372311592102
LOSS: 0.8841670155525208
LOSS: 0.8807140588760376
LOSS: 0.8774521946907043
LOSS: 0.8743630051612854
LOSS: 0.8714349269866943
LOSS: 0.8686617016792297
LOSS: 0.8660359978675842
LOSS: 0.8635509610176086
LOSS: 0.8611989617347717
LOSS: 0.8589726686477661
LOSS: 0.8568670749664307
LOSS: 0.8548783659934998
LOSS: 0.8530032634735107
LOSS: 0.8512392640113831
LOSS: 0.8495825529098511
LOSS: 0.8480298519134521
LOSS: 0.8465760946273804
LOSS: 0.8452171087265015
LOSS: 0.8439454436302185
LOSS: 0.8427545428276062
LOSS: 0.8416373133659363
LOSS: 0.8405876159667969
LOSS: 0.8395993113517761
LOSS: 0.8386675715446472
LOSS: 0.8377874493598938
LOSS: 0.8369531035423279
LOSS: 0.8361595273017883
LOSS: 0.8353999257087708
LOSS: 0.8346711993217468
LOSS: 0.8339687585830688
LOSS: 0.8332898616790771
LOSS: 0.832632303237915
LOSS: 0.8319966197013855
LOSS: 0.8313829302787781
LOSS: 0.8307905197143555
LOSS: 0.8302208781242371
LOSS: 0.829673171043396
LOSS: 0.8291472792625427
LOSS: 0.8286410570144653
LOSS: 0.8281537294387817
LOSS: 0.8276821970939636
LOSS: 0.8272241353988647
LOSS: 0.8267767429351807
LOSS: 0.8263373374938965
LOSS: 0.8259047269821167
LOSS: 0.8254773616790771
LOSS: 0.8250553011894226
LOSS: 0.8246386051177979
LOSS: 0.8242287635803223
LOSS: 0.823826253414154
LOSS: 0.8234331011772156
LOSS: 0.823051393032074
LOSS: 0.8226808309555054
LOSS: 0.8223239183425903
LOSS: 0.8219807147979736
LOSS: 0.8216514587402344
LOSS: 0.8213359117507935
LOSS: 1.3490055799484253
LOSS: 1.3332253694534302
LOSS: 1.3170710802078247
LOSS: 1.3007146120071411
LOSS: 1.2842943668365479
LOSS: 1.2679219245910645
LOSS: 1.251673698425293
LOSS: 1.2356102466583252
LOSS: 1.2197785377502441
LOSS: 1.2042042016983032
LOSS: 1.1889021396636963
LOSS: 1.1738895177841187
LOSS: 1.1591899394989014
LOSS: 1.1448256969451904
LOSS: 1.130812644958496
LOSS: 1.117159366607666
LOSS: 1.1038649082183838
LOSS: 1.0909184217453003
LOSS: 1.0783036947250366
LOSS: 1.0660091638565063
LOSS: 1.0540298223495483
LOSS: 1.0423712730407715
LOSS: 1.0310450792312622
LOSS: 1.0200705528259277
LOSS: 1.0094670057296753
LOSS: 0.9992537498474121
LOSS: 0.989449143409729
LOSS: 0.9800687432289124
LOSS: 0.9711264967918396
LOSS: 0.9626320600509644
LOSS: 0.9545891880989075
LOSS: 0.9469954967498779
LOSS: 0.9398423433303833
LOSS: 0.9331128001213074
LOSS: 0.9267854690551758
LOSS: 0.9208351969718933
LOSS: 0.915235698223114
LOSS: 0.9099623560905457
LOSS: 0.9049937725067139
LOSS: 0.9003104567527771
LOSS: 0.8958976864814758
LOSS: 0.8917434811592102
LOSS: 0.8878372311592102
LOSS: 0.8841670155525208
LOSS: 0.8807140588760376
LOSS: 0.8774521946907043
LOSS: 0.8743630051612854
LOSS: 0.8714349269866943
LOSS: 0.8686617016792297
LOSS: 0.8660359978675842
LOSS: 0.8635509610176086
LOSS: 0.8611989617347717
LOSS: 0.8589726686477661
LOSS: 0.8568670749664307
LOSS: 0.8548783659934998
LOSS: 0.8530032634735107
LOSS: 0.8512392640113831
LOSS: 0.8495825529098511
LOSS: 0.8480298519134521
LOSS: 0.8465760946273804
LOSS: 0.8452171087265015
LOSS: 0.8439454436302185
LOSS: 0.8427545428276062
LOSS: 0.8416373133659363
LOSS: 0.8405876159667969
LOSS: 0.8395993113517761
LOSS: 0.8386675715446472
LOSS: 0.8377874493598938
LOSS: 0.8369531035423279
LOSS: 0.8361595273017883
LOSS: 0.8353999257087708
LOSS: 0.8346711993217468
LOSS: 0.8339687585830688
LOSS: 0.8332898616790771
LOSS: 0.832632303237915
LOSS: 0.8319966197013855
LOSS: 0.8313829302787781
LOSS: 0.8307905197143555
LOSS: 0.8302208781242371
LOSS: 0.829673171043396
LOSS: 0.8291472792625427
LOSS: 0.8286410570144653
LOSS: 0.8281537294387817
LOSS: 0.8276821970939636
LOSS: 0.8272241353988647
LOSS: 0.8267767429351807
LOSS: 0.8263373374938965
LOSS: 0.8259047269821167
LOSS: 0.8254773616790771
LOSS: 0.8250553011894226
LOSS: 0.8246386051177979
LOSS: 0.8242287635803223
LOSS: 0.823826253414154
LOSS: 0.8234331011772156
LOSS: 0.823051393032074
LOSS: 0.8226808309555054
LOSS: 0.8223239183425903
LOSS: 0.8219807147979736
LOSS: 0.8216514587402344
LOSS: 0.8213359117507935
LOSS: 1.3411006927490234
LOSS: 1.3256990909576416
LOSS: 1.3101158142089844
LOSS: 1.2944376468658447
LOSS: 1.2787508964538574
LOSS: 1.263157606124878
LOSS: 1.2477654218673706
LOSS: 1.2326723337173462
LOSS: 1.217958688735962
LOSS: 1.2036844491958618
LOSS: 1.1898868083953857
LOSS: 1.1765838861465454
LOSS: 1.163782000541687
LOSS: 1.151478886604309
LOSS: 1.1396598815917969
LOSS: 1.128296136856079
LOSS: 1.1173382997512817
LOSS: 1.1067193746566772
LOSS: 1.0963596105575562
LOSS: 1.0861762762069702
LOSS: 1.0760945081710815
LOSS: 1.0660581588745117
LOSS: 1.0560357570648193
LOSS: 1.0460216999053955
LOSS: 1.0360326766967773
LOSS: 1.02610182762146
LOSS: 1.0162720680236816
LOSS: 1.0065895318984985
LOSS: 0.9970991015434265
LOSS: 0.9878412485122681
LOSS: 0.9788485169410706
LOSS: 0.9701462984085083
LOSS: 0.961751401424408
LOSS: 0.9536725282669067
LOSS: 0.9459105134010315
LOSS: 0.9384618997573853
LOSS: 0.9313195943832397
LOSS: 0.924475371837616
LOSS: 0.9179225564002991
LOSS: 0.9116566777229309
LOSS: 0.9056767225265503
LOSS: 0.8999834060668945
LOSS: 0.8945807814598083
LOSS: 0.889474093914032
LOSS: 0.8846680521965027
LOSS: 0.88016676902771
LOSS: 0.8759719729423523
LOSS: 0.8720810413360596
LOSS: 0.8684878349304199
LOSS: 0.8651804327964783
LOSS: 0.8621417284011841
LOSS: 0.8593506217002869
LOSS: 0.8567848801612854
LOSS: 0.8544211387634277
LOSS: 0.852240264415741
LOSS: 0.8502260446548462
LOSS: 0.848366379737854
LOSS: 0.8466516137123108
LOSS: 0.8450734615325928
LOSS: 0.8436241745948792
LOSS: 0.8422935009002686
LOSS: 0.841071367263794

LOSS: 0.9074199199676514
LOSS: 0.9026610851287842
LOSS: 0.8982270359992981
LOSS: 0.8941040635108948
LOSS: 0.8902746438980103
LOSS: 0.886714518070221
LOSS: 0.8833999037742615
LOSS: 0.880303144454956
LOSS: 0.8773967027664185
LOSS: 0.8746570348739624
LOSS: 0.8720613718032837
LOSS: 0.8695937395095825
LOSS: 0.8672398924827576
LOSS: 0.8649905323982239
LOSS: 0.8628381490707397
LOSS: 0.8607771396636963
LOSS: 0.8588040471076965
LOSS: 0.856913685798645
LOSS: 0.8551006317138672
LOSS: 0.8533597588539124
LOSS: 0.8516860604286194
LOSS: 0.8500763773918152
LOSS: 0.8485274910926819
LOSS: 0.8470389246940613
LOSS: 0.8456109166145325
LOSS: 0.8442448973655701
LOSS: 0.8429427146911621
LOSS: 0.841705322265625
LOSS: 0.8405340313911438
LOSS: 0.839429497718811
LOSS: 0.8383912444114685
LOSS: 0.8374172449111938
LOSS: 0.836506187915802
LOSS: 0.8356532454490662
LOSS: 0.8348539471626282
LOSS: 0.8341031670570374
LOSS: 0.8333934545516968
LOSS: 0.8327188491821289
LOSS: 0.8320742249488831
LOSS: 0.8314544558525085
LOSS: 0.8308562636375427
LOSS: 0.8302772641181946
LOSS: 0.8297174572944641
LOSS: 0.8291757106781006
LOSS: 0.8286528587341309
LOSS: 0.8281476497650146
LOSS: 0.8276602029800415
LOSS: 0.8271892070770264
LOSS: 0.8267339468002319
LOSS: 0.8262920379638672
LOSS: 0.8258623480796814
LOSS: 0.8254444599151611
LOSS: 0.8250362277030945
LOSS: 0.8246384859085083
LOSS: 0.8242497444152832
LOSS: 0.8238711953163147
LOSS: 0.8235020637512207
LOSS: 0.8231425881385803
LOSS: 0.822792649269104
LOSS: 0.8224514722824097
LOSS: 0.8221191167831421
LOSS: 0.8217945694923401
LOSS: 0.8214783072471619
LOSS: 0.8211694359779358
LOSS: 0.8208678364753723
LOSS: 1.3471133708953857
LOSS: 1.330703854560852
LOSS: 1.3138689994812012
LOSS: 1.296735405921936
LOSS: 1.2793956995010376
LOSS: 1.2619342803955078
LOSS: 1.244388222694397
LOSS: 1.2267637252807617
LOSS: 1.2090774774551392
LOSS: 1.1913800239562988
LOSS: 1.1737561225891113
LOSS: 1.1563160419464111
LOSS: 1.1391801834106445
LOSS: 1.1224671602249146
LOSS: 1.1062837839126587
LOSS: 1.0907212495803833
LOSS: 1.0758475065231323
LOSS: 1.0617061853408813
LOSS: 1.048319697380066
LOSS: 1.0356911420822144
LOSS: 1.023803949356079
LOSS: 1.0126265287399292
LOSS: 1.0021167993545532
LOSS: 0.9922255277633667
LOSS: 0.9828981161117554
LOSS: 0.9740809798240662
LOSS: 0.9657270908355713
LOSS: 0.9577977657318115
LOSS: 0.9502606987953186
LOSS: 0.9430943131446838
LOSS: 0.936284065246582
LOSS: 0.9298242330551147
LOSS: 0.9237107038497925
LOSS: 0.9179408550262451
LOSS: 0.9125117063522339
LOSS: 0.9074199199676514
LOSS: 0.9026610851287842
LOSS: 0.8982270359992981
LOSS: 0.8941040635108948
LOSS: 0.8902746438980103
LOSS: 0.886714518070221
LOSS: 0.8833999037742615
LOSS: 0.880303144454956
LOSS: 0.8773967027664185
LOSS: 0.8746570348739624
LOSS: 0.8720613718032837
LOSS: 0.8695937395095825
LOSS: 0.8672398924827576
LOSS: 0.8649905323982239
LOSS: 0.8628381490707397
LOSS: 0.8607771396636963
LOSS: 0.8588040471076965
LOSS: 0.856913685798645
LOSS: 0.8551006317138672
LOSS: 0.8533597588539124
LOSS: 0.8516860604286194
LOSS: 0.8500763773918152
LOSS: 0.8485274910926819
LOSS: 0.8470389246940613
LOSS: 0.8456109166145325
LOSS: 0.8442448973655701
LOSS: 0.8429427146911621
LOSS: 0.841705322265625
LOSS: 0.8405340313911438
LOSS: 0.839429497718811
LOSS: 0.8383912444114685
LOSS: 0.8374172449111938
LOSS: 0.836506187915802
LOSS: 0.8356532454490662
LOSS: 0.8348539471626282
LOSS: 0.8341031670570374
LOSS: 0.8333934545516968
LOSS: 0.8327188491821289
LOSS: 0.8320742249488831
LOSS: 0.8314544558525085
LOSS: 0.8308562636375427
LOSS: 0.8302772641181946
LOSS: 0.8297174572944641
LOSS: 0.8291757106781006
LOSS: 0.8286528587341309
LOSS: 0.8281476497650146
LOSS: 0.8276602029800415
LOSS: 0.8271892070770264
LOSS: 0.8267339468002319
LOSS: 0.8262920379638672
LOSS: 0.8258623480796814
LOSS: 0.8254444599151611
LOSS: 0.8250362277030945
LOSS: 0.8246384859085083
LOSS: 0.8242497444152832
LOSS: 0.8238711953163147
LOSS: 0.8235020637512207
LOSS: 0.8231425881385803
LOSS: 0.822792649269104
LOSS: 0.8224514722824097
LOSS: 0.8221191167831421
LOSS: 0.8217945694923401
LOSS: 0.8214783072471619
LOSS: 0.8211694359779358
LOSS: 0.8208678364753723
LOSS: 1.341871976852417
LOSS: 1.3247369527816772
LOSS: 1.3069751262664795
LOSS: 1.2886186838150024
LOSS: 1.269662857055664
LOSS: 1.2501885890960693
LOSS: 1.2303094863891602
LOSS: 1.2101333141326904
LOSS: 1.189762830734253
LOSS: 1.169317364692688
LOSS: 1.1489324569702148
LOSS: 1.1287513971328735
LOSS: 1.1089141368865967
LOSS: 1.089553713798523
LOSS: 1.0707905292510986
LOSS: 1.052729845046997
LOSS: 1.0354547500610352
LOSS: 1.019025206565857
LOSS: 1.003477931022644
LOSS: 0.9888278245925903
LOSS: 0.9750707745552063
LOSS: 0.9621864557266235
LOSS: 0.9501428604125977
LOSS: 0.9388986229896545
LOSS: 0.9284086227416992
LOSS: 0.9186270236968994
LOSS: 0.9095094203948975
LOSS: 0.9010117650032043
LOSS: 0.8930878639221191
LOSS: 0.8856886625289917
LOSS: 0.8787609934806824
LOSS: 0.8722519278526306
LOSS: 0.8661120533943176
LOSS: 0.8602966666221619
LOSS: 0.8547697067260742
LOSS: 0.8495016694068909
LOSS: 0.8444725275039673
LOSS: 0.8396695852279663
LOSS: 0.8350903987884521
LOSS: 0.8307381272315979
LOSS: 0.8266201615333557
LOSS: 0.8227435350418091
LOSS: 0.8191121220588684
LOSS: 0.8157291412353516
LOSS: 0.8125857710838318
LOSS: 0.8096703886985779
LOSS: 0.8069632649421692
LOSS: 0.8044411540031433
LOSS: 0.8020802140235901
LOSS: 0.7998594641685486
LOSS: 0.7977588772773743
LOSS: 0.795765221118927
LOSS: 0.7938716411590576
LOSS: 0.7920749187469482
LOSS: 0.7903780937194824
LOSS: 0.7887846231460571
LOSS: 0.7872980833053589
LOSS: 0.7859194874763489
LOSS: 0.7846478223800659
LOSS: 0.7834785580635071
LOSS: 0.7824029326438904
LOSS: 0.7814110517501831
LOSS: 0.7804916501045227
LOSS: 0.7796334624290466
LOSS: 0.7788241505622864
LOSS: 0.7780550122261047
LOSS: 0.7773171663284302
LOSS: 0.7766062617301941
LOSS: 0.7759188413619995
LOSS: 0.7752544283866882
LOSS: 0.7746155858039856
LOSS: 0.7740028500556946
LOSS: 0.7734186053276062
LOSS: 0.7728642821311951
LOSS: 0.7723402380943298
LOSS: 0.7718454003334045
LOSS: 0.771377444267273
LOSS: 0.7709328532218933
LOSS: 0.7705082893371582
LOSS: 0.7701007127761841
LOSS: 0.7697057723999023
LOSS: 0.7693216800689697
LOSS: 0.7689472436904907
LOSS: 0.7685812711715698
LOSS: 0.7682237029075623
LOSS: 0.7678744196891785
LOSS: 0.7675352096557617
LOSS: 0.7672043442726135
LOSS: 0.766883134841919
LOSS: 0.7665723562240601
LOSS: 0.7662706971168518
LOSS: 0.7659788131713867
LOSS: 0.7656961679458618
LOSS: 0.7654216885566711
LOSS: 0.765155017375946
LOSS: 0.7648966908454895
LOSS: 0.7646452188491821
LOSS: 0.7644007205963135
LOSS: 0.7641628980636597
LOSS: 0.7639312148094177
LOSS: 1.3448890447616577
LOSS: 1.3283936977386475
LOSS: 1.3114285469055176
LOSS: 1.2940891981124878
LOSS: 1.2764089107513428
LOSS: 1.2583696842193604
LOSS: 1.2399966716766357
LOSS: 1.221348762512207
LOSS: 1.2025099992752075
LOSS: 1.18359375
LOSS: 1.164737582206726
LOSS: 1.1460902690887451
LOSS: 1.127795696258545
LOSS: 1.1099810600280762
LOSS: 1.09275484085083
LOSS: 1.0762102603912354
LOSS: 1.0604263544082642
LOSS: 1.045464038848877
LOSS: 1.0313626527786255
LOSS: 1.0181361436843872
LOSS: 1.0057744979858398
LOSS: 0.9942485094070435
LOSS: 0.9835136532783508
LOSS: 0.9735140204429626
LOSS: 0.9641863107681274
LOSS: 0.9554657936096191
LOSS: 0.9472904205322266
LOSS: 0.9396020174026489
LOSS: 0.9323478937149048
LOSS: 0.9254832863807678
LOSS: 0.9189733862876892
LOSS: 0.9127970337867737
LOSS: 0.9069429636001587
LOSS: 0.9014065265655518
LOSS: 0.8961837291717529
LOSS: 0.8912675976753235
LOSS: 0.8866497874259949
LOSS: 0.8823202848434448
LOSS: 0.8782681822776794
LOSS: 0.874481201171875
LOSS: 0.8709442615509033
LOSS: 0.867642879486084
LOSS: 0.8645626902580261
LOSS: 0.8616876602172852
LOSS: 0.8590053915977478
LOSS: 0.8565036058425903
LOSS: 0.8541685938835144
LOSS: 0.8519884347915649
LOSS: 0.8499496579170227
LOSS: 0.8480390310287476
LOSS: 0.8462460041046143
LOSS: 0.8445571064949036
LOSS: 0.8429625034332275
LOSS: 0.8414520621299744
LOSS: 0.8400194644927979
LOSS: 0.8386592268943787
LOSS: 0.8373682498931885
LOSS: 0.8361440300941467
LOSS: 0.8349864482879639
LOSS: 0.8338934779167175
LOSS: 0.8328636288642883
LOSS: 0.8318951725959778
LOSS: 0.8309855461120605
LOSS: 0.8301315307617188
LOSS: 0.8983054161071777
LOSS: 0.8929698467254639
LOSS: 0.887956440448761
LOSS: 0.8832616209983826
LOSS: 0.8788783550262451
LOSS: 0.8747932314872742
LOSS: 0.8709877729415894
LOSS: 0.8674384951591492
LOSS: 0.8641217947006226
LOSS: 0.8610121607780457
LOSS: 0.8580883145332336
LOSS: 0.8553327918052673
LOSS: 0.852730929851532
LOSS: 0.8502746224403381
LOSS: 0.8479577302932739
LOSS: 0.8457780480384827
LOSS: 0.8437346816062927
LOSS: 0.8418275117874146
LOSS: 0.8400557041168213
LOSS: 0.8384171724319458
LOSS: 0.8369086980819702
LOSS: 0.8355240821838379
LOSS: 0.8342519402503967
LOSS: 0.8330758213996887
LOSS: 0.8319745659828186
LOSS: 0.830930233001709
LOSS: 0.8299334049224854
LOSS: 0.828980565071106
LOSS: 0.8280717730522156
LOSS: 0.8272079825401306
LOSS: 0.8263891935348511
LOSS: 0.8256136775016785
LOSS: 0.8248798847198486
LOSS: 0.824184238910675
LOSS: 0.823523759841919
LOSS: 0.8228957056999207
LOSS: 0.8222953081130981
LOSS: 0.8217185139656067
LOSS: 0.8211615085601807
LOSS: 0.8206223249435425
LOSS: 0.8200972080230713
LOSS: 0.8195841312408447
LOSS: 0.8190819025039673
LOSS: 0.8185896277427673
LOSS: 0.8181055188179016
LOSS: 0.8176305890083313
LOSS: 0.817163348197937
LOSS: 0.8167057037353516
LOSS: 0.8162570595741272
LOSS: 0.8158191442489624
LOSS: 0.8153931498527527
LOSS: 0.8149787783622742
LOSS: 0.8145758509635925
LOSS: 0.8141846656799316
LOSS: 0.8138045072555542
LOSS: 0.8134350776672363
LOSS: 0.8130763173103333
LOSS: 0.8127269148826599
LOSS: 0.8123870491981506
LOSS: 0.8120559453964233
LOSS: 0.8117331862449646
LOSS: 0.8114179968833923
LOSS: 0.8111096024513245
LOSS: 0.8108071684837341
LOSS: 0.8105100393295288
LOSS: 0.8102185130119324
LOSS: 1.3450928926467896
LOSS: 1.328939437866211
LOSS: 1.312210202217102
LOSS: 1.294981837272644
LOSS: 1.277282953262329
LOSS: 1.259178638458252
LOSS: 1.2407801151275635
LOSS: 1.2222015857696533
LOSS: 1.2035595178604126
LOSS: 1.1849724054336548
LOSS: 1.1665573120117188
LOSS: 1.148429274559021
LOSS: 1.130698800086975
LOSS: 1.113469123840332
LOSS: 1.0968292951583862
LOSS: 1.0808544158935547
LOSS: 1.06560218334198
LOSS: 1.051111102104187
LOSS: 1.0373979806900024
LOSS: 1.0244592428207397
LOSS: 1.0122737884521484
LOSS: 1.000808835029602
LOSS: 0.9900248050689697
LOSS: 0.9798768758773804
LOSS: 0.9703185558319092
LOSS: 0.9613020420074463
LOSS: 0.9527810215950012
LOSS: 0.9447112679481506
LOSS: 0.9370516538619995
LOSS: 0.9297668933868408
LOSS: 0.9228296875953674
LOSS: 0.9162213206291199
LOSS: 0.9099327921867371
LOSS: 0.9039605259895325
LOSS: 0.8983054161071777
LOSS: 0.8929698467254639
LOSS: 0.887956440448761
LOSS: 0.8832616209983826
LOSS: 0.8788783550262451
LOSS: 0.8747932314872742
LOSS: 0.8709877729415894
LOSS: 0.8674384951591492
LOSS: 0.8641217947006226
LOSS: 0.8610121607780457
LOSS: 0.8580883145332336
LOSS: 0.8553327918052673
LOSS: 0.852730929851532
LOSS: 0.8502746224403381
LOSS: 0.8479577302932739
LOSS: 0.8457780480384827
LOSS: 0.8437346816062927
LOSS: 0.8418275117874146
LOSS: 0.8400557041168213
LOSS: 0.8384171724319458
LOSS: 0.8369086980819702
LOSS: 0.8355240821838379
LOSS: 0.8342519402503967
LOSS: 0.8330758213996887
LOSS: 0.8319745659828186
LOSS: 0.830930233001709
LOSS: 0.8299334049224854
LOSS: 0.828980565071106
LOSS: 0.8280717730522156
LOSS: 0.8272079825401306
LOSS: 0.8263891935348511
LOSS: 0.8256136775016785
LOSS: 0.8248798847198486
LOSS: 0.824184238910675
LOSS: 0.823523759841919
LOSS: 0.8228957056999207
LOSS: 0.8222953081130981
LOSS: 0.8217185139656067
LOSS: 0.8211615085601807
LOSS: 0.8206223249435425
LOSS: 0.8200972080230713
LOSS: 0.8195841312408447
LOSS: 0.8190819025039673
LOSS: 0.8185896277427673
LOSS: 0.8181055188179016
LOSS: 0.8176305890083313
LOSS: 0.817163348197937
LOSS: 0.8167057037353516
LOSS: 0.8162570595741272
LOSS: 0.8158191442489624
LOSS: 0.8153931498527527
LOSS: 0.8149787783622742
LOSS: 0.8145758509635925
LOSS: 0.8141846656799316
LOSS: 0.8138045072555542
LOSS: 0.8134350776672363
LOSS: 0.8130763173103333
LOSS: 0.8127269148826599
LOSS: 0.8123870491981506
LOSS: 0.8120559453964233
LOSS: 0.8117331862449646
LOSS: 0.8114179968833923
LOSS: 0.8111096024513245
LOSS: 0.8108071684837341
LOSS: 0.8105100393295288
LOSS: 0.8102185130119324
LOSS: 1.355930209159851
LOSS: 1.3398483991622925
LOSS: 1.323167085647583
LOSS: 1.305975317955017
LOSS: 1.2882938385009766
LOSS: 1.2701358795166016
LOSS: 1.2515552043914795
LOSS: 1.2326455116271973
LOSS: 1.213517427444458
LOSS: 1.1942992210388184
LOSS: 1.1751289367675781
LOSS: 1.1561423540115356
LOSS: 1.1374675035476685
LOSS: 1.1192197799682617
LOSS: 1.1014971733093262
LOSS: 1.0843831300735474
LOSS: 1.067941665649414
LOSS: 1.0522165298461914
LOSS: 1.0372314453125
LOSS: 1.022992491722107
LOSS: 1.0094921588897705
LOSS: 0.9967122673988342
LOSS: 0.9846262335777283
LOSS: 0.9732035398483276
LOSS: 0.9624107480049133
LOSS: 0.9522156119346619
LOSS: 0.9425864219665527
LOSS: 0.9334908127784729
LOSS: 0.9248965382575989
LOSS: 0.916772723197937
LOSS: 0.9090922474861145
LOSS: 0.9018320441246033
LOSS: 0.8949685096740723
LOSS: 0.8884778618812561
LOSS: 0.8823357224464417
LOSS: 0.8765179514884949
LOSS: 0.8710045218467712
LOSS: 0.865780770778656
LOSS: 0.8608339428901672
LOSS: 0.856157660484314
LOSS: 0.8517441153526306
LOSS: 0.8475871682167053
LOSS: 0.8436782956123352
LOSS: 0.8400078415870667
LOSS: 0.8365609645843506
LOSS: 0.8333230018615723
LOSS: 0.8302773833274841
LOSS: 0.8274086117744446
LOSS: 0.8247020840644836
LOSS: 0.8221469521522522
LOSS: 0.8197380900382996
LOSS: 0.8174715042114258
LOSS: 0.8153491020202637
LOSS: 0.813370943069458
LOSS: 0.8115397095680237
LOSS: 0.8098547458648682
LOSS: 0.8083130717277527
LOSS: 0.8069075345993042
LOSS: 0.8056284785270691
LOSS: 0.8044618964195251
LOSS: 0.8033943772315979
LOSS: 0.8024110198020935
LOSS: 0.8014988899230957
LOSS: 0.8006477355957031
LOSS: 0.7998489141464233
LOSS: 0.7990966439247131
LOSS: 0.7983878254890442
LOSS: 0.7977212071418762
LOSS: 0.7970961332321167
LOSS: 0.7965138554573059
LOSS: 0.7959738969802856
LOSS: 0.7954736948013306
LOSS: 0.79500412940979
LOSS: 0.7945526242256165
LOSS: 0.7941110134124756
LOSS: 0.793679416179657
LOSS: 0.7932610511779785
LOSS: 0.7928590178489685
LOSS: 0.7924737334251404
LOSS: 0.7921041250228882
LOSS: 0.7917472124099731
LOSS: 0.7914026379585266
LOSS: 0.7910680174827576
LOSS: 0.7907431721687317
LOSS: 0.7904263734817505
LOSS: 0.7901175618171692
LOSS: 0.7898144125938416
LOSS: 0.7895184755325317
LOSS: 0.7892293930053711
LOSS: 0.7889477014541626
LOSS: 0.7886728644371033
LOSS: 0.7884055972099304
LOSS: 0.7881447672843933
LOSS: 0.7878903150558472
LOSS: 0.7876420021057129
LOSS: 0.7873988151550293
LOSS: 0.7871622443199158
LOSS: 0.7869318127632141
LOSS: 0.786707878112793
LOSS: 0.7864901423454285
LOSS: 1.3489238023757935
LOSS: 1.3328118324279785
LOSS: 1.316159963607788
LOSS: 1.2991162538528442
LOSS: 1.2818167209625244
LOSS: 1.2643266916275024
LOSS: 1.2466810941696167
LOSS: 1.2289329767227173
LOSS: 1.2111526727676392
LOSS: 1.1934303045272827
LOSS: 1.1758677959442139
LOSS: 1.158559799194336
LOSS: 1.1415871381759644
LOSS: 1.125022053718567
LOSS: 1.1089361906051636
LOSS: 1.0934035778045654
LOSS: 1.0784891843795776
LOSS: 1.0642421245574951
LOSS: 1.0506905317306519
LOSS: 1.0378423929214478
LOSS: 1.0256870985031128
LOSS: 1.0141998529434204
LOSS: 1.003346562385559
LOSS: 0.9930850267410278
LOSS: 0.9833683371543884
LOSS: 0.9741480350494385
LOSS: 0.965376615524292
LOSS: 0.9570097923278809
LOSS: 0.9490089416503906
LOSS: 0.9413470029830933
LOSS: 0.9340062141418457
LOSS: 0.9269800782203674
LOSS: 0.9202699065208435
LOSS: 0.9138799905776978
LOSS: 0.9078167080879211
LOSS: 0.9020839929580688
LOSS: 0.8966789841651917
LOSS: 0.8915932178497314
LOSS: 0.886810839176178
LOSS: 0.8823084831237793
LOSS: 0.8780578970909119
LOSS: 0.8740270137786865
LOSS: 0.8701874613761902
LOSS: 0.8665111660957336
LOSS: 0.8629782795906067
LOSS: 0.8595767021179199
LOSS: 0.8563000559806824
LOSS: 0.8531489968299866
LOSS: 0.8501277565956116
LOSS: 0.8472442030906677
LOSS: 0.8445057272911072
LOSS: 0.8419182896614075
LOSS: 0.8394860029220581
LOSS: 0.8372074961662292
LOSS: 0.835078775882721
LOSS: 0.8330923318862915
LOSS: 0.8312368392944336
LOSS: 0.829501211643219
LOSS: 0.8278730511665344
LOSS: 0.826342761516571
LOSS: 0.8248991966247559
LOSS: 0.8235360980033875
LOSS: 0.8222466111183167
LOSS: 0.8471125364303589
LOSS: 0.8464844226837158
LOSS: 0.8458889126777649
LOSS: 0.8453187346458435
LOSS: 0.8447699546813965
LOSS: 0.8442385792732239
LOSS: 0.8437235355377197
LOSS: 0.8432222008705139
LOSS: 0.8427358269691467
LOSS: 0.8422631621360779
LOSS: 0.8418051600456238
LOSS: 0.8413605093955994
LOSS: 0.840929388999939
LOSS: 0.8405134081840515
LOSS: 0.8401110768318176
LOSS: 0.839722216129303
LOSS: 0.8393468260765076
LOSS: 0.8389845490455627
LOSS: 0.8386343717575073
LOSS: 0.8382954597473145
LOSS: 0.8379672169685364
LOSS: 0.8376492857933044
LOSS: 0.8373407125473022
LOSS: 0.8370412588119507
LOSS: 0.8367502689361572
LOSS: 0.8364684581756592
LOSS: 0.8361940383911133
LOSS: 0.8359277844429016
LOSS: 0.8356684446334839
LOSS: 0.8354161977767944
LOSS: 0.8351691961288452
LOSS: 0.8349270820617676
LOSS: 0.8346899151802063
LOSS: 1.3489238023757935
LOSS: 1.3328118324279785
LOSS: 1.316159963607788
LOSS: 1.2991162538528442
LOSS: 1.2818167209625244
LOSS: 1.2643266916275024
LOSS: 1.2466810941696167
LOSS: 1.2289329767227173
LOSS: 1.2111526727676392
LOSS: 1.1934303045272827
LOSS: 1.1758677959442139
LOSS: 1.158559799194336
LOSS: 1.1415871381759644
LOSS: 1.125022053718567
LOSS: 1.1089361906051636
LOSS: 1.0934035778045654
LOSS: 1.0784891843795776
LOSS: 1.0642421245574951
LOSS: 1.0506905317306519
LOSS: 1.0378423929214478
LOSS: 1.0256870985031128
LOSS: 1.0141998529434204
LOSS: 1.003346562385559
LOSS: 0.9930850267410278
LOSS: 0.9833683371543884
LOSS: 0.9741480350494385
LOSS: 0.965376615524292
LOSS: 0.9570097923278809
LOSS: 0.9490089416503906
LOSS: 0.9413470029830933
LOSS: 0.9340062141418457
LOSS: 0.9269800782203674
LOSS: 0.9202699065208435
LOSS: 0.9138799905776978
LOSS: 0.9078167080879211
LOSS: 0.9020839929580688
LOSS: 0.8966789841651917
LOSS: 0.8915932178497314
LOSS: 0.886810839176178
LOSS: 0.8823084831237793
LOSS: 0.8780578970909119
LOSS: 0.8740270137786865
LOSS: 0.8701874613761902
LOSS: 0.8665111660957336
LOSS: 0.8629782795906067
LOSS: 0.8595767021179199
LOSS: 0.8563000559806824
LOSS: 0.8531489968299866
LOSS: 0.8501277565956116
LOSS: 0.8472442030906677
LOSS: 0.8445057272911072
LOSS: 0.8419182896614075
LOSS: 0.8394860029220581
LOSS: 0.8372074961662292
LOSS: 0.835078775882721
LOSS: 0.8330923318862915
LOSS: 0.8312368392944336
LOSS: 0.829501211643219
LOSS: 0.8278730511665344
LOSS: 0.826342761516571
LOSS: 0.8248991966247559
LOSS: 0.8235360980033875
LOSS: 0.8222466111183167
LOSS: 0.8210269212722778
LOSS: 0.819873034954071
LOSS: 0.8187815546989441
LOSS: 0.8177503347396851
LOSS: 0.816775381565094
LOSS: 0.8158526420593262
LOSS: 0.814978301525116
LOSS: 0.8141475319862366
LOSS: 0.8133569955825806
LOSS: 0.8126023411750793
LOSS: 0.8118813037872314
LOSS: 0.8111909031867981
LOSS: 0.8105294704437256
LOSS: 0.8098949790000916
LOSS: 0.8092862963676453
LOSS: 0.8087010383605957
LOSS: 0.8081372380256653
LOSS: 0.8075926303863525
LOSS: 0.8070654273033142
LOSS: 0.8065539598464966
LOSS: 0.8060562610626221
LOSS: 0.805571973323822
LOSS: 0.8051005601882935
LOSS: 0.804641842842102
LOSS: 0.8041960597038269
LOSS: 0.8037633299827576
LOSS: 0.8033434748649597
LOSS: 0.8029366135597229
LOSS: 0.8025418519973755
LOSS: 0.802158534526825
LOSS: 0.8017852306365967
LOSS: 0.8014218211174011
LOSS: 0.8010672926902771
LOSS: 0.8007200956344604
LOSS: 0.8003808259963989
LOSS: 0.800048291683197
LOSS: 0.7997227311134338
LOSS: 1.342081069946289
LOSS: 1.3248881101608276
LOSS: 1.3073116540908813
LOSS: 1.289465308189392
LOSS: 1.271454095840454
LOSS: 1.2533453702926636
LOSS: 1.2351757287979126
LOSS: 1.2169986963272095
LOSS: 1.198898196220398
LOSS: 1.1809617280960083
LOSS: 1.1632647514343262
LOSS: 1.1458748579025269
LOSS: 1.1288577318191528
LOSS: 1.1122777462005615
LOSS: 1.096194863319397
LOSS: 1.08066725730896
LOSS: 1.0657479763031006
LOSS: 1.0514798164367676
LOSS: 1.0378906726837158
LOSS: 1.0249940156936646
LOSS: 1.012789249420166
LOSS: 1.0012646913528442
LOSS: 0.9903967380523682
LOSS: 0.9801534414291382
LOSS: 0.9704998135566711
LOSS: 0.9614019989967346
LOSS: 0.9528307914733887
LOSS: 0.9447576403617859
LOSS: 0.9371528625488281
LOSS: 0.9299856424331665
LOSS: 0.9232308864593506
LOSS: 0.9168661832809448
LOSS: 0.9108732342720032
LOSS: 0.9052332639694214
LOSS: 0.8999297618865967
LOSS: 0.8949494361877441
LOSS: 0.8902799487113953
LOSS: 0.8859096765518188
LOSS: 0.881827175617218
LOSS: 0.8780208826065063
LOSS: 0.8744763731956482
LOSS: 0.8711798191070557
LOSS: 0.8681142330169678
LOSS: 0.8652597665786743
LOSS: 0.8625975251197815
LOSS: 0.8601089119911194
LOSS: 0.8577772974967957
LOSS: 0.8555862903594971
LOSS: 0.8535228371620178
LOSS: 0.8515748977661133
LOSS: 0.8497327566146851
LOSS: 0.8479883670806885
LOSS: 0.8463338017463684
LOSS: 0.8447613716125488
LOSS: 0.8432630896568298
LOSS: 0.8418312668800354
LOSS: 0.840458869934082
LOSS: 0.8391414880752563
LOSS: 0.8378736972808838
LOSS: 0.8366547226905823
LOSS: 0.8354853391647339
LOSS: 0.8343654870986938
LOSS: 0.8332970142364502
LOSS: 0.8322826027870178
LOSS: 0.831321656703949
LOSS: 0.8304153680801392
LOSS: 0.8295596837997437
LOSS: 0.8287513256072998
LOSS: 0.8279833197593689
LOSS: 0.8272504210472107
LOSS: 0.8265447020530701
LOSS: 0.8258606791496277
LOSS: 0.8251934051513672
LOSS: 0.8245392441749573
LOSS: 0.8238972425460815
LOSS: 0.8232676982879639
LOSS: 0.8226506114006042
LOSS: 0.8220489025115967
LOSS: 0.8214643597602844
LOSS: 0.8209002017974854
LOSS: 0.8203588724136353
LOSS: 0.8198419809341431
LOSS: 0.8193491697311401
LOSS: 0.8188813328742981
LOSS: 0.8184364438056946
LOSS: 0.818012535572052
LOSS: 0.8176077008247375
LOSS: 0.8172197341918945
LOSS: 0.8168457746505737
LOSS: 0.8164845108985901
LOSS: 0.8161342144012451
LOSS: 0.8157936930656433
LOSS: 0.8154630661010742
LOSS: 0.8151418566703796
LOSS: 0.8148292899131775
LOSS: 0.8145266771316528
LOSS: 0.8142339587211609
LOSS: 0.813951313495636
LOSS: 0.8136781454086304
LOSS: 0.8134146332740784
LOSS: 1.3448890447616577
LOSS: 1.3283936977386475
LOSS: 1.3114285469055176
LOSS: 1.2940891981124878
LOSS: 1.2764089107513428
LOSS: 1.2583696842193604
LOSS: 1.2399966716766357
LOSS: 1.221348762512207
LOSS: 1.2025099992752075
LOSS: 1.18359375
LOSS: 1.164737582206726
LOSS: 1.1460902690887451
LOSS: 1.127795696258545
LOSS: 1.1099810600280762
LOSS: 1.09275484085083
LOSS: 1.0762102603912354
LOSS: 1.0604263544082642
LOSS: 1.045464038848877
LOSS: 1.0313626527786255
LOSS: 1.0181361436843872
LOSS: 1.0057744979858398
LOSS: 0.9942485094070435
LOSS: 0.9835136532783508
LOSS: 0.9735140204429626
LOSS: 0.9641863107681274
LOSS: 0.9554657936096191
LOSS: 0.9472904205322266
LOSS: 0.9396020174026489
LOSS: 0.9323478937149048
LOSS: 0.9254832863807678
LOSS: 0.9189733862876892
LOSS: 0.9127970337867737
LOSS: 0.9069429636001587
LOSS: 0.9014065265655518
LOSS: 0.8961837291717529
LOSS: 0.8912675976753235
LOSS: 0.8866497874259949
LOSS: 0.8823202848434448
LOSS: 0.8782681822776794
LOSS: 0.874481201171875
LOSS: 0.8709442615509033
LOSS: 0.867642879486084
LOSS: 0.8645626902580261
LOSS: 0.8616876602172852
LOSS: 0.8590053915977478
LOSS: 0.8565036058425903
LOSS: 0.8541685938835144
LOSS: 0.8519884347915649
LOSS: 0.8499496579170227
LOSS: 0.8480390310287476
LOSS: 0.8462460041046143
LOSS: 0.8445571064949036
LOSS: 0.8429625034332275
LOSS: 0.8414520621299744
LOSS: 0.8400194644927979
LOSS: 0.8386592268943787
LOSS: 0.8373682498931885
LOSS: 0.8361440300941467
LOSS: 0.8349864482879639
LOSS: 0.8338934779167175
LOSS: 0.8328636288642883
LOSS: 0.8318951725959778
LOSS: 0.8309855461120605
LOSS: 0.8301315307617188
LOSS: 0.8293280005455017
LOSS: 0.8285706639289856
LOSS: 0.8278555870056152
LOSS: 0.8271762728691101
LOSS: 0.826529324054718
LOSS: 0.8259093165397644
LOSS: 0.825314462184906
LOSS: 0.8247418403625488
LOSS: 0.8241925239562988
LOSS: 0.8236652612686157
LOSS: 0.8231614232063293
LOSS: 0.8226815462112427
LOSS: 0.8222242593765259
LOSS: 0.8217875957489014
LOSS: 0.8213698863983154
LOSS: 0.8209688663482666
LOSS: 0.8205819725990295
LOSS: 0.8202069401741028
LOSS: 0.8198431134223938
LOSS: 0.8194875717163086
LOSS: 0.8191419243812561
LOSS: 0.818804144859314
LOSS: 0.8184751868247986
LOSS: 0.8181548118591309
LOSS: 0.8178435564041138
LOSS: 0.8175409436225891
LOSS: 0.8172469735145569
LOSS: 0.8169612884521484
LOSS: 0.8166831135749817
LOSS: 0.8164117336273193
LOSS: 0.8161470890045166
LOSS: 0.8158883452415466
LOSS: 0.8293280005455017
LOSS: 0.8285706639289856
LOSS: 0.8278555870056152
LOSS: 0.8271762728691101
LOSS: 0.826529324054718
LOSS: 0.8259093165397644
LOSS: 0.825314462184906
LOSS: 0.8247418403625488
LOSS: 0.8241925239562988
LOSS: 0.8236652612686157
LOSS: 0.8231614232063293
LOSS: 0.8226815462112427
LOSS: 0.8222242593765259
LOSS: 0.8217875957489014
LOSS: 0.8213698863983154
LOSS: 0.8209688663482666
LOSS: 0.8205819725990295
LOSS: 0.8202069401741028
LOSS: 0.8198431134223938
LOSS: 0.8194875717163086
LOSS: 0.8191419243812561
LOSS: 0.818804144859314
LOSS: 0.8184751868247986
LOSS: 0.8181548118591309
LOSS: 0.8178435564041138
LOSS: 0.8175409436225891
LOSS: 0.8172469735145569
LOSS: 0.8169612884521484
LOSS: 0.8166831135749817
LOSS: 0.8164117336273193
LOSS: 0.8161470890045166
LOSS: 0.8158883452415466
LOSS: 0.8156359791755676
LOSS: 0.815388560295105
LOSS: 0.8151460289955139
LOSS: 0.8149091005325317
LOSS: 1.3411006927490234
LOSS: 1.3256990909576416
LOSS: 1.3101158142089844
LOSS: 1.2944376468658447
LOSS: 1.2787508964538574
LOSS: 1.263157606124878
LOSS: 1.2477654218673706
LOSS: 1.2326723337173462
LOSS: 1.217958688735962
LOSS: 1.2036844491958618
LOSS: 1.1898868083953857
LOSS: 1.1765838861465454
LOSS: 1.163782000541687
LOSS: 1.151478886604309
LOSS: 1.1396598815917969
LOSS: 1.128296136856079
LOSS: 1.1173382997512817
LOSS: 1.1067193746566772
LOSS: 1.0963596105575562
LOSS: 1.0861762762069702
LOSS: 1.0760945081710815
LOSS: 1.0660581588745117
LOSS: 1.0560357570648193
LOSS: 1.0460216999053955
LOSS: 1.0360326766967773
LOSS: 1.02610182762146
LOSS: 1.0162720680236816
LOSS: 1.0065895318984985
LOSS: 0.9970991015434265
LOSS: 0.9878412485122681
LOSS: 0.9788485169410706
LOSS: 0.9701462984085083
LOSS: 0.961751401424408
LOSS: 0.9536725282669067
LOSS: 0.9459105134010315
LOSS: 0.9384618997573853
LOSS: 0.9313195943832397
LOSS: 0.924475371837616
LOSS: 0.9179225564002991
LOSS: 0.9116566777229309
LOSS: 0.9056767225265503
LOSS: 0.8999834060668945
LOSS: 0.8945807814598083
LOSS: 0.889474093914032
LOSS: 0.8846680521965027
LOSS: 0.88016676902771
LOSS: 0.8759719729423523
LOSS: 0.8720810413360596
LOSS: 0.8684878349304199
LOSS: 0.8651804327964783
LOSS: 0.8621417284011841
LOSS: 0.8593506217002869
LOSS: 0.8567848801612854
LOSS: 0.8544211387634277
LOSS: 0.852240264415741
LOSS: 0.8502260446548462
LOSS: 0.848366379737854
LOSS: 0.8466516137123108
LOSS: 0.8450734615325928
LOSS: 0.8436241745948792
LOSS: 0.8422935009002686
LOSS: 0.841071367263794
LOSS: 0.839945375919342
LOSS: 0.8389019966125488
LOSS: 0.8379294276237488
LOSS: 0.8370162844657898
LOSS: 0.8361535668373108
LOSS: 0.8353344202041626
LOSS: 0.8345538973808289
LOSS: 0.8338085412979126
LOSS: 0.833096444606781
LOSS: 0.8324163556098938
LOSS: 0.8317674994468689
LOSS: 0.8311474323272705
LOSS: 0.8305549621582031
LOSS: 0.8299866914749146
LOSS: 0.829440176486969
LOSS: 0.8289114236831665
LOSS: 0.8283984065055847
LOSS: 0.8278996348381042
LOSS: 0.8274142146110535
LOSS: 0.8269419074058533
LOSS: 0.8264830708503723
LOSS: 0.8260378837585449
LOSS: 0.8256065249443054
LOSS: 0.8251886963844299
LOSS: 0.8247836232185364
LOSS: 0.8243898749351501
LOSS: 0.8240069150924683
LOSS: 0.8236342072486877
LOSS: 0.8232704997062683
LOSS: 0.8229159116744995
LOSS: 0.8225695490837097
LOSS: 0.8222326636314392
LOSS: 0.8219049572944641
LOSS: 0.8215863704681396
LOSS: 0.8212773203849792
LOSS: 0.8209773302078247
LOSS: 0.8206861019134521
LOSS: 0.8204028010368347
LOSS: 1.342081069946289
LOSS: 1.3248881101608276
LOSS: 1.3073116540908813
LOSS: 1.289465308189392
LOSS: 1.271454095840454
LOSS: 1.2533453702926636
LOSS: 1.2351757287979126
LOSS: 1.2169986963272095
LOSS: 1.198898196220398
LOSS: 1.1809617280960083
LOSS: 1.1632647514343262
LOSS: 1.1458748579025269
LOSS: 1.1288577318191528
LOSS: 1.1122777462005615
LOSS: 1.096194863319397
LOSS: 1.08066725730896
LOSS: 1.0657479763031006
LOSS: 1.0514798164367676
LOSS: 1.0378906726837158
LOSS: 1.0249940156936646
LOSS: 1.012789249420166
LOSS: 1.0012646913528442
LOSS: 0.9903967380523682
LOSS: 0.9801534414291382
LOSS: 0.9704998135566711
LOSS: 0.9614019989967346
LOSS: 0.9528307914733887
LOSS: 0.9447576403617859
LOSS: 0.9371528625488281
LOSS: 0.9299856424331665
LOSS: 0.9232308864593506
LOSS: 0.9168661832809448
LOSS: 0.9108732342720032
LOSS: 0.9052332639694214
LOSS: 0.8999297618865967
LOSS: 0.8949494361877441
LOSS: 0.8902799487113953
LOSS: 0.8859096765518188
LOSS: 0.881827175617218
LOSS: 0.8780208826065063
LOSS: 0.8744763731956482
LOSS: 0.8711798191070557
LOSS: 0.8681142330169678
LOSS: 0.8652597665786743
LOSS: 0.8625975251197815
LOSS: 0.8601089119911194
LOSS: 0.8577772974967957
LOSS: 0.8555862903594971
LOSS: 0.8535228371620178
LOSS: 0.8515748977661133
LOSS: 0.8497327566146851
LOSS: 0.8479883670806885
LOSS: 0.8463338017463684
LOSS: 0.8447613716125488
LOSS: 0.8432630896568298
LOSS: 0.8418312668800354
LOSS: 0.840458869934082
LOSS: 0.8391414880752563
LOSS: 0.8378736972808838
LOSS: 0.8366547226905823
LOSS: 0.8354853391647339
LOSS: 0.8343654870986938
LOSS: 0.8332970142364502
LOSS: 0.8322826027870178
LOSS: 0.831321656703949
LOSS: 0.8304153680801392
LOSS: 0.8295596837997437
LOSS: 0.8287513256072998
LOSS: 0.8279833197593689
LOSS: 0.8272504210472107
LOSS: 0.8265447020530701
LOSS: 0.8258606791496277
LOSS: 0.8251934051513672
LOSS: 0.8245392441749573
LOSS: 0.8238972425460815
LOSS: 0.8232676982879639
LOSS: 0.8226506114006042
LOSS: 0.8220489025115967
LOSS: 0.8214643597602844
LOSS: 0.8209002017974854
LOSS: 0.8203588724136353
LOSS: 0.8198419809341431
LOSS: 0.8193491697311401
LOSS: 0.8188813328742981
LOSS: 0.8184364438056946
LOSS: 0.818012535572052
LOSS: 0.8176077008247375
LOSS: 0.8172197341918945
LOSS: 0.8168457746505737
LOSS: 0.8164845108985901
LOSS: 0.8161342144012451
LOSS: 0.8157936930656433
LOSS: 0.8154630661010742
LOSS: 0.8151418566703796
LOSS: 0.8148292899131775
LOSS: 0.8145266771316528
LOSS: 0.8142339587211609
LOSS: 0.813951313495636
LOSS: 0.8136781454086304
LOSS: 0.8134146332740784
LOSS: 1.3524593114852905
LOSS: 1.3362447023391724
LOSS: 1.3195140361785889
LOSS: 1.3023697137832642
LOSS: 1.2848670482635498
LOSS: 1.26705801486969
LOSS: 1.2490226030349731
LOSS: 1.2308542728424072
LOSS: 1.2126621007919312
LOSS: 1.194564700126648
LOSS: 1.1766815185546875
LOSS: 1.159127950668335
LOSS: 1.142015814781189
LOSS: 1.1254489421844482
LOSS: 1.1095157861709595
LOSS: 1.09428870677948
LOSS: 1.0798248052597046
LOSS: 1.0661665201187134
LOSS: 1.053337574005127
LOSS: 1.0413408279418945
LOSS: 1.030157208442688
LOSS: 1.0197497606277466
LOSS: 1.0100653171539307
LOSS: 1.0010416507720947
LOSS: 0.9926106333732605
LOSS: 0.9847062826156616
LOSS: 0.9772686958312988
LOSS: 0.9702459573745728
LOSS: 0.9635953307151794
LOSS: 0.9572818279266357
LOSS: 0.9512794613838196
LOSS: 0.9455694556236267
LOSS: 0.9401384592056274
LOSS: 0.9349780082702637
LOSS: 0.9300827383995056
LOSS: 0.9254485368728638
LOSS: 0.9210718274116516
LOSS: 0.9169489145278931
LOSS: 0.9130727052688599
LOSS: 0.9094380140304565
LOSS: 0.906038224697113
LOSS: 0.9028639793395996
LOSS: 0.8999031782150269
LOSS: 0.897144079208374
LOSS: 0.8945706486701965
LOSS: 0.8921669721603394
LOSS: 0.8899179697036743
LOSS: 0.8878094553947449
LOSS: 0.8858299851417542
LOSS: 0.883969783782959
LOSS: 0.8822211027145386
LOSS: 0.8805766105651855
LOSS: 0.8790302276611328
LOSS: 0.8775752782821655
LOSS: 0.8762052655220032
LOSS: 0.8749130368232727
LOSS: 0.8736916184425354
LOSS: 0.872534990310669
LOSS: 0.871437132358551
LOSS: 0.8703922033309937
LOSS: 0.8693974614143372
LOSS: 0.8684499859809875
LOSS: 0.8675482273101807
LOSS: 0.8666918277740479
LOSS: 0.8658795356750488
LOSS: 0.8651072978973389
LOSS: 0.8643673062324524
LOSS: 0.8636502027511597
LOSS: 0.8629499673843384
LOSS: 0.8622652292251587
LOSS: 0.8615981936454773
LOSS: 0.8609512448310852
LOSS: 0.8603265881538391
LOSS: 0.8597242832183838
LOSS: 0.859143078327179
LOSS: 0.8585818409919739
LOSS: 0.8580387830734253
LOSS: 0.8575121164321899
LOSS: 0.8570024371147156
LOSS: 0.8565079569816589
LOSS: 0.8560284376144409
LOSS: 0.8555630445480347
LOSS: 0.8551119565963745
LOSS: 0.8546751737594604
LOSS: 0.8542525768280029
LOSS: 0.8538437485694885
LOSS: 0.8534481525421143
LOSS: 0.8530654907226562
LOSS: 0.852694571018219
LOSS: 0.8523350954055786
LOSS: 0.851985514163971
LOSS: 0.8516451120376587
LOSS: 0.851313054561615
LOSS: 0.839945375919342
LOSS: 0.8389019966125488
LOSS: 0.8379294276237488
LOSS: 0.8370162844657898
LOSS: 0.8361535668373108
LOSS: 0.8353344202041626
LOSS: 0.8345538973808289
LOSS: 0.8338085412979126
LOSS: 0.833096444606781
LOSS: 0.8324163556098938
LOSS: 0.8317674994468689
LOSS: 0.8311474323272705
LOSS: 0.8305549621582031
LOSS: 0.8299866914749146
LOSS: 0.829440176486969
LOSS: 0.8289114236831665
LOSS: 0.8283984065055847
LOSS: 0.8278996348381042
LOSS: 0.8274142146110535
LOSS: 0.8269419074058533
LOSS: 0.8264830708503723
LOSS: 0.8260378837585449
LOSS: 0.8256065249443054
LOSS: 0.8251886963844299
LOSS: 0.8247836232185364
LOSS: 0.8243898749351501
LOSS: 0.8240069150924683
LOSS: 0.8236342072486877
LOSS: 0.8232704997062683
LOSS: 0.8229159116744995
LOSS: 0.8225695490837097
LOSS: 0.8222326636314392
LOSS: 0.8219049572944641
LOSS: 0.8215863704681396
LOSS: 0.8212773203849792
LOSS: 0.8209773302078247
LOSS: 0.8206861019134521
LOSS: 0.8204028010368347
LOSS: 1.355930209159851
LOSS: 1.3398483991622925
LOSS: 1.323167085647583
LOSS: 1.305975317955017
LOSS: 1.2882938385009766
LOSS: 1.2701358795166016
LOSS: 1.2515552043914795
LOSS: 1.2326455116271973
LOSS: 1.213517427444458
LOSS: 1.1942992210388184
LOSS: 1.1751289367675781
LOSS: 1.1561423540115356
LOSS: 1.1374675035476685
LOSS: 1.1192197799682617
LOSS: 1.1014971733093262
LOSS: 1.0843831300735474
LOSS: 1.067941665649414
LOSS: 1.0522165298461914
LOSS: 1.0372314453125
LOSS: 1.022992491722107
LOSS: 1.0094921588897705
LOSS: 0.9967122673988342
LOSS: 0.9846262335777283
LOSS: 0.9732035398483276
LOSS: 0.9624107480049133
LOSS: 0.9522156119346619
LOSS: 0.9425864219665527
LOSS: 0.9334908127784729
LOSS: 0.9248965382575989
LOSS: 0.916772723197937
LOSS: 0.9090922474861145
LOSS: 0.9018320441246033
LOSS: 0.8949685096740723
LOSS: 0.8884778618812561
LOSS: 0.8823357224464417
LOSS: 0.8765179514884949
LOSS: 0.8710045218467712
LOSS: 0.865780770778656
LOSS: 0.8608339428901672
LOSS: 0.856157660484314
LOSS: 0.8517441153526306
LOSS: 0.8475871682167053
LOSS: 0.8436782956123352
LOSS: 0.8400078415870667
LOSS: 0.8365609645843506
LOSS: 0.8333230018615723
LOSS: 0.8302773833274841
LOSS: 0.8274086117744446
LOSS: 0.8247020840644836
LOSS: 0.8221469521522522
LOSS: 0.8197380900382996
LOSS: 0.8174715042114258
LOSS: 0.8153491020202637
LOSS: 0.813370943069458
LOSS: 0.8115397095680237
LOSS: 0.8098547458648682
LOSS: 0.8083130717277527
LOSS: 0.8069075345993042
LOSS: 0.8056284785270691
LOSS: 0.8044618964195251
LOSS: 0.8033943772315979
LOSS: 0.8024110198020935
LOSS: 0.8014988899230957
LOSS: 0.8006477355957031
LOSS: 0.7998489141464233
LOSS: 0.7990966439247131
LOSS: 0.7983878254890442
LOSS: 0.7977212071418762
LOSS: 0.7970961332321167
LOSS: 0.7965138554573059
LOSS: 0.7959738969802856
LOSS: 0.7954736948013306
LOSS: 0.79500412940979
LOSS: 0.7945526242256165
LOSS: 0.7941110134124756
LOSS: 0.793679416179657
LOSS: 0.7932610511779785
LOSS: 0.7928590178489685
LOSS: 0.7924737334251404
LOSS: 0.7921041250228882
LOSS: 0.7917472124099731
LOSS: 0.7914026379585266
LOSS: 0.7910680174827576
LOSS: 0.7907431721687317
LOSS: 0.7904263734817505
LOSS: 0.7901175618171692
LOSS: 0.7898144125938416
LOSS: 0.7895184755325317
LOSS: 0.7892293930053711
LOSS: 0.7889477014541626
LOSS: 0.7886728644371033
LOSS: 0.7884055972099304
LOSS: 0.7881447672843933
LOSS: 0.7878903150558472
LOSS: 0.7876420021057129
LOSS: 0.7873988151550293
LOSS: 0.7871622443199158
LOSS: 0.7869318127632141
LOSS: 0.786707878112793
LOSS: 0.7864901423454285
LOSS: 1.3452664613723755
LOSS: 1.3285667896270752
LOSS: 1.3113665580749512
LOSS: 1.2937588691711426
LOSS: 1.2758495807647705
LOSS: 1.2576948404312134
LOSS: 1.2393295764923096
LOSS: 1.2208112478256226
LOSS: 1.2022135257720947
LOSS: 1.1836236715316772
LOSS: 1.1651419401168823
LOSS: 1.1468714475631714
LOSS: 1.128912091255188
LOSS: 1.1113563776016235
LOSS: 1.094289779663086
LOSS: 1.0777913331985474
LOSS: 1.0619276762008667
LOSS: 1.0467528104782104
LOSS: 1.0323036909103394
LOSS: 1.0186012983322144
LOSS: 1.0056486129760742
LOSS: 0.9934337735176086
LOSS: 0.9819326996803284
LOSS: 0.9711134433746338
LOSS: 0.9609362483024597
LOSS: 0.9513574242591858
LOSS: 0.942330539226532
LOSS: 0.9338074326515198
LOSS: 0.9257411360740662
LOSS: 0.9180908203125
LOSS: 0.9108222723007202
LOSS: 0.903913140296936
LOSS: 0.8973494172096252
LOSS: 0.8911239504814148
LOSS: 0.8852297067642212
LOSS: 0.8796632289886475
LOSS: 0.8744141459465027
LOSS: 0.8694697022438049
LOSS: 0.8648097515106201
LOSS: 0.8604121208190918
LOSS: 0.8562521934509277
LOSS: 0.8523077368736267
LOSS: 0.8485577702522278
LOSS: 0.8449804186820984
LOSS: 0.8415553569793701
LOSS: 0.8382648825645447
LOSS: 0.8350926637649536
LOSS: 0.832026481628418
LOSS: 0.8290554285049438
LOSS: 0.8261755704879761
LOSS: 0.8233830332756042
LOSS: 0.8206788301467896
LOSS: 0.8180644512176514
LOSS: 0.8155419230461121
LOSS: 0.8131139874458313
LOSS: 0.8107829689979553
LOSS: 0.8085499405860901
LOSS: 0.8064164519309998
LOSS: 0.8043822050094604
LOSS: 0.8024454116821289
LOSS: 0.8006051182746887
LOSS: 0.7988592386245728
LOSS: 0.7972040772438049
LOSS: 0.7956381440162659
LOSS: 0.7941571474075317
LOSS: 0.7927585244178772
LOSS: 0.7914409637451172
LOSS: 0.790199875831604
LOSS: 0.7890340089797974
LOSS: 0.787941575050354
LOSS: 0.786919355392456
LOSS: 0.7859622836112976
LOSS: 0.7850660681724548
LOSS: 0.7842254042625427
LOSS: 0.7834357023239136
LOSS: 0.7826910614967346
LOSS: 0.7819874882698059
LOSS: 0.7813222408294678
LOSS: 0.7806897759437561
LOSS: 0.780087947845459
LOSS: 0.7795136570930481
LOSS: 0.7789658308029175
LOSS: 0.7784414887428284
LOSS: 0.7779408097267151
LOSS: 0.7774627804756165
LOSS: 0.7770066857337952
LOSS: 0.776573121547699
LOSS: 0.7761620283126831
LOSS: 0.775769829750061
LOSS: 0.7753965258598328
LOSS: 0.7750393152236938
LOSS: 0.7746965289115906
LOSS: 0.7743651866912842
LOSS: 0.7740446329116821
LOSS: 0.7737320065498352
LOSS: 0.773428201675415
LOSS: 0.7731314897537231
LOSS: 0.7728418111801147
LOSS: 0.7725586891174316
LOSS: 0.7722819447517395
LOSS: 1.3452664613723755
LOSS: 1.3285667896270752
LOSS: 1.3113665580749512
LOSS: 1.2937588691711426
LOSS: 1.2758495807647705
LOSS: 1.2576948404312134
LOSS: 1.2393295764923096
LOSS: 1.2208112478256226
LOSS: 1.2022135257720947
LOSS: 1.1836236715316772
LOSS: 1.1651419401168823
LOSS: 1.1468714475631714
LOSS: 1.128912091255188
LOSS: 1.1113563776016235
LOSS: 1.094289779663086
LOSS: 1.0777913331985474
LOSS: 1.0619276762008667
LOSS: 1.0467528104782104
LOSS: 1.0323036909103394
LOSS: 1.0186012983322144
LOSS: 1.0056486129760742
LOSS: 0.9934337735176086
LOSS: 0.9819326996803284
LOSS: 0.9711134433746338
LOSS: 0.9609362483024597
LOSS: 0.9513574242591858
LOSS: 0.942330539226532
LOSS: 0.9338074326515198
LOSS: 0.9257411360740662
LOSS: 0.9180908203125
LOSS: 0.9108222723007202
LOSS: 0.903913140296936
LOSS: 0.8973494172096252
LOSS: 0.8911239504814148
LOSS: 0.8852297067642212
LOSS: 0.8796632289886475
LOSS: 0.8744141459465027
LOSS: 0.8694697022438049
LOSS: 0.8648097515106201
LOSS: 0.8604121208190918
LOSS: 0.8562521934509277
LOSS: 0.8523077368736267
LOSS: 0.8485577702522278
LOSS: 0.8449804186820984
LOSS: 0.8415553569793701
LOSS: 0.8382648825645447
LOSS: 0.8350926637649536
LOSS: 0.832026481628418
LOSS: 0.8290554285049438
LOSS: 0.8261755704879761
LOSS: 0.8233830332756042
LOSS: 0.8206788301467896
LOSS: 0.8180644512176514
LOSS: 0.8155419230461121
LOSS: 0.8131139874458313
LOSS: 0.8107829689979553
LOSS: 0.8085499405860901
LOSS: 0.8064164519309998
LOSS: 0.8043822050094604
LOSS: 0.8024454116821289
LOSS: 0.8006051182746887
LOSS: 0.7988592386245728
LOSS: 0.7972040772438049
LOSS: 0.7956381440162659
LOSS: 0.7941571474075317
LOSS: 0.7927585244178772
LOSS: 0.7914409637451172
LOSS: 0.790199875831604
LOSS: 0.7890340089797974
LOSS: 0.787941575050354
LOSS: 0.786919355392456
LOSS: 0.7859622836112976
LOSS: 0.7850660681724548
LOSS: 0.7842254042625427
LOSS: 0.7834357023239136
LOSS: 0.7826910614967346
LOSS: 0.7819874882698059
LOSS: 0.7813222408294678
LOSS: 0.7806897759437561
LOSS: 0.780087947845459
LOSS: 0.7795136570930481
LOSS: 0.7789658308029175
LOSS: 0.7784414887428284
LOSS: 0.7779408097267151
LOSS: 0.7774627804756165
LOSS: 0.7770066857337952
LOSS: 0.776573121547699
LOSS: 0.7761620283126831
LOSS: 0.775769829750061
LOSS: 0.7753965258598328
LOSS: 0.7750393152236938
LOSS: 0.8156359791755676
LOSS: 0.815388560295105
LOSS: 0.8151460289955139
LOSS: 0.8149091005325317
LOSS: 1.3479912281036377
LOSS: 1.3321782350540161
LOSS: 1.3159279823303223
LOSS: 1.2993505001068115
LOSS: 1.2825498580932617
LOSS: 1.265608310699463
LOSS: 1.248579740524292
LOSS: 1.2314943075180054
LOSS: 1.2143813371658325
LOSS: 1.1972949504852295
LOSS: 1.1803151369094849
LOSS: 1.163534164428711
LOSS: 1.1470447778701782
LOSS: 1.1309287548065186
LOSS: 1.1152504682540894
LOSS: 1.1000632047653198
LOSS: 1.0854109525680542
LOSS: 1.0713309049606323
LOSS: 1.0578522682189941
LOSS: 1.044995665550232
LOSS: 1.0327703952789307
LOSS: 1.0211756229400635
LOSS: 1.0101990699768066
LOSS: 0.9998180866241455
LOSS: 0.9900013208389282
LOSS: 0.9807141423225403
LOSS: 0.9719242453575134
LOSS: 0.9636036157608032
LOSS: 0.9557304978370667
LOSS: 0.9482861161231995
LOSS: 0.9412572383880615
LOSS: 0.9346346259117126
LOSS: 0.9284120798110962
LOSS: 0.9225828051567078
LOSS: 0.9171369075775146
LOSS: 0.9120602607727051
LOSS: 0.9073350429534912
LOSS: 0.902937650680542
LOSS: 0.898842990398407
LOSS: 0.8950231075286865
LOSS: 0.8914502859115601
LOSS: 0.8881012797355652
LOSS: 0.884955108165741
LOSS: 0.8819939494132996
LOSS: 0.8792002201080322
LOSS: 0.8765627145767212
LOSS: 0.8740693926811218
LOSS: 0.8717114329338074
LOSS: 0.8694827556610107
LOSS: 0.8673794865608215
LOSS: 0.8653990626335144
LOSS: 0.8635416626930237
LOSS: 0.8618058562278748
LOSS: 0.8601881861686707
LOSS: 0.8586845397949219
LOSS: 0.8572890758514404
LOSS: 0.8559921979904175
LOSS: 0.8547852635383606
LOSS: 0.8536591529846191
LOSS: 0.8526058197021484
LOSS: 0.8516169786453247
LOSS: 0.8506845831871033
LOSS: 0.8498031497001648
LOSS: 0.848966658115387
LOSS: 0.8481701016426086
LOSS: 0.8474084734916687
LOSS: 0.8466770648956299
LOSS: 0.8459734320640564
LOSS: 0.8452935218811035
LOSS: 0.844635546207428
LOSS: 0.8439968824386597
LOSS: 0.8433757424354553
LOSS: 0.8427715301513672
LOSS: 0.842183530330658
LOSS: 0.8416114449501038
LOSS: 0.8410571813583374
LOSS: 0.8405206203460693
LOSS: 0.8400023579597473
LOSS: 0.8395030498504639
LOSS: 0.8390228152275085
LOSS: 0.8385606408119202
LOSS: 0.8381158709526062
LOSS: 0.8376865983009338
LOSS: 0.8372714519500732
LOSS: 0.8368695378303528
LOSS: 0.8364816308021545
LOSS: 0.8361114859580994
LOSS: 0.8357619047164917
LOSS: 0.8354347944259644
LOSS: 0.8351243138313293
LOSS: 0.8348203897476196
LOSS: 0.8345118761062622
LOSS: 0.8341973423957825
LOSS: 0.8338820934295654
LOSS: 0.833574116230011
LOSS: 0.8332769274711609
LOSS: 0.8329924941062927
LOSS: 0.8327175378799438
LOSS: 0.8324517607688904
LOSS: 0.8321911096572876
LOSS: 1.348430871963501
LOSS: 1.3322432041168213
LOSS: 1.315609097480774
LOSS: 1.2986589670181274
LOSS: 1.281427025794983
LOSS: 1.2639305591583252
LOSS: 1.2462081909179688
LOSS: 1.2283028364181519
LOSS: 1.2102543115615845
LOSS: 1.1921156644821167
LOSS: 1.1739633083343506
LOSS: 1.1558964252471924
LOSS: 1.1380289793014526
LOSS: 1.1204781532287598
LOSS: 1.1033563613891602
LOSS: 1.0867680311203003
LOSS: 1.0708043575286865
LOSS: 1.055539846420288
LOSS: 1.041031002998352
LOSS: 1.0273146629333496
LOSS: 1.0144109725952148
LOSS: 1.0023243427276611
LOSS: 0.991043210029602
LOSS: 0.9805404543876648
LOSS: 0.9707735180854797
LOSS: 0.9616915583610535
LOSS: 0.9532375335693359
LOSS: 0.9453538060188293
LOSS: 0.9379890561103821
LOSS: 0.9311005473136902
LOSS: 0.9246534705162048
LOSS: 0.9186190366744995
LOSS: 0.9129730463027954
LOSS: 0.9076955914497375
LOSS: 0.9027723670005798
LOSS: 0.8981869220733643
LOSS: 0.8939252495765686
LOSS: 0.8899726271629333
LOSS: 0.8863096237182617
LOSS: 0.8829144239425659
LOSS: 0.8797648549079895
LOSS: 0.8768379092216492
LOSS: 0.8741108775138855
LOSS: 0.8715643882751465
LOSS: 0.8691811561584473
LOSS: 0.8669435381889343
LOSS: 0.8648379445075989
LOSS: 0.8628509640693665
LOSS: 0.8609710931777954
LOSS: 0.8591869473457336
LOSS: 0.8574900031089783
LOSS: 0.8558725714683533
LOSS: 0.8543294668197632
LOSS: 0.8528576493263245
LOSS: 0.8514544367790222
LOSS: 0.8501187562942505
LOSS: 0.8488490581512451
LOSS: 0.8476426601409912
LOSS: 0.8464978337287903
LOSS: 0.8454151749610901
LOSS: 0.8443926572799683
LOSS: 0.8434314727783203
LOSS: 0.8425319194793701
LOSS: 0.8416941165924072
LOSS: 0.8409180045127869
LOSS: 0.8402002453804016
LOSS: 0.8395379185676575
LOSS: 0.8389233350753784
LOSS: 0.838348925113678
LOSS: 0.8378042578697205
LOSS: 0.8372805714607239
LOSS: 0.836772620677948
LOSS: 0.8362751007080078
LOSS: 0.8357858657836914
LOSS: 0.8353040218353271
LOSS: 0.8348309397697449
LOSS: 0.8343678712844849
LOSS: 0.8339179754257202
LOSS: 0.8334835171699524
LOSS: 0.833068311214447
LOSS: 0.8326729536056519
LOSS: 0.832297682762146
LOSS: 0.8319422006607056
LOSS: 0.8316039443016052
LOSS: 0.8312795758247375
LOSS: 0.8309668302536011
LOSS: 0.8306620717048645
LOSS: 0.8303637504577637
LOSS: 0.8300696611404419
LOSS: 0.8297790884971619
LOSS: 0.8294916749000549
LOSS: 0.8292078971862793
LOSS: 0.8289282321929932
LOSS: 0.8286528587341309
LOSS: 0.8283834457397461
LOSS: 0.8281199336051941
LOSS: 0.8278629183769226
LOSS: 0.8276122212409973
LOSS: 0.8273684978485107
LOSS: 0.8271307945251465
LOSS: 1.3422125577926636
LOSS: 1.3260000944137573
LOSS: 1.3093540668487549
LOSS: 1.2924399375915527
LOSS: 1.2753673791885376
LOSS: 1.2582107782363892
LOSS: 1.2409991025924683
LOSS: 1.223751425743103
LOSS: 1.2065047025680542
LOSS: 1.1893086433410645
LOSS: 1.1722155809402466
LOSS: 1.1552780866622925
LOSS: 1.1385524272918701
LOSS: 1.12209951877594
LOSS: 1.1059775352478027
LOSS: 1.0902379751205444
LOSS: 1.0749212503433228
LOSS: 1.060054063796997
LOSS: 1.0456525087356567
LOSS: 1.0317269563674927
LOSS: 1.0182883739471436
LOSS: 1.0053521394729614
LOSS: 0.9929357171058655
LOSS: 0.9810552000999451
LOSS: 0.969721257686615
LOSS: 0.9589361548423767
LOSS: 0.9486945867538452
LOSS: 0.9389846324920654
LOSS: 0.9297931790351868
LOSS: 0.9211074709892273
LOSS: 0.9129162430763245
LOSS: 0.9052088260650635
LOSS: 0.897972822189331
LOSS: 0.8911916613578796
LOSS: 0.8848448395729065
LOSS: 0.8789075016975403
LOSS: 0.8733540773391724
LOSS: 0.868157684803009
LOSS: 0.8632917404174805
LOSS: 0.858732283115387
LOSS: 0.8544585704803467
LOSS: 0.8504559397697449
LOSS: 0.8467113375663757
LOSS: 0.8432122468948364
LOSS: 0.8399443626403809
LOSS: 0.8368926048278809
LOSS: 0.8340430855751038
LOSS: 0.8313805460929871
LOSS: 0.8288906216621399
LOSS: 0.8265606164932251
LOSS: 0.8243809938430786
LOSS: 0.8223412036895752
LOSS: 0.8204327821731567
LOSS: 0.8186507821083069
LOSS: 0.8169859051704407
LOSS: 0.8154318928718567
LOSS: 0.8139808177947998
LOSS: 0.8126274347305298
LOSS: 0.8113634586334229
LOSS: 0.8101825714111328
LOSS: 0.8090768456459045
LOSS: 0.8080395460128784
LOSS: 0.8070639967918396
LOSS: 0.8061457872390747
LOSS: 0.8052794933319092
LOSS: 0.8044638633728027
LOSS: 0.8036959171295166
LOSS: 0.8029736280441284
LOSS: 0.802293598651886
LOSS: 0.8016530871391296
LOSS: 0.801048755645752
LOSS: 0.8004769086837769
LOSS: 0.7999334335327148
LOSS: 0.79941326379776
LOSS: 0.7989120483398438
LOSS: 0.7984257340431213
LOSS: 0.7979515790939331
LOSS: 0.7974863052368164
LOSS: 0.7970283627510071
LOSS: 0.7965772151947021
LOSS: 0.7961323261260986
LOSS: 0.7956951856613159
LOSS: 0.7952670454978943
LOSS: 0.7948484420776367
LOSS: 0.7944415807723999
LOSS: 0.7940468192100525
LOSS: 0.793664276599884
LOSS: 0.7932959198951721
LOSS: 0.7929401993751526
LOSS: 0.7925971746444702
LOSS: 0.7922666668891907
LOSS: 0.7919480800628662
LOSS: 0.7916420102119446
LOSS: 0.7913470268249512
LOSS: 0.791063666343689
LOSS: 0.7907904386520386
LOSS: 0.790527880191803
LOSS: 0.7902747392654419
LOSS: 0.7900301814079285
LOSS: 0.7897931933403015
LOSS: 1.3557078838348389
LOSS: 1.340206503868103
LOSS: 1.3242875337600708
LOSS: 1.3081204891204834
LOSS: 1.2918622493743896
LOSS: 1.2756487131118774
LOSS: 1.2595832347869873
LOSS: 1.2437314987182617
LOSS: 1.2281349897384644
LOSS: 1.2128316164016724
LOSS: 1.1978601217269897
LOSS: 1.1832548379898071
LOSS: 1.1690373420715332
LOSS: 1.1552115678787231
LOSS: 1.1417652368545532
LOSS: 1.128675937652588
LOSS: 1.1159170866012573
LOSS: 1.1034634113311768
LOSS: 1.0912965536117554
LOSS: 1.079405665397644
LOSS: 1.067785382270813
LOSS: 1.056433916091919
LOSS: 1.0453543663024902
LOSS: 1.034559965133667
LOSS: 1.024073839187622
LOSS: 0.8210269212722778
LOSS: 0.819873034954071
LOSS: 0.8187815546989441
LOSS: 0.8177503347396851
LOSS: 0.816775381565094
LOSS: 0.8158526420593262
LOSS: 0.814978301525116
LOSS: 0.8141475319862366
LOSS: 0.8133569955825806
LOSS: 0.8126023411750793
LOSS: 0.8118813037872314
LOSS: 0.8111909031867981
LOSS: 0.8105294704437256
LOSS: 0.8098949790000916
LOSS: 0.8092862963676453
LOSS: 0.8087010383605957
LOSS: 0.8081372380256653
LOSS: 0.8075926303863525
LOSS: 0.8070654273033142
LOSS: 0.8065539598464966
LOSS: 0.8060562610626221
LOSS: 0.805571973323822
LOSS: 0.8051005601882935
LOSS: 0.804641842842102
LOSS: 0.8041960597038269
LOSS: 0.8037633299827576
LOSS: 0.8033434748649597
LOSS: 0.8029366135597229
LOSS: 0.8025418519973755
LOSS: 0.802158534526825
LOSS: 0.8017852306365967
LOSS: 0.8014218211174011
LOSS: 0.8010672926902771
LOSS: 0.8007200956344604
LOSS: 0.8003808259963989
LOSS: 0.800048291683197
LOSS: 0.7997227311134338
LOSS: 1.348430871963501
LOSS: 1.3322432041168213
LOSS: 1.315609097480774
LOSS: 1.2986589670181274
LOSS: 1.281427025794983
LOSS: 1.2639305591583252
LOSS: 1.2462081909179688
LOSS: 1.2283028364181519
LOSS: 1.2102543115615845
LOSS: 1.1921156644821167
LOSS: 1.1739633083343506
LOSS: 1.1558964252471924
LOSS: 1.1380289793014526
LOSS: 1.1204781532287598
LOSS: 1.1033563613891602
LOSS: 1.0867680311203003
LOSS: 1.0708043575286865
LOSS: 1.055539846420288
LOSS: 1.041031002998352
LOSS: 1.0273146629333496
LOSS: 1.0144109725952148
LOSS: 1.0023243427276611
LOSS: 0.991043210029602
LOSS: 0.9805404543876648
LOSS: 0.9707735180854797
LOSS: 0.9616915583610535
LOSS: 0.9532375335693359
LOSS: 0.9453538060188293
LOSS: 0.9379890561103821
LOSS: 0.9311005473136902
LOSS: 0.9246534705162048
LOSS: 0.9186190366744995
LOSS: 0.9129730463027954
LOSS: 0.9076955914497375
LOSS: 0.9027723670005798
LOSS: 0.8981869220733643
LOSS: 0.8939252495765686
LOSS: 0.8899726271629333
LOSS: 0.8863096237182617
LOSS: 0.8829144239425659
LOSS: 0.8797648549079895
LOSS: 0.8768379092216492
LOSS: 0.8741108775138855
LOSS: 0.8715643882751465
LOSS: 0.8691811561584473
LOSS: 0.8669435381889343
LOSS: 0.8648379445075989
LOSS: 0.8628509640693665
LOSS: 0.8609710931777954
LOSS: 0.8591869473457336
LOSS: 0.8574900031089783
LOSS: 0.8558725714683533
LOSS: 0.8543294668197632
LOSS: 0.8528576493263245
LOSS: 0.8514544367790222
LOSS: 0.8501187562942505
LOSS: 0.8488490581512451
LOSS: 0.8476426601409912
LOSS: 0.8464978337287903
LOSS: 0.8454151749610901
LOSS: 0.8443926572799683
LOSS: 0.8434314727783203
LOSS: 0.8425319194793701
LOSS: 0.8416941165924072
LOSS: 0.8409180045127869
LOSS: 0.8402002453804016
LOSS: 0.8395379185676575
LOSS: 0.8389233350753784
LOSS: 0.838348925113678
LOSS: 0.8378042578697205
LOSS: 0.8372805714607239
LOSS: 0.836772620677948
LOSS: 0.8362751007080078
LOSS: 0.8357858657836914
LOSS: 0.8353040218353271
LOSS: 0.8348309397697449
LOSS: 0.8343678712844849
LOSS: 0.8339179754257202
LOSS: 0.8334835171699524
LOSS: 0.833068311214447
LOSS: 0.8326729536056519
LOSS: 0.832297682762146
LOSS: 0.8319422006607056
LOSS: 0.8316039443016052
LOSS: 0.8312795758247375
LOSS: 0.8309668302536011
LOSS: 0.8306620717048645
LOSS: 0.8303637504577637
LOSS: 0.8300696611404419
LOSS: 0.8297790884971619
LOSS: 0.8294916749000549
LOSS: 0.8292078971862793
LOSS: 0.8289282321929932
LOSS: 0.8286528587341309
LOSS: 0.8283834457397461
LOSS: 0.8281199336051941
LOSS: 0.8278629183769226
LOSS: 0.8276122212409973
LOSS: 0.8273684978485107
LOSS: 0.8271307945251465
LOSS: 1.3479912281036377
LOSS: 1.3321782350540161
LOSS: 1.3159279823303223
LOSS: 1.2993505001068115
LOSS: 1.2825498580932617
LOSS: 1.265608310699463
LOSS: 1.248579740524292
LOSS: 1.2314943075180054
LOSS: 1.2143813371658325
LOSS: 1.1972949504852295
LOSS: 1.1803151369094849
LOSS: 1.163534164428711
LOSS: 1.1470447778701782
LOSS: 1.1309287548065186
LOSS: 1.1152504682540894
LOSS: 1.1000632047653198
LOSS: 1.0854109525680542
LOSS: 1.0713309049606323
LOSS: 1.0578522682189941
LOSS: 1.044995665550232
LOSS: 1.0327703952789307
LOSS: 1.0211756229400635
LOSS: 1.0101990699768066
LOSS: 0.9998180866241455
LOSS: 0.9900013208389282
LOSS: 0.9807141423225403
LOSS: 0.9719242453575134
LOSS: 0.9636036157608032
LOSS: 0.9557304978370667
LOSS: 0.9482861161231995
LOSS: 0.9412572383880615
LOSS: 0.9346346259117126
LOSS: 0.9284120798110962
LOSS: 0.9225828051567078
LOSS: 0.9171369075775146
LOSS: 0.9120602607727051
LOSS: 0.9073350429534912
LOSS: 0.902937650680542
LOSS: 0.898842990398407
LOSS: 0.8950231075286865
LOSS: 0.8914502859115601
LOSS: 0.8881012797355652
LOSS: 0.884955108165741
LOSS: 0.8819939494132996
LOSS: 0.8792002201080322
LOSS: 0.8765627145767212
LOSS: 0.8740693926811218
LOSS: 0.8717114329338074
LOSS: 0.8694827556610107
LOSS: 0.8673794865608215
LOSS: 0.8653990626335144
LOSS: 0.8635416626930237
LOSS: 0.8618058562278748
LOSS: 0.8601881861686707
LOSS: 0.8586845397949219
LOSS: 0.8572890758514404
LOSS: 0.8559921979904175
LOSS: 0.8547852635383606
LOSS: 0.8536591529846191
LOSS: 0.8526058197021484
LOSS: 0.8516169786453247
LOSS: 0.8506845831871033
LOSS: 0.8498031497001648
LOSS: 0.848966658115387
LOSS: 0.8481701016426086
LOSS: 0.8474084734916687
LOSS: 0.8466770648956299
LOSS: 0.8459734320640564
LOSS: 0.8452935218811035
LOSS: 0.844635546207428
LOSS: 0.8439968824386597
LOSS: 0.8433757424354553
LOSS: 0.8427715301513672
LOSS: 0.842183530330658
LOSS: 0.8416114449501038
LOSS: 0.8410571813583374
LOSS: 0.8405206203460693
LOSS: 0.8400023579597473
LOSS: 0.8395030498504639
LOSS: 0.8390228152275085
LOSS: 0.8385606408119202
LOSS: 0.8381158709526062
LOSS: 0.8376865983009338
LOSS: 0.8372714519500732
LOSS: 0.8368695378303528
LOSS: 0.8364816308021545
LOSS: 0.8361114859580994
LOSS: 0.8357619047164917
LOSS: 0.8354347944259644
LOSS: 0.8351243138313293
LOSS: 0.8348203897476196
LOSS: 0.8345118761062622
LOSS: 0.8341973423957825
LOSS: 0.8338820934295654
LOSS: 0.833574116230011
LOSS: 0.8332769274711609
LOSS: 0.8329924941062927
LOSS: 0.8327175378799438
LOSS: 0.8324517607688904
LOSS: 0.8321911096572876
LOSS: 1.3386679887771606
LOSS: 1.3219122886657715
LOSS: 1.3048030138015747
LOSS: 1.2874250411987305
LOSS: 1.2698472738265991
LOSS: 1.2520852088928223
LOSS: 1.234124779701233
LOSS: 1.2159806489944458
LOSS: 1.1977035999298096
LOSS: 1.1793711185455322
LOSS: 1.161070704460144
LOSS: 1.142888069152832
LOSS: 1.124915599822998
LOSS: 1.1072547435760498
LOSS: 1.090009093284607
LOSS: 1.0732799768447876
LOSS: 1.0571564435958862
LOSS: 1.041709542274475
LOSS: 1.0269891023635864
LOSS: 1.0130271911621094
LOSS: 0.9998401403427124
LOSS: 0.9874307513237
LOSS: 0.9757846593856812
LOSS: 0.9648716449737549
LOSS: 0.95464688539505
LOSS: 0.9450570940971375
LOSS: 0.9360466003417969
LOSS: 0.927563488483429
LOSS: 0.9195610880851746
LOSS: 0.9119968414306641
LOSS: 0.9048323035240173
LOSS: 0.8980385661125183
LOSS: 0.8915935158729553
LOSS: 0.8854838013648987
LOSS: 0.879699170589447
LOSS: 0.8742340207099915
LOSS: 0.8690822124481201
LOSS: 0.8642371296882629
LOSS: 0.8596897721290588
LOSS: 0.8554283976554871
LOSS: 0.8514371514320374
LOSS: 0.8476957082748413
LOSS: 0.8441845774650574
LOSS: 0.8408793807029724
LOSS: 0.837759792804718
LOSS: 0.8348055481910706
LOSS: 0.8320006728172302
LOSS: 0.8293332457542419
LOSS: 0.8267961740493774
LOSS: 0.8243856430053711
LOSS: 0.8221023678779602
LOSS: 0.8199471235275269
LOSS: 0.8179242014884949
LOSS: 0.8160325288772583
LOSS: 0.8142724633216858
LOSS: 0.8126410841941833
LOSS: 0.8111346960067749
LOSS: 0.8097458481788635
LOSS: 0.8084679841995239
LOSS: 0.8072925806045532
LOSS: 0.8062102198600769
LOSS: 0.805210530757904
LOSS: 0.804282546043396
LOSS: 0.8034154772758484
LOSS: 0.8025996685028076
LOSS: 0.8018286824226379
LOSS: 0.801097571849823
LOSS: 0.800403892993927
LOSS: 0.7997446060180664
LOSS: 0.7991184592247009
LOSS: 0.7985221743583679
LOSS: 0.7979527115821838
LOSS: 0.7974066734313965
LOSS: 0.7968799471855164
LOSS: 0.7963690161705017
LOSS: 0.7958713173866272
LOSS: 0.7953856587409973
LOSS: 0.7949110865592957
LOSS: 0.7944480776786804
LOSS: 0.7939983010292053
LOSS: 0.7935609817504883
LOSS: 0.7931379079818726
LOSS: 0.7927291393280029
LOSS: 0.7923347353935242
LOSS: 0.7919543981552124
LOSS: 0.7915890216827393
LOSS: 0.7912374138832092
LOSS: 0.7909001111984253
LOSS: 0.7905780076980591
LOSS: 0.7902696132659912
LOSS: 0.7899758219718933
LOSS: 0.7896962761878967
LOSS: 0.8509888648986816
LOSS: 0.8506726622581482
LOSS: 0.8503634929656982
LOSS: 0.8500614762306213
LOSS: 0.849765419960022
LOSS: 0.8494738936424255
LOSS: 0.8491864800453186
LOSS: 1.3422125577926636
LOSS: 1.3260000944137573
LOSS: 1.3093540668487549
LOSS: 1.2924399375915527
LOSS: 1.2753673791885376
LOSS: 1.2582107782363892
LOSS: 1.2409991025924683
LOSS: 1.223751425743103
LOSS: 1.2065047025680542
LOSS: 1.1893086433410645
LOSS: 1.1722155809402466
LOSS: 1.1552780866622925
LOSS: 1.1385524272918701
LOSS: 1.12209951877594
LOSS: 1.1059775352478027
LOSS: 1.0902379751205444
LOSS: 1.0749212503433228
LOSS: 1.060054063796997
LOSS: 1.0456525087356567
LOSS: 1.0317269563674927
LOSS: 1.0182883739471436
LOSS: 1.0053521394729614
LOSS: 0.9929357171058655
LOSS: 0.9810552000999451
LOSS: 0.969721257686615
LOSS: 0.9589361548423767
LOSS: 0.9486945867538452
LOSS: 0.9389846324920654
LOSS: 0.9297931790351868
LOSS: 0.9211074709892273
LOSS: 0.9129162430763245
LOSS: 0.9052088260650635
LOSS: 0.897972822189331
LOSS: 0.8911916613578796
LOSS: 0.8848448395729065
LOSS: 0.8789075016975403
LOSS: 0.8733540773391724
LOSS: 0.868157684803009
LOSS: 0.8632917404174805
LOSS: 0.858732283115387
LOSS: 0.8544585704803467
LOSS: 0.8504559397697449
LOSS: 0.8467113375663757
LOSS: 0.8432122468948364
LOSS: 0.8399443626403809
LOSS: 0.8368926048278809
LOSS: 0.8340430855751038
LOSS: 0.8313805460929871
LOSS: 0.8288906216621399
LOSS: 0.8265606164932251
LOSS: 0.8243809938430786
LOSS: 0.8223412036895752
LOSS: 0.8204327821731567
LOSS: 0.8186507821083069
LOSS: 0.8169859051704407
LOSS: 0.8154318928718567
LOSS: 0.8139808177947998
LOSS: 0.8126274347305298
LOSS: 0.8113634586334229
LOSS: 0.8101825714111328
LOSS: 0.8090768456459045
LOSS: 0.8080395460128784
LOSS: 0.8070639967918396
LOSS: 0.8061457872390747
LOSS: 0.8052794933319092
LOSS: 0.8044638633728027
LOSS: 0.8036959171295166
LOSS: 0.8029736280441284
LOSS: 0.802293598651886
LOSS: 0.8016530871391296
LOSS: 0.801048755645752
LOSS: 0.8004769086837769
LOSS: 0.7999334335327148
LOSS: 0.79941326379776
LOSS: 0.7989120483398438
LOSS: 0.7984257340431213
LOSS: 0.7979515790939331
LOSS: 0.7974863052368164
LOSS: 0.7970283627510071
LOSS: 0.7965772151947021
LOSS: 0.7961323261260986
LOSS: 0.7956951856613159
LOSS: 0.7952670454978943
LOSS: 0.7948484420776367
LOSS: 0.7944415807723999
LOSS: 0.7940468192100525
LOSS: 0.793664276599884
LOSS: 0.7932959198951721
LOSS: 0.7929401993751526
LOSS: 0.7925971746444702
LOSS: 0.7922666668891907
LOSS: 0.7919480800628662
LOSS: 0.7916420102119446
LOSS: 0.7913470268249512
LOSS: 0.791063666343689
LOSS: 0.7907904386520386
LOSS: 0.790527880191803
LOSS: 0.7902747392654419
LOSS: 0.7900301814079285
LOSS: 0.7897931933403015
LOSS: 1.3524593114852905
LOSS: 1.3362447023391724
LOSS: 1.3195140361785889
LOSS: 1.3023697137832642
LOSS: 1.2848670482635498
LOSS: 1.26705801486969
LOSS: 1.2490226030349731
LOSS: 1.2308542728424072
LOSS: 1.2126621007919312
LOSS: 1.194564700126648
LOSS: 1.1766815185546875
LOSS: 1.159127950668335
LOSS: 1.142015814781189
LOSS: 1.1254489421844482
LOSS: 1.1095157861709595
LOSS: 1.09428870677948
LOSS: 1.0798248052597046
LOSS: 1.0661665201187134
LOSS: 1.053337574005127
LOSS: 1.0413408279418945
LOSS: 1.030157208442688
LOSS: 1.0197497606277466
LOSS: 1.0100653171539307
LOSS: 1.0010416507720947
LOSS: 0.9926106333732605
LOSS: 0.9847062826156616
LOSS: 0.9772686958312988
LOSS: 0.9702459573745728
LOSS: 0.9635953307151794
LOSS: 0.9572818279266357
LOSS: 0.9512794613838196
LOSS: 0.9455694556236267
LOSS: 0.9401384592056274
LOSS: 0.9349780082702637
LOSS: 0.9300827383995056
LOSS: 0.9254485368728638
LOSS: 0.9210718274116516
LOSS: 0.9169489145278931
LOSS: 0.9130727052688599
LOSS: 0.9094380140304565
LOSS: 0.906038224697113
LOSS: 0.9028639793395996
LOSS: 0.8999031782150269
LOSS: 0.897144079208374
LOSS: 0.8945706486701965
LOSS: 0.8921669721603394
LOSS: 0.8899179697036743
LOSS: 0.8878094553947449
LOSS: 0.8858299851417542
LOSS: 0.883969783782959
LOSS: 0.8822211027145386
LOSS: 0.8805766105651855
LOSS: 0.8790302276611328
LOSS: 0.8775752782821655
LOSS: 0.8762052655220032
LOSS: 0.8749130368232727
LOSS: 0.8736916184425354
LOSS: 0.872534990310669
LOSS: 0.871437132358551
LOSS: 0.8703922033309937
LOSS: 0.8693974614143372
LOSS: 0.8684499859809875
LOSS: 0.8675482273101807
LOSS: 0.8666918277740479
LOSS: 0.8658795356750488
LOSS: 0.8651072978973389
LOSS: 0.8643673062324524
LOSS: 0.8636502027511597
LOSS: 0.8629499673843384
LOSS: 0.8622652292251587
LOSS: 0.8615981936454773
LOSS: 0.8609512448310852
LOSS: 0.8603265881538391
LOSS: 0.8597242832183838
LOSS: 0.859143078327179
LOSS: 0.8585818409919739
LOSS: 0.8580387830734253
LOSS: 0.8575121164321899
LOSS: 0.8570024371147156
LOSS: 0.8565079569816589
LOSS: 0.8560284376144409
LOSS: 0.8555630445480347
LOSS: 0.8551119565963745
LOSS: 0.8546751737594604
LOSS: 0.8542525768280029
LOSS: 0.8538437485694885
LOSS: 0.8534481525421143
LOSS: 0.8530654907226562
LOSS: 0.852694571018219
LOSS: 0.8523350954055786
LOSS: 0.851985514163971
LOSS: 0.8516451120376587
LOSS: 0.851313054561615
LOSS: 0.8509888648986816
LOSS: 0.8506726622581482
LOSS: 0.8503634929656982
LOSS: 0.8500614762306213
LOSS: 0.849765419960022
LOSS: 0.8494738936424255
LOSS: 0.8491864800453186
LOSS: 1.3527240753173828
LOSS: 1.3371007442474365
LOSS: 1.3210976123809814
LOSS: 1.3047511577606201
LOSS: 1.288081169128418
LOSS: 1.2710888385772705
LOSS: 1.2537964582443237
LOSS: 1.2362695932388306
LOSS: 1.218593716621399
LOSS: 1.2008650302886963
LOSS: 1.1831871271133423
LOSS: 1.16566801071167
LOSS: 1.1484140157699585
LOSS: 1.1315231323242188
LOSS: 1.11508309841156
LOSS: 1.0991708040237427
LOSS: 1.0838500261306763
LOSS: 1.0691732168197632
LOSS: 1.0551804304122925
LOSS: 1.0418963432312012
LOSS: 1.029329538345337
LOSS: 1.0174756050109863
LOSS: 1.0063189268112183
LOSS: 0.9958360195159912
LOSS: 0.9859960675239563
LOSS: 0.9767632484436035
LOSS: 0.9680966734886169
LOSS: 0.9599509239196777
LOSS: 0.9522810578346252
LOSS: 0.9450430274009705
LOSS: 0.9381980895996094
LOSS: 0.9317152500152588
LOSS: 0.9255735874176025
LOSS: 0.919761061668396
LOSS: 0.9142737984657288
LOSS: 0.9091112613677979
LOSS: 0.9042714238166809
LOSS: 0.8997504115104675
LOSS: 0.8955398201942444
LOSS: 0.89162278175354
LOSS: 0.8879801034927368
LOSS: 0.8845880031585693
LOSS: 0.8814224600791931
LOSS: 0.878456711769104
LOSS: 0.87566739320755
LOSS: 0.873032808303833
LOSS: 0.870535135269165
LOSS: 0.8681635856628418
LOSS: 0.8659098744392395
LOSS: 0.863768994808197
LOSS: 0.8617388606071472
LOSS: 0.859816312789917
LOSS: 0.8579980731010437
LOSS: 0.8562812209129333
LOSS: 0.854660153388977
LOSS: 0.8531302809715271
LOSS: 0.851685106754303
LOSS: 0.8503209948539734
LOSS: 0.8490328192710876
LOSS: 0.8478166460990906
LOSS: 0.8466702699661255
LOSS: 0.845589280128479
LOSS: 0.8445726633071899
LOSS: 0.8436164259910583
LOSS: 0.8427178263664246
LOSS: 0.8418716192245483
LOSS: 0.8410724997520447
LOSS: 0.8403136730194092
LOSS: 0.8395876884460449
LOSS: 0.8388858437538147
LOSS: 0.8381996154785156
LOSS: 0.8375231623649597
LOSS: 0.8368494510650635
LOSS: 0.8361736536026001
LOSS: 0.8354925513267517
LOSS: 0.8348026275634766
LOSS: 0.8341037034988403
LOSS: 0.8333972692489624
LOSS: 0.8326873183250427
LOSS: 0.8319782018661499
LOSS: 0.8312767148017883
LOSS: 0.8305888772010803
LOSS: 0.8299208879470825
LOSS: 0.8292766213417053
LOSS: 0.8286584615707397
LOSS: 0.82806795835495
LOSS: 0.8275048732757568
LOSS: 0.8269691467285156
LOSS: 0.8264593482017517
LOSS: 0.8259747624397278
LOSS: 0.825514554977417
LOSS: 0.8250771164894104
LOSS: 0.8246620297431946
LOSS: 0.8242673873901367
LOSS: 0.8238927721977234
LOSS: 0.8235369920730591
LOSS: 0.823198676109314
LOSS: 0.8228769898414612
LOSS: 0.8225690722465515
LOSS: 0.8222753405570984
LOSS: 1.3527240753173828
LOSS: 1.3371007442474365
LOSS: 1.3210976123809814
LOSS: 1.3047511577606201
LOSS: 1.288081169128418
LOSS: 1.2710888385772705
LOSS: 1.2537964582443237
LOSS: 1.2362695932388306
LOSS: 1.218593716621399
LOSS: 1.2008650302886963
LOSS: 1.1831871271133423
LOSS: 1.16566801071167
LOSS: 1.1484140157699585
LOSS: 1.1315231323242188
LOSS: 1.11508309841156
LOSS: 1.0991708040237427
LOSS: 1.0838500261306763
LOSS: 1.0691732168197632
LOSS: 1.0551804304122925
LOSS: 1.0418963432312012
LOSS: 1.029329538345337
LOSS: 1.0174756050109863
LOSS: 1.0063189268112183
LOSS: 0.7746965289115906
LOSS: 0.7743651866912842
LOSS: 0.7740446329116821
LOSS: 0.7737320065498352
LOSS: 0.773428201675415
LOSS: 0.7731314897537231
LOSS: 0.7728418111801147
LOSS: 0.7725586891174316
LOSS: 0.7722819447517395
LOSS: 1.344185471534729
LOSS: 1.3271397352218628
LOSS: 1.3095903396606445
LOSS: 1.2916308641433716
LOSS: 1.2733049392700195
LOSS: 1.2546266317367554
LOSS: 1.235643982887268
LOSS: 1.216449499130249
LOSS: 1.1971515417099
LOSS: 1.177866816520691
LOSS: 1.1587210893630981
LOSS: 1.1398441791534424
LOSS: 1.1213617324829102
LOSS: 1.1033867597579956
LOSS: 1.0860133171081543
LOSS: 1.069320559501648
LOSS: 1.0533713102340698
LOSS: 1.0382095575332642
LOSS: 1.0238590240478516
LOSS: 1.0103230476379395
LOSS: 0.9975890517234802
LOSS: 0.9856289625167847
LOSS: 0.9744018912315369
LOSS: 0.9638546705245972
LOSS: 0.9539273381233215
LOSS: 0.9445595741271973
LOSS: 0.9356967210769653
LOSS: 0.9272909164428711
LOSS: 0.9193034768104553
LOSS: 0.9117067456245422
LOSS: 0.9044826030731201
LOSS: 0.8976199626922607
LOSS: 0.8911099433898926
LOSS: 0.8849427700042725
LOSS: 0.8791102766990662
LOSS: 0.8736003041267395
LOSS: 0.8684014678001404
LOSS: 0.8635004758834839
LOSS: 0.8588873744010925
LOSS: 0.8545554280281067
LOSS: 0.8505003452301025
LOSS: 0.8467211723327637
LOSS: 0.843214750289917
LOSS: 0.8399759531021118
LOSS: 0.8369964361190796
LOSS: 0.8342620730400085
LOSS: 0.8317559361457825
LOSS: 0.8294580578804016
LOSS: 0.8273504972457886
LOSS: 0.8254144787788391
LOSS: 0.8236358761787415
LOSS: 0.8220038414001465
LOSS: 0.8205068707466125
LOSS: 0.8191357254981995
LOSS: 0.8178836703300476
LOSS: 0.8167378306388855
LOSS: 0.815687894821167
LOSS: 0.8147208094596863
LOSS: 0.8138220906257629
LOSS: 0.812980592250824
LOSS: 0.812187135219574
LOSS: 0.8114331960678101
LOSS: 0.8107147812843323
LOSS: 0.8100295662879944
LOSS: 0.8093757033348083
LOSS: 0.8087526559829712
LOSS: 0.8081601858139038
LOSS: 0.8075969219207764
LOSS: 0.8070605993270874
LOSS: 0.8065505623817444
LOSS: 0.8060624599456787
LOSS: 0.8055938482284546
LOSS: 0.805142879486084
LOSS: 0.8047065138816833
LOSS: 0.8042827248573303
LOSS: 0.8038696646690369
LOSS: 0.8034664392471313
LOSS: 0.8030731081962585
LOSS: 0.802689790725708
LOSS: 0.8023163080215454
LOSS: 0.8019534945487976
LOSS: 0.8016019463539124
LOSS: 0.8012617230415344
LOSS: 0.8009330034255981
LOSS: 0.8006153702735901
LOSS: 0.8003074526786804
LOSS: 0.8000085949897766
LOSS: 0.7997177243232727
LOSS: 0.7994344830513
LOSS: 0.799159049987793
LOSS: 0.7988900542259216
LOSS: 0.7986283302307129
LOSS: 0.798374354839325
LOSS: 0.798126757144928
LOSS: 0.7978872656822205
LOSS: 0.7976545691490173
LOSS: 0.7974284291267395
LOSS: 0.7972093820571899
LOSS: 0.7969961166381836
LOSS: 0.7967888116836548
LOSS: 1.3436731100082397
LOSS: 1.3277162313461304
LOSS: 1.311358094215393
LOSS: 1.2946051359176636
LOSS: 1.2775224447250366
LOSS: 1.2601392269134521
LOSS: 1.242506980895996
LOSS: 1.2247127294540405
LOSS: 1.2068581581115723
LOSS: 1.18904709815979
LOSS: 1.1713781356811523
LOSS: 1.1539431810379028
LOSS: 1.1368342638015747
LOSS: 1.120147705078125
LOSS: 1.1039791107177734
LOSS: 1.0884140729904175
LOSS: 1.0735210180282593
LOSS: 1.0593483448028564
LOSS: 1.0459250211715698
LOSS: 1.0332599878311157
LOSS: 1.0213433504104614
LOSS: 1.0101534128189087
LOSS: 0.99965500831604
LOSS: 0.9898019433021545
LOSS: 0.9805388450622559
LOSS: 0.9718076586723328
LOSS: 0.9635511040687561
LOSS: 0.9557156562805176
LOSS: 0.9482559561729431
LOSS: 0.941133975982666
LOSS: 0.9343203902244568
LOSS: 0.9277962446212769
LOSS: 0.9215499758720398
LOSS: 0.9155769944190979
LOSS: 0.9098774194717407
LOSS: 0.9044559597969055
LOSS: 0.8993172645568848
LOSS: 0.894467294216156
LOSS: 0.8899052739143372
LOSS: 0.8856269717216492
LOSS: 0.8816189765930176
LOSS: 0.8778631091117859
LOSS: 0.8743391036987305
LOSS: 0.8710243105888367
LOSS: 0.867900013923645
LOSS: 0.8649506568908691
LOSS: 0.8621639013290405
LOSS: 0.8595300912857056
LOSS: 0.8570412397384644
LOSS: 0.8546900153160095
LOSS: 0.8524696826934814
LOSS: 0.8503766059875488
LOSS: 0.848408579826355
LOSS: 0.8465638756752014
LOSS: 0.8448423743247986
LOSS: 0.8432410955429077
LOSS: 0.8417564630508423
LOSS: 0.8403833508491516
LOSS: 0.8391140699386597
LOSS: 0.8379397392272949
LOSS: 0.8368521928787231
LOSS: 0.8358411192893982
LOSS: 0.8348977565765381
LOSS: 0.8340139985084534
LOSS: 0.8331832885742188
LOSS: 0.8323996067047119
LOSS: 0.8316587805747986
LOSS: 0.8309565186500549
LOSS: 0.8302889466285706
LOSS: 0.8296529054641724
LOSS: 0.8290430307388306
LOSS: 0.8284560441970825
LOSS: 0.8278882503509521
LOSS: 0.827336311340332
LOSS: 0.826798141002655
LOSS: 0.8262722492218018
LOSS: 0.82575923204422
LOSS: 0.8252581357955933
LOSS: 0.824769914150238
LOSS: 0.8242964148521423
LOSS: 0.823838472366333
LOSS: 0.8233969211578369
LOSS: 0.8229726552963257
LOSS: 0.8225648999214172
LOSS: 0.8221735954284668
LOSS: 0.8217964172363281
LOSS: 0.8214321732521057
LOSS: 0.8210793137550354
LOSS: 0.8207363486289978
LOSS: 0.8204015493392944
LOSS: 0.8200744390487671
LOSS: 0.819754421710968
LOSS: 0.8194407820701599
LOSS: 0.8191336989402771
LOSS: 0.8188319802284241
LOSS: 0.8185364603996277
LOSS: 0.8182451725006104
LOSS: 0.8179576396942139
LOSS: 0.8176741003990173
LOSS: 0.81739342212677
LOSS: 1.3557078838348389
LOSS: 1.340206503868103
LOSS: 1.3242875337600708
LOSS: 1.3081204891204834
LOSS: 1.2918622493743896
LOSS: 1.2756487131118774
LOSS: 1.2595832347869873
LOSS: 1.2437314987182617
LOSS: 1.2281349897384644
LOSS: 1.2128316164016724
LOSS: 1.1978601217269897
LOSS: 1.1832548379898071
LOSS: 1.1690373420715332
LOSS: 1.1552115678787231
LOSS: 1.1417652368545532
LOSS: 1.128675937652588
LOSS: 1.1159170866012573
LOSS: 1.1034634113311768
LOSS: 1.0912965536117554
LOSS: 1.079405665397644
LOSS: 1.067785382270813
LOSS: 1.056433916091919
LOSS: 1.0453543663024902
LOSS: 1.034559965133667
LOSS: 1.024073839187622
LOSS: 1.0139248371124268
LOSS: 1.004143238067627
LOSS: 0.9947525858879089
LOSS: 0.9857690930366516
LOSS: 0.9771985411643982
LOSS: 0.9690367579460144
LOSS: 0.961273729801178
LOSS: 0.9538989067077637
LOSS: 0.9469020962715149
LOSS: 0.9402760863304138
LOSS: 0.9340128302574158
LOSS: 0.9281028509140015
LOSS: 0.9225327372550964
LOSS: 0.9172853231430054
LOSS: 0.9123414158821106
LOSS: 0.9076800346374512
LOSS: 0.9032785296440125
LOSS: 0.8991160988807678
LOSS: 0.8951722979545593
LOSS: 0.8914291262626648
LOSS: 0.8878708481788635
LOSS: 0.8844851851463318
LOSS: 0.8812628388404846
LOSS: 0.8781963586807251
LOSS: 0.8752794861793518
LOSS: 0.8725070953369141
LOSS: 0.8698726892471313
LOSS: 0.8673710227012634
LOSS: 0.8649958968162537
LOSS: 0.8627430200576782
LOSS: 0.8606076240539551
LOSS: 0.8585842847824097
LOSS: 0.856666624546051
LOSS: 0.8548485636711121
LOSS: 0.8531222343444824
LOSS: 0.8514809608459473
LOSS: 0.8499197363853455
LOSS: 0.8484326601028442
LOSS: 0.8470155596733093
LOSS: 0.8456630706787109
LOSS: 0.8443728685379028
LOSS: 0.8431434035301208
LOSS: 0.8419743776321411
LOSS: 0.8408641219139099
LOSS: 0.839812695980072
LOSS: 0.8388189673423767
LOSS: 0.8378817439079285
LOSS: 0.8369995951652527
LOSS: 0.8361708521842957
LOSS: 0.8353904485702515
LOSS: 0.8346551060676575
LOSS: 0.8339586853981018
LOSS: 0.8332976698875427
LOSS: 0.8326660394668579
LOSS: 0.8320611715316772
LOSS: 0.8314785361289978
LOSS: 0.8309162259101868
LOSS: 0.8303698897361755
LOSS: 0.8298379182815552
LOSS: 0.8293169140815735
LOSS: 0.8288058042526245
LOSS: 0.8283027410507202
LOSS: 0.827806830406189
LOSS: 0.8273173570632935
LOSS: 0.8268348574638367
LOSS: 0.8263586759567261
LOSS: 0.8258881568908691
LOSS: 0.8254233002662659
LOSS: 0.8249645829200745
LOSS: 0.8245131373405457
LOSS: 0.8240678906440735
LOSS: 0.8236290812492371
LOSS: 0.8231972455978394
LOSS: 0.8227726221084595
LOSS: 0.8223544955253601
LOSS: 1.3487545251846313
LOSS: 1.3329179286956787
LOSS: 1.3166556358337402
LOSS: 1.3001346588134766
LOSS: 1.283534049987793
LOSS: 1.2670035362243652
LOSS: 1.2506613731384277
LOSS: 1.2345956563949585
LOSS: 1.2188611030578613
LOSS: 1.2034873962402344
LOSS: 1.1884880065917969
LOSS: 1.1738691329956055
LOSS: 1.1596382856369019
LOSS: 1.1458016633987427
LOSS: 1.1323517560958862
LOSS: 1.119261622428894
LOSS: 1.1064894199371338
LOSS: 1.0939894914627075
LOSS: 1.0817182064056396
LOSS: 1.0696380138397217
LOSS: 0.789429783821106
LOSS: 0.7891759276390076
LOSS: 0.788933515548706
LOSS: 0.7887001037597656
LOSS: 0.7884747982025146
LOSS: 0.788255512714386
LOSS: 0.7880414724349976
LOSS: 0.78783118724823
LOSS: 1.3386679887771606
LOSS: 1.3219122886657715
LOSS: 1.3048030138015747
LOSS: 1.2874250411987305
LOSS: 1.2698472738265991
LOSS: 1.2520852088928223
LOSS: 1.234124779701233
LOSS: 1.2159806489944458
LOSS: 1.1977035999298096
LOSS: 1.1793711185455322
LOSS: 1.161070704460144
LOSS: 1.142888069152832
LOSS: 1.124915599822998
LOSS: 1.1072547435760498
LOSS: 1.090009093284607
LOSS: 1.0732799768447876
LOSS: 1.0571564435958862
LOSS: 1.041709542274475
LOSS: 1.0269891023635864
LOSS: 1.0130271911621094
LOSS: 0.9998401403427124
LOSS: 0.9874307513237
LOSS: 0.9757846593856812
LOSS: 0.9648716449737549
LOSS: 0.95464688539505
LOSS: 0.9450570940971375
LOSS: 0.9360466003417969
LOSS: 0.927563488483429
LOSS: 0.9195610880851746
LOSS: 0.9119968414306641
LOSS: 0.9048323035240173
LOSS: 0.8980385661125183
LOSS: 0.8915935158729553
LOSS: 0.8854838013648987
LOSS: 0.879699170589447
LOSS: 0.8742340207099915
LOSS: 0.8690822124481201
LOSS: 0.8642371296882629
LOSS: 0.8596897721290588
LOSS: 0.8554283976554871
LOSS: 0.8514371514320374
LOSS: 0.8476957082748413
LOSS: 0.8441845774650574
LOSS: 0.8408793807029724
LOSS: 0.837759792804718
LOSS: 0.8348055481910706
LOSS: 0.8320006728172302
LOSS: 0.8293332457542419
LOSS: 0.8267961740493774
LOSS: 0.8243856430053711
LOSS: 0.8221023678779602
LOSS: 0.8199471235275269
LOSS: 0.8179242014884949
LOSS: 0.8160325288772583
LOSS: 0.8142724633216858
LOSS: 0.8126410841941833
LOSS: 0.8111346960067749
LOSS: 0.8097458481788635
LOSS: 0.8084679841995239
LOSS: 0.8072925806045532
LOSS: 0.8062102198600769
LOSS: 0.805210530757904
LOSS: 0.804282546043396
LOSS: 0.8034154772758484
LOSS: 0.8025996685028076
LOSS: 0.8018286824226379
LOSS: 0.801097571849823
LOSS: 0.800403892993927
LOSS: 0.7997446060180664
LOSS: 0.7991184592247009
LOSS: 0.7985221743583679
LOSS: 0.7979527115821838
LOSS: 0.7974066734313965
LOSS: 0.7968799471855164
LOSS: 0.7963690161705017
LOSS: 0.7958713173866272
LOSS: 0.7953856587409973
LOSS: 0.7949110865592957
LOSS: 0.7944480776786804
LOSS: 0.7939983010292053
LOSS: 0.7935609817504883
LOSS: 0.7931379079818726
LOSS: 0.7927291393280029
LOSS: 0.7923347353935242
LOSS: 0.7919543981552124
LOSS: 0.7915890216827393
LOSS: 0.7912374138832092
LOSS: 0.7909001111984253
LOSS: 0.7905780076980591
LOSS: 0.7902696132659912
LOSS: 0.7899758219718933
LOSS: 0.7896962761878967
LOSS: 0.789429783821106
LOSS: 0.7891759276390076
LOSS: 0.788933515548706
LOSS: 0.7887001037597656
LOSS: 0.7884747982025146
LOSS: 0.788255512714386
LOSS: 0.7880414724349976
LOSS: 0.78783118724823
LOSS: 1.3436731100082397
LOSS: 1.3277162313461304
LOSS: 1.311358094215393
LOSS: 1.2946051359176636
LOSS: 1.2775224447250366
LOSS: 1.2601392269134521
LOSS: 1.242506980895996
LOSS: 1.2247127294540405
LOSS: 1.2068581581115723
LOSS: 1.18904709815979
LOSS: 1.1713781356811523
LOSS: 1.1539431810379028
LOSS: 1.1368342638015747
LOSS: 1.120147705078125
LOSS: 1.1039791107177734
LOSS: 1.0884140729904175
LOSS: 1.0735210180282593
LOSS: 1.0593483448028564
LOSS: 1.0459250211715698
LOSS: 1.0332599878311157
LOSS: 1.0213433504104614
LOSS: 1.0101534128189087
LOSS: 0.99965500831604
LOSS: 0.9898019433021545
LOSS: 0.9805388450622559
LOSS: 0.9718076586723328
LOSS: 0.9635511040687561
LOSS: 0.9557156562805176
LOSS: 0.9482559561729431
LOSS: 0.941133975982666
LOSS: 0.9343203902244568
LOSS: 0.9277962446212769
LOSS: 0.9215499758720398
LOSS: 0.9155769944190979
LOSS: 0.9098774194717407
LOSS: 0.9044559597969055
LOSS: 0.8993172645568848
LOSS: 0.894467294216156
LOSS: 0.8899052739143372
LOSS: 0.8856269717216492
LOSS: 0.8816189765930176
LOSS: 0.8778631091117859
LOSS: 0.8743391036987305
LOSS: 0.8710243105888367
LOSS: 0.867900013923645
LOSS: 0.8649506568908691
LOSS: 0.8621639013290405
LOSS: 0.8595300912857056
LOSS: 0.8570412397384644
LOSS: 0.8546900153160095
LOSS: 0.8524696826934814
LOSS: 0.8503766059875488
LOSS: 0.848408579826355
LOSS: 0.8465638756752014
LOSS: 0.8448423743247986
LOSS: 0.8432410955429077
LOSS: 0.8417564630508423
LOSS: 0.8403833508491516
LOSS: 0.8391140699386597
LOSS: 0.8379397392272949
LOSS: 0.8368521928787231
LOSS: 0.8358411192893982
LOSS: 0.8348977565765381
LOSS: 0.8340139985084534
LOSS: 0.8331832885742188
LOSS: 0.8323996067047119
LOSS: 0.8316587805747986
LOSS: 0.8309565186500549
LOSS: 0.8302889466285706
LOSS: 0.8296529054641724
LOSS: 0.8290430307388306
LOSS: 0.8284560441970825
LOSS: 0.8278882503509521
LOSS: 0.827336311340332
LOSS: 0.826798141002655
LOSS: 0.8262722492218018
LOSS: 0.82575923204422
LOSS: 0.8252581357955933
LOSS: 0.824769914150238
LOSS: 0.8242964148521423
LOSS: 0.823838472366333
LOSS: 0.8233969211578369
LOSS: 0.8229726552963257
LOSS: 0.8225648999214172
LOSS: 0.8221735954284668
LOSS: 0.8217964172363281
LOSS: 0.8214321732521057
LOSS: 0.8210793137550354
LOSS: 0.8207363486289978
LOSS: 0.8204015493392944
LOSS: 0.8200744390487671
LOSS: 0.819754421710968
LOSS: 0.8194407820701599
LOSS: 0.8191336989402771
LOSS: 0.8188319802284241
LOSS: 0.8185364603996277
LOSS: 0.8182451725006104
LOSS: 0.8179576396942139
LOSS: 0.8176741003990173
LOSS: 0.81739342212677
LOSS: 1.342894434928894
LOSS: 1.3266730308532715
LOSS: 1.309985637664795
LOSS: 1.2928839921951294
LOSS: 1.2753880023956299
LOSS: 1.257567048072815
LOSS: 1.2394928932189941
LOSS: 1.2212274074554443
LOSS: 1.2028371095657349
LOSS: 1.1844087839126587
LOSS: 1.1660486459732056
LOSS: 1.1478729248046875
LOSS: 1.129995346069336
LOSS: 1.1125198602676392
LOSS: 1.0955387353897095
LOSS: 1.0791361331939697
LOSS: 1.0633866786956787
LOSS: 1.0483523607254028
LOSS: 1.0340763330459595
LOSS: 1.0205821990966797
LOSS: 1.0078736543655396
LOSS: 0.9959369897842407
LOSS: 0.9847419857978821
LOSS: 0.9742468595504761
LOSS: 0.964402437210083
LOSS: 0.9551563858985901
LOSS: 0.9464588761329651
LOSS: 0.9382623434066772
LOSS: 0.9305254817008972
LOSS: 0.9232116341590881
LOSS: 0.9162943363189697
LOSS: 0.9097549915313721
LOSS: 0.9035835862159729
LOSS: 0.8977730870246887
LOSS: 0.8923203349113464
LOSS: 0.8872188925743103
LOSS: 0.8824618458747864
LOSS: 0.8780359029769897
LOSS: 0.8739224076271057
LOSS: 0.870097279548645
LOSS: 0.8665340542793274
LOSS: 0.8632065057754517
LOSS: 0.8600897789001465
LOSS: 0.8571633100509644
LOSS: 0.854407787322998
LOSS: 0.8518067002296448
LOSS: 0.8493446111679077
LOSS: 0.847008228302002
LOSS: 0.8447851538658142
LOSS: 0.842666745185852
LOSS: 0.8406446576118469
LOSS: 0.8387149572372437
LOSS: 0.8368750214576721
LOSS: 0.8351228833198547
LOSS: 0.8334605097770691
LOSS: 0.8318893909454346
LOSS: 0.830410361289978
LOSS: 0.8290240168571472
LOSS: 0.8277287483215332
LOSS: 0.8265218138694763
LOSS: 0.8253987431526184
LOSS: 0.8243529796600342
LOSS: 0.8233768343925476
LOSS: 0.8224632740020752
LOSS: 0.82160484790802
LOSS: 0.8207948803901672
LOSS: 0.8200280666351318
LOSS: 0.8193002343177795
LOSS: 0.8186084032058716
LOSS: 0.8179513812065125
LOSS: 0.8173264265060425
LOSS: 0.8167324066162109
LOSS: 0.8161669969558716
LOSS: 0.8156276941299438
LOSS: 0.8151119351387024
LOSS: 0.8146164417266846
LOSS: 0.814136266708374
LOSS: 0.8136690855026245
LOSS: 0.8132119178771973
LOSS: 0.8127618432044983
LOSS: 0.812317430973053
LOSS: 0.8118794560432434
LOSS: 0.8114468455314636
LOSS: 0.8110207319259644
LOSS: 0.8106023073196411
LOSS: 0.8101925253868103
LOSS: 0.8097924590110779
LOSS: 0.8094025254249573
LOSS: 0.8090229034423828
LOSS: 0.8086541295051575
LOSS: 0.8082957863807678
LOSS: 0.8079473376274109
LOSS: 0.807607889175415
LOSS: 0.8072778582572937
LOSS: 0.8069559335708618
LOSS: 0.8066422343254089
LOSS: 0.8063364028930664
LOSS: 0.8060377836227417
LOSS: 0.8057462573051453
LOSS: 0.8054617643356323
LOSS: 1.3487545251846313
LOSS: 1.3329179286956787
LOSS: 1.3166556358337402
LOSS: 1.3001346588134766
LOSS: 1.283534049987793
LOSS: 1.2670035362243652
LOSS: 1.2506613731384277
LOSS: 1.2345956563949585
LOSS: 1.2188611030578613
LOSS: 1.2034873962402344
LOSS: 1.1884880065917969
LOSS: 1.1738691329956055
LOSS: 1.1596382856369019
LOSS: 1.1458016633987427
LOSS: 1.1323517560958862
LOSS: 1.119261622428894
LOSS: 1.1064894199371338
LOSS: 1.0939894914627075
LOSS: 1.0817182064056396
LOSS: 1.0696380138397217
LOSS: 1.0577199459075928
LOSS: 1.0459480285644531
LOSS: 1.0139248371124268
LOSS: 1.004143238067627
LOSS: 0.9947525858879089
LOSS: 0.9857690930366516
LOSS: 0.9771985411643982
LOSS: 0.9690367579460144
LOSS: 0.961273729801178
LOSS: 0.9538989067077637
LOSS: 0.9469020962715149
LOSS: 0.9402760863304138
LOSS: 0.9340128302574158
LOSS: 0.9281028509140015
LOSS: 0.9225327372550964
LOSS: 0.9172853231430054
LOSS: 0.9123414158821106
LOSS: 0.9076800346374512
LOSS: 0.9032785296440125
LOSS: 0.8991160988807678
LOSS: 0.8951722979545593
LOSS: 0.8914291262626648
LOSS: 0.8878708481788635
LOSS: 0.8844851851463318
LOSS: 0.8812628388404846
LOSS: 0.8781963586807251
LOSS: 0.8752794861793518
LOSS: 0.8725070953369141
LOSS: 0.8698726892471313
LOSS: 0.8673710227012634
LOSS: 0.8649958968162537
LOSS: 0.8627430200576782
LOSS: 0.8606076240539551
LOSS: 0.8585842847824097
LOSS: 0.856666624546051
LOSS: 0.8548485636711121
LOSS: 0.8531222343444824
LOSS: 0.8514809608459473
LOSS: 0.8499197363853455
LOSS: 0.8484326601028442
LOSS: 0.8470155596733093
LOSS: 0.8456630706787109
LOSS: 0.8443728685379028
LOSS: 0.8431434035301208
LOSS: 0.8419743776321411
LOSS: 0.8408641219139099
LOSS: 0.839812695980072
LOSS: 0.8388189673423767
LOSS: 0.8378817439079285
LOSS: 0.8369995951652527
LOSS: 0.8361708521842957
LOSS: 0.8353904485702515
LOSS: 0.8346551060676575
LOSS: 0.8339586853981018
LOSS: 0.8332976698875427
LOSS: 0.8326660394668579
LOSS: 0.8320611715316772
LOSS: 0.8314785361289978
LOSS: 0.8309162259101868
LOSS: 0.8303698897361755
LOSS: 0.8298379182815552
LOSS: 0.8293169140815735
LOSS: 0.8288058042526245
LOSS: 0.8283027410507202
LOSS: 0.827806830406189
LOSS: 0.8273173570632935
LOSS: 0.8268348574638367
LOSS: 0.8263586759567261
LOSS: 0.8258881568908691
LOSS: 0.8254233002662659
LOSS: 0.8249645829200745
LOSS: 0.8245131373405457
LOSS: 0.8240678906440735
LOSS: 0.8236290812492371
LOSS: 0.8231972455978394
LOSS: 0.8227726221084595
LOSS: 0.8223544955253601
LOSS: 1.344185471534729
LOSS: 1.3271397352218628
LOSS: 1.3095903396606445
LOSS: 1.2916308641433716
LOSS: 1.2733049392700195
LOSS: 1.2546266317367554
LOSS: 1.235643982887268
LOSS: 1.216449499130249
LOSS: 1.1971515417099
LOSS: 1.177866816520691
LOSS: 1.1587210893630981
LOSS: 1.1398441791534424
LOSS: 1.1213617324829102
LOSS: 1.1033867597579956
LOSS: 1.0860133171081543
LOSS: 1.069320559501648
LOSS: 1.0533713102340698
LOSS: 1.0382095575332642
LOSS: 1.0238590240478516
LOSS: 1.0103230476379395
LOSS: 0.9975890517234802
LOSS: 0.9856289625167847
LOSS: 0.9744018912315369
LOSS: 0.9638546705245972
LOSS: 0.9539273381233215
LOSS: 0.9445595741271973
LOSS: 0.9356967210769653
LOSS: 0.9272909164428711
LOSS: 0.9193034768104553
LOSS: 0.9117067456245422
LOSS: 0.9044826030731201
LOSS: 0.8976199626922607
LOSS: 0.8911099433898926
LOSS: 0.8849427700042725
LOSS: 0.8791102766990662
LOSS: 0.8736003041267395
LOSS: 0.8684014678001404
LOSS: 0.8635004758834839
LOSS: 0.8588873744010925
LOSS: 0.8545554280281067
LOSS: 0.8505003452301025
LOSS: 0.8467211723327637
LOSS: 0.843214750289917
LOSS: 0.8399759531021118
LOSS: 0.8369964361190796
LOSS: 0.8342620730400085
LOSS: 0.8317559361457825
LOSS: 0.8294580578804016
LOSS: 0.8273504972457886
LOSS: 0.8254144787788391
LOSS: 0.8236358761787415
LOSS: 0.8220038414001465
LOSS: 0.8205068707466125
LOSS: 0.8191357254981995
LOSS: 0.8178836703300476
LOSS: 0.8167378306388855
LOSS: 0.815687894821167
LOSS: 0.8147208094596863
LOSS: 0.8138220906257629
LOSS: 0.812980592250824
LOSS: 0.812187135219574
LOSS: 0.8114331960678101
LOSS: 0.8107147812843323
LOSS: 0.8100295662879944
LOSS: 0.8093757033348083
LOSS: 0.8087526559829712
LOSS: 0.8081601858139038
LOSS: 0.8075969219207764
LOSS: 0.8070605993270874
LOSS: 0.8065505623817444
LOSS: 0.8060624599456787
LOSS: 0.8055938482284546
LOSS: 0.805142879486084
LOSS: 0.8047065138816833
LOSS: 0.8042827248573303
LOSS: 0.8038696646690369
LOSS: 0.8034664392471313
LOSS: 0.8030731081962585
LOSS: 0.802689790725708
LOSS: 0.8023163080215454
LOSS: 0.8019534945487976
LOSS: 0.8016019463539124
LOSS: 0.8012617230415344
LOSS: 0.8009330034255981
LOSS: 0.8006153702735901
LOSS: 0.8003074526786804
LOSS: 0.8000085949897766
LOSS: 0.7997177243232727
LOSS: 0.7994344830513
LOSS: 0.799159049987793
LOSS: 0.7988900542259216
LOSS: 0.7986283302307129
LOSS: 0.798374354839325
LOSS: 0.798126757144928
LOSS: 0.7978872656822205
LOSS: 0.7976545691490173
LOSS: 0.7974284291267395
LOSS: 0.7972093820571899
LOSS: 0.7969961166381836
LOSS: 0.7967888116836548
LOSS: 1.3484550714492798
LOSS: 1.3321155309677124
LOSS: 1.315307378768921
LOSS: 1.2981288433074951
LOSS: 1.2807016372680664
LOSS: 1.263121247291565
LOSS: 1.245423674583435
LOSS: 1.2276337146759033
LOSS: 1.2098056077957153
LOSS: 1.1920222043991089
LOSS: 1.1743820905685425
LOSS: 1.1569859981536865
LOSS: 1.1399312019348145
LOSS: 1.1233147382736206
LOSS: 1.1072323322296143
LOSS: 1.091772437095642
LOSS: 1.077008843421936
LOSS: 1.0629948377609253
LOSS: 1.0497585535049438
LOSS: 1.037302017211914
LOSS: 1.0256046056747437
LOSS: 1.0146299600601196
LOSS: 1.0043307542800903
LOSS: 0.9946551322937012
LOSS: 0.9855480194091797
LOSS: 0.9769551157951355
LOSS: 0.968825101852417
LOSS: 0.9611121416091919
LOSS: 0.9537792801856995
LOSS: 0.9468007683753967
LOSS: 0.9401613473892212
LOSS: 0.9338537454605103
LOSS: 0.9278749227523804
LOSS: 0.9222241044044495
LOSS: 0.9169006943702698
LOSS: 0.9119048714637756
LOSS: 0.9072303771972656
LOSS: 0.902866542339325
LOSS: 0.8987959027290344
LOSS: 0.8949954509735107
LOSS: 0.8914356827735901
LOSS: 0.8880869746208191
LOSS: 0.8849213719367981
LOSS: 0.8819142580032349
LOSS: 0.8790452480316162
LOSS: 0.8762996196746826
LOSS: 0.8736647367477417
LOSS: 0.8711336851119995
LOSS: 0.8687021136283875
LOSS: 0.8663709163665771
LOSS: 0.8641413450241089
LOSS: 0.8620156645774841
LOSS: 0.8599968552589417
LOSS: 0.8580854535102844
LOSS: 0.8562787175178528
LOSS: 0.8545750975608826
LOSS: 0.8529694676399231
LOSS: 0.851456344127655
LOSS: 0.8500297665596008
LOSS: 0.8486844301223755
LOSS: 0.8474175930023193
LOSS: 0.8462269306182861
LOSS: 0.845112144947052
LOSS: 0.8440717458724976
LOSS: 0.8431037664413452
LOSS: 0.8422055244445801
LOSS: 0.8413718938827515
LOSS: 0.8405970335006714
LOSS: 0.8398723006248474
LOSS: 0.8391897678375244
LOSS: 0.8385418653488159
LOSS: 0.8379207253456116
LOSS: 0.8373205065727234
LOSS: 0.8367363214492798
LOSS: 0.8361648321151733
LOSS: 0.8356050848960876
LOSS: 0.8350563049316406
LOSS: 0.8345190286636353
LOSS: 0.8339939713478088
LOSS: 0.83348149061203
LOSS: 0.8329826593399048
LOSS: 0.832497775554657
LOSS: 0.8320263028144836
LOSS: 0.8315684199333191
LOSS: 0.8311229944229126
LOSS: 0.8306898474693298
LOSS: 0.830267071723938
LOSS: 0.8298546671867371
LOSS: 0.829450786113739
LOSS: 0.8290551900863647
LOSS: 0.8286668062210083
LOSS: 0.8282853364944458
LOSS: 0.8279092311859131
LOSS: 0.8275392055511475
LOSS: 0.8271744251251221
LOSS: 0.826815664768219
LOSS: 0.826462984085083
LOSS: 0.8261159062385559
LOSS: 0.8257747292518616
LOSS: 0.8254393339157104
LOSS: 1.3484550714492798
LOSS: 1.3321155309677124
LOSS: 1.315307378768921
LOSS: 1.2981288433074951
LOSS: 1.2807016372680664
LOSS: 1.263121247291565
LOSS: 1.245423674583435
LOSS: 1.2276337146759033
LOSS: 1.2098056077957153
LOSS: 1.1920222043991089
LOSS: 1.1743820905685425
LOSS: 1.1569859981536865
LOSS: 1.1399312019348145
LOSS: 1.1233147382736206
LOSS: 1.1072323322296143
LOSS: 1.091772437095642
LOSS: 1.077008843421936
LOSS: 1.0629948377609253
LOSS: 1.0497585535049438
LOSS: 1.037302017211914
LOSS: 1.0256046056747437
LOSS: 1.0146299600601196
LOSS: 1.0043307542800903
LOSS: 0.9946551322937012
LOSS: 0.9855480194091797
LOSS: 0.9769551157951355
LOSS: 0.968825101852417
LOSS: 0.9611121416091919
LOSS: 0.9537792801856995
LOSS: 0.9468007683753967
LOSS: 0.9401613473892212
LOSS: 0.9338537454605103
LOSS: 0.9278749227523804
LOSS: 0.9222241044044495
LOSS: 0.9169006943702698
LOSS: 0.9119048714637756
LOSS: 0.9072303771972656
LOSS: 0.902866542339325
LOSS: 0.8987959027290344
LOSS: 0.8949954509735107
LOSS: 0.8914356827735901
LOSS: 0.8880869746208191
LOSS: 0.8849213719367981
LOSS: 0.8819142580032349
LOSS: 0.8790452480316162
LOSS: 0.8762996196746826
LOSS: 0.8736647367477417
LOSS: 0.8711336851119995
LOSS: 0.8687021136283875
LOSS: 0.8663709163665771
LOSS: 0.8641413450241089
LOSS: 0.8620156645774841
LOSS: 0.8599968552589417
LOSS: 0.8580854535102844
LOSS: 1.0577199459075928
LOSS: 1.0459480285644531
LOSS: 1.0343241691589355
LOSS: 1.0228697061538696
LOSS: 1.0116255283355713
LOSS: 1.000646710395813
LOSS: 0.9899939894676208
LOSS: 0.9797226190567017
LOSS: 0.9698767066001892
LOSS: 0.9604861736297607
LOSS: 0.9515658617019653
LOSS: 0.9431155323982239
LOSS: 0.9351241588592529
LOSS: 0.9275739789009094
LOSS: 0.9204457402229309
LOSS: 0.9137218594551086
LOSS: 0.9073866605758667
LOSS: 0.9014273285865784
LOSS: 0.8958315849304199
LOSS: 0.8905871510505676
LOSS: 0.8856807351112366
LOSS: 0.8810976147651672
LOSS: 0.8768200874328613
LOSS: 0.8728299736976624
LOSS: 0.869105339050293
LOSS: 0.8656287789344788
LOSS: 0.8623811602592468
LOSS: 0.8593485951423645
LOSS: 0.8565191030502319
LOSS: 0.8538808822631836
LOSS: 0.8514251112937927
LOSS: 0.8491378426551819
LOSS: 0.8470045924186707
LOSS: 0.8450092673301697
LOSS: 0.843136727809906
LOSS: 0.841370701789856
LOSS: 0.8396980166435242
LOSS: 0.8381078243255615
LOSS: 0.836590588092804
LOSS: 0.83514004945755
LOSS: 0.8337507247924805
LOSS: 0.8324199318885803
LOSS: 0.8311466574668884
LOSS: 0.8299325108528137
LOSS: 0.8287786245346069
LOSS: 0.8276875019073486
LOSS: 0.8266597390174866
LOSS: 0.8256943225860596
LOSS: 0.8247902989387512
LOSS: 0.8239442110061646
LOSS: 0.8231526613235474
LOSS: 0.822412371635437
LOSS: 0.8217189311981201
LOSS: 0.821067214012146
LOSS: 0.8204545378684998
LOSS: 0.8198767304420471
LOSS: 0.8193306922912598
LOSS: 0.818812906742096
LOSS: 0.8183202147483826
LOSS: 0.8178505897521973
LOSS: 0.8174012303352356
LOSS: 0.8169703483581543
LOSS: 0.816555917263031
LOSS: 0.816156268119812
LOSS: 0.8157694339752197
LOSS: 0.815394937992096
LOSS: 0.8150304555892944
LOSS: 0.8146756291389465
LOSS: 0.8143293857574463
LOSS: 0.8139913082122803
LOSS: 0.8136609196662903
LOSS: 0.813336968421936
LOSS: 0.813018798828125
LOSS: 0.8127067685127258
LOSS: 0.8124017119407654
LOSS: 0.8121035695075989
LOSS: 0.8118128776550293
LOSS: 0.8115291595458984
LOSS: 0.8112531304359436
LOSS: 0.810982882976532
LOSS: 1.3532947301864624
LOSS: 1.3373150825500488
LOSS: 1.3207594156265259
LOSS: 1.3037549257278442
LOSS: 1.2863212823867798
LOSS: 1.2684873342514038
LOSS: 1.250331997871399
LOSS: 1.2319647073745728
LOSS: 1.213512897491455
LOSS: 1.1951072216033936
LOSS: 1.1768702268600464
LOSS: 1.1589124202728271
LOSS: 1.141342282295227
LOSS: 1.1242729425430298
LOSS: 1.1078170537948608
LOSS: 1.0920792818069458
LOSS: 1.0771459341049194
LOSS: 1.063078761100769
LOSS: 1.049910545349121
LOSS: 1.0376441478729248
LOSS: 1.026253342628479
LOSS: 1.0156903266906738
LOSS: 1.0058925151824951
LOSS: 0.9967890381813049
LOSS: 0.988304078578949
LOSS: 0.9803614020347595
LOSS: 0.9728882312774658
LOSS: 0.9658188223838806
LOSS: 0.9590983390808105
LOSS: 0.9526808857917786
LOSS: 0.9465308785438538
LOSS: 0.9406213760375977
LOSS: 0.9349352717399597
LOSS: 0.9294619560241699
LOSS: 0.9241933822631836
LOSS: 0.9191240668296814
LOSS: 0.9142484664916992
LOSS: 0.9095666408538818
LOSS: 0.9050801992416382
LOSS: 0.9007924795150757
LOSS: 0.8967058658599854
LOSS: 0.892818033695221
LOSS: 0.8891229033470154
LOSS: 0.8856124877929688
LOSS: 0.882276177406311
LOSS: 0.8791045546531677
LOSS: 0.8760907053947449
LOSS: 0.8732285499572754
LOSS: 0.8705176711082458
LOSS: 0.8679570555686951
LOSS: 0.8655477166175842
LOSS: 0.8632897138595581
LOSS: 0.8611799478530884
LOSS: 0.8592148423194885
LOSS: 0.857387125492096
LOSS: 0.8556876182556152
LOSS: 0.8541072607040405
LOSS: 0.8526342511177063
LOSS: 0.8512592315673828
LOSS: 0.8499735593795776
LOSS: 0.848768413066864
LOSS: 0.8476355671882629
LOSS: 0.8465701341629028
LOSS: 0.8455637097358704
LOSS: 0.8446123003959656
LOSS: 0.8437119126319885
LOSS: 0.8428584933280945
LOSS: 0.8420493006706238
LOSS: 0.841282844543457
LOSS: 0.8405565023422241
LOSS: 0.8398669362068176
LOSS: 0.8392118811607361
LOSS: 0.8385885953903198
LOSS: 0.8379926085472107
LOSS: 0.8374230861663818
LOSS: 0.8368764519691467
LOSS: 0.8363515734672546
LOSS: 0.8358475565910339
LOSS: 0.8353629112243652
LOSS: 0.8348966836929321
LOSS: 0.8344486951828003
LOSS: 0.8340171575546265
LOSS: 0.8336015343666077
LOSS: 0.833200216293335
LOSS: 0.832811713218689
LOSS: 0.832434356212616
LOSS: 0.8320675492286682
LOSS: 0.831710159778595
LOSS: 0.8313623070716858
LOSS: 0.8310243487358093
LOSS: 0.830695629119873
LOSS: 0.8303773999214172
LOSS: 0.8300690054893494
LOSS: 0.8297706842422485
LOSS: 0.8294819593429565
LOSS: 0.8292019367218018
LOSS: 0.8289296627044678
LOSS: 0.8286644816398621
LOSS: 0.8284050226211548
LOSS: 0.8281522989273071
LOSS: 1.342894434928894
LOSS: 1.3266730308532715
LOSS: 1.309985637664795
LOSS: 1.2928839921951294
LOSS: 1.2753880023956299
LOSS: 1.257567048072815
LOSS: 1.2394928932189941
LOSS: 1.2212274074554443
LOSS: 1.2028371095657349
LOSS: 1.1844087839126587
LOSS: 1.1660486459732056
LOSS: 1.1478729248046875
LOSS: 1.129995346069336
LOSS: 1.1125198602676392
LOSS: 1.0955387353897095
LOSS: 1.0791361331939697
LOSS: 1.0633866786956787
LOSS: 1.0483523607254028
LOSS: 1.0340763330459595
LOSS: 1.0205821990966797
LOSS: 1.0078736543655396
LOSS: 0.9959369897842407
LOSS: 0.9847419857978821
LOSS: 0.9742468595504761
LOSS: 0.964402437210083
LOSS: 0.9551563858985901
LOSS: 0.9464588761329651
LOSS: 0.9382623434066772
LOSS: 0.9305254817008972
LOSS: 0.9232116341590881
LOSS: 0.9162943363189697
LOSS: 0.9097549915313721
LOSS: 0.9035835862159729
LOSS: 0.8977730870246887
LOSS: 0.8923203349113464
LOSS: 0.8872188925743103
LOSS: 0.8824618458747864
LOSS: 0.8780359029769897
LOSS: 0.8739224076271057
LOSS: 0.870097279548645
LOSS: 0.8665340542793274
LOSS: 0.8632065057754517
LOSS: 0.8600897789001465
LOSS: 0.8571633100509644
LOSS: 0.854407787322998
LOSS: 0.8518067002296448
LOSS: 0.8493446111679077
LOSS: 0.847008228302002
LOSS: 0.8447851538658142
LOSS: 0.842666745185852
LOSS: 0.8406446576118469
LOSS: 0.8387149572372437
LOSS: 0.8368750214576721
LOSS: 0.8351228833198547
LOSS: 0.8334605097770691
LOSS: 0.8318893909454346
LOSS: 0.830410361289978
LOSS: 0.8290240168571472
LOSS: 0.8277287483215332
LOSS: 0.8265218138694763
LOSS: 0.8253987431526184
LOSS: 0.8243529796600342
LOSS: 0.8233768343925476
LOSS: 0.8224632740020752
LOSS: 0.82160484790802
LOSS: 0.8207948803901672
LOSS: 0.8200280666351318
LOSS: 0.8193002343177795
LOSS: 0.8186084032058716
LOSS: 0.8179513812065125
LOSS: 0.8173264265060425
LOSS: 0.8167324066162109
LOSS: 0.8161669969558716
LOSS: 0.8156276941299438
LOSS: 0.8151119351387024
LOSS: 0.8146164417266846
LOSS: 0.814136266708374
LOSS: 0.8136690855026245
LOSS: 0.8132119178771973
LOSS: 0.8127618432044983
LOSS: 0.812317430973053
LOSS: 0.8118794560432434
LOSS: 0.8114468455314636
LOSS: 0.8110207319259644
LOSS: 0.8106023073196411
LOSS: 0.8101925253868103
LOSS: 0.8097924590110779
LOSS: 0.8094025254249573
LOSS: 0.8090229034423828
LOSS: 0.8086541295051575
LOSS: 0.8082957863807678
LOSS: 0.8079473376274109
LOSS: 0.807607889175415
LOSS: 0.8072778582572937
LOSS: 0.8069559335708618
LOSS: 0.8066422343254089
LOSS: 0.8063364028930664
LOSS: 0.8060377836227417
LOSS: 0.8057462573051453
LOSS: 0.8054617643356323
LOSS: 1.3532947301864624
LOSS: 1.3373150825500488
LOSS: 1.3207594156265259
LOSS: 1.3037549257278442
LOSS: 1.2863212823867798
LOSS: 1.2684873342514038
LOSS: 1.250331997871399
LOSS: 1.2319647073745728
LOSS: 1.213512897491455
LOSS: 1.1951072216033936
LOSS: 1.1768702268600464
LOSS: 1.1589124202728271
LOSS: 1.141342282295227
LOSS: 1.1242729425430298
LOSS: 1.1078170537948608
LOSS: 1.0920792818069458
LOSS: 1.0771459341049194
LOSS: 1.063078761100769
LOSS: 1.049910545349121
LOSS: 1.0376441478729248
LOSS: 1.026253342628479
LOSS: 1.0156903266906738
LOSS: 1.0058925151824951
LOSS: 0.9967890381813049
LOSS: 0.988304078578949
LOSS: 0.9803614020347595
LOSS: 0.9728882312774658
LOSS: 0.9658188223838806
LOSS: 0.9590983390808105
LOSS: 0.9526808857917786
LOSS: 0.9465308785438538
LOSS: 0.9406213760375977
LOSS: 0.9349352717399597
LOSS: 0.9294619560241699
LOSS: 0.9241933822631836
LOSS: 0.9191240668296814
LOSS: 0.9142484664916992
LOSS: 0.9095666408538818
LOSS: 0.9050801992416382
LOSS: 0.9007924795150757
LOSS: 0.8967058658599854
LOSS: 0.892818033695221
LOSS: 0.8891229033470154
LOSS: 0.8856124877929688
LOSS: 0.882276177406311
LOSS: 0.8791045546531677
LOSS: 0.8760907053947449
LOSS: 0.8732285499572754
LOSS: 0.8705176711082458
LOSS: 0.8679570555686951LOSS: 0.9958360195159912
LOSS: 0.9859960675239563
LOSS: 0.9767632484436035
LOSS: 0.9680966734886169
LOSS: 0.9599509239196777
LOSS: 0.9522810578346252
LOSS: 0.9450430274009705
LOSS: 0.9381980895996094
LOSS: 0.9317152500152588
LOSS: 0.9255735874176025
LOSS: 0.919761061668396
LOSS: 0.9142737984657288
LOSS: 0.9091112613677979
LOSS: 0.9042714238166809
LOSS: 0.8997504115104675
LOSS: 0.8955398201942444
LOSS: 0.89162278175354
LOSS: 0.8879801034927368
LOSS: 0.8845880031585693
LOSS: 0.8814224600791931
LOSS: 0.878456711769104
LOSS: 0.87566739320755
LOSS: 0.873032808303833
LOSS: 0.870535135269165
LOSS: 0.8681635856628418
LOSS: 0.8659098744392395
LOSS: 0.863768994808197
LOSS: 0.8617388606071472
LOSS: 0.859816312789917
LOSS: 0.8579980731010437
LOSS: 0.8562812209129333
LOSS: 0.854660153388977
LOSS: 0.8531302809715271
LOSS: 0.851685106754303
LOSS: 0.8503209948539734
LOSS: 0.8490328192710876
LOSS: 0.8478166460990906
LOSS: 0.8466702699661255
LOSS: 0.845589280128479
LOSS: 0.8445726633071899
LOSS: 0.8436164259910583
LOSS: 0.8427178263664246
LOSS: 0.8418716192245483
LOSS: 0.8410724997520447
LOSS: 0.8403136730194092
LOSS: 0.8395876884460449
LOSS: 0.8388858437538147
LOSS: 0.8381996154785156
LOSS: 0.8375231623649597
LOSS: 0.8368494510650635
LOSS: 0.8361736536026001
LOSS: 0.8354925513267517
LOSS: 0.8348026275634766
LOSS: 0.8341037034988403
LOSS: 0.8333972692489624
LOSS: 0.8326873183250427
LOSS: 0.8319782018661499
LOSS: 0.8312767148017883
LOSS: 0.8305888772010803
LOSS: 0.8299208879470825
LOSS: 0.8292766213417053
LOSS: 0.8286584615707397
LOSS: 0.82806795835495
LOSS: 0.8275048732757568
LOSS: 0.8269691467285156
LOSS: 0.8264593482017517
LOSS: 0.8259747624397278
LOSS: 0.825514554977417
LOSS: 0.8250771164894104
LOSS: 0.8246620297431946
LOSS: 0.8242673873901367
LOSS: 0.8238927721977234
LOSS: 0.8235369920730591
LOSS: 0.823198676109314
LOSS: 0.8228769898414612
LOSS: 0.8225690722465515
LOSS: 0.8222753405570984
LOSS: 1.3435261249542236
LOSS: 1.3265827894210815
LOSS: 1.3091542720794678
LOSS: 1.2913339138031006
LOSS: 1.2731082439422607
LOSS: 1.2544788122177124
LOSS: 1.2354986667633057
LOSS: 1.216247797012329
LOSS: 1.1968350410461426
LOSS: 1.1773946285247803
LOSS: 1.158068060874939
LOSS: 1.138985514640808
LOSS: 1.1202549934387207
LOSS: 1.1019648313522339
LOSS: 1.0841939449310303
LOSS: 1.0670210123062134
LOSS: 1.0505201816558838
LOSS: 1.0347545146942139
LOSS: 1.0197709798812866
LOSS: 1.0055965185165405
LOSS: 0.9922400712966919
LOSS: 0.9796946048736572
LOSS: 0.9679390788078308
LOSS: 0.9569423794746399
LOSS: 0.9466667175292969
LOSS: 0.9370703101158142
LOSS: 0.9281088709831238
LOSS: 0.9197354316711426
LOSS: 0.9119002819061279
LOSS: 0.9045537710189819
LOSS: 0.8976476192474365
LOSS: 0.8911382555961609
LOSS: 0.8849920034408569
LOSS: 0.8791849613189697
LOSS: 0.8737019896507263
LOSS: 0.8685357570648193
LOSS: 0.8636839985847473
LOSS: 0.8591436147689819
LOSS: 0.8549084663391113
LOSS: 0.8509671092033386
LOSS: 0.847305953502655
LOSS: 0.8439042568206787
LOSS: 0.8407435417175293
LOSS: 0.8378022909164429
LOSS: 0.8350623250007629
LOSS: 0.8325029611587524
LOSS: 0.8301063179969788
LOSS: 0.8278545141220093
LOSS: 0.8257307410240173
LOSS: 0.8237214088439941
LOSS: 0.8218145966529846
LOSS: 0.8200019598007202
LOSS: 0.8182809352874756
LOSS: 0.8166507482528687
LOSS: 0.8151133060455322
LOSS: 0.8136708736419678
LOSS: 0.8123227953910828
LOSS: 0.8110660910606384
LOSS: 0.8098931312561035
LOSS: 0.8087937831878662
LOSS: 0.8077588081359863
LOSS: 0.8067775964736938
LOSS: 0.8058425188064575
LOSS: 0.8049477934837341
LOSS: 0.8040913939476013
LOSS: 0.8032727837562561
LOSS: 0.8024945855140686
LOSS: 0.8017578721046448
LOSS: 0.8010638356208801
LOSS: 0.8004143834114075
LOSS: 0.799809455871582
LOSS: 0.799247145652771
LOSS: 0.7987257838249207
LOSS: 0.7982427477836609
LOSS: 0.7977942824363708
LOSS: 0.7973772287368774
LOSS: 0.7969869375228882
LOSS: 0.7966193556785583
LOSS: 0.7962701320648193
LOSS: 0.7959362864494324
LOSS: 0.7956134080886841
LOSS: 0.7953000664710999
LOSS: 0.7949931621551514
LOSS: 0.794691801071167
LOSS: 0.7943958044052124
LOSS: 0.7941044569015503
LOSS: 0.7938180565834045
LOSS: 0.793537437915802
LOSS: 0.7932629585266113
LOSS: 0.7929952144622803
LOSS: 0.7927341461181641
LOSS: 0.7924814224243164
LOSS: 0.7922362685203552
LOSS: 0.7919985055923462
LOSS: 0.7917688488960266
LOSS: 0.7915461659431458
LOSS: 0.7913310527801514
LOSS: 0.7911222577095032
LOSS: 0.7909195423126221
LOSS: 0.7907230257987976
LOSS: 1.3435261249542236
LOSS: 1.3265827894210815
LOSS: 1.3091542720794678
LOSS: 1.2913339138031006
LOSS: 1.2731082439422607
LOSS: 1.2544788122177124
LOSS: 1.2354986667633057
LOSS: 1.216247797012329
LOSS: 1.1968350410461426
LOSS: 1.1773946285247803
LOSS: 1.158068060874939
LOSS: 1.138985514640808
LOSS: 1.1202549934387207
LOSS: 1.1019648313522339
LOSS: 1.0841939449310303
LOSS: 1.0670210123062134
LOSS: 1.0505201816558838
LOSS: 1.0347545146942139
LOSS: 1.0197709798812866
LOSS: 1.0055965185165405
LOSS: 0.9922400712966919
LOSS: 0.9796946048736572
LOSS: 0.9679390788078308
LOSS: 0.9569423794746399
LOSS: 0.9466667175292969
LOSS: 0.9370703101158142
LOSS: 0.9281088709831238
LOSS: 0.9197354316711426
LOSS: 0.9119002819061279
LOSS: 0.9045537710189819
LOSS: 0.8976476192474365
LOSS: 0.8911382555961609
LOSS: 0.8849920034408569
LOSS: 0.8791849613189697
LOSS: 0.8737019896507263
LOSS: 0.8685357570648193
LOSS: 0.8636839985847473
LOSS: 0.8591436147689819
LOSS: 0.8549084663391113
LOSS: 0.8509671092033386
LOSS: 0.847305953502655
LOSS: 0.8439042568206787
LOSS: 0.8407435417175293
LOSS: 0.8378022909164429
LOSS: 0.8350623250007629
LOSS: 0.8325029611587524
LOSS: 0.8301063179969788
LOSS: 0.8278545141220093
LOSS: 0.8257307410240173
LOSS: 0.8237214088439941
LOSS: 0.8218145966529846
LOSS: 0.8200019598007202
LOSS: 0.8182809352874756
LOSS: 0.8166507482528687
LOSS: 0.8151133060455322
LOSS: 0.8136708736419678
LOSS: 0.8123227953910828
LOSS: 0.8110660910606384
LOSS: 0.8098931312561035
LOSS: 0.8087937831878662
LOSS: 0.8077588081359863
LOSS: 0.8067775964736938
LOSS: 0.8058425188064575
LOSS: 0.8049477934837341
LOSS: 0.8040913939476013
LOSS: 0.8032727837562561
LOSS: 0.8024945855140686
LOSS: 0.8017578721046448
LOSS: 0.8010638356208801
LOSS: 0.8004143834114075
LOSS: 0.799809455871582
LOSS: 0.799247145652771
LOSS: 0.7987257838249207
LOSS: 0.7982427477836609
LOSS: 0.7977942824363708
LOSS: 0.7973772287368774
LOSS: 0.7969869375228882
LOSS: 0.7966193556785583
LOSS: 0.7962701320648193
LOSS: 0.7959362864494324
LOSS: 0.7956134080886841
LOSS: 0.7953000664710999
LOSS: 0.7949931621551514
LOSS: 0.794691801071167
LOSS: 0.7943958044052124
LOSS: 0.7941044569015503
LOSS: 0.7938180565834045
LOSS: 0.793537437915802
LOSS: 0.7932629585266113
LOSS: 0.7929952144622803
LOSS: 0.7927341461181641
LOSS: 0.7924814224243164
LOSS: 0.7922362685203552
LOSS: 0.7919985055923462
LOSS: 0.7917688488960266
LOSS: 0.7915461659431458
LOSS: 0.7913310527801514
LOSS: 0.7911222577095032
LOSS: 0.7909195423126221
LOSS: 0.7907230257987976
LOSS: 1.3438860177993774
LOSS: 1.3277429342269897
LOSS: 1.3112109899520874
LOSS: 1.2943991422653198
LOSS: 1.2773966789245605
LOSS: 1.260266661643982
LOSS: 1.2430485486984253
LOSS: 1.2257919311523438
LOSS: 1.2085680961608887
LOSS: 1.1914563179016113
LOSS: 1.1745390892028809
LOSS: 1.1578948497772217
LOSS: 1.1415992975234985
LOSS: 1.1257258653640747
LOSS: 1.1103438138961792
LOSS: 1.0955159664154053
LOSS: 1.081292748451233
LOSS: 1.0677083730697632
LOSS: 1.0547770261764526
LOSS: 1.042494297027588
LOSS: 1.030839204788208
LOSS: 1.019778847694397
LOSS: 1.0092723369598389
LOSS: 0.999274492263794
LOSS: 0.9897391200065613
LOSS: 0.9806239604949951
LOSS: 0.9718908071517944
LOSS: 0.9635064601898193
LOSS: 0.9554463624954224
LOSS: 0.9476945400238037
LOSS: 0.9402439594268799
LOSS: 0.9330954551696777
LOSS: 0.9262558221817017
LOSS: 0.9197333455085754
LOSS: 0.9135348796844482
LOSS: 0.9076658487319946
LOSS: 0.9021276235580444
LOSS: 0.8969179391860962
LOSS: 0.892028272151947
LOSS: 0.8874432444572449
LOSS: 0.8831440806388855
LOSS: 0.8791083693504333
LOSS: 0.8753108978271484
LOSS: 0.8717331886291504
LOSS: 0.8683568239212036
LOSS: 0.8651697635650635
LOSS: 0.8621612787246704
LOSS: 0.8593266010284424
LOSS: 0.8566616177558899
LOSS: 0.8541638851165771
LOSS: 0.8518300652503967
LOSS: 0.8496549725532532
LOSS: 0.8562787175178528
LOSS: 0.8545750975608826
LOSS: 0.8529694676399231
LOSS: 0.851456344127655
LOSS: 0.8500297665596008
LOSS: 0.8486844301223755
LOSS: 0.8474175930023193
LOSS: 0.8462269306182861
LOSS: 0.845112144947052
LOSS: 0.8440717458724976
LOSS: 0.8431037664413452
LOSS: 0.8422055244445801
LOSS: 0.8413718938827515
LOSS: 0.8405970335006714
LOSS: 0.8398723006248474
LOSS: 0.8391897678375244
LOSS: 0.8385418653488159
LOSS: 0.8379207253456116
LOSS: 0.8373205065727234
LOSS: 0.8367363214492798
LOSS: 0.8361648321151733
LOSS: 0.8356050848960876
LOSS: 0.8350563049316406
LOSS: 0.8345190286636353
LOSS: 0.8339939713478088
LOSS: 0.83348149061203
LOSS: 0.8329826593399048
LOSS: 0.832497775554657
LOSS: 0.8320263028144836
LOSS: 0.8315684199333191
LOSS: 0.8311229944229126
LOSS: 0.8306898474693298
LOSS: 0.830267071723938
LOSS: 0.8298546671867371
LOSS: 0.829450786113739
LOSS: 0.8290551900863647
LOSS: 0.8286668062210083
LOSS: 0.8282853364944458
LOSS: 0.8279092311859131
LOSS: 0.8275392055511475
LOSS: 0.8271744251251221
LOSS: 0.826815664768219
LOSS: 0.826462984085083
LOSS: 0.8261159062385559
LOSS: 0.8257747292518616
LOSS: 0.8254393339157104
LOSS: 1.34933602809906
LOSS: 1.3334295749664307
LOSS: 1.3170086145401
LOSS: 1.3001437187194824
LOSS: 1.2828878164291382
LOSS: 1.2652665376663208
LOSS: 1.247326135635376
LOSS: 1.2291266918182373
LOSS: 1.2107480764389038
LOSS: 1.1922969818115234
LOSS: 1.1739013195037842
LOSS: 1.1556957960128784
LOSS: 1.137807011604309
LOSS: 1.1203477382659912
LOSS: 1.103417992591858
LOSS: 1.0871100425720215
LOSS: 1.0715043544769287
LOSS: 1.0566657781600952
LOSS: 1.0426385402679443
LOSS: 1.029444694519043
LOSS: 1.0170851945877075
LOSS: 1.0055444240570068
LOSS: 0.9947890043258667
LOSS: 0.9847703576087952
LOSS: 0.9754292368888855
LOSS: 0.9667015075683594
LOSS: 0.9585272669792175
LOSS: 0.95085209608078
LOSS: 0.9436271786689758
LOSS: 0.9368107318878174
LOSS: 0.9303656220436096
LOSS: 0.9242618083953857
LOSS: 0.9184733629226685
LOSS: 0.9129831790924072
LOSS: 0.9077804684638977
LOSS: 0.902862548828125
LOSS: 0.8982309699058533
LOSS: 0.8938869833946228
LOSS: 0.8898292779922485
LOSS: 0.8860518336296082
LOSS: 0.8825412392616272
LOSS: 0.879281759262085
LOSS: 0.8762539625167847
LOSS: 0.8734365701675415
LOSS: 0.8708096742630005
LOSS: 0.8683546185493469
LOSS: 0.8660537004470825
LOSS: 0.8638883233070374
LOSS: 0.8618440628051758
LOSS: 0.8599076271057129
LOSS: 0.8580718636512756
LOSS: 0.856332540512085
LOSS: 0.8546885848045349
LOSS: 0.8531404733657837
LOSS: 0.851690411567688
LOSS: 0.850339949131012
LOSS: 0.8490895628929138
LOSS: 0.847937822341919
LOSS: 0.8468818068504333
LOSS: 0.8459154963493347
LOSS: 0.8450285792350769
LOSS: 0.8442086577415466
LOSS: 0.8434421420097351
LOSS: 0.8427154421806335
LOSS: 0.8420202136039734
LOSS: 0.8413493633270264
LOSS: 0.8407002687454224
LOSS: 0.8400736451148987
LOSS: 0.8394708037376404
LOSS: 0.8388938307762146
LOSS: 0.8383447527885437
LOSS: 0.8378244042396545
LOSS: 0.8373328447341919
LOSS: 0.8368683457374573
LOSS: 0.8364297747612
LOSS: 0.8360142707824707
LOSS: 0.8356194496154785
LOSS: 0.8352429866790771
LOSS: 0.8348824977874756
LOSS: 0.8345363140106201
LOSS: 0.8342032432556152
LOSS: 0.8338811993598938
LOSS: 0.833568811416626
LOSS: 0.8332657814025879
LOSS: 0.8329702019691467
LOSS: 0.8326819539070129
LOSS: 0.8324010372161865
LOSS: 0.8321262001991272
LOSS: 0.8318575620651245
LOSS: 0.8315951228141785
LOSS: 0.8313377499580383
LOSS: 0.831086277961731
LOSS: 0.8308411240577698
LOSS: 0.8306024074554443
LOSS: 0.8303698897361755
LOSS: 0.8301438093185425
LOSS: 0.8299240469932556
LOSS: 0.8297101855278015
LOSS: 0.8295025825500488
LOSS: 0.8293004035949707
LOSS: 1.3492063283920288
LOSS: 1.3325457572937012
LOSS: 1.3153133392333984
LOSS: 1.297615647315979
LOSS: 1.2795473337173462
LOSS: 1.261146068572998
LOSS: 1.2424468994140625
LOSS: 1.2235230207443237
LOSS: 1.2044721841812134
LOSS: 1.1854078769683838
LOSS: 1.1664541959762573
LOSS: 1.1477370262145996
LOSS: 1.1293808221817017
LOSS: 1.1115024089813232
LOSS: 1.0942015647888184
LOSS: 1.077559471130371
LOSS: 1.0616352558135986
LOSS: 1.0464671850204468
LOSS: 1.032076358795166
LOSS: 1.0184693336486816
LOSS: 1.0056431293487549
LOSS: 0.9935868978500366
LOSS: 0.9822806715965271
LOSS: 0.9716956615447998
LOSS: 0.9617953896522522
LOSS: 0.9525392055511475
LOSS: 0.9438849091529846
LOSS: 0.9357897043228149
LOSS: 0.9282125234603882
LOSS: 0.9211142063140869
LOSS: 0.9144592881202698
LOSS: 0.9082176089286804
LOSS: 0.9023619294166565
LOSS: 0.8968683481216431
LOSS: 0.8917134404182434
LOSS: 0.8868745565414429
LOSS: 0.8823280930519104
LOSS: 0.8780539631843567
LOSS: 0.8740345239639282
LOSS: 0.8702561855316162
LOSS: 0.8667078614234924
LOSS: 0.8633794188499451
LOSS: 0.8602601289749146
LOSS: 0.8573403358459473
LOSS: 0.8546063899993896
LOSS: 0.8520448207855225
LOSS: 0.8496451377868652
LOSS: 0.8473965525627136
LOSS: 0.8452908396720886
LOSS: 0.8433191776275635
LOSS: 0.841473400592804
LOSS: 0.8397462368011475
LOSS: 0.8381287455558777
LOSS: 0.8366135358810425
LOSS: 0.835196316242218
LOSS: 0.8338697552680969
LOSS: 0.8326281905174255
LOSS: 0.8314658403396606
LOSS: 0.8303763270378113
LOSS: 0.829354465007782
LOSS: 0.8283937573432922
LOSS: 0.8274886608123779
LOSS: 0.8266346454620361
LOSS: 0.8258278965950012
LOSS: 0.8250657320022583
LOSS: 0.824346125125885
LOSS: 0.8236675262451172
LOSS: 0.8230272531509399
LOSS: 0.8224226236343384
LOSS: 0.8218511939048767
LOSS: 0.821308970451355
LOSS: 0.8207936882972717
LOSS: 0.8203022480010986
LOSS: 0.8198325634002686
LOSS: 0.8193819522857666
LOSS: 0.8189500570297241
LOSS: 0.8185343146324158
LOSS: 0.8181338906288147
LOSS: 0.8177458643913269
LOSS: 0.8173686265945435
LOSS: 0.8169999718666077
LOSS: 0.8166387677192688
LOSS: 0.8162835836410522
LOSS: 0.8159326910972595
LOSS: 0.8155856132507324
LOSS: 0.8152422904968262
LOSS: 0.8149020671844482
LOSS: 0.8145670294761658
LOSS: 0.8142361640930176
LOSS: 0.8139109015464783
LOSS: 0.813592255115509
LOSS: 0.8132812976837158
LOSS: 0.8129783272743225
LOSS: 0.812684178352356
LOSS: 0.8123992085456848
LOSS: 0.8121228218078613
LOSS: 0.8118553757667542
LOSS: 0.8115957379341125
LOSS: 0.8113433122634888
LOSS: 0.8110979795455933
LOSS: 1.3493558168411255
LOSS: 1.3335986137390137
LOSS: 1.317452311515808
LOSS: 1.3010472059249878
LOSS: 1.2844822406768799
LOSS: 1.2678076028823853
LOSS: 1.2510415315628052
LOSS: 1.2342157363891602
LOSS: 1.2173774242401123
LOSS: 1.2005833387374878
LOSS: 1.1838977336883545
LOSS: 1.1673885583877563
LOSS: 1.1511226892471313
LOSS: 1.1351592540740967
LOSS: 1.1195480823516846
LOSS: 1.1043338775634766
LOSS: 1.089559555053711
LOSS: 1.0752646923065186
LOSS: 1.0614840984344482
LOSS: 1.0482442378997803
LOSS: 1.0355632305145264
LOSS: 1.023453950881958
LOSS: 1.0119234323501587
LOSS: 1.0009711980819702
LOSS: 0.9905881881713867
LOSS: 0.9807575345039368
LOSS: 0.971455454826355
LOSS: 0.9626546502113342
LOSS: 0.9543250799179077
LOSS: 0.9464355707168579
LOSS: 0.9389585852622986
LOSS: 0.9318708777427673
LOSS: 0.9251602292060852
LOSS: 0.9188200831413269
LOSS: 0.9128490090370178
LOSS: 0.9072477221488953
LOSS: 0.9020130634307861
LOSS: 0.8971379399299622
LOSS: 0.8926081657409668
LOSS: 0.8884037137031555
LOSS: 0.8844971656799316
LOSS: 0.8808574676513672
LOSS: 0.8774511814117432
LOSS: 0.8742477297782898
LOSS: 0.8712193965911865
LOSS: 0.8683441877365112
LOSS: 0.8656052947044373
LOSS: 0.862992525100708
LOSS: 0.8604991436004639
LOSS: 0.8581223487854004
LOSS: 0.8558604717254639
LOSS: 0.8537112474441528
LOSS: 0.8516701459884644
LOSS: 0.849733829498291
LOSS: 0.8478943705558777
LOSS: 0.8461428880691528
LOSS: 0.8444715738296509
LOSS: 0.8428716063499451
LOSS: 0.8413375616073608
LOSS: 0.8398643732070923
LOSS: 0.838449239730835
LOSS: 0.8370932936668396
LOSS: 0.8357983827590942
LOSS: 0.8345682621002197
LOSS: 0.8334068059921265
LOSS: 0.832317590713501
LOSS: 0.8313019871711731
LOSS: 0.8303603529930115
LOSS: 0.8294897675514221
LOSS: 0.8286857604980469
LOSS: 0.827940821647644
LOSS: 0.8272460103034973
LOSS: 0.8265911936759949
LOSS: 0.8259670734405518
LOSS: 0.8253653645515442
LOSS: 0.8247807621955872
LOSS: 0.8242095112800598
LOSS: 0.8236484527587891
LOSS: 0.8230980634689331
LOSS: 0.8225581049919128
LOSS: 0.822028636932373
LOSS: 0.8215112090110779
LOSS: 0.8210054636001587
LOSS: 1.0343241691589355
LOSS: 1.0228697061538696
LOSS: 1.0116255283355713
LOSS: 1.000646710395813
LOSS: 0.9899939894676208
LOSS: 0.9797226190567017
LOSS: 0.9698767066001892
LOSS: 0.9604861736297607
LOSS: 0.9515658617019653
LOSS: 0.9431155323982239
LOSS: 0.9351241588592529
LOSS: 0.9275739789009094
LOSS: 0.9204457402229309
LOSS: 0.9137218594551086
LOSS: 0.9073866605758667
LOSS: 0.9014273285865784
LOSS: 0.8958315849304199
LOSS: 0.8905871510505676
LOSS: 0.8856807351112366
LOSS: 0.8810976147651672
LOSS: 0.8768200874328613
LOSS: 0.8728299736976624
LOSS: 0.869105339050293
LOSS: 0.8656287789344788
LOSS: 0.8623811602592468
LOSS: 0.8593485951423645
LOSS: 0.8565191030502319
LOSS: 0.8538808822631836
LOSS: 0.8514251112937927
LOSS: 0.8491378426551819
LOSS: 0.8470045924186707
LOSS: 0.8450092673301697
LOSS: 0.843136727809906
LOSS: 0.841370701789856
LOSS: 0.8396980166435242
LOSS: 0.8381078243255615
LOSS: 0.836590588092804
LOSS: 0.83514004945755
LOSS: 0.8337507247924805
LOSS: 0.8324199318885803
LOSS: 0.8311466574668884
LOSS: 0.8299325108528137
LOSS: 0.8287786245346069
LOSS: 0.8276875019073486
LOSS: 0.8266597390174866
LOSS: 0.8256943225860596
LOSS: 0.8247902989387512
LOSS: 0.8239442110061646
LOSS: 0.8231526613235474
LOSS: 0.822412371635437
LOSS: 0.8217189311981201
LOSS: 0.821067214012146
LOSS: 0.8204545378684998
LOSS: 0.8198767304420471
LOSS: 0.8193306922912598
LOSS: 0.818812906742096
LOSS: 0.8183202147483826
LOSS: 0.8178505897521973
LOSS: 0.8174012303352356
LOSS: 0.8169703483581543
LOSS: 0.816555917263031
LOSS: 0.816156268119812
LOSS: 0.8157694339752197
LOSS: 0.815394937992096
LOSS: 0.8150304555892944
LOSS: 0.8146756291389465
LOSS: 0.8143293857574463
LOSS: 0.8139913082122803
LOSS: 0.8136609196662903
LOSS: 0.813336968421936
LOSS: 0.813018798828125
LOSS: 0.8127067685127258
LOSS: 0.8124017119407654
LOSS: 0.8121035695075989
LOSS: 0.8118128776550293
LOSS: 0.8115291595458984
LOSS: 0.8112531304359436
LOSS: 0.810982882976532
LOSS: 1.345861554145813
LOSS: 1.3290725946426392
LOSS: 1.3119359016418457
LOSS: 1.2945395708084106
LOSS: 1.2769583463668823
LOSS: 1.2592440843582153
LOSS: 1.2413939237594604
LOSS: 1.223410725593567
LOSS: 1.2053475379943848
LOSS: 1.1872913837432861
LOSS: 1.1693400144577026
LOSS: 1.1515921354293823
LOSS: 1.1341485977172852
LOSS: 1.1171048879623413
LOSS: 1.1005476713180542
LOSS: 1.0845555067062378
LOSS: 1.0691967010498047
LOSS: 1.0545234680175781
LOSS: 1.0405679941177368
LOSS: 1.0273425579071045
LOSS: 1.014841079711914
LOSS: 1.0030447244644165
LOSS: 0.9919243454933167
LOSS: 0.9814452528953552
LOSS: 0.9715710282325745
LOSS: 0.9622640013694763
LOSS: 0.9534873962402344
LOSS: 0.9452049136161804
LOSS: 0.9373799562454224
LOSS: 0.92997807264328
LOSS: 0.9229683876037598
LOSS: 0.916329026222229
LOSS: 0.9100437760353088
LOSS: 0.9041029810905457
LOSS: 0.8984965085983276
LOSS: 0.8932151794433594
LOSS: 0.88824862241745
LOSS: 0.8835822939872742
LOSS: 0.8792020678520203
LOSS: 0.8750908970832825
LOSS: 0.8712274432182312
LOSS: 0.8675891757011414
LOSS: 0.8641539216041565
LOSS: 0.8608973026275635
LOSS: 0.8577993512153625
LOSS: 0.8548427820205688
LOSS: 0.8520193696022034
LOSS: 0.8493207693099976
LOSS: 0.8467440605163574
LOSS: 0.8442879319190979
LOSS: 0.8419519066810608
LOSS: 0.8397361040115356
LOSS: 0.8376368880271912
LOSS: 0.8356510996818542
LOSS: 0.8337705731391907
LOSS: 0.8319883942604065
LOSS: 0.830296516418457
LOSS: 0.8286877274513245
LOSS: 0.8271552920341492
LOSS: 0.8256965279579163
LOSS: 0.8243076205253601
LOSS: 0.8229882121086121
LOSS: 0.8217380046844482
LOSS: 0.8205574154853821
LOSS: 0.8194447159767151
LOSS: 0.818398118019104
LOSS: 0.8174150586128235
LOSS: 0.8164919018745422
LOSS: 0.8156223893165588
LOSS: 0.8148050904273987
LOSS: 0.8140347599983215
LOSS: 0.8133067488670349
LOSS: 0.8126174211502075
LOSS: 0.8119634389877319
LOSS: 0.8113416433334351
LOSS: 0.8107491135597229
LOSS: 0.8101820349693298
LOSS: 0.809639573097229
LOSS: 0.8091180324554443
LOSS: 0.8086165189743042
LOSS: 0.8081322312355042
LOSS: 0.8076643943786621
LOSS: 0.8072117567062378
LOSS: 0.8067732453346252
LOSS: 0.806349515914917
LOSS: 0.8059399724006653
LOSS: 0.8055443167686462
LOSS: 0.805164098739624
LOSS: 0.804798424243927
LOSS: 0.8044466376304626
LOSS: 0.8041080236434937
LOSS: 0.8037822246551514
LOSS: 0.803466796875
LOSS: 0.8031617999076843
LOSS: 0.8028649091720581
LOSS: 0.8025756478309631
LOSS: 0.8022923469543457
LOSS: 0.8020156621932983
LOSS: 0.8017449378967285
LOSS: 0.8014805316925049
LOSS: 1.345861554145813
LOSS: 1.3290725946426392
LOSS: 1.3119359016418457
LOSS: 1.2945395708084106
LOSS: 1.2769583463668823
LOSS: 1.2592440843582153
LOSS: 1.2413939237594604
LOSS: 1.223410725593567
LOSS: 1.2053475379943848
LOSS: 1.1872913837432861
LOSS: 1.1693400144577026
LOSS: 1.1515921354293823
LOSS: 1.1341485977172852
LOSS: 1.1171048879623413
LOSS: 1.1005476713180542
LOSS: 1.0845555067062378
LOSS: 1.0691967010498047
LOSS: 1.0545234680175781
LOSS: 1.0405679941177368
LOSS: 1.0273425579071045
LOSS: 1.014841079711914
LOSS: 1.0030447244644165
LOSS: 0.9919243454933167
LOSS: 0.9814452528953552
LOSS: 0.9715710282325745
LOSS: 0.9622640013694763
LOSS: 0.9534873962402344
LOSS: 0.9452049136161804
LOSS: 0.9373799562454224
LOSS: 0.92997807264328
LOSS: 0.9229683876037598
LOSS: 0.916329026222229
LOSS: 0.9100437760353088
LOSS: 0.9041029810905457
LOSS: 0.8984965085983276
LOSS: 0.8932151794433594
LOSS: 0.88824862241745
LOSS: 0.8835822939872742
LOSS: 0.8792020678520203
LOSS: 0.8750908970832825
LOSS: 0.8712274432182312
LOSS: 0.8675891757011414
LOSS: 0.8641539216041565
LOSS: 0.8608973026275635
LOSS: 0.8577993512153625
LOSS: 0.8548427820205688
LOSS: 0.8520193696022034
LOSS: 0.8493207693099976
LOSS: 0.8467440605163574
LOSS: 0.8442879319190979
LOSS: 0.8419519066810608
LOSS: 0.8397361040115356
LOSS: 0.8376368880271912
LOSS: 0.8356510996818542
LOSS: 0.8337705731391907
LOSS: 0.8319883942604065
LOSS: 0.830296516418457
LOSS: 0.8286877274513245
LOSS: 0.8271552920341492
LOSS: 0.8256965279579163
LOSS: 0.8243076205253601
LOSS: 0.8229882121086121
LOSS: 0.8217380046844482
LOSS: 0.8205574154853821
LOSS: 0.8194447159767151
LOSS: 0.818398118019104
LOSS: 0.8174150586128235
LOSS: 0.8164919018745422
LOSS: 0.8156223893165588
LOSS: 0.8148050904273987
LOSS: 0.8140347599983215
LOSS: 0.8133067488670349
LOSS: 0.8126174211502075
LOSS: 0.8119634389877319
LOSS: 0.8113416433334351
LOSS: 0.8107491135597229
LOSS: 0.8101820349693298
LOSS: 0.809639573097229
LOSS: 0.8091180324554443
LOSS: 0.8086165189743042
LOSS: 0.8081322312355042
LOSS: 0.8076643943786621
LOSS: 0.8072117567062378
LOSS: 0.8067732453346252
LOSS: 0.806349515914917
LOSS: 0.8059399724006653
LOSS: 0.8055443167686462
LOSS: 0.805164098739624
LOSS: 0.804798424243927
LOSS: 0.8044466376304626
LOSS: 0.8041080236434937
LOSS: 0.8037822246551514
LOSS: 0.803466796875
LOSS: 0.8031617999076843
LOSS: 0.8028649091720581
LOSS: 0.8025756478309631
LOSS: 0.8022923469543457
LOSS: 0.8020156621932983
LOSS: 0.8017449378967285
LOSS: 0.8014805316925049
LOSS: 1.3492063283920288
LOSS: 1.3325457572937012
LOSS: 1.3153133392333984
LOSS: 1.297615647315979
LOSS: 1.2795473337173462
LOSS: 1.261146068572998
LOSS: 1.2424468994140625
LOSS: 1.2235230207443237
LOSS: 1.2044721841812134
LOSS: 1.1854078769683838
LOSS: 1.1664541959762573
LOSS: 1.1477370262145996
LOSS: 1.1293808221817017
LOSS: 1.1115024089813232
LOSS: 1.0942015647888184
LOSS: 1.077559471130371
LOSS: 1.0616352558135986
LOSS: 1.0464671850204468
LOSS: 1.032076358795166
LOSS: 1.0184693336486816
LOSS: 1.0056431293487549
LOSS: 0.9935868978500366
LOSS: 0.9822806715965271
LOSS: 0.9716956615447998
LOSS: 0.9617953896522522
LOSS: 0.9525392055511475
LOSS: 0.9438849091529846
LOSS: 0.9357897043228149
LOSS: 0.9282125234603882
LOSS: 0.9211142063140869
LOSS: 0.9144592881202698
LOSS: 0.9082176089286804
LOSS: 0.9023619294166565
LOSS: 0.8968683481216431
LOSS: 0.8917134404182434
LOSS: 0.8868745565414429
LOSS: 0.8823280930519104
LOSS: 0.8780539631843567
LOSS: 0.8740345239639282
LOSS: 0.8702561855316162
LOSS: 0.8667078614234924
LOSS: 0.8633794188499451
LOSS: 0.8602601289749146
LOSS: 0.8573403358459473
LOSS: 0.8546063899993896
LOSS: 0.8520448207855225
LOSS: 0.8496451377868652
LOSS: 0.8473965525627136
LOSS: 0.8452908396720886
LOSS: 0.8433191776275635
LOSS: 0.841473400592804
LOSS: 0.8397462368011475
LOSS: 0.8655477166175842
LOSS: 0.8632897138595581
LOSS: 0.8611799478530884
LOSS: 0.8592148423194885
LOSS: 0.857387125492096
LOSS: 0.8556876182556152
LOSS: 0.8541072607040405
LOSS: 0.8526342511177063
LOSS: 0.8512592315673828
LOSS: 0.8499735593795776
LOSS: 0.848768413066864
LOSS: 0.8476355671882629
LOSS: 0.8465701341629028
LOSS: 0.8455637097358704
LOSS: 0.8446123003959656
LOSS: 0.8437119126319885
LOSS: 0.8428584933280945
LOSS: 0.8420493006706238
LOSS: 0.841282844543457
LOSS: 0.8405565023422241
LOSS: 0.8398669362068176
LOSS: 0.8392118811607361
LOSS: 0.8385885953903198
LOSS: 0.8379926085472107
LOSS: 0.8374230861663818
LOSS: 0.8368764519691467
LOSS: 0.8363515734672546
LOSS: 0.8358475565910339
LOSS: 0.8353629112243652
LOSS: 0.8348966836929321
LOSS: 0.8344486951828003
LOSS: 0.8340171575546265
LOSS: 0.8336015343666077
LOSS: 0.833200216293335
LOSS: 0.832811713218689
LOSS: 0.832434356212616
LOSS: 0.8320675492286682
LOSS: 0.831710159778595
LOSS: 0.8313623070716858
LOSS: 0.8310243487358093
LOSS: 0.830695629119873
LOSS: 0.8303773999214172
LOSS: 0.8300690054893494
LOSS: 0.8297706842422485
LOSS: 0.8294819593429565
LOSS: 0.8292019367218018
LOSS: 0.8289296627044678
LOSS: 0.8286644816398621
LOSS: 0.8284050226211548
LOSS: 0.8281522989273071
LOSS: 1.3493558168411255
LOSS: 1.3335986137390137
LOSS: 1.317452311515808
LOSS: 1.3010472059249878
LOSS: 1.2844822406768799
LOSS: 1.2678076028823853
LOSS: 1.2510415315628052
LOSS: 1.2342157363891602
LOSS: 1.2173774242401123
LOSS: 1.2005833387374878
LOSS: 1.1838977336883545
LOSS: 1.1673885583877563
LOSS: 1.1511226892471313
LOSS: 1.1351592540740967
LOSS: 1.1195480823516846
LOSS: 1.1043338775634766
LOSS: 1.089559555053711
LOSS: 1.0752646923065186
LOSS: 1.0614840984344482
LOSS: 1.0482442378997803
LOSS: 1.0355632305145264
LOSS: 1.023453950881958
LOSS: 1.0119234323501587
LOSS: 1.0009711980819702
LOSS: 0.9905881881713867
LOSS: 0.9807575345039368
LOSS: 0.971455454826355
LOSS: 0.9626546502113342
LOSS: 0.9543250799179077
LOSS: 0.9464355707168579
LOSS: 0.9389585852622986
LOSS: 0.9318708777427673
LOSS: 0.9251602292060852
LOSS: 0.9188200831413269
LOSS: 0.9128490090370178
LOSS: 0.9072477221488953
LOSS: 0.9020130634307861
LOSS: 0.8971379399299622
LOSS: 0.8926081657409668
LOSS: 0.8884037137031555
LOSS: 0.8844971656799316
LOSS: 0.8808574676513672
LOSS: 0.8774511814117432
LOSS: 0.8742477297782898
LOSS: 0.8712193965911865
LOSS: 0.8683441877365112
LOSS: 0.8656052947044373
LOSS: 0.862992525100708
LOSS: 0.8604991436004639
LOSS: 0.8581223487854004
LOSS: 0.8558604717254639
LOSS: 0.8537112474441528
LOSS: 0.8516701459884644
LOSS: 0.849733829498291
LOSS: 0.8478943705558777
LOSS: 0.8461428880691528
LOSS: 0.8444715738296509
LOSS: 0.8428716063499451
LOSS: 0.8413375616073608
LOSS: 0.8398643732070923
LOSS: 0.838449239730835
LOSS: 0.8370932936668396
LOSS: 0.8357983827590942
LOSS: 0.8345682621002197
LOSS: 0.8334068059921265
LOSS: 0.832317590713501
LOSS: 0.8313019871711731
LOSS: 0.8303603529930115
LOSS: 0.8294897675514221
LOSS: 0.8286857604980469
LOSS: 0.827940821647644
LOSS: 0.8272460103034973
LOSS: 0.8265911936759949
LOSS: 0.8259670734405518
LOSS: 0.8253653645515442
LOSS: 0.8247807621955872
LOSS: 0.8242095112800598
LOSS: 0.8236484527587891
LOSS: 0.8230980634689331
LOSS: 0.8225581049919128
LOSS: 0.822028636932373
LOSS: 0.8215112090110779
LOSS: 0.8210054636001587
LOSS: 0.820512056350708
LOSS: 0.8200299739837646
LOSS: 0.8195608258247375
LOSS: 0.8191031217575073
LOSS: 0.8186575174331665
LOSS: 0.8182231187820435
LOSS: 0.8177996873855591
LOSS: 0.817387044429779
LOSS: 0.8169848322868347
LOSS: 0.8165937066078186
LOSS: 0.8162118196487427
LOSS: 0.8158401250839233
LOSS: 0.8154774904251099
LOSS: 0.8151236772537231
LOSS: 0.8147780299186707
LOSS: 0.8144404292106628
LOSS: 0.8141095638275146
LOSS: 1.3438860177993774
LOSS: 1.3277429342269897
LOSS: 1.3112109899520874
LOSS: 1.2943991422653198
LOSS: 1.2773966789245605
LOSS: 1.260266661643982
LOSS: 1.2430485486984253
LOSS: 1.2257919311523438
LOSS: 1.2085680961608887
LOSS: 1.1914563179016113
LOSS: 1.1745390892028809
LOSS: 1.1578948497772217
LOSS: 1.1415992975234985
LOSS: 1.1257258653640747
LOSS: 1.1103438138961792
LOSS: 1.0955159664154053
LOSS: 1.081292748451233
LOSS: 1.0677083730697632
LOSS: 1.0547770261764526
LOSS: 1.042494297027588
LOSS: 1.030839204788208
LOSS: 1.019778847694397
LOSS: 1.0092723369598389
LOSS: 0.999274492263794
LOSS: 0.9897391200065613
LOSS: 0.9806239604949951
LOSS: 0.9718908071517944
LOSS: 0.9635064601898193
LOSS: 0.9554463624954224
LOSS: 0.9476945400238037
LOSS: 0.9402439594268799
LOSS: 0.9330954551696777
LOSS: 0.9262558221817017
LOSS: 0.9197333455085754
LOSS: 0.9135348796844482
LOSS: 0.9076658487319946
LOSS: 0.9021276235580444
LOSS: 0.8969179391860962
LOSS: 0.892028272151947
LOSS: 0.8874432444572449
LOSS: 0.8831440806388855
LOSS: 0.8791083693504333
LOSS: 0.8753108978271484
LOSS: 0.8717331886291504
LOSS: 0.8683568239212036
LOSS: 0.8651697635650635
LOSS: 0.8621612787246704
LOSS: 0.8593266010284424
LOSS: 0.8566616177558899
LOSS: 0.8541638851165771
LOSS: 0.8518300652503967
LOSS: 0.8496549725532532
LOSS: 0.8476320505142212
LOSS: 0.8457514047622681
LOSS: 0.8440015912055969
LOSS: 0.8423712253570557
LOSS: 0.8408472537994385
LOSS: 0.8394191265106201
LOSS: 0.8380786180496216
LOSS: 0.8368192315101624
LOSS: 0.8356356024742126
LOSS: 0.8345239758491516
LOSS: 0.8334789276123047
LOSS: 0.8324940800666809
LOSS: 0.8315628170967102
LOSS: 0.830677330493927
LOSS: 0.8298313617706299
LOSS: 0.8290200233459473
LOSS: 0.8282381892204285
LOSS: 0.8274824023246765
LOSS: 0.8267513513565063
LOSS: 0.8260444402694702
LOSS: 0.8253617286682129
LOSS: 0.8247034549713135
LOSS: 0.8240699172019958
LOSS: 0.8234610557556152
LOSS: 0.8228756189346313
LOSS: 0.822313129901886
LOSS: 0.8217727541923523
LOSS: 0.8212514519691467
LOSS: 0.8207476735115051
LOSS: 0.820260763168335
LOSS: 0.8197876811027527
LOSS: 0.8193284273147583
LOSS: 0.8188815712928772
LOSS: 0.8184463977813721
LOSS: 0.818021833896637
LOSS: 0.8176075220108032
LOSS: 0.81720370054245
LOSS: 0.8168088793754578
LOSS: 0.816422700881958
LOSS: 0.8160446882247925
LOSS: 0.815674364566803
LOSS: 0.8153119087219238
LOSS: 0.8149561882019043
LOSS: 0.8146083354949951
LOSS: 0.8142671585083008
LOSS: 0.8139334917068481
LOSS: 0.8136075139045715
LOSS: 0.8132887482643127
LOSS: 1.3673166036605835
LOSS: 1.3608299493789673
LOSS: 1.3547190427780151
LOSS: 1.3490206003189087
LOSS: 1.343767523765564
LOSS: 1.3389900922775269
LOSS: 1.3347039222717285
LOSS: 1.330911636352539
LOSS: 1.3275871276855469
LOSS: 1.3246703147888184
LOSS: 1.3220651149749756
LOSS: 1.3196481466293335
LOSS: 1.3172869682312012
LOSS: 1.3148653507232666
LOSS: 1.3122949600219727
LOSS: 1.3095207214355469
LOSS: 1.3065139055252075
LOSS: 1.3032655715942383
LOSS: 1.2997791767120361
LOSS: 1.2960647344589233
LOSS: 1.2921391725540161
LOSS: 1.2880191802978516
LOSS: 1.2837247848510742
LOSS: 1.2792778015136719
LOSS: 1.2746986150741577
LOSS: 1.2700080871582031
LOSS: 1.2652255296707153
LOSS: 1.260367512702942
LOSS: 1.2554433345794678
LOSS: 1.2504551410675049
LOSS: 1.245396614074707
LOSS: 1.240260362625122
LOSS: 1.2350351810455322
LOSS: 1.2297160625457764
LOSS: 1.2243053913116455
LOSS: 1.2188127040863037
LOSS: 1.2132551670074463
LOSS: 1.2076592445373535
LOSS: 1.2020606994628906
LOSS: 1.1964987516403198
LOSS: 1.1910144090652466
LOSS: 1.1856484413146973
LOSS: 1.180432677268982
LOSS: 1.1753880977630615
LOSS: 1.170526385307312
LOSS: 1.1658583879470825
LOSS: 1.1613993644714355
LOSS: 1.157153844833374
LOSS: 1.1531072854995728
LOSS: 1.1492400169372559
LOSS: 1.1455397605895996
LOSS: 1.1419880390167236
LOSS: 1.1385608911514282
LOSS: 1.1352462768554688
LOSS: 1.1320483684539795
LOSS: 1.1289796829223633
LOSS: 1.1260570287704468
LOSS: 1.1232863664627075
LOSS: 1.1206510066986084
LOSS: 1.1181107759475708
LOSS: 1.1156156063079834
LOSS: 1.1131268739700317
LOSS: 1.1106265783309937
LOSS: 1.1081111431121826
LOSS: 1.105599045753479
LOSS: 1.1031064987182617
LOSS: 1.1006500720977783
LOSS: 1.0982404947280884
LOSS: 1.0958889722824097
LOSS: 1.0936150550842285
LOSS: 1.0914340019226074
LOSS: 1.089351773262024
LOSS: 1.0873616933822632
LOSS: 1.08545982837677
LOSS: 1.083644986152649
LOSS: 1.081911325454712
LOSS: 1.0802305936813354
LOSS: 1.0785738229751587
LOSS: 1.0769282579421997
LOSS: 0.8476320505142212
LOSS: 0.8457514047622681
LOSS: 0.8440015912055969
LOSS: 0.8423712253570557
LOSS: 0.8408472537994385
LOSS: 0.8394191265106201
LOSS: 0.8380786180496216
LOSS: 0.8368192315101624
LOSS: 0.8356356024742126
LOSS: 0.8345239758491516
LOSS: 0.8334789276123047
LOSS: 0.8324940800666809
LOSS: 0.8315628170967102
LOSS: 0.830677330493927
LOSS: 0.8298313617706299
LOSS: 0.8290200233459473
LOSS: 0.8282381892204285
LOSS: 0.8274824023246765
LOSS: 0.8267513513565063
LOSS: 0.8260444402694702
LOSS: 0.8253617286682129
LOSS: 0.8247034549713135
LOSS: 0.8240699172019958
LOSS: 0.8234610557556152
LOSS: 0.8228756189346313
LOSS: 0.822313129901886
LOSS: 0.8217727541923523
LOSS: 0.8212514519691467
LOSS: 0.8207476735115051
LOSS: 0.820260763168335
LOSS: 0.8197876811027527
LOSS: 0.8193284273147583
LOSS: 0.8188815712928772
LOSS: 0.8184463977813721
LOSS: 0.818021833896637
LOSS: 0.8176075220108032
LOSS: 0.81720370054245
LOSS: 0.8168088793754578
LOSS: 0.816422700881958
LOSS: 0.8160446882247925
LOSS: 0.815674364566803
LOSS: 0.8153119087219238
LOSS: 0.8149561882019043
LOSS: 0.8146083354949951
LOSS: 0.8142671585083008
LOSS: 0.8139334917068481
LOSS: 0.8136075139045715
LOSS: 0.8132887482643127
LOSS: 1.34933602809906
LOSS: 1.3334295749664307
LOSS: 1.3170086145401
LOSS: 1.3001437187194824
LOSS: 1.2828878164291382
LOSS: 1.2652665376663208
LOSS: 1.247326135635376
LOSS: 1.2291266918182373
LOSS: 1.2107480764389038
LOSS: 1.1922969818115234
LOSS: 1.1739013195037842
LOSS: 1.1556957960128784
LOSS: 1.137807011604309
LOSS: 1.1203477382659912
LOSS: 1.103417992591858
LOSS: 1.0871100425720215
LOSS: 1.0715043544769287
LOSS: 1.0566657781600952
LOSS: 1.0426385402679443
LOSS: 1.029444694519043
LOSS: 1.0170851945877075
LOSS: 1.0055444240570068
LOSS: 0.9947890043258667
LOSS: 0.9847703576087952
LOSS: 0.9754292368888855
LOSS: 0.9667015075683594
LOSS: 0.9585272669792175
LOSS: 0.95085209608078
LOSS: 0.9436271786689758
LOSS: 0.9368107318878174
LOSS: 0.9303656220436096
LOSS: 0.9242618083953857
LOSS: 0.9184733629226685
LOSS: 0.9129831790924072
LOSS: 0.9077804684638977
LOSS: 0.902862548828125
LOSS: 0.8982309699058533
LOSS: 0.8938869833946228
LOSS: 0.8898292779922485
LOSS: 0.8860518336296082
LOSS: 0.8825412392616272
LOSS: 0.879281759262085
LOSS: 0.8762539625167847
LOSS: 0.8734365701675415
LOSS: 0.8708096742630005
LOSS: 0.8683546185493469
LOSS: 0.8660537004470825
LOSS: 0.8638883233070374
LOSS: 0.8618440628051758
LOSS: 0.8599076271057129
LOSS: 0.8580718636512756
LOSS: 0.856332540512085
LOSS: 0.8546885848045349
LOSS: 0.8531404733657837
LOSS: 0.851690411567688
LOSS: 0.850339949131012
LOSS: 0.8490895628929138
LOSS: 0.847937822341919
LOSS: 0.8468818068504333
LOSS: 0.8459154963493347
LOSS: 0.8450285792350769
LOSS: 0.8442086577415466
LOSS: 0.8434421420097351
LOSS: 0.8427154421806335
LOSS: 0.8420202136039734
LOSS: 0.8413493633270264
LOSS: 0.8407002687454224
LOSS: 0.8400736451148987
LOSS: 0.8394708037376404
LOSS: 0.8388938307762146
LOSS: 0.8383447527885437
LOSS: 0.8378244042396545
LOSS: 0.8373328447341919
LOSS: 0.8368683457374573
LOSS: 0.8364297747612
LOSS: 0.8360142707824707
LOSS: 0.8356194496154785
LOSS: 0.8352429866790771
LOSS: 0.8348824977874756
LOSS: 0.8345363140106201
LOSS: 0.8342032432556152
LOSS: 0.8338811993598938
LOSS: 0.833568811416626
LOSS: 0.8332657814025879
LOSS: 0.8329702019691467
LOSS: 0.8326819539070129
LOSS: 0.8324010372161865
LOSS: 0.8321262001991272
LOSS: 0.8318575620651245
LOSS: 0.8315951228141785
LOSS: 0.8313377499580383
LOSS: 0.831086277961731
LOSS: 0.8308411240577698
LOSS: 0.8306024074554443
LOSS: 0.8303698897361755
LOSS: 0.8301438093185425
LOSS: 0.8299240469932556
LOSS: 0.8297101855278015
LOSS: 0.8295025825500488
LOSS: 0.8293004035949707
LOSS: 1.3880085945129395
LOSS: 1.382020354270935
LOSS: 1.376430630683899
LOSS: 1.37127685546875
LOSS: 1.3665940761566162
LOSS: 1.3624111413955688
LOSS: 1.3587453365325928
LOSS: 1.3555952310562134
LOSS: 1.352929711341858
LOSS: 1.3506847620010376
LOSS: 1.348755121231079
LOSS: 1.347008228302002
LOSS: 1.3453043699264526
LOSS: 1.3435204029083252
LOSS: 1.341564416885376
LOSS: 1.3393776416778564
LOSS: 1.3369290828704834
LOSS: 1.334202527999878
LOSS: 1.3311902284622192
LOSS: 1.3278833627700806
LOSS: 1.324275016784668
LOSS: 1.3203587532043457
LOSS: 1.3161303997039795
LOSS: 1.3115874528884888
LOSS: 1.3067330121994019
LOSS: 1.30157470703125
LOSS: 1.296125054359436
LOSS: 1.290403962135315
LOSS: 1.284433364868164
LOSS: 1.2782392501831055
LOSS: 1.2718449831008911
LOSS: 1.2652738094329834
LOSS: 1.2585501670837402
LOSS: 1.2517002820968628
LOSS: 1.2447572946548462
LOSS: 1.237761378288269
LOSS: 1.230760931968689
LOSS: 1.2238115072250366
LOSS: 1.2169766426086426
LOSS: 1.2103255987167358
LOSS: 1.2039240598678589
LOSS: 1.1978309154510498
LOSS: 1.1920926570892334
LOSS: 1.1867406368255615
LOSS: 1.1817876100540161
LOSS: 1.1772291660308838
LOSS: 1.1730412244796753
LOSS: 1.1691824197769165
LOSS: 1.1655997037887573
LOSS: 1.1622370481491089
LOSS: 1.159042477607727
LOSS: 1.155971646308899
LOSS: 1.1529885530471802
LOSS: 1.1500672101974487
LOSS: 1.1472053527832031
LOSS: 1.1444199085235596
LOSS: 1.141734004020691
LOSS: 1.139167070388794
LOSS: 1.1367300748825073
LOSS: 1.1344281435012817
LOSS: 1.1322585344314575
LOSS: 1.130210518836975
LOSS: 1.12826669216156
LOSS: 1.126403570175171
LOSS: 1.1245956420898438
LOSS: 1.1228243112564087
LOSS: 1.121078372001648
LOSS: 1.1193513870239258
LOSS: 1.1176403760910034
LOSS: 1.1159436702728271
LOSS: 1.1142606735229492
LOSS: 1.1125928163528442
LOSS: 1.1109437942504883
LOSS: 1.109318494796753
LOSS: 1.1077182292938232
LOSS: 1.1061385869979858
LOSS: 1.1045687198638916
LOSS: 1.1029940843582153
LOSS: 1.1013990640640259
LOSS: 1.0997728109359741
LOSS: 1.0981134176254272
LOSS: 1.0964317321777344
LOSS: 1.0947520732879639
LOSS: 1.093109369277954
LOSS: 1.0915454626083374
LOSS: 1.0900996923446655
LOSS: 1.0887960195541382
LOSS: 1.0876307487487793
LOSS: 1.086569905281067
LOSS: 1.085572600364685
LOSS: 1.0846070051193237
LOSS: 1.0836454629898071
LOSS: 1.0826609134674072
LOSS: 1.081641435623169
LOSS: 1.0805974006652832
LOSS: 1.0795552730560303
LOSS: 1.0785428285598755
LOSS: 1.0775820016860962
LOSS: 1.0766820907592773
LOSS: 1.075842261314392
LOSS: 1.3728927373886108
LOSS: 1.3665779829025269
LOSS: 1.360646367073059
LOSS: 1.3551334142684937
LOSS: 1.3500725030899048
LOSS: 1.3454906940460205
LOSS: 1.3414058685302734
LOSS: 1.3378167152404785
LOSS: 1.3346939086914062
LOSS: 1.3319745063781738
LOSS: 1.3295570611953735
LOSS: 1.327317714691162
LOSS: 1.3251276016235352
LOSS: 1.322877049446106
LOSS: 1.3204894065856934
LOSS: 1.3179199695587158
LOSS: 1.3151488304138184
LOSS: 1.312171459197998
LOSS: 1.3089945316314697
LOSS: 1.3056234121322632
LOSS: 1.3020687103271484
LOSS: 1.29833984375
LOSS: 1.2944453954696655
LOSS: 1.2903939485549927
LOSS: 1.2861928939819336
LOSS: 1.2818478345870972
LOSS: 1.2773617506027222
LOSS: 1.2727361917495728
LOSS: 1.2679693698883057
LOSS: 1.2630577087402344
LOSS: 1.2579987049102783
LOSS: 1.2527923583984375
LOSS: 1.2474409341812134
LOSS: 1.2419534921646118
LOSS: 1.236344814300537
LOSS: 1.2306358814239502
LOSS: 1.2248567342758179
LOSS: 1.2190401554107666
LOSS: 1.2132267951965332
LOSS: 1.2074605226516724
LOSS: 1.201784372329712
LOSS: 1.1962404251098633
LOSS: 1.1908689737319946
LOSS: 1.185701847076416
LOSS: 1.1807698011398315
LOSS: 1.1760945320129395
LOSS: 1.1716893911361694
LOSS: 1.1675574779510498
LOSS: 1.1636894941329956
LOSS: 1.1600669622421265
LOSS: 1.1566650867462158
LOSS: 1.1534537076950073
LOSS: 1.1503955125808716
LOSS: 1.1474508047103882
LOSS: 1.1445809602737427
LOSS: 1.1417571306228638
LOSS: 1.1389663219451904
LOSS: 1.1362141370773315
LOSS: 1.1335182189941406
LOSS: 1.1308997869491577
LOSS: 1.128379225730896
LOSS: 1.1259747743606567
LOSS: 1.1236886978149414
LOSS: 1.1215053796768188
LOSS: 1.1194016933441162
LOSS: 1.117363452911377
LOSS: 1.1153830289840698
LOSS: 1.1134557723999023
LOSS: 1.1115789413452148
LOSS: 1.1097486019134521
LOSS: 1.1079565286636353
LOSS: 1.1061928272247314
LOSS: 1.1044458150863647
LOSS: 1.102706789970398
LOSS: 1.1009682416915894
LOSS: 1.0992261171340942
LOSS: 1.0974794626235962
LOSS: 1.0957292318344116
LOSS: 1.0939780473709106
LOSS: 1.0922261476516724
LOSS: 1.09047532081604
LOSS: 1.0887311697006226

LOSS: 0.8381287455558777
LOSS: 0.8366135358810425
LOSS: 0.835196316242218
LOSS: 0.8338697552680969
LOSS: 0.8326281905174255
LOSS: 0.8314658403396606
LOSS: 0.8303763270378113
LOSS: 0.829354465007782
LOSS: 0.8283937573432922
LOSS: 0.8274886608123779
LOSS: 0.8266346454620361
LOSS: 0.8258278965950012
LOSS: 0.8250657320022583
LOSS: 0.824346125125885
LOSS: 0.8236675262451172
LOSS: 0.8230272531509399
LOSS: 0.8224226236343384
LOSS: 0.8218511939048767
LOSS: 0.821308970451355
LOSS: 0.8207936882972717
LOSS: 0.8203022480010986
LOSS: 0.8198325634002686
LOSS: 0.8193819522857666
LOSS: 0.8189500570297241
LOSS: 0.8185343146324158
LOSS: 0.8181338906288147
LOSS: 0.8177458643913269
LOSS: 0.8173686265945435
LOSS: 0.8169999718666077
LOSS: 0.8166387677192688
LOSS: 0.8162835836410522
LOSS: 0.8159326910972595
LOSS: 0.8155856132507324
LOSS: 0.8152422904968262
LOSS: 0.8149020671844482
LOSS: 0.8145670294761658
LOSS: 0.8142361640930176
LOSS: 0.8139109015464783
LOSS: 0.813592255115509
LOSS: 0.8132812976837158
LOSS: 0.8129783272743225
LOSS: 0.812684178352356
LOSS: 0.8123992085456848
LOSS: 0.8121228218078613
LOSS: 0.8118553757667542
LOSS: 0.8115957379341125
LOSS: 0.8113433122634888
LOSS: 0.8110979795455933
LOSS: 1.3809418678283691
LOSS: 1.3747531175613403
LOSS: 1.3689568042755127
LOSS: 1.3635891675949097
LOSS: 1.3586878776550293
LOSS: 1.3542839288711548
LOSS: 1.3504003286361694
LOSS: 1.3470447063446045
LOSS: 1.3442015647888184
LOSS: 1.3418232202529907
LOSS: 1.3398267030715942
LOSS: 1.338098406791687
LOSS: 1.3365073204040527
LOSS: 1.3349277973175049
LOSS: 1.333258867263794
LOSS: 1.3314319849014282
LOSS: 1.3294093608856201
LOSS: 1.3271763324737549
LOSS: 1.3247376680374146
LOSS: 1.3221063613891602
LOSS: 1.3193014860153198
LOSS: 1.3163408041000366
LOSS: 1.313246488571167
LOSS: 1.3100334405899048
LOSS: 1.3067176342010498
LOSS: 1.3033099174499512
LOSS: 1.2998203039169312
LOSS: 1.2962536811828613
LOSS: 1.2926156520843506
LOSS: 1.2889093160629272
LOSS: 1.2851431369781494
LOSS: 1.2813283205032349
LOSS: 1.2774763107299805
LOSS: 1.2736047506332397
LOSS: 1.269727110862732
LOSS: 1.2658554315567017
LOSS: 1.2620031833648682
LOSS: 1.2581783533096313
LOSS: 1.2543883323669434
LOSS: 1.2506365776062012
LOSS: 1.2469236850738525
LOSS: 1.2432489395141602
LOSS: 1.2396095991134644
LOSS: 1.2360050678253174
LOSS: 1.2324389219284058
LOSS: 1.2289180755615234
LOSS: 1.2254537343978882
LOSS: 1.222060203552246
LOSS: 1.218752145767212
LOSS: 1.215537667274475
LOSS: 1.2124176025390625
LOSS: 1.2093781232833862
LOSS: 1.2064050436019897
LOSS: 1.2034783363342285
LOSS: 1.2005804777145386
LOSS: 1.1977016925811768
LOSS: 1.194840908050537
LOSS: 1.1920140981674194
LOSS: 1.1892420053482056
LOSS: 1.1865495443344116
LOSS: 1.183953046798706
LOSS: 1.1814377307891846
LOSS: 1.1789748668670654
LOSS: 1.1765377521514893
LOSS: 1.1741079092025757
LOSS: 1.1716749668121338
LOSS: 1.1692482233047485
LOSS: 1.1668269634246826
LOSS: 1.164444088935852
LOSS: 1.1621179580688477
LOSS: 1.1598687171936035
LOSS: 1.1577272415161133
LOSS: 1.15569007396698
LOSS: 1.1537832021713257
LOSS: 1.1519702672958374
LOSS: 1.1502734422683716
LOSS: 1.1486269235610962
LOSS: 1.1470367908477783
LOSS: 1.1454964876174927
LOSS: 1.1439919471740723
LOSS: 1.142561912536621
LOSS: 1.1411770582199097
LOSS: 1.139908790588379
LOSS: 1.1386994123458862
LOSS: 1.1375312805175781
LOSS: 1.1364011764526367
LOSS: 1.1352397203445435
LOSS: 1.1341431140899658
LOSS: 1.133091688156128
LOSS: 1.1320918798446655
LOSS: 1.1311014890670776
LOSS: 1.1301440000534058
LOSS: 1.1292181015014648
LOSS: 1.1283003091812134
LOSS: 1.1273856163024902
LOSS: 1.1265004873275757
LOSS: 1.1256872415542603
LOSS: 1.1248056888580322
LOSS: 1.124068021774292
LOSS: 1.1233512163162231
LOSS: 1.3809418678283691
LOSS: 1.3747531175613403
LOSS: 1.3689568042755127
LOSS: 1.3635891675949097
LOSS: 1.3586878776550293
LOSS: 1.3542839288711548
LOSS: 1.3504003286361694
LOSS: 1.3470447063446045
LOSS: 1.3442015647888184
LOSS: 1.3418232202529907
LOSS: 1.3398267030715942
LOSS: 1.338098406791687
LOSS: 1.3365073204040527
LOSS: 1.3349277973175049
LOSS: 1.333258867263794
LOSS: 1.3314319849014282
LOSS: 1.3294093608856201
LOSS: 1.3271763324737549
LOSS: 1.3247376680374146
LOSS: 1.3221063613891602
LOSS: 1.3193014860153198
LOSS: 1.3163408041000366
LOSS: 1.313246488571167
LOSS: 1.3100334405899048
LOSS: 1.3067176342010498
LOSS: 1.3033099174499512
LOSS: 1.2998203039169312
LOSS: 1.2962536811828613
LOSS: 1.2926156520843506
LOSS: 1.2889093160629272
LOSS: 1.2851431369781494
LOSS: 1.2813283205032349
LOSS: 1.2774763107299805
LOSS: 1.2736047506332397
LOSS: 1.269727110862732
LOSS: 1.2658554315567017
LOSS: 1.2620031833648682
LOSS: 1.2581783533096313
LOSS: 1.2543883323669434
LOSS: 1.2506365776062012
LOSS: 1.2469236850738525
LOSS: 1.2432489395141602
LOSS: 1.2396095991134644
LOSS: 1.2360050678253174
LOSS: 1.2324389219284058
LOSS: 1.2289180755615234
LOSS: 1.2254537343978882
LOSS: 1.222060203552246
LOSS: 1.218752145767212
LOSS: 1.215537667274475
LOSS: 1.2124176025390625
LOSS: 1.2093781232833862
LOSS: 1.2064050436019897
LOSS: 1.2034783363342285
LOSS: 1.2005804777145386
LOSS: 1.1977016925811768
LOSS: 1.194840908050537
LOSS: 1.1920140981674194
LOSS: 1.1892420053482056
LOSS: 1.1865495443344116
LOSS: 1.183953046798706
LOSS: 1.1814377307891846
LOSS: 1.1789748668670654
LOSS: 1.1765377521514893
LOSS: 1.1741079092025757
LOSS: 1.1716749668121338
LOSS: 1.1692482233047485
LOSS: 1.1668269634246826
LOSS: 1.164444088935852
LOSS: 1.1621179580688477
LOSS: 1.1598687171936035
LOSS: 1.1577272415161133
LOSS: 1.15569007396698
LOSS: 1.1537832021713257
LOSS: 1.1519702672958374
LOSS: 1.1502734422683716
LOSS: 1.1486269235610962
LOSS: 1.1470367908477783
LOSS: 1.1454964876174927
LOSS: 1.1439919471740723
LOSS: 1.142561912536621
LOSS: 1.1411770582199097
LOSS: 1.139908790588379
LOSS: 1.1386994123458862
LOSS: 1.1375312805175781
LOSS: 1.1364011764526367
LOSS: 1.1352397203445435
LOSS: 1.1341431140899658
LOSS: 1.133091688156128
LOSS: 1.1320918798446655
LOSS: 1.1311014890670776
LOSS: 1.1301440000534058
LOSS: 1.1292181015014648
LOSS: 1.1283003091812134
LOSS: 1.1273856163024902
LOSS: 1.1265004873275757
LOSS: 1.1256872415542603
LOSS: 1.1248056888580322
LOSS: 1.124068021774292
LOSS: 1.1233512163162231
LOSS: 1.3692761659622192
LOSS: 1.3627443313598633
LOSS: 1.3565858602523804
LOSS: 1.350837230682373
LOSS: 1.3455331325531006
LOSS: 1.3407031297683716
LOSS: 1.336368441581726
LOSS: 1.3325345516204834
LOSS: 1.329184889793396
LOSS: 1.3262702226638794
LOSS: 1.3237086534500122
LOSS: 1.3213878870010376
LOSS: 1.3191802501678467
LOSS: 1.316962480545044
LOSS: 1.314636468887329
LOSS: 1.312137246131897
LOSS: 1.3094288110733032
LOSS: 1.3064959049224854
LOSS: 1.3033428192138672
LOSS: 1.2999801635742188
LOSS: 1.2964221239089966
LOSS: 1.2926909923553467
LOSS: 1.288806438446045
LOSS: 1.2847875356674194
LOSS: 1.2806494235992432
LOSS: 1.2764043807983398
LOSS: 1.2720587253570557
LOSS: 1.2676141262054443
LOSS: 1.2630705833435059
LOSS: 1.2584254741668701
LOSS: 1.2536736726760864
LOSS: 1.2488083839416504
LOSS: 1.2438230514526367
LOSS: 1.2387149333953857
LOSS: 1.2334866523742676
LOSS: 1.2281522750854492
LOSS: 1.2227360010147095
LOSS: 1.2172712087631226
LOSS: 1.211801528930664
LOSS: 1.2063747644424438
LOSS: 1.2010424137115479
LOSS: 1.1958526372909546
LOSS: 1.1908515691757202
LOSS: 1.1860798597335815
LOSS: 1.1815688610076904
LOSS: 1.177341103553772
LOSS: 1.173402190208435
LOSS: 1.1697381734848022
LOSS: 1.1663215160369873
LOSS: 1.1631168127059937
LOSS: 1.1600899696350098
LOSS: 1.1572175025939941
LOSS: 1.1544867753982544
LOSS: 1.1518912315368652
LOSS: 1.1494115591049194
LOSS: 1.147020936012268
LOSS: 1.1446852684020996
LOSS: 1.1423715353012085
LOSS: 1.1400684118270874
LOSS: 1.13779616355896
LOSS: 1.1355904340744019
LOSS: 1.1334840059280396
LOSS: 1.1314972639083862
LOSS: 1.1296412944793701
LOSS: 1.127889633178711
LOSS: 1.126187801361084
LOSS: 1.1245019435882568
LOSS: 1.12277090549469
LOSS: 1.1210042238235474
LOSS: 1.1192114353179932
LOSS: 1.1174383163452148
LOSS: 1.1157170534133911
LOSS: 1.1140800714492798
LOSS: 1.1125410795211792
LOSS: 1.1111118793487549
LOSS: 1.1097880601882935
LOSS: 1.1085376739501953
LOSS: 1.107351303100586
LOSS: 1.1062122583389282
LOSS: 1.1050915718078613
LOSS: 1.1039471626281738
LOSS: 1.0753041505813599
LOSS: 1.073715329170227
LOSS: 1.0721758604049683
LOSS: 1.0706909894943237
LOSS: 1.0692692995071411
LOSS: 1.0679194927215576
LOSS: 1.0666602849960327
LOSS: 1.065494179725647
LOSS: 1.064408540725708
LOSS: 1.0633705854415894
LOSS: 1.0623407363891602
LOSS: 1.061296820640564
LOSS: 1.0602312088012695
LOSS: 1.0591554641723633
LOSS: 1.0580801963806152
LOSS: 1.057022213935852
LOSS: 1.0559905767440796
LOSS: 1.054993748664856
LOSS: 1.0540356636047363
LOSS: 1.053115963935852
LOSS: 1.0522366762161255
LOSS: 1.3880085945129395
LOSS: 1.382020354270935
LOSS: 1.376430630683899
LOSS: 1.37127685546875
LOSS: 1.3665940761566162
LOSS: 1.3624111413955688
LOSS: 1.3587453365325928
LOSS: 1.3555952310562134
LOSS: 1.352929711341858
LOSS: 1.3506847620010376
LOSS: 1.348755121231079
LOSS: 1.347008228302002
LOSS: 1.3453043699264526
LOSS: 1.3435204029083252
LOSS: 1.341564416885376
LOSS: 1.3393776416778564
LOSS: 1.3369290828704834
LOSS: 1.334202527999878
LOSS: 1.3311902284622192
LOSS: 1.3278833627700806
LOSS: 1.324275016784668
LOSS: 1.3203587532043457
LOSS: 1.3161303997039795
LOSS: 1.3115874528884888
LOSS: 1.3067330121994019
LOSS: 1.30157470703125
LOSS: 1.296125054359436
LOSS: 1.290403962135315
LOSS: 1.284433364868164
LOSS: 1.2782392501831055
LOSS: 1.2718449831008911
LOSS: 1.2652738094329834
LOSS: 1.2585501670837402
LOSS: 1.2517002820968628
LOSS: 1.2447572946548462
LOSS: 1.237761378288269
LOSS: 1.230760931968689
LOSS: 1.2238115072250366
LOSS: 1.2169766426086426
LOSS: 1.2103255987167358
LOSS: 1.2039240598678589
LOSS: 1.1978309154510498
LOSS: 1.1920926570892334
LOSS: 1.1867406368255615
LOSS: 1.1817876100540161
LOSS: 1.1772291660308838
LOSS: 1.1730412244796753
LOSS: 1.1691824197769165
LOSS: 1.1655997037887573
LOSS: 1.1622370481491089
LOSS: 1.159042477607727
LOSS: 1.155971646308899
LOSS: 1.1529885530471802
LOSS: 1.1500672101974487
LOSS: 1.1472053527832031
LOSS: 1.1444199085235596
LOSS: 1.141734004020691
LOSS: 1.139167070388794
LOSS: 1.1367300748825073
LOSS: 1.1344281435012817
LOSS: 1.1322585344314575
LOSS: 1.130210518836975
LOSS: 1.12826669216156
LOSS: 1.126403570175171
LOSS: 1.1245956420898438
LOSS: 1.1228243112564087
LOSS: 1.121078372001648
LOSS: 1.1193513870239258
LOSS: 1.1176403760910034
LOSS: 1.1159436702728271
LOSS: 1.1142606735229492
LOSS: 1.1125928163528442
LOSS: 1.1109437942504883
LOSS: 1.109318494796753
LOSS: 1.1077182292938232
LOSS: 1.1061385869979858
LOSS: 1.1045687198638916
LOSS: 1.1029940843582153
LOSS: 1.1013990640640259
LOSS: 1.0997728109359741
LOSS: 1.0981134176254272
LOSS: 1.0964317321777344
LOSS: 1.0947520732879639
LOSS: 1.093109369277954
LOSS: 1.0915454626083374
LOSS: 1.0900996923446655
LOSS: 1.0887960195541382
LOSS: 1.0876307487487793
LOSS: 1.086569905281067
LOSS: 1.085572600364685
LOSS: 1.0846070051193237
LOSS: 1.0836454629898071
LOSS: 1.0826609134674072
LOSS: 1.081641435623169
LOSS: 1.0805974006652832
LOSS: 1.0795552730560303
LOSS: 1.0785428285598755
LOSS: 1.0775820016860962
LOSS: 1.0766820907592773
LOSS: 1.075842261314392
LOSS: 1.3728927373886108
LOSS: 1.3665779829025269
LOSS: 1.360646367073059
LOSS: 1.3551334142684937
LOSS: 1.3500725030899048
LOSS: 1.3454906940460205
LOSS: 1.3414058685302734
LOSS: 1.3378167152404785
LOSS: 1.3346939086914062
LOSS: 1.3319745063781738
LOSS: 1.3295570611953735
LOSS: 1.327317714691162
LOSS: 1.3251276016235352
LOSS: 1.322877049446106
LOSS: 1.3204894065856934
LOSS: 1.3179199695587158
LOSS: 1.3151488304138184
LOSS: 1.312171459197998
LOSS: 1.3089945316314697
LOSS: 1.3056234121322632
LOSS: 1.3020687103271484
LOSS: 1.29833984375
LOSS: 1.2944453954696655
LOSS: 1.2903939485549927
LOSS: 1.2861928939819336
LOSS: 1.2818478345870972
LOSS: 1.2773617506027222
LOSS: 1.2727361917495728
LOSS: 1.2679693698883057
LOSS: 1.2630577087402344
LOSS: 1.2579987049102783
LOSS: 1.2527923583984375
LOSS: 1.2474409341812134
LOSS: 1.2419534921646118
LOSS: 1.236344814300537
LOSS: 1.2306358814239502
LOSS: 1.2248567342758179
LOSS: 1.2190401554107666
LOSS: 1.2132267951965332
LOSS: 1.2074605226516724
LOSS: 1.201784372329712
LOSS: 1.1962404251098633
LOSS: 1.1908689737319946
LOSS: 1.185701847076416
LOSS: 1.1807698011398315
LOSS: 1.1760945320129395
LOSS: 1.1716893911361694
LOSS: 1.1675574779510498
LOSS: 1.1636894941329956
LOSS: 1.1600669622421265
LOSS: 1.1566650867462158
LOSS: 1.1534537076950073
LOSS: 1.1503955125808716
LOSS: 1.1474508047103882
LOSS: 1.1445809602737427
LOSS: 1.1417571306228638
LOSS: 1.1389663219451904
LOSS: 1.1362141370773315
LOSS: 1.1335182189941406
LOSS: 1.1308997869491577
LOSS: 1.128379225730896
LOSS: 1.1259747743606567
LOSS: 1.1236886978149414
LOSS: 1.1215053796768188
LOSS: 1.1194016933441162
LOSS: 1.117363452911377
LOSS: 1.1153830289840698
LOSS: 1.1134557723999023
LOSS: 1.1115789413452148
LOSS: 1.1097486019134521
LOSS: 1.1079565286636353
LOSS: 1.1061928272247314
LOSS: 1.1044458150863647
LOSS: 1.102706789970398
LOSS: 1.1009682416915894
LOSS: 1.0992261171340942
LOSS: 1.0974794626235962
LOSS: 1.0957292318344116
LOSS: 1.0939780473709106
LOSS: 1.0922261476516724
LOSS: 1.09047532081604
LOSS: 1.0887311697006226
LOSS: 1.0870100259780884
LOSS: 1.085333228111267
LOSS: 1.0837141275405884
LOSS: 1.0821508169174194
LOSS: 1.0806360244750977
LOSS: 1.0791727304458618
LOSS: 1.0777689218521118
LOSS: 1.076429009437561
LOSS: 1.0751535892486572
LOSS: 1.0739400386810303
LOSS: 1.0727827548980713
LOSS: 1.0716675519943237
LOSS: 1.070570945739746
LOSS: 1.0694694519042969
LOSS: 1.0683515071868896
LOSS: 1.0672205686569214
LOSS: 1.066090703010559
LOSS: 1.0649784803390503
LOSS: 1.3812413215637207
LOSS: 1.3751020431518555
LOSS: 1.369356393814087
LOSS: 1.3640379905700684
LOSS: 1.359183430671692
LOSS: 1.3548206090927124
LOSS: 1.3509671688079834
LOSS: 1.3476266860961914
LOSS: 1.3447725772857666
LOSS: 1.3423454761505127
LOSS: 1.3402479887008667
LOSS: 1.3383530378341675
LOSS: 1.3365236520767212
LOSS: 1.3346354961395264
LOSS: 1.3325934410095215
LOSS: 1.3303357362747192
LOSS: 1.3278288841247559
LOSS: 1.325060248374939
LOSS: 1.3220304250717163
LOSS: 1.3187471628189087
LOSS: 1.3152210712432861
LOSS: 1.3114575147628784
LOSS: 1.3074593544006348
LOSS: 1.3032242059707642
LOSS: 1.2987483739852905
LOSS: 1.2940353155136108
LOSS: 1.2890928983688354
LOSS: 1.283935785293579
LOSS: 1.2785817384719849
LOSS: 1.2730512619018555
LOSS: 1.267365574836731
LOSS: 1.261545181274414
LOSS: 1.2556124925613403
LOSS: 1.2495901584625244
LOSS: 1.243504285812378
LOSS: 1.2373872995376587
LOSS: 1.2312769889831543
LOSS: 1.2252177000045776
LOSS: 1.2192574739456177
LOSS: 1.2134467363357544
LOSS: 1.2078369855880737
LOSS: 1.2024749517440796
LOSS: 1.197399616241455
LOSS: 1.1926425695419312
LOSS: 1.188224196434021
LOSS: 1.1841545104980469
LOSS: 1.1804300546646118
LOSS: 1.1770343780517578
LOSS: 1.1739344596862793
LOSS: 1.1710765361785889
LOSS: 1.168400526046753
LOSS: 1.1658475399017334
LOSS: 1.1633763313293457
LOSS: 1.1609731912612915
LOSS: 1.1586350202560425
LOSS: 1.1563746929168701
LOSS: 1.154206395149231
LOSS: 1.1521440744400024
LOSS: 1.1501942873001099
LOSS: 1.1483559608459473
LOSS: 1.1466187238693237
LOSS: 1.1449646949768066
LOSS: 1.143376350402832
LOSS: 1.1418378353118896
LOSS: 1.1403348445892334
LOSS: 1.138850212097168
LOSS: 1.1373730897903442
LOSS: 1.1359018087387085
LOSS: 1.1344352960586548
LOSS: 1.1329752206802368
LOSS: 1.131512999534607
LOSS: 1.1300393342971802
LOSS: 1.1285380125045776
LOSS: 1.1270073652267456
LOSS: 1.1254454851150513
LOSS: 1.12385892868042
LOSS: 1.1222573518753052
LOSS: 1.1206507682800293
LOSS: 1.1190769672393799
LOSS: 1.117566704750061
LOSS: 1.116134762763977
LOSS: 1.1147868633270264
LOSS: 1.1135069131851196
LOSS: 1.1122939586639404
LOSS: 1.1111453771591187
LOSS: 1.1100620031356812
LOSS: 1.109034538269043
LOSS: 1.1080491542816162
LOSS: 1.107092022895813
LOSS: 1.1061575412750244
LOSS: 1.105247139930725
LOSS: 1.1043634414672852
LOSS: 1.1035094261169434
LOSS: 1.1026803255081177
LOSS: 1.1018717288970947
LOSS: 1.1010760068893433
LOSS: 1.1002906560897827
LOSS: 1.0995076894760132
LOSS: 1.0987308025360107
LOSS: 1.0979597568511963
LOSS: 1.3812413215637207
LOSS: 1.3751020431518555
LOSS: 1.369356393814087
LOSS: 1.3640379905700684
LOSS: 1.359183430671692
LOSS: 1.3548206090927124
LOSS: 1.3509671688079834
LOSS: 1.3476266860961914
LOSS: 1.3447725772857666
LOSS: 1.3423454761505127
LOSS: 1.0870100259780884
LOSS: 1.085333228111267
LOSS: 1.0837141275405884
LOSS: 1.0821508169174194
LOSS: 1.0806360244750977
LOSS: 1.0791727304458618
LOSS: 1.0777689218521118
LOSS: 1.076429009437561
LOSS: 1.0751535892486572
LOSS: 1.0739400386810303
LOSS: 1.0727827548980713
LOSS: 1.0716675519943237
LOSS: 1.070570945739746
LOSS: 1.0694694519042969
LOSS: 1.0683515071868896
LOSS: 1.0672205686569214
LOSS: 1.066090703010559
LOSS: 1.0649784803390503
LOSS: 1.3673667907714844
LOSS: 1.3608602285385132
LOSS: 1.3547316789627075
LOSS: 1.349016785621643
LOSS: 1.3437526226043701
LOSS: 1.3389694690704346
LOSS: 1.3346905708312988
LOSS: 1.3309239149093628
LOSS: 1.3276573419570923
LOSS: 1.3248485326766968
LOSS: 1.3224200010299683
LOSS: 1.3202643394470215
LOSS: 1.3182542324066162
LOSS: 1.3162641525268555
LOSS: 1.3141883611679077
LOSS: 1.311949610710144
LOSS: 1.3094993829727173
LOSS: 1.3068119287490845
LOSS: 1.30387544631958
LOSS: 1.3006870746612549
LOSS: 1.2972487211227417
LOSS: 1.2935630083084106
LOSS: 1.2896331548690796
LOSS: 1.2854622602462769
LOSS: 1.2810536623001099
LOSS: 1.2764108180999756
LOSS: 1.2715367078781128
LOSS: 1.266436219215393
LOSS: 1.2611148357391357
LOSS: 1.2555813789367676
LOSS: 1.2498486042022705
LOSS: 1.2439327239990234
LOSS: 1.2378524541854858
LOSS: 1.2316293716430664
LOSS: 1.2252888679504395
LOSS: 1.2188640832901
LOSS: 1.2123955488204956
LOSS: 1.2059327363967896
LOSS: 1.1995328664779663
LOSS: 1.1932631731033325
LOSS: 1.1871939897537231
LOSS: 1.181397557258606
LOSS: 1.1759364604949951
LOSS: 1.1708546876907349
LOSS: 1.1661670207977295
LOSS: 1.161879301071167
LOSS: 1.1579926013946533
LOSS: 1.1545002460479736
LOSS: 1.151379942893982
LOSS: 1.1485912799835205
LOSS: 1.1460802555084229
LOSS: 1.1437815427780151
LOSS: 1.141629695892334
LOSS: 1.1395704746246338
LOSS: 1.1375702619552612
LOSS: 1.1356149911880493
LOSS: 1.1337089538574219
LOSS: 1.1318647861480713
LOSS: 1.130091667175293
LOSS: 1.1283924579620361
LOSS: 1.1267671585083008
LOSS: 1.125208854675293
LOSS: 1.1237128973007202
LOSS: 1.1222813129425049
LOSS: 1.1209118366241455
LOSS: 1.1195987462997437
LOSS: 1.1183377504348755
LOSS: 1.117120385169983
LOSS: 1.1159334182739258
LOSS: 1.1147654056549072
LOSS: 1.1136128902435303
LOSS: 1.1124680042266846
LOSS: 1.1113331317901611
LOSS: 1.1102107763290405
LOSS: 1.1091095209121704
LOSS: 1.1080456972122192
LOSS: 1.1070141792297363
LOSS: 1.106016755104065
LOSS: 1.105047583580017
LOSS: 1.1041077375411987
LOSS: 1.1031920909881592
LOSS: 1.1023119688034058
LOSS: 1.101468563079834
LOSS: 1.1006667613983154
LOSS: 1.0999112129211426
LOSS: 1.0991929769515991
LOSS: 1.0984952449798584
LOSS: 1.0978025197982788
LOSS: 1.0971068143844604
LOSS: 1.096412181854248
LOSS: 1.0957216024398804
LOSS: 1.0950446128845215
LOSS: 1.0943840742111206
LOSS: 1.0937420129776
LOSS: 1.0931205749511719
LOSS: 1.092520833015442
LOSS: 1.0919451713562012
LOSS: 1.0913952589035034
LOSS: 1.0908650159835815
LOSS: 1.0903518199920654
LOSS: 1.3770174980163574
LOSS: 1.3708534240722656
LOSS: 1.365078330039978
LOSS: 1.3597300052642822
LOSS: 1.354841947555542
LOSS: 1.3504441976547241
LOSS: 1.3465546369552612
LOSS: 1.3431732654571533
LOSS: 1.3402715921401978
LOSS: 1.3377898931503296
LOSS: 1.3356276750564575
LOSS: 1.3336559534072876
LOSS: 1.3317397832870483
LOSS: 1.3297574520111084
LOSS: 1.3276183605194092
LOSS: 1.325262188911438
LOSS: 1.3226553201675415
LOSS: 1.3197827339172363
LOSS: 1.3166402578353882
LOSS: 1.3132315874099731
LOSS: 1.3095635175704956
LOSS: 1.3056443929672241
LOSS: 1.3014811277389526
LOSS: 1.2970823049545288
LOSS: 1.2924542427062988
LOSS: 1.287599802017212
LOSS: 1.282523512840271
LOSS: 1.2772291898727417
LOSS: 1.271722435951233
LOSS: 1.2660099267959595
LOSS: 1.2600982189178467
LOSS: 1.253989577293396
LOSS: 1.247686743736267
LOSS: 1.2411967515945435
LOSS: 1.2345389127731323
LOSS: 1.2277470827102661
LOSS: 1.2208722829818726
LOSS: 1.2139801979064941
LOSS: 1.2071486711502075
LOSS: 1.200461506843567
LOSS: 1.1940022706985474
LOSS: 1.1878465414047241
LOSS: 1.182058334350586
LOSS: 1.1766835451126099
LOSS: 1.1717476844787598
LOSS: 1.1672544479370117
LOSS: 1.1631836891174316
LOSS: 1.1594948768615723
LOSS: 1.1561312675476074
LOSS: 1.1530251502990723
LOSS: 1.1501024961471558
LOSS: 1.147287368774414
LOSS: 1.1445072889328003
LOSS: 1.141709804534912
LOSS: 1.1388756036758423
LOSS: 1.136019229888916
LOSS: 1.133174180984497
LOSS: 1.130376935005188
LOSS: 1.1276521682739258
LOSS: 1.125009536743164
LOSS: 1.1224509477615356
LOSS: 1.1199713945388794
LOSS: 1.117560863494873
LOSS: 1.1152052879333496
LOSS: 1.112890362739563
LOSS: 1.110603928565979
LOSS: 1.1083354949951172
LOSS: 1.1060774326324463
LOSS: 1.1038280725479126
LOSS: 1.1015973091125488
LOSS: 1.0994057655334473
LOSS: 1.0972793102264404
LOSS: 1.09524405002594
LOSS: 1.093316674232483
LOSS: 1.0915019512176514
LOSS: 1.0897884368896484
LOSS: 1.088153600692749
LOSS: 1.0865648984909058
LOSS: 1.0849931240081787
LOSS: 1.0834187269210815
LOSS: 1.0818378925323486
LOSS: 1.080263376235962
LOSS: 1.0787193775177002
LOSS: 1.077231526374817
LOSS: 1.0758167505264282
LOSS: 1.0744733810424805
LOSS: 1.073183536529541
LOSS: 1.071932315826416
LOSS: 1.0707231760025024
LOSS: 1.0695724487304688
LOSS: 1.0684953927993774
LOSS: 1.067489743232727
LOSS: 1.0665370225906372
LOSS: 1.0656172037124634
LOSS: 1.0647172927856445
LOSS: 1.0638370513916016
LOSS: 1.0629825592041016
LOSS: 1.062159538269043
LOSS: 1.0613667964935303
LOSS: 1.060597538948059
LOSS: 1.3783811330795288
LOSS: 1.3722401857376099
LOSS: 1.3664932250976562
LOSS: 1.3611767292022705
LOSS: 1.356327772140503
LOSS: 1.3519721031188965
LOSS: 1.3481279611587524
LOSS: 1.3447908163070679
LOSS: 1.3419280052185059
LOSS: 1.3394688367843628
LOSS: 1.3373072147369385
LOSS: 1.3353118896484375
LOSS: 1.3333524465560913
LOSS: 1.3313190937042236
LOSS: 1.3291362524032593
LOSS: 1.3267585039138794
LOSS: 1.324164628982544
LOSS: 1.321351408958435
LOSS: 1.3183238506317139
LOSS: 1.3150910139083862
LOSS: 1.311659574508667
LOSS: 1.3080339431762695
LOSS: 1.3042163848876953
LOSS: 1.3002097606658936
LOSS: 1.2960145473480225
LOSS: 1.2916306257247925
LOSS: 1.287056565284729
LOSS: 1.2822887897491455
LOSS: 1.2773233652114868
LOSS: 1.2721561193466187
LOSS: 1.2667837142944336
LOSS: 1.2612067461013794
LOSS: 1.255431056022644
LOSS: 1.249472975730896
LOSS: 1.2433596849441528
LOSS: 1.2371318340301514
LOSS: 1.2308377027511597
LOSS: 1.2245368957519531
LOSS: 1.2182908058166504
LOSS: 1.2121626138687134
LOSS: 1.206210970878601
LOSS: 1.2004845142364502
LOSS: 1.1950173377990723
LOSS: 1.189839243888855
LOSS: 1.1849734783172607
LOSS: 1.1804347038269043
LOSS: 1.1762290000915527
LOSS: 1.1723504066467285
LOSS: 1.1687839031219482
LOSS: 1.1655042171478271
LOSS: 1.1624795198440552
LOSS: 1.1596704721450806
LOSS: 1.157034993171692
LOSS: 1.1545240879058838
LOSS: 1.152086615562439
LOSS: 1.1496890783309937
LOSS: 1.1473140716552734
LOSS: 1.1449640989303589
LOSS: 1.1426446437835693
LOSS: 1.1403675079345703
LOSS: 1.1381471157073975
LOSS: 1.1359882354736328
LOSS: 1.1338938474655151
LOSS: 1.1318613290786743
LOSS: 1.129892110824585
LOSS: 1.1279922723770142
LOSS: 1.1261653900146484
LOSS: 1.124418020248413
LOSS: 1.1227531433105469
LOSS: 1.1211707592010498
LOSS: 1.1196646690368652
LOSS: 1.1182276010513306
LOSS: 1.1168521642684937
LOSS: 1.1155357360839844
LOSS: 1.1142754554748535
LOSS: 1.1130720376968384
LOSS: 1.111924171447754
LOSS: 1.1108272075653076
LOSS: 1.1097798347473145
LOSS: 1.1087839603424072
LOSS: 1.1078404188156128
LOSS: 1.106941819190979
LOSS: 1.1060733795166016
LOSS: 1.1052225828170776
LOSS: 1.1043871641159058
LOSS: 1.103567361831665
LOSS: 1.1027618646621704
LOSS: 1.1019750833511353
LOSS: 1.101215124130249
LOSS: 1.1004899740219116
LOSS: 1.0998061895370483
LOSS: 1.0991644859313965
LOSS: 1.0985620021820068
LOSS: 1.0979859828948975
LOSS: 1.0974233150482178
LOSS: 1.0968620777130127
LOSS: 1.096298336982727
LOSS: 1.095732569694519
LOSS: 1.0951725244522095
LOSS: 1.0946234464645386
LOSS: 1.3770174980163574
LOSS: 1.3708534240722656
LOSS: 1.365078330039978
LOSS: 1.3597300052642822
LOSS: 1.354841947555542
LOSS: 1.3504441976547241
LOSS: 1.3465546369552612
LOSS: 1.3431732654571533
LOSS: 1.3402715921401978
LOSS: 1.3377898931503296
LOSS: 1.3356276750564575
LOSS: 1.3336559534072876
LOSS: 0.820512056350708
LOSS: 0.8200299739837646
LOSS: 0.8195608258247375
LOSS: 0.8191031217575073
LOSS: 0.8186575174331665
LOSS: 0.8182231187820435
LOSS: 0.8177996873855591
LOSS: 0.817387044429779
LOSS: 0.8169848322868347
LOSS: 0.8165937066078186
LOSS: 0.8162118196487427
LOSS: 0.8158401250839233
LOSS: 0.8154774904251099
LOSS: 0.8151236772537231
LOSS: 0.8147780299186707
LOSS: 0.8144404292106628
LOSS: 0.8141095638275146
LOSS: 1.3673667907714844
LOSS: 1.3608602285385132
LOSS: 1.3547316789627075
LOSS: 1.349016785621643
LOSS: 1.3437526226043701
LOSS: 1.3389694690704346
LOSS: 1.3346905708312988
LOSS: 1.3309239149093628
LOSS: 1.3276573419570923
LOSS: 1.3248485326766968
LOSS: 1.3224200010299683
LOSS: 1.3202643394470215
LOSS: 1.3182542324066162
LOSS: 1.3162641525268555
LOSS: 1.3141883611679077
LOSS: 1.311949610710144
LOSS: 1.3094993829727173
LOSS: 1.3068119287490845
LOSS: 1.30387544631958
LOSS: 1.3006870746612549
LOSS: 1.2972487211227417
LOSS: 1.2935630083084106
LOSS: 1.2896331548690796
LOSS: 1.2854622602462769
LOSS: 1.2810536623001099
LOSS: 1.2764108180999756
LOSS: 1.2715367078781128
LOSS: 1.266436219215393
LOSS: 1.2611148357391357
LOSS: 1.2555813789367676
LOSS: 1.2498486042022705
LOSS: 1.2439327239990234
LOSS: 1.2378524541854858
LOSS: 1.2316293716430664
LOSS: 1.2252888679504395
LOSS: 1.2188640832901
LOSS: 1.2123955488204956
LOSS: 1.2059327363967896
LOSS: 1.1995328664779663
LOSS: 1.1932631731033325
LOSS: 1.1871939897537231
LOSS: 1.181397557258606
LOSS: 1.1759364604949951
LOSS: 1.1708546876907349
LOSS: 1.1661670207977295
LOSS: 1.161879301071167
LOSS: 1.1579926013946533
LOSS: 1.1545002460479736
LOSS: 1.151379942893982
LOSS: 1.1485912799835205
LOSS: 1.1460802555084229
LOSS: 1.1437815427780151
LOSS: 1.141629695892334
LOSS: 1.1395704746246338
LOSS: 1.1375702619552612
LOSS: 1.1356149911880493
LOSS: 1.1337089538574219
LOSS: 1.1318647861480713
LOSS: 1.130091667175293
LOSS: 1.1283924579620361
LOSS: 1.1267671585083008
LOSS: 1.125208854675293
LOSS: 1.1237128973007202
LOSS: 1.1222813129425049
LOSS: 1.1209118366241455
LOSS: 1.1195987462997437
LOSS: 1.1183377504348755
LOSS: 1.117120385169983
LOSS: 1.1159334182739258
LOSS: 1.1147654056549072
LOSS: 1.1136128902435303
LOSS: 1.1124680042266846
LOSS: 1.1113331317901611
LOSS: 1.1102107763290405
LOSS: 1.1091095209121704
LOSS: 1.1080456972122192
LOSS: 1.1070141792297363
LOSS: 1.106016755104065
LOSS: 1.105047583580017
LOSS: 1.1041077375411987
LOSS: 1.1031920909881592
LOSS: 1.1023119688034058
LOSS: 1.101468563079834
LOSS: 1.1006667613983154
LOSS: 1.0999112129211426
LOSS: 1.0991929769515991
LOSS: 1.0984952449798584
LOSS: 1.0978025197982788
LOSS: 1.0971068143844604
LOSS: 1.096412181854248
LOSS: 1.0957216024398804
LOSS: 1.0950446128845215
LOSS: 1.0943840742111206
LOSS: 1.0937420129776
LOSS: 1.0931205749511719
LOSS: 1.092520833015442
LOSS: 1.0919451713562012
LOSS: 1.0913952589035034
LOSS: 1.0908650159835815
LOSS: 1.0903518199920654
LOSS: 1.3673166036605835
LOSS: 1.3608299493789673
LOSS: 1.3547190427780151
LOSS: 1.3490206003189087
LOSS: 1.343767523765564
LOSS: 1.3389900922775269
LOSS: 1.3347039222717285
LOSS: 1.330911636352539
LOSS: 1.3275871276855469
LOSS: 1.3246703147888184
LOSS: 1.3220651149749756
LOSS: 1.3196481466293335
LOSS: 1.3172869682312012
LOSS: 1.3148653507232666
LOSS: 1.3122949600219727
LOSS: 1.3095207214355469
LOSS: 1.3065139055252075
LOSS: 1.3032655715942383
LOSS: 1.2997791767120361
LOSS: 1.2960647344589233
LOSS: 1.2921391725540161
LOSS: 1.2880191802978516
LOSS: 1.2837247848510742
LOSS: 1.2792778015136719
LOSS: 1.2746986150741577
LOSS: 1.2700080871582031
LOSS: 1.2652255296707153
LOSS: 1.260367512702942
LOSS: 1.2554433345794678
LOSS: 1.2504551410675049
LOSS: 1.245396614074707
LOSS: 1.240260362625122
LOSS: 1.2350351810455322
LOSS: 1.2297160625457764
LOSS: 1.2243053913116455
LOSS: 1.2188127040863037
LOSS: 1.2132551670074463
LOSS: 1.2076592445373535
LOSS: 1.2020606994628906
LOSS: 1.1964987516403198
LOSS: 1.1910144090652466
LOSS: 1.1856484413146973
LOSS: 1.180432677268982
LOSS: 1.1753880977630615
LOSS: 1.170526385307312
LOSS: 1.1658583879470825
LOSS: 1.1613993644714355
LOSS: 1.157153844833374
LOSS: 1.1531072854995728
LOSS: 1.1492400169372559
LOSS: 1.1455397605895996
LOSS: 1.1419880390167236
LOSS: 1.1385608911514282
LOSS: 1.1352462768554688
LOSS: 1.1320483684539795
LOSS: 1.1289796829223633
LOSS: 1.1260570287704468
LOSS: 1.1232863664627075
LOSS: 1.1206510066986084
LOSS: 1.1181107759475708
LOSS: 1.1156156063079834
LOSS: 1.1131268739700317
LOSS: 1.1106265783309937
LOSS: 1.1081111431121826
LOSS: 1.105599045753479
LOSS: 1.1031064987182617
LOSS: 1.1006500720977783
LOSS: 1.0982404947280884
LOSS: 1.0958889722824097
LOSS: 1.0936150550842285
LOSS: 1.0914340019226074
LOSS: 1.089351773262024
LOSS: 1.0873616933822632
LOSS: 1.08545982837677
LOSS: 1.083644986152649
LOSS: 1.081911325454712
LOSS: 1.0802305936813354
LOSS: 1.0785738229751587
LOSS: 1.0769282579421997
LOSS: 1.0753041505813599
LOSS: 1.073715329170227
LOSS: 1.0721758604049683
LOSS: 1.0706909894943237
LOSS: 1.0692692995071411
LOSS: 1.0679194927215576
LOSS: 1.0666602849960327
LOSS: 1.065494179725647
LOSS: 1.064408540725708
LOSS: 1.0633705854415894
LOSS: 1.0623407363891602
LOSS: 1.061296820640564
LOSS: 1.0602312088012695
LOSS: 1.0591554641723633
LOSS: 1.0580801963806152
LOSS: 1.057022213935852
LOSS: 1.0559905767440796
LOSS: 1.054993748664856
LOSS: 1.0540356636047363
LOSS: 1.053115963935852
LOSS: 1.0522366762161255
LOSS: 1.3696153163909912
LOSS: 1.3630179166793823
LOSS: 1.3567923307418823
LOSS: 1.3509740829467773
LOSS: 1.3455992937088013
LOSS: 1.340698480606079
LOSS: 1.336296558380127
LOSS: 1.3324002027511597
LOSS: 1.32900071144104
LOSS: 1.3260575532913208
LOSS: 1.3234989643096924
LOSS: 1.3212225437164307
LOSS: 1.3191059827804565
LOSS: 1.3170294761657715
LOSS: 1.3148926496505737
LOSS: 1.3126258850097656
LOSS: 1.310189127922058
LOSS: 1.3075673580169678
LOSS: 1.3047635555267334
LOSS: 1.3017882108688354
LOSS: 1.2986586093902588
LOSS: 1.2953890562057495
LOSS: 1.2919923067092896
LOSS: 1.2884774208068848
LOSS: 1.2848494052886963
LOSS: 1.2811108827590942
LOSS: 1.277260422706604
LOSS: 1.2732951641082764
LOSS: 1.2692086696624756
LOSS: 1.2649924755096436
LOSS: 1.2606374025344849
LOSS: 1.2561309337615967
LOSS: 1.251466989517212
LOSS: 1.2466415166854858
LOSS: 1.2416566610336304
LOSS: 1.236519455909729
LOSS: 1.2312443256378174
LOSS: 1.2258541584014893
LOSS: 1.2203861474990845
LOSS: 1.2148826122283936
LOSS: 1.2093919515609741
LOSS: 1.2039674520492554
LOSS: 1.1986650228500366
LOSS: 1.1935416460037231
LOSS: 1.188644528388977
LOSS: 1.1840109825134277
LOSS: 1.1796590089797974
LOSS: 1.1755858659744263
LOSS: 1.1717703342437744
LOSS: 1.1681832075119019
LOSS: 1.1647931337356567
LOSS: 1.1615644693374634
LOSS: 1.1584556102752686
LOSS: 1.1554287672042847
LOSS: 1.1524481773376465
LOSS: 1.1494914293289185
LOSS: 1.146552562713623
LOSS: 1.1436394453048706
LOSS: 1.1407663822174072
LOSS: 1.137939691543579
LOSS: 1.1351537704467773
LOSS: 1.1323992013931274
LOSS: 1.1296744346618652
LOSS: 1.1269810199737549
LOSS: 1.1243152618408203
LOSS: 1.1216731071472168
LOSS: 1.1190464496612549
LOSS: 1.1164294481277466
LOSS: 1.113822102546692
LOSS: 1.1112279891967773
LOSS: 1.1086552143096924
LOSS: 1.1061151027679443
LOSS: 1.1036230325698853
LOSS: 1.1011980772018433
LOSS: 1.0988624095916748
LOSS: 1.0966342687606812
LOSS: 1.0945234298706055
LOSS: 1.0925253629684448
LOSS: 1.0906291007995605
LOSS: 1.088828682899475
LOSS: 1.0871200561523438
LOSS: 1.0854911804199219
LOSS: 1.0839139223098755
LOSS: 1.0823601484298706
LOSS: 1.0808122158050537
LOSS: 1.0792620182037354
LOSS: 1.0777077674865723
LOSS: 1.0761531591415405
LOSS: 1.07460618019104
LOSS: 1.0730760097503662
LOSS: 1.0715651512145996
LOSS: 1.0700618028640747
LOSS: 1.0685449838638306
LOSS: 1.0670011043548584
LOSS: 1.0654360055923462
LOSS: 1.0638706684112549
LOSS: 1.0623341798782349
LOSS: 1.0608561038970947
LOSS: 1.0594613552093506
LOSS: 1.058163046836853
LOSS: 1.3696153163909912
LOSS: 1.3630179166793823
LOSS: 1.3567923307418823
LOSS: 1.3509740829467773
LOSS: 1.3455992937088013
LOSS: 1.340698480606079
LOSS: 1.336296558380127
LOSS: 1.3324002027511597
LOSS: 1.32900071144104
LOSS: 1.3260575532913208
LOSS: 1.3234989643096924
LOSS: 1.3212225437164307
LOSS: 1.3191059827804565
LOSS: 1.102817416191101
LOSS: 1.101708173751831
LOSS: 1.1006138324737549
LOSS: 1.0995447635650635
LOSS: 1.098559021949768
LOSS: 1.097629189491272
LOSS: 1.0967214107513428
LOSS: 1.0958348512649536
LOSS: 1.0949475765228271
LOSS: 1.0941154956817627
LOSS: 1.0932480096817017
LOSS: 1.0924229621887207
LOSS: 1.0916197299957275
LOSS: 1.0908432006835938
LOSS: 1.0900678634643555
LOSS: 1.0892969369888306
LOSS: 1.0885326862335205
LOSS: 1.0878020524978638
LOSS: 1.0870375633239746
LOSS: 1.3692761659622192
LOSS: 1.3627443313598633
LOSS: 1.3565858602523804
LOSS: 1.350837230682373
LOSS: 1.3455331325531006
LOSS: 1.3407031297683716
LOSS: 1.336368441581726
LOSS: 1.3325345516204834
LOSS: 1.329184889793396
LOSS: 1.3262702226638794
LOSS: 1.3237086534500122
LOSS: 1.3213878870010376
LOSS: 1.3191802501678467
LOSS: 1.316962480545044
LOSS: 1.314636468887329
LOSS: 1.312137246131897
LOSS: 1.3094288110733032
LOSS: 1.3064959049224854
LOSS: 1.3033428192138672
LOSS: 1.2999801635742188
LOSS: 1.2964221239089966
LOSS: 1.2926909923553467
LOSS: 1.288806438446045
LOSS: 1.2847875356674194
LOSS: 1.2806494235992432
LOSS: 1.2764043807983398
LOSS: 1.2720587253570557
LOSS: 1.2676141262054443
LOSS: 1.2630705833435059
LOSS: 1.2584254741668701
LOSS: 1.2536736726760864
LOSS: 1.2488083839416504
LOSS: 1.2438230514526367
LOSS: 1.2387149333953857
LOSS: 1.2334866523742676
LOSS: 1.2281522750854492
LOSS: 1.2227360010147095
LOSS: 1.2172712087631226
LOSS: 1.211801528930664
LOSS: 1.2063747644424438
LOSS: 1.2010424137115479
LOSS: 1.1958526372909546
LOSS: 1.1908515691757202
LOSS: 1.1860798597335815
LOSS: 1.1815688610076904
LOSS: 1.177341103553772
LOSS: 1.173402190208435
LOSS: 1.1697381734848022
LOSS: 1.1663215160369873
LOSS: 1.1631168127059937
LOSS: 1.1600899696350098
LOSS: 1.1572175025939941
LOSS: 1.1544867753982544
LOSS: 1.1518912315368652
LOSS: 1.1494115591049194
LOSS: 1.147020936012268
LOSS: 1.1446852684020996
LOSS: 1.1423715353012085
LOSS: 1.1400684118270874
LOSS: 1.13779616355896
LOSS: 1.1355904340744019
LOSS: 1.1334840059280396
LOSS: 1.1314972639083862
LOSS: 1.1296412944793701
LOSS: 1.127889633178711
LOSS: 1.126187801361084
LOSS: 1.1245019435882568
LOSS: 1.12277090549469
LOSS: 1.1210042238235474
LOSS: 1.1192114353179932
LOSS: 1.1174383163452148
LOSS: 1.1157170534133911
LOSS: 1.1140800714492798
LOSS: 1.1125410795211792
LOSS: 1.1111118793487549
LOSS: 1.1097880601882935
LOSS: 1.1085376739501953
LOSS: 1.107351303100586
LOSS: 1.1062122583389282
LOSS: 1.1050915718078613
LOSS: 1.1039471626281738
LOSS: 1.102817416191101
LOSS: 1.101708173751831
LOSS: 1.1006138324737549
LOSS: 1.0995447635650635
LOSS: 1.098559021949768
LOSS: 1.097629189491272
LOSS: 1.0967214107513428
LOSS: 1.0958348512649536
LOSS: 1.0949475765228271
LOSS: 1.0941154956817627
LOSS: 1.0932480096817017
LOSS: 1.0924229621887207
LOSS: 1.0916197299957275
LOSS: 1.0908432006835938
LOSS: 1.0900678634643555
LOSS: 1.0892969369888306
LOSS: 1.0885326862335205
LOSS: 1.0878020524978638
LOSS: 1.0870375633239746
LOSS: 1.3774211406707764
LOSS: 1.3711754083633423
LOSS: 1.3653154373168945
LOSS: 1.359877347946167
LOSS: 1.3548963069915771
LOSS: 1.3504018783569336
LOSS: 1.3464131355285645
LOSS: 1.3429334163665771
LOSS: 1.339939832687378
LOSS: 1.3373759984970093
LOSS: 1.3351496458053589
LOSS: 1.333138108253479
LOSS: 1.3312116861343384
LOSS: 1.3292510509490967
LOSS: 1.3271660804748535
LOSS: 1.3249012231826782
LOSS: 1.3224283456802368
LOSS: 1.319740891456604
LOSS: 1.3168436288833618
LOSS: 1.3137469291687012
LOSS: 1.3104594945907593
LOSS: 1.3069841861724854
LOSS: 1.3033206462860107
LOSS: 1.2994667291641235
LOSS: 1.295420527458191
LOSS: 1.2911781072616577
LOSS: 1.2867345809936523
LOSS: 1.2820820808410645
LOSS: 1.2772167921066284
LOSS: 1.2721370458602905
LOSS: 1.266846776008606
LOSS: 1.2613550424575806
LOSS: 1.2556719779968262
LOSS: 1.249813199043274
LOSS: 1.2437958717346191
LOSS: 1.2376408576965332
LOSS: 1.2313761711120605
LOSS: 1.2250386476516724
LOSS: 1.2186716794967651
LOSS: 1.2123247385025024
LOSS: 1.2060530185699463
LOSS: 1.1999109983444214
LOSS: 1.1939544677734375
LOSS: 1.1882433891296387
LOSS: 1.1828367710113525
LOSS: 1.1777836084365845
LOSS: 1.1731237173080444
LOSS: 1.168879508972168
LOSS: 1.1650503873825073
LOSS: 1.1616162061691284
LOSS: 1.158535122871399
LOSS: 1.155748963356018
LOSS: 1.1531833410263062
LOSS: 1.1507600545883179
LOSS: 1.1484142541885376
LOSS: 1.1460925340652466
LOSS: 1.1437547206878662
LOSS: 1.1413769721984863
LOSS: 1.1389483213424683
LOSS: 1.1364718675613403
LOSS: 1.1339590549468994
LOSS: 1.1314266920089722
LOSS: 1.128892183303833
LOSS: 1.1263668537139893
LOSS: 1.1238583326339722
LOSS: 1.1213723421096802
LOSS: 1.1189144849777222
LOSS: 1.1164891719818115
LOSS: 1.1141057014465332
LOSS: 1.1117756366729736
LOSS: 1.1095155477523804
LOSS: 1.1073436737060547
LOSS: 1.1052786111831665
LOSS: 1.1033354997634888
LOSS: 1.1015245914459229
LOSS: 1.0998451709747314
LOSS: 1.0982822179794312
LOSS: 1.096808671951294
LOSS: 1.0953929424285889
LOSS: 1.0940065383911133
LOSS: 1.0926305055618286
LOSS: 1.0912561416625977
LOSS: 1.0898808240890503
LOSS: 1.0885026454925537
LOSS: 1.0871236324310303
LOSS: 1.0857545137405396
LOSS: 1.0844134092330933
LOSS: 1.0831186771392822
LOSS: 1.08187735080719
LOSS: 1.0806881189346313
LOSS: 1.0795440673828125
LOSS: 1.0784326791763306
LOSS: 1.077338695526123
LOSS: 1.07625150680542
LOSS: 1.0751677751541138
LOSS: 1.074087142944336
LOSS: 1.0730057954788208
LOSS: 1.0719245672225952
LOSS: 1.070854902267456
LOSS: 1.0698156356811523
LOSS: 1.3727694749832153
LOSS: 1.3664493560791016
LOSS: 1.3605120182037354
LOSS: 1.3549914360046387
LOSS: 1.3499231338500977
LOSS: 1.3453344106674194
LOSS: 1.341243863105774
LOSS: 1.3376482725143433
LOSS: 1.3345223665237427
LOSS: 1.3318029642105103
LOSS: 1.3293894529342651
LOSS: 1.3271561861038208
LOSS: 1.3249704837799072
LOSS: 1.3227155208587646
LOSS: 1.3203057050704956
LOSS: 1.3176854848861694
LOSS: 1.314825177192688
LOSS: 1.3117091655731201
LOSS: 1.3083282709121704
LOSS: 1.304673671722412
LOSS: 1.3007359504699707
LOSS: 1.2965071201324463
LOSS: 1.291985273361206
LOSS: 1.2871711254119873
LOSS: 1.282070517539978
LOSS: 1.2766964435577393
LOSS: 1.2710665464401245
LOSS: 1.2652052640914917
LOSS: 1.2591376304626465
LOSS: 1.2528903484344482
LOSS: 1.246484398841858
LOSS: 1.2399364709854126
LOSS: 1.233272671699524
LOSS: 1.2265275716781616
LOSS: 1.2197479009628296
LOSS: 1.2129936218261719
LOSS: 1.2063343524932861
LOSS: 1.1998462677001953
LOSS: 1.193604588508606
LOSS: 1.1876813173294067
LOSS: 1.182139277458191
LOSS: 1.1770284175872803
LOSS: 1.1723829507827759
LOSS: 1.1682137250900269
LOSS: 1.1645082235336304
LOSS: 1.1612297296524048
LOSS: 1.1583173274993896
LOSS: 1.155687689781189
LOSS: 1.1532460451126099
LOSS: 1.1509013175964355
LOSS: 1.1485792398452759
LOSS: 1.1462287902832031
LOSS: 1.1438236236572266
LOSS: 1.141361117362976
LOSS: 1.138857126235962
LOSS: 1.1363381147384644
LOSS: 1.1338359117507935
LOSS: 1.1313804388046265
LOSS: 1.1289936304092407
LOSS: 1.1266857385635376
LOSS: 1.1244550943374634
LOSS: 1.122291922569275
LOSS: 1.1201823949813843
LOSS: 1.11811363697052
LOSS: 1.1160744428634644
LOSS: 1.1140564680099487
LOSS: 1.1120562553405762
LOSS: 1.1100773811340332
LOSS: 1.1081279516220093
LOSS: 1.1062164306640625
LOSS: 1.104349970817566
LOSS: 1.1025347709655762
LOSS: 1.100779414176941
LOSS: 1.0990920066833496
LOSS: 1.0974780321121216
LOSS: 1.0959376096725464
LOSS: 1.0944644212722778
LOSS: 1.0930463075637817
LOSS: 1.0916707515716553
LOSS: 1.0903280973434448
LOSS: 1.089011788368225
LOSS: 1.0877182483673096
LOSS: 1.0864450931549072
LOSS: 1.0851916074752808
LOSS: 1.083957552909851
LOSS: 1.0827414989471436
LOSS: 1.0815446376800537
LOSS: 1.0803735256195068
LOSS: 1.0792344808578491
LOSS: 1.0781326293945312
LOSS: 1.0770713090896606
LOSS: 1.0760555267333984
LOSS: 1.0750879049301147
LOSS: 1.074161410331726
LOSS: 1.0732592344284058
LOSS: 1.0723692178726196
LOSS: 1.0714926719665527
LOSS: 1.070637583732605
LOSS: 1.0698046684265137
LOSS: 1.0689847469329834
LOSS: 1.3788703680038452
LOSS: 1.3726036548614502
LOSS: 1.3667277097702026
LOSS: 1.361279010772705
LOSS: 1.3562939167022705
LOSS: 1.3518025875091553
LOSS: 1.3478264808654785
LOSS: 1.3443702459335327
LOSS: 1.341413140296936
LOSS: 1.3389052152633667
LOSS: 1.3367578983306885
LOSS: 1.3317397832870483
LOSS: 1.3297574520111084
LOSS: 1.3276183605194092
LOSS: 1.325262188911438
LOSS: 1.3226553201675415
LOSS: 1.3197827339172363
LOSS: 1.3166402578353882
LOSS: 1.3132315874099731
LOSS: 1.3095635175704956
LOSS: 1.3056443929672241
LOSS: 1.3014811277389526
LOSS: 1.2970823049545288
LOSS: 1.2924542427062988
LOSS: 1.287599802017212
LOSS: 1.282523512840271
LOSS: 1.2772291898727417
LOSS: 1.271722435951233
LOSS: 1.2660099267959595
LOSS: 1.2600982189178467
LOSS: 1.253989577293396
LOSS: 1.247686743736267
LOSS: 1.2411967515945435
LOSS: 1.2345389127731323
LOSS: 1.2277470827102661
LOSS: 1.2208722829818726
LOSS: 1.2139801979064941
LOSS: 1.2071486711502075
LOSS: 1.200461506843567
LOSS: 1.1940022706985474
LOSS: 1.1878465414047241
LOSS: 1.182058334350586
LOSS: 1.1766835451126099
LOSS: 1.1717476844787598
LOSS: 1.1672544479370117
LOSS: 1.1631836891174316
LOSS: 1.1594948768615723
LOSS: 1.1561312675476074
LOSS: 1.1530251502990723
LOSS: 1.1501024961471558
LOSS: 1.147287368774414
LOSS: 1.1445072889328003
LOSS: 1.141709804534912
LOSS: 1.1388756036758423
LOSS: 1.136019229888916
LOSS: 1.133174180984497
LOSS: 1.130376935005188
LOSS: 1.1276521682739258
LOSS: 1.125009536743164
LOSS: 1.1224509477615356
LOSS: 1.1199713945388794
LOSS: 1.117560863494873
LOSS: 1.1152052879333496
LOSS: 1.112890362739563
LOSS: 1.110603928565979
LOSS: 1.1083354949951172
LOSS: 1.1060774326324463
LOSS: 1.1038280725479126
LOSS: 1.1015973091125488
LOSS: 1.0994057655334473
LOSS: 1.0972793102264404
LOSS: 1.09524405002594
LOSS: 1.093316674232483
LOSS: 1.0915019512176514
LOSS: 1.0897884368896484
LOSS: 1.088153600692749
LOSS: 1.0865648984909058
LOSS: 1.0849931240081787
LOSS: 1.0834187269210815
LOSS: 1.0818378925323486
LOSS: 1.080263376235962
LOSS: 1.0787193775177002
LOSS: 1.077231526374817
LOSS: 1.0758167505264282
LOSS: 1.0744733810424805
LOSS: 1.073183536529541
LOSS: 1.071932315826416
LOSS: 1.0707231760025024
LOSS: 1.0695724487304688
LOSS: 1.0684953927993774
LOSS: 1.067489743232727
LOSS: 1.0665370225906372
LOSS: 1.0656172037124634
LOSS: 1.0647172927856445
LOSS: 1.0638370513916016
LOSS: 1.0629825592041016
LOSS: 1.062159538269043
LOSS: 1.0613667964935303
LOSS: 1.060597538948059
LOSS: 1.3783811330795288
LOSS: 1.3722401857376099
LOSS: 1.3664932250976562
LOSS: 1.3611767292022705
LOSS: 1.356327772140503
LOSS: 1.3519721031188965
LOSS: 1.3481279611587524
LOSS: 1.3447908163070679
LOSS: 1.3419280052185059
LOSS: 1.3394688367843628
LOSS: 1.3373072147369385
LOSS: 1.3353118896484375
LOSS: 1.3333524465560913
LOSS: 1.3313190937042236
LOSS: 1.3291362524032593
LOSS: 1.3267585039138794
LOSS: 1.324164628982544
LOSS: 1.321351408958435
LOSS: 1.3183238506317139
LOSS: 1.3150910139083862
LOSS: 1.311659574508667
LOSS: 1.3080339431762695
LOSS: 1.3042163848876953
LOSS: 1.3002097606658936
LOSS: 1.2960145473480225
LOSS: 1.2916306257247925
LOSS: 1.287056565284729
LOSS: 1.2822887897491455
LOSS: 1.2773233652114868
LOSS: 1.2721561193466187
LOSS: 1.2667837142944336
LOSS: 1.2612067461013794
LOSS: 1.255431056022644
LOSS: 1.249472975730896
LOSS: 1.2433596849441528
LOSS: 1.2371318340301514
LOSS: 1.2308377027511597
LOSS: 1.2245368957519531
LOSS: 1.2182908058166504
LOSS: 1.2121626138687134
LOSS: 1.206210970878601
LOSS: 1.2004845142364502
LOSS: 1.1950173377990723
LOSS: 1.189839243888855
LOSS: 1.1849734783172607
LOSS: 1.1804347038269043
LOSS: 1.1762290000915527
LOSS: 1.1723504066467285
LOSS: 1.1687839031219482
LOSS: 1.1655042171478271
LOSS: 1.1624795198440552
LOSS: 1.1596704721450806
LOSS: 1.157034993171692
LOSS: 1.1545240879058838
LOSS: 1.152086615562439
LOSS: 1.1496890783309937
LOSS: 1.1473140716552734
LOSS: 1.1449640989303589
LOSS: 1.1426446437835693
LOSS: 1.1403675079345703
LOSS: 1.1381471157073975
LOSS: 1.1359882354736328
LOSS: 1.1338938474655151
LOSS: 1.1318613290786743
LOSS: 1.129892110824585
LOSS: 1.1279922723770142
LOSS: 1.1261653900146484
LOSS: 1.124418020248413
LOSS: 1.1227531433105469
LOSS: 1.1211707592010498
LOSS: 1.1196646690368652
LOSS: 1.1182276010513306
LOSS: 1.1168521642684937
LOSS: 1.1155357360839844
LOSS: 1.1142754554748535
LOSS: 1.1130720376968384
LOSS: 1.111924171447754
LOSS: 1.1108272075653076
LOSS: 1.1097798347473145
LOSS: 1.1087839603424072
LOSS: 1.1078404188156128
LOSS: 1.106941819190979
LOSS: 1.1060733795166016
LOSS: 1.1052225828170776
LOSS: 1.1043871641159058
LOSS: 1.103567361831665
LOSS: 1.1027618646621704
LOSS: 1.1019750833511353
LOSS: 1.101215124130249
LOSS: 1.1004899740219116
LOSS: 1.0998061895370483
LOSS: 1.0991644859313965
LOSS: 1.0985620021820068
LOSS: 1.0979859828948975
LOSS: 1.0974233150482178
LOSS: 1.0968620777130127
LOSS: 1.096298336982727
LOSS: 1.095732569694519
LOSS: 1.0951725244522095
LOSS: 1.0946234464645386
LOSS: 1.368350863456726
LOSS: 1.3618849515914917
LOSS: 1.3557921648025513
LOSS: 1.350107192993164
LOSS: 1.3448655605316162
LOSS: 1.3400952816009521
LOSS: 1.3358185291290283
LOSS: 1.3320403099060059
LOSS: 1.3287433385849
LOSS: 1.3258799314498901
LOSS: 1.3233673572540283
LOSS: 1.321093201637268
LOSS: 1.3189316987991333
LOSS: 1.3167651891708374
LOSS: 1.3145012855529785
LOSS: 1.3120814561843872
LOSS: 1.3094762563705444
LOSS: 1.3066792488098145
LOSS: 1.3036972284317017
LOSS: 1.3005484342575073
LOSS: 1.2972503900527954
LOSS: 1.293822169303894
LOSS: 1.2902761697769165
LOSS: 1.2866185903549194
LOSS: 1.2828487157821655
LOSS: 1.2789627313613892
LOSS: 1.2749525308609009
LOSS: 1.2708085775375366
LOSS: 1.2665191888809204
LOSS: 1.2620735168457031
LOSS: 1.2574645280838013
LOSS: 1.2526872158050537
LOSS: 1.24774169921875
LOSS: 1.2426316738128662
LOSS: 1.237365961074829
LOSS: 1.2319576740264893
LOSS: 1.226426362991333
LOSS: 1.2208000421524048
LOSS: 1.2151122093200684
LOSS: 1.2094024419784546
LOSS: 1.203712821006775
LOSS: 1.1980842351913452
LOSS: 1.19255793094635
LOSS: 1.1871716976165771
LOSS: 1.1819604635238647
LOSS: 1.1769577264785767
LOSS: 1.1721913814544678
LOSS: 1.167678713798523
LOSS: 1.1634306907653809
LOSS: 1.1594533920288086
LOSS: 1.1557462215423584
LOSS: 1.1523000001907349
LOSS: 1.149091362953186
LOSS: 1.146074891090393
LOSS: 1.1431970596313477
LOSS: 1.1404138803482056
LOSS: 1.1377044916152954
LOSS: 1.1350690126419067
LOSS: 1.1325163841247559
LOSS: 1.1300603151321411
LOSS: 1.1277145147323608
LOSS: 1.12548828125
LOSS: 1.1233799457550049
LOSS: 1.1213687658309937
LOSS: 1.1194218397140503
LOSS: 1.1175001859664917
LOSS: 1.1155738830566406
LOSS: 1.1136279106140137
LOSS: 1.1116610765457153
LOSS: 1.1096757650375366
LOSS: 1.1076754331588745
LOSS: 1.1056628227233887
LOSS: 1.103642225265503
LOSS: 1.101619005203247
LOSS: 1.0995979309082031
LOSS: 1.0975768566131592
LOSS: 1.0955414772033691
LOSS: 1.093470811843872
LOSS: 1.0913546085357666
LOSS: 1.089208960533142
LOSS: 1.087064504623413
LOSS: 1.0849573612213135
LOSS: 1.0829228162765503
LOSS: 1.0809910297393799
LOSS: 1.0791724920272827
LOSS: 1.077451229095459
LOSS: 1.075783371925354
LOSS: 1.0741238594055176
LOSS: 1.0724462270736694
LOSS: 1.070740818977356
LOSS: 1.0690033435821533
LOSS: 1.0672341585159302
LOSS: 1.0654346942901611
LOSS: 1.063612699508667
LOSS: 1.0617786645889282
LOSS: 1.0599476099014282
LOSS: 1.0581406354904175
LOSS: 1.0563820600509644
LOSS: 1.0546939373016357
LOSS: 1.0530930757522583
LOSS: 1.3736728429794312
LOSS: 1.3672797679901123
LOSS: 1.361268162727356
LOSS: 1.3556768894195557
LOSS: 1.3505395650863647
LOSS: 1.3458881378173828
LOSS: 1.3417414426803589
LOSS: 1.3381048440933228
LOSS: 1.3349554538726807
LOSS: 1.3322384357452393
LOSS: 1.329865574836731
LOSS: 1.3277156352996826
LOSS: 1.3256603479385376
LOSS: 1.3235834836959839
LOSS: 1.3213974237442017
LOSS: 1.3190462589263916
LOSS: 1.3165011405944824
LOSS: 1.3137555122375488
LOSS: 1.3108125925064087
LOSS: 1.3076813220977783
LOSS: 1.3043700456619263
LOSS: 1.3008836507797241
LOSS: 1.2972263097763062
LOSS: 1.2933976650238037
LOSS: 1.2893929481506348
LOSS: 1.2852063179016113
LOSS: 1.2808284759521484
LOSS: 1.2762506008148193
LOSS: 1.2714647054672241
LOSS: 1.2664647102355957
LOSS: 1.2612488269805908
LOSS: 1.2558211088180542
LOSS: 1.2501918077468872
LOSS: 1.2443777322769165
LOSS: 1.2384005784988403
LOSS: 1.2322877645492554
LOSS: 1.226072907447815
LOSS: 1.2197967767715454
LOSS: 1.2135053873062134
LOSS: 1.2072510719299316
LOSS: 1.2010904550552368
LOSS: 1.1950790882110596
LOSS: 1.3170294761657715
LOSS: 1.3148926496505737
LOSS: 1.3126258850097656
LOSS: 1.310189127922058
LOSS: 1.3075673580169678
LOSS: 1.3047635555267334
LOSS: 1.3017882108688354
LOSS: 1.2986586093902588
LOSS: 1.2953890562057495
LOSS: 1.2919923067092896
LOSS: 1.2884774208068848
LOSS: 1.2848494052886963
LOSS: 1.2811108827590942
LOSS: 1.277260422706604
LOSS: 1.2732951641082764
LOSS: 1.2692086696624756
LOSS: 1.2649924755096436
LOSS: 1.2606374025344849
LOSS: 1.2561309337615967
LOSS: 1.251466989517212
LOSS: 1.2466415166854858
LOSS: 1.2416566610336304
LOSS: 1.236519455909729
LOSS: 1.2312443256378174
LOSS: 1.2258541584014893
LOSS: 1.2203861474990845
LOSS: 1.2148826122283936
LOSS: 1.2093919515609741
LOSS: 1.2039674520492554
LOSS: 1.1986650228500366
LOSS: 1.1935416460037231
LOSS: 1.188644528388977
LOSS: 1.1840109825134277
LOSS: 1.1796590089797974
LOSS: 1.1755858659744263
LOSS: 1.1717703342437744
LOSS: 1.1681832075119019
LOSS: 1.1647931337356567
LOSS: 1.1615644693374634
LOSS: 1.1584556102752686
LOSS: 1.1554287672042847
LOSS: 1.1524481773376465
LOSS: 1.1494914293289185
LOSS: 1.146552562713623
LOSS: 1.1436394453048706
LOSS: 1.1407663822174072
LOSS: 1.137939691543579
LOSS: 1.1351537704467773
LOSS: 1.1323992013931274
LOSS: 1.1296744346618652
LOSS: 1.1269810199737549
LOSS: 1.1243152618408203
LOSS: 1.1216731071472168
LOSS: 1.1190464496612549
LOSS: 1.1164294481277466
LOSS: 1.113822102546692
LOSS: 1.1112279891967773
LOSS: 1.1086552143096924
LOSS: 1.1061151027679443
LOSS: 1.1036230325698853
LOSS: 1.1011980772018433
LOSS: 1.0988624095916748
LOSS: 1.0966342687606812
LOSS: 1.0945234298706055
LOSS: 1.0925253629684448
LOSS: 1.0906291007995605
LOSS: 1.088828682899475
LOSS: 1.0871200561523438
LOSS: 1.0854911804199219
LOSS: 1.0839139223098755
LOSS: 1.0823601484298706
LOSS: 1.0808122158050537
LOSS: 1.0792620182037354
LOSS: 1.0777077674865723
LOSS: 1.0761531591415405
LOSS: 1.07460618019104
LOSS: 1.0730760097503662
LOSS: 1.0715651512145996
LOSS: 1.0700618028640747
LOSS: 1.0685449838638306
LOSS: 1.0670011043548584
LOSS: 1.0654360055923462
LOSS: 1.0638706684112549
LOSS: 1.0623341798782349
LOSS: 1.0608561038970947
LOSS: 1.0594613552093506
LOSS: 1.058163046836853
LOSS: 1.3736728429794312
LOSS: 1.3672797679901123
LOSS: 1.361268162727356
LOSS: 1.3556768894195557
LOSS: 1.3505395650863647
LOSS: 1.3458881378173828
LOSS: 1.3417414426803589
LOSS: 1.3381048440933228
LOSS: 1.3349554538726807
LOSS: 1.3322384357452393
LOSS: 1.329865574836731
LOSS: 1.3277156352996826
LOSS: 1.3256603479385376
LOSS: 1.3235834836959839
LOSS: 1.3213974237442017
LOSS: 1.3190462589263916
LOSS: 1.3165011405944824
LOSS: 1.3137555122375488
LOSS: 1.3108125925064087
LOSS: 1.3076813220977783
LOSS: 1.3043700456619263
LOSS: 1.3008836507797241
LOSS: 1.2972263097763062
LOSS: 1.2933976650238037
LOSS: 1.2893929481506348
LOSS: 1.2852063179016113
LOSS: 1.2808284759521484
LOSS: 1.2762506008148193
LOSS: 1.2714647054672241
LOSS: 1.2664647102355957
LOSS: 1.2612488269805908
LOSS: 1.2558211088180542
LOSS: 1.2501918077468872
LOSS: 1.2443777322769165
LOSS: 1.2384005784988403
LOSS: 1.2322877645492554
LOSS: 1.226072907447815
LOSS: 1.2197967767715454
LOSS: 1.2135053873062134
LOSS: 1.2072510719299316
LOSS: 1.2010904550552368
LOSS: 1.1950790882110596
LOSS: 1.1892699003219604
LOSS: 1.183711290359497
LOSS: 1.1784443855285645
LOSS: 1.1734988689422607
LOSS: 1.1688932180404663
LOSS: 1.1646311283111572
LOSS: 1.1607049703598022
LOSS: 1.157096266746521
LOSS: 1.1537761688232422
LOSS: 1.1507107019424438
LOSS: 1.1478643417358398
LOSS: 1.1452080011367798
LOSS: 1.1427137851715088
LOSS: 1.1403664350509644
LOSS: 1.1381596326828003
LOSS: 1.1360939741134644
LOSS: 1.1341688632965088
LOSS: 1.1323766708374023
LOSS: 1.1306906938552856
LOSS: 1.1290674209594727
LOSS: 1.127461552619934
LOSS: 1.1258410215377808
LOSS: 1.1241916418075562
LOSS: 1.1225073337554932
LOSS: 1.1208018064498901
LOSS: 1.1190787553787231
LOSS: 1.1173657178878784
LOSS: 1.1156516075134277
LOSS: 1.1139525175094604
LOSS: 1.1122807264328003
LOSS: 1.1106356382369995
LOSS: 1.1090002059936523
LOSS: 1.1074104309082031
LOSS: 1.105819821357727
LOSS: 1.1042687892913818
LOSS: 1.1027636528015137
LOSS: 1.1012877225875854
LOSS: 1.099877953529358
LOSS: 1.0984774827957153
LOSS: 1.0971349477767944
LOSS: 1.0958532094955444
LOSS: 1.0946071147918701
LOSS: 1.093436360359192
LOSS: 1.092285394668579
LOSS: 1.0912424325942993
LOSS: 1.0902718305587769
LOSS: 1.0893551111221313
LOSS: 1.0884453058242798
LOSS: 1.0876028537750244
LOSS: 1.0867135524749756
LOSS: 1.085833191871643
LOSS: 1.0849686861038208
LOSS: 1.0840708017349243
LOSS: 1.0832037925720215
LOSS: 1.082310676574707
LOSS: 1.0814988613128662
LOSS: 1.0806821584701538
LOSS: 1.0798656940460205
LOSS: 1.3774211406707764
LOSS: 1.3711754083633423
LOSS: 1.3653154373168945
LOSS: 1.359877347946167
LOSS: 1.3548963069915771
LOSS: 1.3504018783569336
LOSS: 1.3464131355285645
LOSS: 1.3429334163665771
LOSS: 1.339939832687378
LOSS: 1.3373759984970093
LOSS: 1.3351496458053589
LOSS: 1.333138108253479
LOSS: 1.3312116861343384
LOSS: 1.3292510509490967
LOSS: 1.3271660804748535
LOSS: 1.3249012231826782
LOSS: 1.3224283456802368
LOSS: 1.319740891456604
LOSS: 1.3168436288833618
LOSS: 1.3137469291687012
LOSS: 1.3104594945907593
LOSS: 1.3069841861724854
LOSS: 1.3033206462860107
LOSS: 1.2994667291641235
LOSS: 1.295420527458191
LOSS: 1.2911781072616577
LOSS: 1.2867345809936523
LOSS: 1.2820820808410645
LOSS: 1.2772167921066284
LOSS: 1.2721370458602905
LOSS: 1.266846776008606
LOSS: 1.2613550424575806
LOSS: 1.2556719779968262
LOSS: 1.249813199043274
LOSS: 1.2437958717346191
LOSS: 1.2376408576965332
LOSS: 1.2313761711120605
LOSS: 1.2250386476516724
LOSS: 1.2186716794967651
LOSS: 1.2123247385025024
LOSS: 1.2060530185699463
LOSS: 1.1999109983444214
LOSS: 1.1939544677734375
LOSS: 1.1882433891296387
LOSS: 1.1828367710113525
LOSS: 1.1777836084365845
LOSS: 1.1731237173080444
LOSS: 1.168879508972168
LOSS: 1.1650503873825073
LOSS: 1.1616162061691284
LOSS: 1.158535122871399
LOSS: 1.155748963356018
LOSS: 1.1531833410263062
LOSS: 1.1507600545883179
LOSS: 1.1484142541885376
LOSS: 1.1460925340652466
LOSS: 1.1437547206878662
LOSS: 1.1413769721984863
LOSS: 1.1389483213424683
LOSS: 1.1364718675613403
LOSS: 1.1339590549468994
LOSS: 1.1314266920089722
LOSS: 1.128892183303833
LOSS: 1.1263668537139893
LOSS: 1.1238583326339722
LOSS: 1.1213723421096802
LOSS: 1.1189144849777222
LOSS: 1.1164891719818115
LOSS: 1.1141057014465332
LOSS: 1.1117756366729736
LOSS: 1.1095155477523804
LOSS: 1.1073436737060547
LOSS: 1.1052786111831665
LOSS: 1.1033354997634888
LOSS: 1.1015245914459229
LOSS: 1.0998451709747314
LOSS: 1.0982822179794312
LOSS: 1.096808671951294
LOSS: 1.0953929424285889
LOSS: 1.0940065383911133
LOSS: 1.0926305055618286
LOSS: 1.0912561416625977
LOSS: 1.0898808240890503
LOSS: 1.0885026454925537
LOSS: 1.0871236324310303
LOSS: 1.0857545137405396
LOSS: 1.0844134092330933
LOSS: 1.0831186771392822
LOSS: 1.08187735080719
LOSS: 1.0806881189346313
LOSS: 1.0795440673828125
LOSS: 1.0784326791763306
LOSS: 1.077338695526123
LOSS: 1.07625150680542
LOSS: 1.0751677751541138
LOSS: 1.074087142944336
LOSS: 1.0730057954788208
LOSS: 1.0719245672225952
LOSS: 1.070854902267456
LOSS: 1.0698156356811523
LOSS: 1.3727694749832153
LOSS: 1.3664493560791016
LOSS: 1.3605120182037354
LOSS: 1.3549914360046387
LOSS: 1.3499231338500977
LOSS: 1.3453344106674194
LOSS: 1.341243863105774
LOSS: 1.3376482725143433
LOSS: 1.3345223665237427
LOSS: 1.3318029642105103
LOSS: 1.3293894529342651
LOSS: 1.3271561861038208
LOSS: 1.3249704837799072
LOSS: 1.3227155208587646
LOSS: 1.3203057050704956
LOSS: 1.3176854848861694
LOSS: 1.314825177192688
LOSS: 1.3117091655731201
LOSS: 1.3083282709121704
LOSS: 1.304673671722412
LOSS: 1.3007359504699707
LOSS: 1.2965071201324463
LOSS: 1.291985273361206
LOSS: 1.2871711254119873
LOSS: 1.282070517539978
LOSS: 1.2766964435577393
LOSS: 1.2710665464401245
LOSS: 1.2652052640914917
LOSS: 1.2591376304626465
LOSS: 1.2528903484344482
LOSS: 1.246484398841858
LOSS: 1.2399364709854126
LOSS: 1.233272671699524
LOSS: 1.2265275716781616
LOSS: 1.2197479009628296
LOSS: 1.2129936218261719
LOSS: 1.2063343524932861
LOSS: 1.1998462677001953
LOSS: 1.193604588508606
LOSS: 1.1876813173294067
LOSS: 1.182139277458191
LOSS: 1.1770284175872803
LOSS: 1.3402479887008667
LOSS: 1.3383530378341675
LOSS: 1.3365236520767212
LOSS: 1.3346354961395264
LOSS: 1.3325934410095215
LOSS: 1.3303357362747192
LOSS: 1.3278288841247559
LOSS: 1.325060248374939
LOSS: 1.3220304250717163
LOSS: 1.3187471628189087
LOSS: 1.3152210712432861
LOSS: 1.3114575147628784
LOSS: 1.3074593544006348
LOSS: 1.3032242059707642
LOSS: 1.2987483739852905
LOSS: 1.2940353155136108
LOSS: 1.2890928983688354
LOSS: 1.283935785293579
LOSS: 1.2785817384719849
LOSS: 1.2730512619018555
LOSS: 1.267365574836731
LOSS: 1.261545181274414
LOSS: 1.2556124925613403
LOSS: 1.2495901584625244
LOSS: 1.243504285812378
LOSS: 1.2373872995376587
LOSS: 1.2312769889831543
LOSS: 1.2252177000045776
LOSS: 1.2192574739456177
LOSS: 1.2134467363357544
LOSS: 1.2078369855880737
LOSS: 1.2024749517440796
LOSS: 1.197399616241455
LOSS: 1.1926425695419312
LOSS: 1.188224196434021
LOSS: 1.1841545104980469
LOSS: 1.1804300546646118
LOSS: 1.1770343780517578
LOSS: 1.1739344596862793
LOSS: 1.1710765361785889
LOSS: 1.168400526046753
LOSS: 1.1658475399017334
LOSS: 1.1633763313293457
LOSS: 1.1609731912612915
LOSS: 1.1586350202560425
LOSS: 1.1563746929168701
LOSS: 1.154206395149231
LOSS: 1.1521440744400024
LOSS: 1.1501942873001099
LOSS: 1.1483559608459473
LOSS: 1.1466187238693237
LOSS: 1.1449646949768066
LOSS: 1.143376350402832
LOSS: 1.1418378353118896
LOSS: 1.1403348445892334
LOSS: 1.138850212097168
LOSS: 1.1373730897903442
LOSS: 1.1359018087387085
LOSS: 1.1344352960586548
LOSS: 1.1329752206802368
LOSS: 1.131512999534607
LOSS: 1.1300393342971802
LOSS: 1.1285380125045776
LOSS: 1.1270073652267456
LOSS: 1.1254454851150513
LOSS: 1.12385892868042
LOSS: 1.1222573518753052
LOSS: 1.1206507682800293
LOSS: 1.1190769672393799
LOSS: 1.117566704750061
LOSS: 1.116134762763977
LOSS: 1.1147868633270264
LOSS: 1.1135069131851196
LOSS: 1.1122939586639404
LOSS: 1.1111453771591187
LOSS: 1.1100620031356812
LOSS: 1.109034538269043
LOSS: 1.1080491542816162
LOSS: 1.107092022895813
LOSS: 1.1061575412750244
LOSS: 1.105247139930725
LOSS: 1.1043634414672852
LOSS: 1.1035094261169434
LOSS: 1.1026803255081177
LOSS: 1.1018717288970947
LOSS: 1.1010760068893433
LOSS: 1.1002906560897827
LOSS: 1.0995076894760132
LOSS: 1.0987308025360107
LOSS: 1.0979597568511963
LOSS: 1.3788703680038452
LOSS: 1.3726036548614502
LOSS: 1.3667277097702026
LOSS: 1.361279010772705
LOSS: 1.3562939167022705
LOSS: 1.3518025875091553
LOSS: 1.3478264808654785
LOSS: 1.3443702459335327
LOSS: 1.341413140296936
LOSS: 1.3389052152633667
LOSS: 1.3367578983306885
LOSS: 1.3348524570465088
LOSS: 1.333058476448059
LOSS: 1.3312517404556274
LOSS: 1.329336166381836
LOSS: 1.3272477388381958
LOSS: 1.3249520063400269
LOSS: 1.3224374055862427
LOSS: 1.319704294204712
LOSS: 1.3167624473571777
LOSS: 1.3136241436004639
LOSS: 1.3103023767471313
LOSS: 1.306807279586792
LOSS: 1.3031470775604248
LOSS: 1.2993249893188477
LOSS: 1.2953405380249023
LOSS: 1.291190266609192
LOSS: 1.2868701219558716
LOSS: 1.282374382019043
LOSS: 1.277696132659912
LOSS: 1.2728321552276611
LOSS: 1.2677819728851318
LOSS: 1.2625484466552734
LOSS: 1.2571440935134888
LOSS: 1.2515861988067627
LOSS: 1.2459053993225098
LOSS: 1.2401388883590698
LOSS: 1.2343374490737915
LOSS: 1.2285547256469727
LOSS: 1.2228528261184692
LOSS: 1.2172929048538208
LOSS: 1.2119358777999878
LOSS: 1.2068341970443726
LOSS: 1.2020316123962402
LOSS: 1.1975579261779785
LOSS: 1.193421721458435
LOSS: 1.1896166801452637
LOSS: 1.1861172914505005
LOSS: 1.182887315750122
LOSS: 1.1798783540725708
LOSS: 1.177040457725525
LOSS: 1.174324631690979
LOSS: 1.1716912984848022
LOSS: 1.1691091060638428
LOSS: 1.1665587425231934
LOSS: 1.1640344858169556
LOSS: 1.1615432500839233
LOSS: 1.1591044664382935
LOSS: 1.1567423343658447
LOSS: 1.154482126235962
LOSS: 1.1523464918136597
LOSS: 1.1503480672836304
LOSS: 1.1484880447387695
LOSS: 1.1467366218566895
LOSS: 1.1450414657592773
LOSS: 1.1433343887329102
LOSS: 1.1415584087371826
LOSS: 1.1396899223327637
LOSS: 1.13773775100708
LOSS: 1.1357375383377075
LOSS: 1.1337326765060425
LOSS: 1.131775975227356
LOSS: 1.1299115419387817
LOSS: 1.1281702518463135
LOSS: 1.126559853553772
LOSS: 1.1250674724578857
LOSS: 1.1236625909805298
LOSS: 1.1223046779632568
LOSS: 1.1209547519683838
LOSS: 1.1195889711380005
LOSS: 1.1182022094726562
LOSS: 1.11680006980896
LOSS: 1.1154052019119263
LOSS: 1.1140460968017578
LOSS: 1.11275053024292
LOSS: 1.1115344762802124
LOSS: 1.110391616821289
LOSS: 1.109303593635559
LOSS: 1.108253002166748
LOSS: 1.1072324514389038
LOSS: 1.106241226196289
LOSS: 1.1052743196487427
LOSS: 1.1043217182159424
LOSS: 1.1033705472946167
LOSS: 1.1024041175842285
LOSS: 1.1014087200164795
LOSS: 1.1003673076629639
LOSS: 1.0992681980133057
LOSS: 1.0981061458587646
LOSS: 1.0968867540359497
LOSS: 1.368350863456726
LOSS: 1.3618849515914917
LOSS: 1.3557921648025513
LOSS: 1.350107192993164
LOSS: 1.3448655605316162
LOSS: 1.3400952816009521
LOSS: 1.3358185291290283
LOSS: 1.3320403099060059
LOSS: 1.3287433385849
LOSS: 1.3258799314498901
LOSS: 1.3233673572540283
LOSS: 1.321093201637268
LOSS: 1.3189316987991333
LOSS: 1.3167651891708374
LOSS: 1.3145012855529785
LOSS: 1.3120814561843872
LOSS: 1.3094762563705444
LOSS: 1.3066792488098145
LOSS: 1.3036972284317017
LOSS: 1.3005484342575073
LOSS: 1.2972503900527954
LOSS: 1.293822169303894
LOSS: 1.2902761697769165
LOSS: 1.2866185903549194
LOSS: 1.2828487157821655
LOSS: 1.2789627313613892
LOSS: 1.2749525308609009
LOSS: 1.2708085775375366
LOSS: 1.2665191888809204
LOSS: 1.2620735168457031
LOSS: 1.2574645280838013
LOSS: 1.2526872158050537
LOSS: 1.24774169921875
LOSS: 1.2426316738128662
LOSS: 1.237365961074829
LOSS: 1.2319576740264893
LOSS: 1.226426362991333
LOSS: 1.2208000421524048
LOSS: 1.2151122093200684
LOSS: 1.2094024419784546
LOSS: 1.203712821006775
LOSS: 1.1980842351913452
LOSS: 1.19255793094635
LOSS: 1.1871716976165771
LOSS: 1.1819604635238647
LOSS: 1.1769577264785767
LOSS: 1.1721913814544678
LOSS: 1.167678713798523
LOSS: 1.1634306907653809
LOSS: 1.1594533920288086
LOSS: 1.1557462215423584
LOSS: 1.1523000001907349
LOSS: 1.149091362953186
LOSS: 1.146074891090393
LOSS: 1.1431970596313477
LOSS: 1.1404138803482056
LOSS: 1.1377044916152954
LOSS: 1.1350690126419067
LOSS: 1.1325163841247559
LOSS: 1.1300603151321411
LOSS: 1.1277145147323608
LOSS: 1.12548828125
LOSS: 1.1233799457550049
LOSS: 1.1213687658309937
LOSS: 1.1194218397140503
LOSS: 1.1175001859664917
LOSS: 1.1155738830566406
LOSS: 1.1136279106140137
LOSS: 1.1116610765457153
LOSS: 1.1096757650375366
LOSS: 1.1076754331588745
LOSS: 1.1056628227233887
LOSS: 1.103642225265503
LOSS: 1.101619005203247
LOSS: 1.0995979309082031
LOSS: 1.0975768566131592
LOSS: 1.0955414772033691
LOSS: 1.093470811843872
LOSS: 1.0913546085357666
LOSS: 1.089208960533142
LOSS: 1.087064504623413
LOSS: 1.0849573612213135
LOSS: 1.0829228162765503
LOSS: 1.0809910297393799
LOSS: 1.0791724920272827
LOSS: 1.077451229095459
LOSS: 1.075783371925354
LOSS: 1.0741238594055176
LOSS: 1.0724462270736694
LOSS: 1.070740818977356
LOSS: 1.0690033435821533
LOSS: 1.0672341585159302
LOSS: 1.0654346942901611
LOSS: 1.063612699508667
LOSS: 1.0617786645889282
LOSS: 1.0599476099014282
LOSS: 1.0581406354904175
LOSS: 1.0563820600509644
LOSS: 1.0546939373016357
LOSS: 1.0530930757522583
LOSS: 1.3731873035430908
LOSS: 1.3668854236602783
LOSS: 1.3609739542007446
LOSS: 1.35548734664917
LOSS: 1.3504618406295776
LOSS: 1.3459266424179077
LOSS: 1.341903567314148
LOSS: 1.3383985757827759
LOSS: 1.3353909254074097
LOSS: 1.332833170890808
LOSS: 1.3306384086608887
LOSS: 1.3286911249160767
LOSS: 1.326862096786499
LOSS: 1.325034260749817
LOSS: 1.3231146335601807
LOSS: 1.3210408687591553
LOSS: 1.318784475326538
LOSS: 1.3163350820541382
LOSS: 1.3136978149414062
LOSS: 1.3108831644058228
LOSS: 1.3079020977020264
LOSS: 1.3047624826431274
LOSS: 1.3014665842056274
LOSS: 1.2980127334594727
LOSS: 1.2943966388702393
LOSS: 1.2906105518341064
LOSS: 1.2866462469100952
LOSS: 1.2824945449829102
LOSS: 1.2781486511230469
LOSS: 1.2736049890518188
LOSS: 1.2688647508621216
LOSS: 1.2639358043670654
LOSS: 1.2588316202163696
LOSS: 1.2535699605941772
LOSS: 1.248176097869873
LOSS: 1.2426793575286865
LOSS: 1.2371152639389038
LOSS: 1.2315244674682617
LOSS: 1.2259502410888672
LOSS: 1.2204359769821167
LOSS: 1.215026617050171
LOSS: 1.1892699003219604
LOSS: 1.183711290359497
LOSS: 1.1784443855285645
LOSS: 1.1734988689422607
LOSS: 1.1688932180404663
LOSS: 1.1646311283111572
LOSS: 1.1607049703598022
LOSS: 1.157096266746521
LOSS: 1.1537761688232422
LOSS: 1.1507107019424438
LOSS: 1.1478643417358398
LOSS: 1.1452080011367798
LOSS: 1.1427137851715088
LOSS: 1.1403664350509644
LOSS: 1.1381596326828003
LOSS: 1.1360939741134644
LOSS: 1.1341688632965088
LOSS: 1.1323766708374023
LOSS: 1.1306906938552856
LOSS: 1.1290674209594727
LOSS: 1.127461552619934
LOSS: 1.1258410215377808
LOSS: 1.1241916418075562
LOSS: 1.1225073337554932
LOSS: 1.1208018064498901
LOSS: 1.1190787553787231
LOSS: 1.1173657178878784
LOSS: 1.1156516075134277
LOSS: 1.1139525175094604
LOSS: 1.1122807264328003
LOSS: 1.1106356382369995
LOSS: 1.1090002059936523
LOSS: 1.1074104309082031
LOSS: 1.105819821357727
LOSS: 1.1042687892913818
LOSS: 1.1027636528015137
LOSS: 1.1012877225875854
LOSS: 1.099877953529358
LOSS: 1.0984774827957153
LOSS: 1.0971349477767944
LOSS: 1.0958532094955444
LOSS: 1.0946071147918701
LOSS: 1.093436360359192
LOSS: 1.092285394668579
LOSS: 1.0912424325942993
LOSS: 1.0902718305587769
LOSS: 1.0893551111221313
LOSS: 1.0884453058242798
LOSS: 1.0876028537750244
LOSS: 1.0867135524749756
LOSS: 1.085833191871643
LOSS: 1.0849686861038208
LOSS: 1.0840708017349243
LOSS: 1.0832037925720215
LOSS: 1.082310676574707
LOSS: 1.0814988613128662
LOSS: 1.0806821584701538
LOSS: 1.0798656940460205
LOSS: 1.3853819370269775
LOSS: 1.379346251487732
LOSS: 1.3737090826034546
LOSS: 1.3685085773468018
LOSS: 1.3637793064117432
LOSS: 1.359552264213562
LOSS: 1.3558461666107178
LOSS: 1.3526623249053955
LOSS: 1.3499749898910522
LOSS: 1.3477239608764648
LOSS: 1.345811128616333
LOSS: 1.3441102504730225
LOSS: 1.3424867391586304
LOSS: 1.3408211469650269
LOSS: 1.3390257358551025
LOSS: 1.3370437622070312
LOSS: 1.3348475694656372
LOSS: 1.332425832748413
LOSS: 1.3297783136367798
LOSS: 1.3269096612930298
LOSS: 1.3238238096237183
LOSS: 1.3205246925354004
LOSS: 1.3170119524002075
LOSS: 1.3132821321487427
LOSS: 1.3093297481536865
LOSS: 1.3051503896713257
LOSS: 1.3007409572601318
LOSS: 1.2961019277572632
LOSS: 1.2912390232086182
LOSS: 1.2861615419387817
LOSS: 1.2808778285980225
LOSS: 1.2753963470458984
LOSS: 1.2697229385375977
LOSS: 1.2638641595840454
LOSS: 1.25783109664917
LOSS: 1.251644492149353
LOSS: 1.2453378438949585
LOSS: 1.2389556169509888
LOSS: 1.232557773590088
LOSS: 1.2262119054794312
LOSS: 1.2199944257736206
LOSS: 1.213987946510315
LOSS: 1.2082746028900146
LOSS: 1.2029259204864502
LOSS: 1.1979899406433105
LOSS: 1.1934916973114014
LOSS: 1.1894328594207764
LOSS: 1.1857900619506836
LOSS: 1.1825119256973267
LOSS: 1.1795316934585571
LOSS: 1.1767793893814087
LOSS: 1.1741886138916016
LOSS: 1.1717031002044678
LOSS: 1.169283151626587
LOSS: 1.1669096946716309
LOSS: 1.1645827293395996
LOSS: 1.1623153686523438
LOSS: 1.1601253747940063
LOSS: 1.158027172088623
LOSS: 1.156020998954773
LOSS: 1.1540915966033936
LOSS: 1.1522160768508911
LOSS: 1.1503729820251465
LOSS: 1.1485446691513062
LOSS: 1.1467161178588867
LOSS: 1.1448800563812256
LOSS: 1.1430386304855347
LOSS: 1.14120352268219
LOSS: 1.1393917798995972
LOSS: 1.1376242637634277
LOSS: 1.135923147201538
LOSS: 1.1343055963516235
LOSS: 1.1327828168869019
LOSS: 1.1313552856445312
LOSS: 1.1300121545791626
LOSS: 1.128732442855835
LOSS: 1.1274902820587158
LOSS: 1.1262617111206055
LOSS: 1.125030755996704
LOSS: 1.1237932443618774
LOSS: 1.1225582361221313
LOSS: 1.1213421821594238
LOSS: 1.1201564073562622
LOSS: 1.1189996004104614
LOSS: 1.1178582906723022
LOSS: 1.116719126701355
LOSS: 1.1155779361724854
LOSS: 1.1144399642944336
LOSS: 1.113316535949707
LOSS: 1.1122190952301025
LOSS: 1.111154556274414
LOSS: 1.1101218461990356
LOSS: 1.1091153621673584
LOSS: 1.1081262826919556
LOSS: 1.1071465015411377
LOSS: 1.10616934299469
LOSS: 1.105189323425293
LOSS: 1.1042016744613647
LOSS: 1.1032073497772217
LOSS: 1.1022136211395264
LOSS: 1.3703076839447021
LOSS: 1.363905668258667
LOSS: 1.3578847646713257
LOSS: 1.3522816896438599
LOSS: 1.347130298614502
LOSS: 1.3424586057662964
LOSS: 1.3382834196090698
LOSS: 1.3346045017242432
LOSS: 1.3313930034637451
LOSS: 1.328584909439087
LOSS: 1.326080083847046
LOSS: 1.3237507343292236
LOSS: 1.3214625120162964
LOSS: 1.3190983533859253
LOSS: 1.3165708780288696
LOSS: 1.3138247728347778
LOSS: 1.3108313083648682
LOSS: 1.3075789213180542
LOSS: 1.3040673732757568
LOSS: 1.3003027439117432
LOSS: 1.2962946891784668
LOSS: 1.2920535802841187
LOSS: 1.287588357925415
LOSS: 1.2829065322875977
LOSS: 1.2780131101608276
LOSS: 1.272911548614502
LOSS: 1.2676007747650146
LOSS: 1.2620859146118164
LOSS: 1.256371021270752
LOSS: 1.2504642009735107
LOSS: 1.2443755865097046
LOSS: 1.238116979598999
LOSS: 1.2317042350769043
LOSS: 1.225157380104065
LOSS: 1.2185043096542358
LOSS: 1.211782455444336
LOSS: 1.2050392627716064
LOSS: 1.198326826095581
LOSS: 1.1917014122009277
LOSS: 1.1852197647094727
LOSS: 1.1789387464523315
LOSS: 1.1729124784469604
LOSS: 1.1671899557113647
LOSS: 1.1618176698684692
LOSS: 1.1568320989608765
LOSS: 1.1522555351257324
LOSS: 1.1480892896652222
LOSS: 1.1443077325820923
LOSS: 1.1408610343933105
LOSS: 1.1376889944076538
LOSS: 1.134730577468872
LOSS: 1.1319292783737183
LOSS: 1.1292388439178467
LOSS: 1.126621961593628
LOSS: 1.1240545511245728
LOSS: 1.1215296983718872
LOSS: 1.1190613508224487
LOSS: 1.1166731119155884
LOSS: 1.1143823862075806
LOSS: 1.1121888160705566
LOSS: 1.110090732574463
LOSS: 1.1080870628356934
LOSS: 1.1061731576919556
LOSS: 1.1043328046798706
LOSS: 1.1025488376617432
LOSS: 1.100799798965454
LOSS: 1.0990707874298096
LOSS: 1.0973565578460693
LOSS: 1.0956569910049438
LOSS: 1.0939784049987793
LOSS: 1.0923244953155518
LOSS: 1.0906970500946045
LOSS: 1.089101791381836
LOSS: 1.0875458717346191
LOSS: 1.0860399007797241
LOSS: 1.0845829248428345
LOSS: 1.0831729173660278
LOSS: 1.0817973613739014
LOSS: 1.080452561378479
LOSS: 1.07913339138031
LOSS: 1.0778368711471558
LOSS: 1.0765626430511475
LOSS: 1.0753129720687866
LOSS: 1.0740900039672852
LOSS: 1.0728931427001953
LOSS: 1.0717179775238037
LOSS: 1.0705618858337402
LOSS: 1.069420337677002
LOSS: 1.068298578262329
LOSS: 1.0671985149383545
LOSS: 1.066122055053711
LOSS: 1.0650721788406372
LOSS: 1.0640413761138916
LOSS: 1.0630279779434204
LOSS: 1.0620354413986206
LOSS: 1.0610696077346802
LOSS: 1.0601376295089722
LOSS: 1.0592457056045532
LOSS: 1.058393955230713
LOSS: 1.0575734376907349
LOSS: 1.3853819370269775
LOSS: 1.379346251487732
LOSS: 1.3737090826034546
LOSS: 1.3685085773468018
LOSS: 1.3637793064117432
LOSS: 1.359552264213562
LOSS: 1.3558461666107178
LOSS: 1.3526623249053955
LOSS: 1.3499749898910522
LOSS: 1.3477239608764648
LOSS: 1.345811128616333
LOSS: 1.3441102504730225
LOSS: 1.3424867391586304
LOSS: 1.3408211469650269
LOSS: 1.3390257358551025
LOSS: 1.3370437622070312
LOSS: 1.3348475694656372
LOSS: 1.332425832748413
LOSS: 1.3297783136367798
LOSS: 1.3269096612930298
LOSS: 1.3238238096237183
LOSS: 1.3205246925354004
LOSS: 1.3170119524002075
LOSS: 1.3132821321487427
LOSS: 1.3093297481536865
LOSS: 1.3051503896713257
LOSS: 1.3007409572601318
LOSS: 1.2961019277572632
LOSS: 1.2912390232086182
LOSS: 1.2861615419387817
LOSS: 1.2808778285980225
LOSS: 1.2753963470458984
LOSS: 1.2697229385375977
LOSS: 1.2638641595840454
LOSS: 1.25783109664917
LOSS: 1.251644492149353
LOSS: 1.2453378438949585
LOSS: 1.2389556169509888
LOSS: 1.232557773590088
LOSS: 1.2262119054794312
LOSS: 1.2199944257736206
LOSS: 1.213987946510315
LOSS: 1.2082746028900146
LOSS: 1.2029259204864502
LOSS: 1.1979899406433105
LOSS: 1.1934916973114014
LOSS: 1.1894328594207764
LOSS: 1.1857900619506836
LOSS: 1.1825119256973267
LOSS: 1.1795316934585571
LOSS: 1.1767793893814087
LOSS: 1.1741886138916016
LOSS: 1.1717031002044678
LOSS: 1.169283151626587
LOSS: 1.1669096946716309
LOSS: 1.1645827293395996
LOSS: 1.1623153686523438
LOSS: 1.1601253747940063
LOSS: 1.158027172088623
LOSS: 1.156020998954773
LOSS: 1.1540915966033936
LOSS: 1.1522160768508911
LOSS: 1.1503729820251465
LOSS: 1.1485446691513062
LOSS: 1.1467161178588867
LOSS: 1.1448800563812256
LOSS: 1.1430386304855347
LOSS: 1.14120352268219
LOSS: 1.1393917798995972
LOSS: 1.1376242637634277
LOSS: 1.135923147201538
LOSS: 1.1343055963516235
LOSS: 1.3348524570465088
LOSS: 1.333058476448059
LOSS: 1.3312517404556274
LOSS: 1.329336166381836
LOSS: 1.3272477388381958
LOSS: 1.3249520063400269
LOSS: 1.3224374055862427
LOSS: 1.319704294204712
LOSS: 1.3167624473571777
LOSS: 1.3136241436004639
LOSS: 1.3103023767471313
LOSS: 1.306807279586792
LOSS: 1.3031470775604248
LOSS: 1.2993249893188477
LOSS: 1.2953405380249023
LOSS: 1.291190266609192
LOSS: 1.2868701219558716
LOSS: 1.282374382019043
LOSS: 1.277696132659912
LOSS: 1.2728321552276611
LOSS: 1.2677819728851318
LOSS: 1.2625484466552734
LOSS: 1.2571440935134888
LOSS: 1.2515861988067627
LOSS: 1.2459053993225098
LOSS: 1.2401388883590698
LOSS: 1.2343374490737915
LOSS: 1.2285547256469727
LOSS: 1.2228528261184692
LOSS: 1.2172929048538208
LOSS: 1.2119358777999878
LOSS: 1.2068341970443726
LOSS: 1.2020316123962402
LOSS: 1.1975579261779785
LOSS: 1.193421721458435
LOSS: 1.1896166801452637
LOSS: 1.1861172914505005
LOSS: 1.182887315750122
LOSS: 1.1798783540725708
LOSS: 1.177040457725525
LOSS: 1.174324631690979
LOSS: 1.1716912984848022
LOSS: 1.1691091060638428
LOSS: 1.1665587425231934
LOSS: 1.1640344858169556
LOSS: 1.1615432500839233
LOSS: 1.1591044664382935
LOSS: 1.1567423343658447
LOSS: 1.154482126235962
LOSS: 1.1523464918136597
LOSS: 1.1503480672836304
LOSS: 1.1484880447387695
LOSS: 1.1467366218566895
LOSS: 1.1450414657592773
LOSS: 1.1433343887329102
LOSS: 1.1415584087371826
LOSS: 1.1396899223327637
LOSS: 1.13773775100708
LOSS: 1.1357375383377075
LOSS: 1.1337326765060425
LOSS: 1.131775975227356
LOSS: 1.1299115419387817
LOSS: 1.1281702518463135
LOSS: 1.126559853553772
LOSS: 1.1250674724578857
LOSS: 1.1236625909805298
LOSS: 1.1223046779632568
LOSS: 1.1209547519683838
LOSS: 1.1195889711380005
LOSS: 1.1182022094726562
LOSS: 1.11680006980896
LOSS: 1.1154052019119263
LOSS: 1.1140460968017578
LOSS: 1.11275053024292
LOSS: 1.1115344762802124
LOSS: 1.110391616821289
LOSS: 1.109303593635559
LOSS: 1.108253002166748
LOSS: 1.1072324514389038
LOSS: 1.106241226196289
LOSS: 1.1052743196487427
LOSS: 1.1043217182159424
LOSS: 1.1033705472946167
LOSS: 1.1024041175842285
LOSS: 1.1014087200164795
LOSS: 1.1003673076629639
LOSS: 1.0992681980133057
LOSS: 1.0981061458587646
LOSS: 1.0968867540359497
LOSS: 1.3668519258499146
LOSS: 1.3602213859558105
LOSS: 1.3539583683013916
LOSS: 1.3480989933013916
LOSS: 1.342677354812622
LOSS: 1.337722897529602
LOSS: 1.3332560062408447
LOSS: 1.3292828798294067
LOSS: 1.3257859945297241
LOSS: 1.3227169513702393
LOSS: 1.3199923038482666
LOSS: 1.3174980878829956
LOSS: 1.3151050806045532
LOSS: 1.3126897811889648
LOSS: 1.31015145778656
LOSS: 1.3074203729629517
LOSS: 1.304453730583191
LOSS: 1.3012323379516602
LOSS: 1.2977491617202759
LOSS: 1.2940051555633545
LOSS: 1.2900056838989258
LOSS: 1.2857609987258911
LOSS: 1.2812855243682861
LOSS: 1.2765965461730957
LOSS: 1.2717124223709106
LOSS: 1.266653299331665
LOSS: 1.261439561843872
LOSS: 1.2560895681381226
LOSS: 1.2506152391433716
LOSS: 1.2450244426727295
LOSS: 1.2393182516098022
LOSS: 1.2334994077682495
LOSS: 1.2275704145431519
LOSS: 1.221540927886963
LOSS: 1.2154275178909302
LOSS: 1.2092572450637817
LOSS: 1.2030638456344604
LOSS: 1.196885347366333
LOSS: 1.1907645463943481
LOSS: 1.1847504377365112
LOSS: 1.1788930892944336
LOSS: 1.1732385158538818
LOSS: 1.1678271293640137
LOSS: 1.162685751914978
LOSS: 1.1578305959701538
LOSS: 1.153266429901123
LOSS: 1.1489869356155396
LOSS: 1.1449754238128662
LOSS: 1.141212821006775
LOSS: 1.1376746892929077
LOSS: 1.1343320608139038
LOSS: 1.131161093711853
LOSS: 1.1281479597091675
LOSS: 1.1252881288528442
LOSS: 1.1225816011428833
LOSS: 1.1200246810913086
LOSS: 1.1176077127456665
LOSS: 1.1153128147125244
LOSS: 1.1131185293197632
LOSS: 1.1110103130340576
LOSS: 1.108971118927002
LOSS: 1.1069962978363037
LOSS: 1.1050751209259033
LOSS: 1.1032040119171143
LOSS: 1.10137939453125
LOSS: 1.099599003791809
LOSS: 1.0978535413742065
LOSS: 1.0961277484893799
LOSS: 1.0944221019744873
LOSS: 1.0927226543426514
LOSS: 1.0910283327102661
LOSS: 1.0893330574035645
LOSS: 1.0876327753067017
LOSS: 1.0859109163284302
LOSS: 1.0841511487960815
LOSS: 1.082338809967041
LOSS: 1.0804685354232788
LOSS: 1.0785481929779053
LOSS: 1.0766019821166992
LOSS: 1.0746605396270752
LOSS: 1.072759985923767
LOSS: 1.0709331035614014
LOSS: 1.0692063570022583
LOSS: 1.0675806999206543
LOSS: 1.0660334825515747
LOSS: 1.064529299736023
LOSS: 1.063046932220459
LOSS: 1.0615887641906738
LOSS: 1.0601799488067627
LOSS: 1.0588527917861938
LOSS: 1.0576210021972656
LOSS: 1.0564695596694946
LOSS: 1.0553733110427856
LOSS: 1.0543218851089478
LOSS: 1.0533149242401123
LOSS: 1.0523521900177002
LOSS: 1.0514270067214966
LOSS: 1.0505300760269165
LOSS: 1.0496532917022705
LOSS: 1.0487909317016602
LOSS: 1.3731873035430908
LOSS: 1.3668854236602783
LOSS: 1.3609739542007446
LOSS: 1.35548734664917
LOSS: 1.3504618406295776
LOSS: 1.3459266424179077
LOSS: 1.341903567314148
LOSS: 1.3383985757827759
LOSS: 1.3353909254074097
LOSS: 1.332833170890808
LOSS: 1.3306384086608887
LOSS: 1.3286911249160767
LOSS: 1.326862096786499
LOSS: 1.325034260749817
LOSS: 1.3231146335601807
LOSS: 1.3210408687591553
LOSS: 1.318784475326538
LOSS: 1.3163350820541382
LOSS: 1.3136978149414062
LOSS: 1.3108831644058228
LOSS: 1.3079020977020264
LOSS: 1.3047624826431274
LOSS: 1.3014665842056274
LOSS: 1.2980127334594727
LOSS: 1.2943966388702393
LOSS: 1.2906105518341064
LOSS: 1.2866462469100952
LOSS: 1.2824945449829102
LOSS: 1.2781486511230469
LOSS: 1.2736049890518188
LOSS: 1.2688647508621216
LOSS: 1.2639358043670654
LOSS: 1.2588316202163696
LOSS: 1.2535699605941772
LOSS: 1.248176097869873
LOSS: 1.2426793575286865
LOSS: 1.2371152639389038
LOSS: 1.2315244674682617
LOSS: 1.2259502410888672
LOSS: 1.2204359769821167
LOSS: 1.215026617050171
LOSS: 1.2097620964050293
LOSS: 1.2046806812286377
LOSS: 1.199813723564148
LOSS: 1.195181131362915
LOSS: 1.190786361694336
LOSS: 1.1866215467453003
LOSS: 1.18267023563385
LOSS: 1.1789090633392334
LOSS: 1.1753084659576416
LOSS: 1.1718323230743408
LOSS: 1.1684412956237793
LOSS: 1.1651101112365723
LOSS: 1.1618282794952393
LOSS: 1.158586025238037
LOSS: 1.1553632020950317
LOSS: 1.1521505117416382
LOSS: 1.148963451385498
LOSS: 1.145830512046814
LOSS: 1.142780065536499
LOSS: 1.1398471593856812
LOSS: 1.1370737552642822
LOSS: 1.1344980001449585
LOSS: 1.1321454048156738
LOSS: 1.1300266981124878
LOSS: 1.1281239986419678
LOSS: 1.1263824701309204
LOSS: 1.124722957611084
LOSS: 1.1230767965316772
LOSS: 1.1214057207107544
LOSS: 1.119707703590393
LOSS: 1.1180098056793213
LOSS: 1.1163493394851685
LOSS: 1.1147518157958984
LOSS: 1.1132309436798096
LOSS: 1.111782431602478
LOSS: 1.1103869676589966
LOSS: 1.1090164184570312
LOSS: 1.1076463460922241
LOSS: 1.1062564849853516
LOSS: 1.10483717918396
LOSS: 1.103390097618103
LOSS: 1.1019234657287598
LOSS: 1.1004486083984375
LOSS: 1.098976731300354
LOSS: 1.0975123643875122
LOSS: 1.096054196357727
LOSS: 1.0945987701416016
LOSS: 1.093143105506897
LOSS: 1.0916770696640015
LOSS: 1.0901832580566406
LOSS: 1.0886497497558594
LOSS: 1.0870816707611084
LOSS: 1.0854978561401367
LOSS: 1.083924412727356
LOSS: 1.0823925733566284
LOSS: 1.0809326171875
LOSS: 1.0795563459396362
LOSS: 1.078248381614685
LOSS: 1.0769872665405273
LOSS: 1.3753741979599
LOSS: 1.3690857887268066
LOSS: 1.363183856010437
LOSS: 1.357704758644104
LOSS: 1.3526835441589355
LOSS: 1.3481500148773193
LOSS: 1.3441218137741089
LOSS: 1.3405992984771729
LOSS: 1.3375575542449951
LOSS: 1.3349376916885376
LOSS: 1.332642912864685
LOSS: 1.3305480480194092
LOSS: 1.328519344329834
LOSS: 1.326436161994934
LOSS: 1.324206829071045
LOSS: 1.3217707872390747
LOSS: 1.3190956115722656
LOSS: 1.316165804862976
LOSS: 1.3129760026931763
LOSS: 1.3095237016677856
LOSS: 1.305802822113037
LOSS: 1.3018053770065308
LOSS: 1.2975198030471802
LOSS: 1.2929394245147705
LOSS: 1.2880582809448242
LOSS: 1.2828747034072876
LOSS: 1.2773921489715576
LOSS: 1.271619439125061
LOSS: 1.2655667066574097
LOSS: 1.259246587753296
LOSS: 1.2526710033416748
LOSS: 1.245851993560791
LOSS: 1.2387975454330444
LOSS: 1.2315194606781006
LOSS: 1.224035382270813
LOSS: 1.2163726091384888
LOSS: 1.2085719108581543
LOSS: 1.2006891965866089
LOSS: 1.1927944421768188
LOSS: 1.1849701404571533
LOSS: 1.1773037910461426
LOSS: 1.1698819398880005
LOSS: 1.1723829507827759
LOSS: 1.1682137250900269
LOSS: 1.1645082235336304
LOSS: 1.1612297296524048
LOSS: 1.1583173274993896
LOSS: 1.155687689781189
LOSS: 1.1532460451126099
LOSS: 1.1509013175964355
LOSS: 1.1485792398452759
LOSS: 1.1462287902832031
LOSS: 1.1438236236572266
LOSS: 1.141361117362976
LOSS: 1.138857126235962
LOSS: 1.1363381147384644
LOSS: 1.1338359117507935
LOSS: 1.1313804388046265
LOSS: 1.1289936304092407
LOSS: 1.1266857385635376
LOSS: 1.1244550943374634
LOSS: 1.122291922569275
LOSS: 1.1201823949813843
LOSS: 1.11811363697052
LOSS: 1.1160744428634644
LOSS: 1.1140564680099487
LOSS: 1.1120562553405762
LOSS: 1.1100773811340332
LOSS: 1.1081279516220093
LOSS: 1.1062164306640625
LOSS: 1.104349970817566
LOSS: 1.1025347709655762
LOSS: 1.100779414176941
LOSS: 1.0990920066833496
LOSS: 1.0974780321121216
LOSS: 1.0959376096725464
LOSS: 1.0944644212722778
LOSS: 1.0930463075637817
LOSS: 1.0916707515716553
LOSS: 1.0903280973434448
LOSS: 1.089011788368225
LOSS: 1.0877182483673096
LOSS: 1.0864450931549072
LOSS: 1.0851916074752808
LOSS: 1.083957552909851
LOSS: 1.0827414989471436
LOSS: 1.0815446376800537
LOSS: 1.0803735256195068
LOSS: 1.0792344808578491
LOSS: 1.0781326293945312
LOSS: 1.0770713090896606
LOSS: 1.0760555267333984
LOSS: 1.0750879049301147
LOSS: 1.074161410331726
LOSS: 1.0732592344284058
LOSS: 1.0723692178726196
LOSS: 1.0714926719665527
LOSS: 1.070637583732605
LOSS: 1.0698046684265137
LOSS: 1.0689847469329834
LOSS: 1.3755828142166138
LOSS: 1.3692920207977295
LOSS: 1.36338472366333
LOSS: 1.3578978776931763
LOSS: 1.352866768836975
LOSS: 1.3483219146728516
LOSS: 1.3442842960357666
LOSS: 1.3407599925994873
LOSS: 1.337729811668396
LOSS: 1.3351439237594604
LOSS: 1.3329145908355713
LOSS: 1.3309283256530762
LOSS: 1.329055666923523
LOSS: 1.3271790742874146
LOSS: 1.3252054452896118
LOSS: 1.3230787515640259
LOSS: 1.3207708597183228
LOSS: 1.3182742595672607
LOSS: 1.3155959844589233
LOSS: 1.3127477169036865
LOSS: 1.3097413778305054
LOSS: 1.3065898418426514
LOSS: 1.30330228805542
LOSS: 1.2998889684677124
LOSS: 1.2963554859161377
LOSS: 1.2927031517028809
LOSS: 1.2889281511306763
LOSS: 1.2850219011306763
LOSS: 1.2809724807739258
LOSS: 1.2767672538757324
LOSS: 1.2723958492279053
LOSS: 1.26785147190094
LOSS: 1.2631325721740723
LOSS: 1.2582465410232544
LOSS: 1.2532063722610474
LOSS: 1.2480318546295166
LOSS: 1.2427496910095215
LOSS: 1.2373926639556885
LOSS: 1.2319998741149902
LOSS: 1.2266154289245605
LOSS: 1.2212848663330078
LOSS: 1.2160524129867554
LOSS: 1.2109614610671997
LOSS: 1.206048846244812
LOSS: 1.2013435363769531
LOSS: 1.1968687772750854
LOSS: 1.1926429271697998
LOSS: 1.1886767148971558
LOSS: 1.1849690675735474
LOSS: 1.1815094947814941
LOSS: 1.1782644987106323
LOSS: 1.1751899719238281
LOSS: 1.172245979309082
LOSS: 1.1693980693817139
LOSS: 1.1666173934936523
LOSS: 1.1638859510421753
LOSS: 1.1611913442611694
LOSS: 1.1585233211517334
LOSS: 1.1558696031570435
LOSS: 1.15321946144104
LOSS: 1.1505786180496216
LOSS: 1.1479655504226685
LOSS: 1.1454042196273804
LOSS: 1.1429158449172974
LOSS: 1.140515685081482
LOSS: 1.1382070779800415
LOSS: 1.1359792947769165
LOSS: 1.133813738822937
LOSS: 1.131688117980957
LOSS: 1.1295803785324097
LOSS: 1.1274744272232056
LOSS: 1.125364065170288
LOSS: 1.1232576370239258
LOSS: 1.1211748123168945
LOSS: 1.1191418170928955
LOSS: 1.117185115814209
LOSS: 1.1153228282928467
LOSS: 1.1135635375976562
LOSS: 1.1119041442871094
LOSS: 1.1103326082229614
LOSS: 1.108830213546753
LOSS: 1.1073724031448364
LOSS: 1.1059317588806152
LOSS: 1.1044856309890747
LOSS: 1.103023648262024
LOSS: 1.1015465259552002
LOSS: 1.100064754486084
LOSS: 1.0985956192016602
LOSS: 1.0971574783325195
LOSS: 1.0957648754119873
LOSS: 1.0944279432296753
LOSS: 1.093151330947876
LOSS: 1.0919328927993774
LOSS: 1.09076726436615
LOSS: 1.0896449089050293
LOSS: 1.0885611772537231
LOSS: 1.0875160694122314
LOSS: 1.086513876914978
LOSS: 1.0855547189712524
LOSS: 1.0846294164657593
LOSS: 1.3668519258499146
LOSS: 1.3602213859558105
LOSS: 1.3539583683013916
LOSS: 1.3480989933013916
LOSS: 1.342677354812622
LOSS: 1.337722897529602
LOSS: 1.3332560062408447
LOSS: 1.3292828798294067
LOSS: 1.3257859945297241
LOSS: 1.3227169513702393
LOSS: 1.3199923038482666
LOSS: 1.3174980878829956
LOSS: 1.3151050806045532
LOSS: 1.3126897811889648
LOSS: 1.31015145778656
LOSS: 1.3074203729629517
LOSS: 1.304453730583191
LOSS: 1.3012323379516602
LOSS: 1.2977491617202759
LOSS: 1.2940051555633545
LOSS: 1.2900056838989258
LOSS: 1.2857609987258911
LOSS: 1.2812855243682861
LOSS: 1.2765965461730957
LOSS: 1.2717124223709106
LOSS: 1.266653299331665
LOSS: 1.261439561843872
LOSS: 1.2560895681381226
LOSS: 1.2506152391433716
LOSS: 1.2450244426727295
LOSS: 1.2393182516098022
LOSS: 1.2334994077682495
LOSS: 1.2275704145431519
LOSS: 1.221540927886963
LOSS: 1.2154275178909302
LOSS: 1.2092572450637817
LOSS: 1.2030638456344604
LOSS: 1.196885347366333
LOSS: 1.1907645463943481
LOSS: 1.1847504377365112
LOSS: 1.1788930892944336
LOSS: 1.1732385158538818
LOSS: 1.1678271293640137
LOSS: 1.162685751914978
LOSS: 1.1578305959701538
LOSS: 1.153266429901123
LOSS: 1.1489869356155396
LOSS: 1.1449754238128662
LOSS: 1.141212821006775
LOSS: 1.1376746892929077
LOSS: 1.1343320608139038
LOSS: 1.131161093711853
LOSS: 1.1281479597091675
LOSS: 1.1252881288528442
LOSS: 1.1225816011428833
LOSS: 1.1200246810913086
LOSS: 1.1176077127456665
LOSS: 1.1153128147125244
LOSS: 1.1131185293197632
LOSS: 1.1110103130340576
LOSS: 1.108971118927002
LOSS: 1.1069962978363037
LOSS: 1.1050751209259033
LOSS: 1.1032040119171143
LOSS: 1.10137939453125
LOSS: 1.099599003791809
LOSS: 1.0978535413742065
LOSS: 1.0961277484893799
LOSS: 1.0944221019744873
LOSS: 1.0927226543426514
LOSS: 1.0910283327102661
LOSS: 1.0893330574035645
LOSS: 1.0876327753067017
LOSS: 1.0859109163284302
LOSS: 1.0841511487960815
LOSS: 1.082338809967041
LOSS: 1.0804685354232788
LOSS: 1.0785481929779053
LOSS: 1.0766019821166992
LOSS: 1.0746605396270752
LOSS: 1.072759985923767
LOSS: 1.0709331035614014
LOSS: 1.0692063570022583
LOSS: 1.0675806999206543
LOSS: 1.0660334825515747
LOSS: 1.064529299736023
LOSS: 1.063046932220459
LOSS: 1.0615887641906738
LOSS: 1.0601799488067627
LOSS: 1.0588527917861938
LOSS: 1.0576210021972656
LOSS: 1.0564695596694946
LOSS: 1.0553733110427856
LOSS: 1.0543218851089478
LOSS: 1.0533149242401123
LOSS: 1.0523521900177002
LOSS: 1.0514270067214966
LOSS: 1.0505300760269165
LOSS: 1.0496532917022705
LOSS: 1.0487909317016602
LOSS: 1.3773398399353027
LOSS: 1.3710651397705078
LOSS: 1.3651753664016724
LOSS: 1.359708309173584
LOSS: 1.3546984195709229
LOSS: 1.3501756191253662
LOSS: 1.346159815788269
LOSS: 1.3426541090011597
LOSS: 1.3396347761154175
LOSS: 1.3370459079742432
LOSS: 1.3347938060760498
LOSS: 1.3327584266662598
LOSS: 1.3308078050613403
LOSS: 1.328827142715454
LOSS: 1.3267298936843872
LOSS: 1.324461817741394
LOSS: 1.3219962120056152
LOSS: 1.3193250894546509
LOSS: 1.316450834274292
LOSS: 1.3133785724639893
LOSS: 1.3101146221160889
LOSS: 1.3066672086715698
LOSS: 1.3030439615249634
LOSS: 1.2992509603500366
LOSS: 1.2952946424484253
LOSS: 1.2911790609359741
LOSS: 1.286909818649292
LOSS: 1.2824897766113281
LOSS: 1.2779185771942139
LOSS: 1.2731924057006836
LOSS: 1.268304705619812
LOSS: 1.2632495164871216
LOSS: 1.2580260038375854
LOSS: 1.2526414394378662
LOSS: 1.2471132278442383
LOSS: 1.241466760635376
LOSS: 1.2357310056686401
LOSS: 1.2299470901489258
LOSS: 1.2241684198379517
LOSS: 1.2184562683105469
LOSS: 1.2128733396530151
LOSS: 1.2074776887893677
LOSS: 1.2023200988769531
LOSS: 1.1974341869354248
LOSS: 1.1928399801254272
LOSS: 1.1885439157485962
LOSS: 1.1845481395721436
LOSS: 1.1808496713638306
LOSS: 1.1774379014968872
LOSS: 1.1742956638336182
LOSS: 1.1714006662368774
LOSS: 1.1687264442443848
LOSS: 1.1662365198135376
LOSS: 1.1638710498809814
LOSS: 1.1615599393844604
LOSS: 1.1592453718185425
LOSS: 1.1568994522094727
LOSS: 1.1545199155807495
LOSS: 1.152095079421997
LOSS: 1.1496073007583618
LOSS: 1.1470609903335571
LOSS: 1.144478678703308
LOSS: 1.1419001817703247
LOSS: 1.1393662691116333
LOSS: 1.1369160413742065
LOSS: 1.134577989578247
LOSS: 1.1323622465133667
LOSS: 1.1302623748779297
LOSS: 1.1282576322555542
LOSS: 1.1263235807418823
LOSS: 1.124436855316162
LOSS: 1.1225783824920654
LOSS: 1.2097620964050293
LOSS: 1.2046806812286377
LOSS: 1.199813723564148
LOSS: 1.195181131362915
LOSS: 1.190786361694336
LOSS: 1.1866215467453003
LOSS: 1.18267023563385
LOSS: 1.1789090633392334
LOSS: 1.1753084659576416
LOSS: 1.1718323230743408
LOSS: 1.1684412956237793
LOSS: 1.1651101112365723
LOSS: 1.1618282794952393
LOSS: 1.158586025238037
LOSS: 1.1553632020950317
LOSS: 1.1521505117416382
LOSS: 1.148963451385498
LOSS: 1.145830512046814
LOSS: 1.142780065536499
LOSS: 1.1398471593856812
LOSS: 1.1370737552642822
LOSS: 1.1344980001449585
LOSS: 1.1321454048156738
LOSS: 1.1300266981124878
LOSS: 1.1281239986419678
LOSS: 1.1263824701309204
LOSS: 1.124722957611084
LOSS: 1.1230767965316772
LOSS: 1.1214057207107544
LOSS: 1.119707703590393
LOSS: 1.1180098056793213
LOSS: 1.1163493394851685
LOSS: 1.1147518157958984
LOSS: 1.1132309436798096
LOSS: 1.111782431602478
LOSS: 1.1103869676589966
LOSS: 1.1090164184570312
LOSS: 1.1076463460922241
LOSS: 1.1062564849853516
LOSS: 1.10483717918396
LOSS: 1.103390097618103
LOSS: 1.1019234657287598
LOSS: 1.1004486083984375
LOSS: 1.098976731300354
LOSS: 1.0975123643875122
LOSS: 1.096054196357727
LOSS: 1.0945987701416016
LOSS: 1.093143105506897
LOSS: 1.0916770696640015
LOSS: 1.0901832580566406
LOSS: 1.0886497497558594
LOSS: 1.0870816707611084
LOSS: 1.0854978561401367
LOSS: 1.083924412727356
LOSS: 1.0823925733566284
LOSS: 1.0809326171875
LOSS: 1.0795563459396362
LOSS: 1.078248381614685
LOSS: 1.0769872665405273
LOSS: 1.3755828142166138
LOSS: 1.3692920207977295
LOSS: 1.36338472366333
LOSS: 1.3578978776931763
LOSS: 1.352866768836975
LOSS: 1.3483219146728516
LOSS: 1.3442842960357666
LOSS: 1.3407599925994873
LOSS: 1.337729811668396
LOSS: 1.3351439237594604
LOSS: 1.3329145908355713
LOSS: 1.3309283256530762
LOSS: 1.329055666923523
LOSS: 1.3271790742874146
LOSS: 1.3252054452896118
LOSS: 1.3230787515640259
LOSS: 1.3207708597183228
LOSS: 1.3182742595672607
LOSS: 1.3155959844589233
LOSS: 1.3127477169036865
LOSS: 1.3097413778305054
LOSS: 1.3065898418426514
LOSS: 1.30330228805542
LOSS: 1.2998889684677124
LOSS: 1.2963554859161377
LOSS: 1.2927031517028809
LOSS: 1.2889281511306763
LOSS: 1.2850219011306763
LOSS: 1.2809724807739258
LOSS: 1.2767672538757324
LOSS: 1.2723958492279053
LOSS: 1.26785147190094
LOSS: 1.2631325721740723
LOSS: 1.2582465410232544
LOSS: 1.2532063722610474
LOSS: 1.2480318546295166
LOSS: 1.2427496910095215
LOSS: 1.2373926639556885
LOSS: 1.2319998741149902
LOSS: 1.2266154289245605
LOSS: 1.2212848663330078
LOSS: 1.2160524129867554
LOSS: 1.2109614610671997
LOSS: 1.206048846244812
LOSS: 1.2013435363769531
LOSS: 1.1968687772750854
LOSS: 1.1926429271697998
LOSS: 1.1886767148971558
LOSS: 1.1849690675735474
LOSS: 1.1815094947814941
LOSS: 1.1782644987106323
LOSS: 1.1751899719238281
LOSS: 1.172245979309082
LOSS: 1.1693980693817139
LOSS: 1.1666173934936523
LOSS: 1.1638859510421753
LOSS: 1.1611913442611694
LOSS: 1.1585233211517334
LOSS: 1.1558696031570435
LOSS: 1.15321946144104
LOSS: 1.1505786180496216
LOSS: 1.1479655504226685
LOSS: 1.1454042196273804
LOSS: 1.1429158449172974
LOSS: 1.140515685081482
LOSS: 1.1382070779800415
LOSS: 1.1359792947769165
LOSS: 1.133813738822937
LOSS: 1.131688117980957
LOSS: 1.1295803785324097
LOSS: 1.1274744272232056
LOSS: 1.125364065170288
LOSS: 1.1232576370239258
LOSS: 1.1211748123168945
LOSS: 1.1191418170928955
LOSS: 1.117185115814209
LOSS: 1.1153228282928467
LOSS: 1.1135635375976562
LOSS: 1.1119041442871094
LOSS: 1.1103326082229614
LOSS: 1.108830213546753
LOSS: 1.1073724031448364
LOSS: 1.1059317588806152
LOSS: 1.1044856309890747
LOSS: 1.103023648262024
LOSS: 1.1015465259552002
LOSS: 1.100064754486084
LOSS: 1.0985956192016602
LOSS: 1.0971574783325195
LOSS: 1.0957648754119873
LOSS: 1.0944279432296753
LOSS: 1.093151330947876
LOSS: 1.0919328927993774
LOSS: 1.09076726436615
LOSS: 1.0896449089050293
LOSS: 1.0885611772537231
LOSS: 1.0875160694122314
LOSS: 1.086513876914978
LOSS: 1.0855547189712524
LOSS: 1.0846294164657593
LOSS: 1.378225564956665
LOSS: 1.372001051902771
LOSS: 1.3661649227142334
LOSS: 1.3607536554336548
LOSS: 1.3558027744293213
LOSS: 1.351340889930725
LOSS: 1.347386121749878
LOSS: 1.3439399003982544
LOSS: 1.3409769535064697
LOSS: 1.3384371995925903
LOSS: 1.3362230062484741
LOSS: 1.3342092037200928
LOSS: 1.3322606086730957
LOSS: 1.3302569389343262
LOSS: 1.3281066417694092
LOSS: 1.3257503509521484
LOSS: 1.323154330253601
LOSS: 1.320302128791809
LOSS: 1.3171873092651367
LOSS: 1.3138083219528198
LOSS: 1.3101686239242554
LOSS: 1.3062735795974731
LOSS: 1.3021302223205566
LOSS: 1.297747015953064
LOSS: 1.2931337356567383
LOSS: 1.288301944732666
LOSS: 1.283262848854065
LOSS: 1.278031587600708
LOSS: 1.2726248502731323
LOSS: 1.267061471939087
LOSS: 1.2613615989685059
LOSS: 1.2555441856384277
LOSS: 1.2496273517608643
LOSS: 1.243630051612854
LOSS: 1.237575888633728
LOSS: 1.2314924001693726
LOSS: 1.2254117727279663
LOSS: 1.2193704843521118
LOSS: 1.21340811252594
LOSS: 1.2075660228729248
LOSS: 1.2018873691558838
LOSS: 1.1964105367660522
LOSS: 1.1911722421646118
LOSS: 1.1861999034881592
LOSS: 1.1815149784088135
LOSS: 1.1771273612976074
LOSS: 1.1730345487594604
LOSS: 1.1692209243774414
LOSS: 1.1656620502471924
LOSS: 1.1623250246047974
LOSS: 1.1591777801513672
LOSS: 1.1561864614486694
LOSS: 1.153328537940979
LOSS: 1.1505934000015259
LOSS: 1.1479828357696533
LOSS: 1.145501971244812
LOSS: 1.1431540250778198
LOSS: 1.1409318447113037
LOSS: 1.1388124227523804
LOSS: 1.1367639303207397
LOSS: 1.1347553730010986
LOSS: 1.1327662467956543
LOSS: 1.1307895183563232
LOSS: 1.1288316249847412
LOSS: 1.1269065141677856
LOSS: 1.125027060508728
LOSS: 1.1232022047042847
LOSS: 1.1214337348937988
LOSS: 1.119714617729187
LOSS: 1.118029236793518
LOSS: 1.116358995437622
LOSS: 1.1146918535232544
LOSS: 1.1130250692367554
LOSS: 1.111364722251892
LOSS: 1.1097221374511719
LOSS: 1.108109712600708
LOSS: 1.106541633605957
LOSS: 1.1050320863723755
LOSS: 1.1035906076431274
LOSS: 1.1022123098373413
LOSS: 1.1008824110031128
LOSS: 1.0995852947235107
LOSS: 1.0983103513717651
LOSS: 1.0970494747161865
LOSS: 1.095798134803772
LOSS: 1.0945509672164917
LOSS: 1.093294620513916
LOSS: 1.0920114517211914
LOSS: 1.0906916856765747
LOSS: 1.0893385410308838
LOSS: 1.0879632234573364
LOSS: 1.0865808725357056
LOSS: 1.0852080583572388
LOSS: 1.0838665962219238
LOSS: 1.0825791358947754
LOSS: 1.0813655853271484
LOSS: 1.0802334547042847
LOSS: 1.079171061515808
LOSS: 1.0781314373016357
LOSS: 1.0770827531814575
LOSS: 1.3773398399353027
LOSS: 1.3710651397705078
LOSS: 1.3651753664016724
LOSS: 1.359708309173584
LOSS: 1.3546984195709229
LOSS: 1.3501756191253662
LOSS: 1.346159815788269
LOSS: 1.3426541090011597
LOSS: 1.3396347761154175
LOSS: 1.3370459079742432
LOSS: 1.3347938060760498
LOSS: 1.3327584266662598
LOSS: 1.3308078050613403
LOSS: 1.328827142715454
LOSS: 1.3267298936843872
LOSS: 1.324461817741394
LOSS: 1.3219962120056152
LOSS: 1.3193250894546509
LOSS: 1.316450834274292
LOSS: 1.3133785724639893
LOSS: 1.3101146221160889
LOSS: 1.3066672086715698
LOSS: 1.3030439615249634
LOSS: 1.2992509603500366
LOSS: 1.2952946424484253
LOSS: 1.2911790609359741
LOSS: 1.286909818649292
LOSS: 1.2824897766113281
LOSS: 1.2779185771942139
LOSS: 1.2731924057006836
LOSS: 1.268304705619812
LOSS: 1.2632495164871216
LOSS: 1.2580260038375854
LOSS: 1.2526414394378662
LOSS: 1.2471132278442383
LOSS: 1.241466760635376
LOSS: 1.2357310056686401
LOSS: 1.2299470901489258
LOSS: 1.2241684198379517
LOSS: 1.2184562683105469
LOSS: 1.2128733396530151
LOSS: 1.2074776887893677
LOSS: 1.2023200988769531
LOSS: 1.1974341869354248
LOSS: 1.1928399801254272
LOSS: 1.1885439157485962
LOSS: 1.1845481395721436
LOSS: 1.1808496713638306
LOSS: 1.1774379014968872
LOSS: 1.1742956638336182
LOSS: 1.1714006662368774
LOSS: 1.1687264442443848
LOSS: 1.1662365198135376
LOSS: 1.1638710498809814
LOSS: 1.1615599393844604
LOSS: 1.1592453718185425
LOSS: 1.1568994522094727
LOSS: 1.1545199155807495
LOSS: 1.152095079421997
LOSS: 1.1496073007583618
LOSS: 1.1470609903335571
LOSS: 1.144478678703308
LOSS: 1.1419001817703247
LOSS: 1.1393662691116333
LOSS: 1.1369160413742065
LOSS: 1.134577989578247
LOSS: 1.1323622465133667
LOSS: 1.1302623748779297
LOSS: 1.1282576322555542
LOSS: 1.1263235807418823
LOSS: 1.124436855316162
LOSS: 1.1225783824920654
LOSS: 1.1327828168869019
LOSS: 1.1313552856445312
LOSS: 1.1300121545791626
LOSS: 1.128732442855835
LOSS: 1.1274902820587158
LOSS: 1.1262617111206055
LOSS: 1.125030755996704
LOSS: 1.1237932443618774
LOSS: 1.1225582361221313
LOSS: 1.1213421821594238
LOSS: 1.1201564073562622
LOSS: 1.1189996004104614
LOSS: 1.1178582906723022
LOSS: 1.116719126701355
LOSS: 1.1155779361724854
LOSS: 1.1144399642944336
LOSS: 1.113316535949707
LOSS: 1.1122190952301025
LOSS: 1.111154556274414
LOSS: 1.1101218461990356
LOSS: 1.1091153621673584
LOSS: 1.1081262826919556
LOSS: 1.1071465015411377
LOSS: 1.10616934299469
LOSS: 1.105189323425293
LOSS: 1.1042016744613647
LOSS: 1.1032073497772217
LOSS: 1.1022136211395264
LOSS: 1.3703076839447021
LOSS: 1.363905668258667
LOSS: 1.3578847646713257
LOSS: 1.3522816896438599
LOSS: 1.347130298614502
LOSS: 1.3424586057662964
LOSS: 1.3382834196090698
LOSS: 1.3346045017242432
LOSS: 1.3313930034637451
LOSS: 1.328584909439087
LOSS: 1.326080083847046
LOSS: 1.3237507343292236
LOSS: 1.3214625120162964
LOSS: 1.3190983533859253
LOSS: 1.3165708780288696
LOSS: 1.3138247728347778
LOSS: 1.3108313083648682
LOSS: 1.3075789213180542
LOSS: 1.3040673732757568
LOSS: 1.3003027439117432
LOSS: 1.2962946891784668
LOSS: 1.2920535802841187
LOSS: 1.287588357925415
LOSS: 1.2829065322875977
LOSS: 1.2780131101608276
LOSS: 1.272911548614502
LOSS: 1.2676007747650146
LOSS: 1.2620859146118164
LOSS: 1.256371021270752
LOSS: 1.2504642009735107
LOSS: 1.2443755865097046
LOSS: 1.238116979598999
LOSS: 1.2317042350769043
LOSS: 1.225157380104065
LOSS: 1.2185043096542358
LOSS: 1.211782455444336
LOSS: 1.2050392627716064
LOSS: 1.198326826095581
LOSS: 1.1917014122009277
LOSS: 1.1852197647094727
LOSS: 1.1789387464523315
LOSS: 1.1729124784469604
LOSS: 1.1671899557113647
LOSS: 1.1618176698684692
LOSS: 1.1568320989608765
LOSS: 1.1522555351257324
LOSS: 1.1480892896652222
LOSS: 1.1443077325820923
LOSS: 1.1408610343933105
LOSS: 1.1376889944076538
LOSS: 1.134730577468872
LOSS: 1.1319292783737183
LOSS: 1.1292388439178467
LOSS: 1.126621961593628
LOSS: 1.1240545511245728
LOSS: 1.1215296983718872
LOSS: 1.1190613508224487
LOSS: 1.1166731119155884
LOSS: 1.1143823862075806
LOSS: 1.1121888160705566
LOSS: 1.110090732574463
LOSS: 1.1080870628356934
LOSS: 1.1061731576919556
LOSS: 1.1043328046798706
LOSS: 1.1025488376617432
LOSS: 1.100799798965454
LOSS: 1.0990707874298096
LOSS: 1.0973565578460693
LOSS: 1.0956569910049438
LOSS: 1.0939784049987793
LOSS: 1.0923244953155518
LOSS: 1.0906970500946045
LOSS: 1.089101791381836
LOSS: 1.0875458717346191
LOSS: 1.0860399007797241
LOSS: 1.0845829248428345
LOSS: 1.0831729173660278
LOSS: 1.0817973613739014
LOSS: 1.080452561378479
LOSS: 1.07913339138031
LOSS: 1.0778368711471558
LOSS: 1.0765626430511475
LOSS: 1.0753129720687866
LOSS: 1.0740900039672852
LOSS: 1.0728931427001953
LOSS: 1.0717179775238037
LOSS: 1.0705618858337402
LOSS: 1.069420337677002
LOSS: 1.068298578262329
LOSS: 1.0671985149383545
LOSS: 1.066122055053711
LOSS: 1.0650721788406372
LOSS: 1.0640413761138916
LOSS: 1.0630279779434204
LOSS: 1.0620354413986206
LOSS: 1.0610696077346802
LOSS: 1.0601376295089722
LOSS: 1.0592457056045532
LOSS: 1.058393955230713
LOSS: 1.0575734376907349
LOSS: 1.3722831010818481
LOSS: 1.365846037864685
LOSS: 1.3597897291183472
LOSS: 1.3541513681411743
LOSS: 1.3489681482315063
LOSS: 1.344269871711731
LOSS: 1.3400812149047852
LOSS: 1.3364096879959106
LOSS: 1.3332436084747314
LOSS: 1.3305418491363525
LOSS: 1.3282268047332764
LOSS: 1.3261924982070923
LOSS: 1.324312448501587
LOSS: 1.3224622011184692
LOSS: 1.320538878440857
LOSS: 1.3184723854064941
LOSS: 1.3162208795547485
LOSS: 1.313766598701477
LOSS: 1.311110258102417
LOSS: 1.308262825012207
LOSS: 1.3052384853363037
LOSS: 1.3020535707473755
LOSS: 1.298722267150879
LOSS: 1.295255184173584
LOSS: 1.291658878326416
LOSS: 1.2879339456558228
LOSS: 1.2840783596038818
LOSS: 1.2800829410552979
LOSS: 1.275937795639038
LOSS: 1.2716295719146729
LOSS: 1.2671512365341187
LOSS: 1.262498378753662
LOSS: 1.2576744556427002
LOSS: 1.2526891231536865
LOSS: 1.247558832168579
LOSS: 1.2423079013824463
LOSS: 1.2369650602340698
LOSS: 1.2315646409988403
LOSS: 1.2261464595794678
LOSS: 1.22075355052948
LOSS: 1.2154299020767212
LOSS: 1.2102192640304565
LOSS: 1.2051606178283691
LOSS: 1.2002869844436646
LOSS: 1.1956250667572021
LOSS: 1.1911940574645996
LOSS: 1.1870033740997314
LOSS: 1.183058738708496
LOSS: 1.1793551445007324
LOSS: 1.175884485244751
LOSS: 1.1726410388946533
LOSS: 1.1696209907531738
LOSS: 1.1668143272399902
LOSS: 1.1642074584960938
LOSS: 1.161778450012207
LOSS: 1.1595028638839722
LOSS: 1.1573615074157715
LOSS: 1.1553329229354858
LOSS: 1.1534010171890259
LOSS: 1.1515510082244873
LOSS: 1.1497783660888672
LOSS: 1.1480664014816284
LOSS: 1.146399736404419
LOSS: 1.1447690725326538
LOSS: 1.14316725730896
LOSS: 1.1415964365005493
LOSS: 1.1400455236434937
LOSS: 1.1384950876235962
LOSS: 1.1369426250457764
LOSS: 1.135377287864685
LOSS: 1.1338114738464355
LOSS: 1.132242202758789
LOSS: 1.1306774616241455
LOSS: 1.1291197538375854
LOSS: 1.1275854110717773
LOSS: 1.1260895729064941
LOSS: 1.124656081199646
LOSS: 1.1233017444610596
LOSS: 1.1220184564590454
LOSS: 1.1208109855651855
LOSS: 1.1196751594543457
LOSS: 1.1186144351959229
LOSS: 1.1176166534423828
LOSS: 1.1166465282440186
LOSS: 1.1156797409057617
LOSS: 1.1146870851516724
LOSS: 1.1137102842330933
LOSS: 1.1127644777297974
LOSS: 1.111855149269104
LOSS: 1.111000657081604
LOSS: 1.1101785898208618
LOSS: 1.1093761920928955
LOSS: 1.1086012125015259
LOSS: 1.107846975326538
LOSS: 1.1071285009384155
LOSS: 1.1064262390136719
LOSS: 1.1057473421096802
LOSS: 1.1050735712051392
LOSS: 1.1044299602508545
LOSS: 1.103798508644104
LOSS: 1.3722831010818481
LOSS: 1.365846037864685
LOSS: 1.3597897291183472
LOSS: 1.3541513681411743
LOSS: 1.3489681482315063
LOSS: 1.344269871711731
LOSS: 1.3400812149047852
LOSS: 1.3364096879959106
LOSS: 1.3332436084747314
LOSS: 1.3305418491363525
LOSS: 1.3282268047332764
LOSS: 1.3261924982070923
LOSS: 1.324312448501587
LOSS: 1.3224622011184692
LOSS: 1.320538878440857
LOSS: 1.3184723854064941
LOSS: 1.3162208795547485
LOSS: 1.313766598701477
LOSS: 1.311110258102417
LOSS: 1.308262825012207
LOSS: 1.3052384853363037
LOSS: 1.3020535707473755
LOSS: 1.298722267150879
LOSS: 1.295255184173584
LOSS: 1.291658878326416
LOSS: 1.2879339456558228
LOSS: 1.2840783596038818
LOSS: 1.2800829410552979
LOSS: 1.275937795639038
LOSS: 1.2716295719146729
LOSS: 1.2671512365341187
LOSS: 1.262498378753662
LOSS: 1.2576744556427002
LOSS: 1.2526891231536865
LOSS: 1.247558832168579
LOSS: 1.2423079013824463
LOSS: 1.2369650602340698
LOSS: 1.2315646409988403
LOSS: 1.2261464595794678
LOSS: 1.22075355052948
LOSS: 1.2154299020767212
LOSS: 1.2102192640304565
LOSS: 1.2051606178283691
LOSS: 1.2002869844436646
LOSS: 1.1956250667572021
LOSS: 1.1911940574645996
LOSS: 1.1870033740997314
LOSS: 1.183058738708496
LOSS: 1.1793551445007324
LOSS: 1.175884485244751
LOSS: 1.1726410388946533
LOSS: 1.1696209907531738
LOSS: 1.1668143272399902
LOSS: 1.1642074584960938
LOSS: 1.161778450012207
LOSS: 1.1595028638839722
LOSS: 1.1573615074157715
LOSS: 1.1553329229354858
LOSS: 1.1534010171890259
LOSS: 1.1515510082244873
LOSS: 1.1497783660888672
LOSS: 1.1480664014816284
LOSS: 1.146399736404419
LOSS: 1.1447690725326538
LOSS: 1.14316725730896
LOSS: 1.1415964365005493
LOSS: 1.1400455236434937
LOSS: 1.1384950876235962
LOSS: 1.1369426250457764
LOSS: 1.135377287864685
LOSS: 1.1338114738464355
LOSS: 1.132242202758789
LOSS: 1.1306774616241455
LOSS: 1.1291197538375854
LOSS: 1.1275854110717773
LOSS: 1.1260895729064941
LOSS: 1.124656081199646
LOSS: 1.1233017444610596
LOSS: 1.1220184564590454
LOSS: 1.1208109855651855
LOSS: 1.1196751594543457
LOSS: 1.1186144351959229
LOSS: 1.1176166534423828
LOSS: 1.1166465282440186
LOSS: 1.1156797409057617
LOSS: 1.1146870851516724
LOSS: 1.1137102842330933
LOSS: 1.1127644777297974
LOSS: 1.111855149269104
LOSS: 1.111000657081604
LOSS: 1.1101785898208618
LOSS: 1.1093761920928955
LOSS: 1.1086012125015259
LOSS: 1.107846975326538
LOSS: 1.1071285009384155
LOSS: 1.1064262390136719
LOSS: 1.1057473421096802
LOSS: 1.1050735712051392
LOSS: 1.1044299602508545
LOSS: 1.103798508644104
LOSS: 1.3705490827560425
LOSS: 1.364116907119751
LOSS: 1.3580635786056519
LOSS: 1.1627869606018066
LOSS: 1.1560934782028198
LOSS: 1.1498589515686035
LOSS: 1.1441200971603394
LOSS: 1.1388908624649048
LOSS: 1.134159803390503
LOSS: 1.1298936605453491
LOSS: 1.126039743423462
LOSS: 1.122534155845642
LOSS: 1.1193081140518188
LOSS: 1.116299033164978
LOSS: 1.1134576797485352
LOSS: 1.110753059387207
LOSS: 1.1081641912460327
LOSS: 1.1056660413742065
LOSS: 1.1032228469848633
LOSS: 1.1008058786392212
LOSS: 1.0984089374542236
LOSS: 1.0960408449172974
LOSS: 1.0937094688415527
LOSS: 1.0914115905761719
LOSS: 1.0891313552856445
LOSS: 1.086847186088562
LOSS: 1.084538221359253
LOSS: 1.082192301750183
LOSS: 1.0798084735870361
LOSS: 1.0773996114730835
LOSS: 1.0749880075454712
LOSS: 1.072601556777954
LOSS: 1.070268988609314
LOSS: 1.068016529083252
LOSS: 1.0658631324768066
LOSS: 1.063819169998169
LOSS: 1.0618844032287598
LOSS: 1.060050368309021
LOSS: 1.058302640914917
LOSS: 1.0566273927688599
LOSS: 1.0550174713134766
LOSS: 1.0534738302230835
LOSS: 1.0519989728927612
LOSS: 1.0505861043930054
LOSS: 1.0492162704467773
LOSS: 1.0478613376617432
LOSS: 1.0464924573898315
LOSS: 1.045086145401001
LOSS: 1.0436314344406128
LOSS: 1.0421305894851685
LOSS: 1.0406007766723633
LOSS: 1.039068341255188
LOSS: 1.0375641584396362
LOSS: 1.0361140966415405
LOSS: 1.0347352027893066
LOSS: 1.0334333181381226
LOSS: 1.032213807106018
LOSS: 1.0310837030410767
LOSS: 1.0300424098968506
LOSS: 1.0290756225585938
LOSS: 1.028157114982605
LOSS: 1.3733500242233276
LOSS: 1.3670432567596436
LOSS: 1.3611189126968384
LOSS: 1.3556126356124878
LOSS: 1.3505581617355347
LOSS: 1.3459830284118652
LOSS: 1.3419073820114136
LOSS: 1.3383302688598633
LOSS: 1.3352285623550415
LOSS: 1.3325448036193848
LOSS: 1.3301851749420166
LOSS: 1.3280274868011475
LOSS: 1.325943946838379
LOSS: 1.323819637298584
LOSS: 1.3215711116790771
LOSS: 1.319145679473877
LOSS: 1.3165204524993896
LOSS: 1.3136907815933228
LOSS: 1.3106608390808105
LOSS: 1.3074363470077515
LOSS: 1.3040227890014648
LOSS: 1.3004237413406372
LOSS: 1.2966433763504028
LOSS: 1.2926851511001587
LOSS: 1.2885537147521973
LOSS: 1.2842535972595215
LOSS: 1.2797901630401611
LOSS: 1.2751686573028564
LOSS: 1.2703980207443237
LOSS: 1.2654871940612793
LOSS: 1.2604491710662842
LOSS: 1.2552978992462158
LOSS: 1.2500475645065308
LOSS: 1.2447121143341064
LOSS: 1.2393070459365845
LOSS: 1.2338509559631348
LOSS: 1.2283674478530884
LOSS: 1.2228851318359375
LOSS: 1.2174391746520996
LOSS: 1.2120697498321533
LOSS: 1.2068208456039429
LOSS: 1.2017396688461304
LOSS: 1.1968621015548706
LOSS: 1.1922082901000977
LOSS: 1.1877851486206055
LOSS: 1.1835994720458984
LOSS: 1.1796586513519287
LOSS: 1.1759706735610962
LOSS: 1.1725428104400635
LOSS: 1.169371485710144
LOSS: 1.166428565979004
LOSS: 1.1636558771133423
LOSS: 1.161002278327942
LOSS: 1.158431887626648
LOSS: 1.155915379524231
LOSS: 1.153432846069336
LOSS: 1.1509802341461182
LOSS: 1.1485651731491089
LOSS: 1.146201491355896
LOSS: 1.1438921689987183
LOSS: 1.1416229009628296
LOSS: 1.139377474784851
LOSS: 1.1371537446975708
LOSS: 1.1349645853042603
LOSS: 1.1328262090682983
LOSS: 1.1307514905929565
LOSS: 1.128746747970581
LOSS: 1.1268106698989868
LOSS: 1.1249334812164307
LOSS: 1.1230950355529785
LOSS: 1.1212681531906128
LOSS: 1.119428038597107
LOSS: 1.117569088935852
LOSS: 1.1157039403915405
LOSS: 1.113855004310608
LOSS: 1.1120471954345703
LOSS: 1.1103025674819946
LOSS: 1.1086369752883911
LOSS: 1.107060194015503
LOSS: 1.1055749654769897
LOSS: 1.1041721105575562
LOSS: 1.1028244495391846
LOSS: 1.101489782333374
LOSS: 1.1001307964324951
LOSS: 1.0987391471862793
LOSS: 1.0973361730575562
LOSS: 1.09595787525177
LOSS: 1.0946351289749146
LOSS: 1.0933829545974731
LOSS: 1.0921988487243652
LOSS: 1.0910727977752686
LOSS: 1.089992642402649
LOSS: 1.0889426469802856
LOSS: 1.0879038572311401
LOSS: 1.0868622064590454
LOSS: 1.0858170986175537
LOSS: 1.0847805738449097
LOSS: 1.0837713479995728
LOSS: 1.0828051567077637
LOSS: 1.081887125968933
LOSS: 1.378225564956665
LOSS: 1.372001051902771
LOSS: 1.3661649227142334
LOSS: 1.3607536554336548
LOSS: 1.3558027744293213
LOSS: 1.351340889930725
LOSS: 1.347386121749878
LOSS: 1.3439399003982544
LOSS: 1.3409769535064697
LOSS: 1.3384371995925903
LOSS: 1.3362230062484741
LOSS: 1.3342092037200928
LOSS: 1.3322606086730957
LOSS: 1.3302569389343262
LOSS: 1.3281066417694092
LOSS: 1.3257503509521484
LOSS: 1.323154330253601
LOSS: 1.320302128791809
LOSS: 1.3171873092651367
LOSS: 1.3138083219528198
LOSS: 1.3101686239242554
LOSS: 1.3062735795974731
LOSS: 1.3021302223205566
LOSS: 1.297747015953064
LOSS: 1.2931337356567383
LOSS: 1.288301944732666
LOSS: 1.283262848854065
LOSS: 1.278031587600708
LOSS: 1.2726248502731323
LOSS: 1.267061471939087
LOSS: 1.2613615989685059
LOSS: 1.2555441856384277
LOSS: 1.2496273517608643
LOSS: 1.243630051612854
LOSS: 1.237575888633728
LOSS: 1.2314924001693726
LOSS: 1.2254117727279663
LOSS: 1.2193704843521118
LOSS: 1.21340811252594
LOSS: 1.2075660228729248
LOSS: 1.2018873691558838
LOSS: 1.1964105367660522
LOSS: 1.1911722421646118
LOSS: 1.1861999034881592
LOSS: 1.1815149784088135
LOSS: 1.1771273612976074
LOSS: 1.1730345487594604
LOSS: 1.1692209243774414
LOSS: 1.1656620502471924
LOSS: 1.1623250246047974
LOSS: 1.1591777801513672
LOSS: 1.1561864614486694
LOSS: 1.153328537940979
LOSS: 1.1505934000015259
LOSS: 1.1479828357696533
LOSS: 1.145501971244812
LOSS: 1.1431540250778198
LOSS: 1.1409318447113037
LOSS: 1.1388124227523804
LOSS: 1.1367639303207397
LOSS: 1.1347553730010986
LOSS: 1.1327662467956543
LOSS: 1.1307895183563232
LOSS: 1.1288316249847412
LOSS: 1.1269065141677856
LOSS: 1.125027060508728
LOSS: 1.1232022047042847
LOSS: 1.1214337348937988
LOSS: 1.119714617729187
LOSS: 1.118029236793518
LOSS: 1.116358995437622
LOSS: 1.1146918535232544
LOSS: 1.1130250692367554
LOSS: 1.111364722251892
LOSS: 1.1097221374511719
LOSS: 1.108109712600708
LOSS: 1.106541633605957
LOSS: 1.1050320863723755
LOSS: 1.1035906076431274
LOSS: 1.1022123098373413
LOSS: 1.1008824110031128
LOSS: 1.0995852947235107
LOSS: 1.0983103513717651
LOSS: 1.0970494747161865
LOSS: 1.095798134803772
LOSS: 1.0945509672164917
LOSS: 1.093294620513916
LOSS: 1.0920114517211914
LOSS: 1.0906916856765747
LOSS: 1.0893385410308838
LOSS: 1.0879632234573364
LOSS: 1.0865808725357056
LOSS: 1.0852080583572388
LOSS: 1.0838665962219238
LOSS: 1.0825791358947754
LOSS: 1.0813655853271484
LOSS: 1.0802334547042847
LOSS: 1.079171061515808
LOSS: 1.0781314373016357
LOSS: 1.0770827531814575
LOSS: 1.3708332777023315
LOSS: 1.3643684387207031
LOSS: 1.358278512954712
LOSS: 1.352599859237671
LOSS: 1.3473669290542603
LOSS: 1.3426108360290527
LOSS: 1.338353157043457
LOSS: 1.3346002101898193
LOSS: 1.3313368558883667
LOSS: 1.3285174369812012
LOSS: 1.3260600566864014
LOSS: 1.323851466178894
LOSS: 1.3217604160308838
LOSS: 1.3196580410003662
LOSS: 1.3174384832382202
LOSS: 1.315024971961975
LOSS: 1.3123711347579956
LOSS: 1.3094502687454224
LOSS: 1.3062539100646973
LOSS: 1.3027797937393188
LOSS: 1.2990330457687378
LOSS: 1.2950204610824585
LOSS: 1.2907500267028809
LOSS: 1.2862318754196167
LOSS: 1.281476378440857
LOSS: 1.2764939069747925
LOSS: 1.2712972164154053
LOSS: 1.265899658203125
LOSS: 1.2603176832199097
LOSS: 1.254571557044983
LOSS: 1.2486822605133057
LOSS: 1.2426719665527344
LOSS: 1.236562728881836
LOSS: 1.2303773164749146
LOSS: 1.2241408824920654
LOSS: 1.2178809642791748
LOSS: 1.2116265296936035
LOSS: 1.205411434173584
LOSS: 1.1992743015289307
LOSS: 1.193255066871643
LOSS: 1.1873935461044312
LOSS: 1.1817275285720825
LOSS: 1.1762930154800415
LOSS: 1.1711206436157227
LOSS: 1.1662288904190063
LOSS: 1.1616296768188477
LOSS: 1.1573233604431152
LOSS: 1.1533011198043823
LOSS: 1.1495481729507446
LOSS: 1.1460477113723755
LOSS: 1.1427844762802124
LOSS: 1.139747142791748
LOSS: 1.1369308233261108
LOSS: 1.1343270540237427
LOSS: 1.1319074630737305
LOSS: 1.1296190023422241
LOSS: 1.1273964643478394
LOSS: 1.1251939535140991
LOSS: 1.1230026483535767
LOSS: 1.1208406686782837
LOSS: 1.1187361478805542
LOSS: 1.1167069673538208
LOSS: 1.1147887706756592
LOSS: 1.1129975318908691
LOSS: 1.1113399267196655
LOSS: 1.109806776046753
LOSS: 1.1083643436431885
LOSS: 1.1069883108139038
LOSS: 1.1056252717971802
LOSS: 1.1042464971542358
LOSS: 1.1028374433517456
LOSS: 1.1013939380645752
LOSS: 1.1207289695739746
LOSS: 1.1188764572143555
LOSS: 1.1170191764831543
LOSS: 1.1151705980300903
LOSS: 1.1133527755737305
LOSS: 1.1115870475769043
LOSS: 1.1098928451538086
LOSS: 1.1082885265350342
LOSS: 1.1067813634872437
LOSS: 1.1053708791732788
LOSS: 1.104052186012268
LOSS: 1.1028183698654175
LOSS: 1.10164213180542
LOSS: 1.1004799604415894
LOSS: 1.0992945432662964
LOSS: 1.0980702638626099
LOSS: 1.0968116521835327
LOSS: 1.0955383777618408
LOSS: 1.0942742824554443
LOSS: 1.0930298566818237
LOSS: 1.0917965173721313
LOSS: 1.0905659198760986
LOSS: 1.0893391370773315
LOSS: 1.0881197452545166
LOSS: 1.0869113206863403
LOSS: 1.085720181465149
LOSS: 1.0845519304275513
LOSS: 1.0834015607833862
LOSS: 1.3753741979599
LOSS: 1.3690857887268066
LOSS: 1.363183856010437
LOSS: 1.357704758644104
LOSS: 1.3526835441589355
LOSS: 1.3481500148773193
LOSS: 1.3441218137741089
LOSS: 1.3405992984771729
LOSS: 1.3375575542449951
LOSS: 1.3349376916885376
LOSS: 1.332642912864685
LOSS: 1.3305480480194092
LOSS: 1.328519344329834
LOSS: 1.326436161994934
LOSS: 1.324206829071045
LOSS: 1.3217707872390747
LOSS: 1.3190956115722656
LOSS: 1.316165804862976
LOSS: 1.3129760026931763
LOSS: 1.3095237016677856
LOSS: 1.305802822113037
LOSS: 1.3018053770065308
LOSS: 1.2975198030471802
LOSS: 1.2929394245147705
LOSS: 1.2880582809448242
LOSS: 1.2828747034072876
LOSS: 1.2773921489715576
LOSS: 1.271619439125061
LOSS: 1.2655667066574097
LOSS: 1.259246587753296
LOSS: 1.2526710033416748
LOSS: 1.245851993560791
LOSS: 1.2387975454330444
LOSS: 1.2315194606781006
LOSS: 1.224035382270813
LOSS: 1.2163726091384888
LOSS: 1.2085719108581543
LOSS: 1.2006891965866089
LOSS: 1.1927944421768188
LOSS: 1.1849701404571533
LOSS: 1.1773037910461426
LOSS: 1.1698819398880005
LOSS: 1.1627869606018066
LOSS: 1.1560934782028198
LOSS: 1.1498589515686035
LOSS: 1.1441200971603394
LOSS: 1.1388908624649048
LOSS: 1.134159803390503
LOSS: 1.1298936605453491
LOSS: 1.126039743423462
LOSS: 1.122534155845642
LOSS: 1.1193081140518188
LOSS: 1.116299033164978
LOSS: 1.1134576797485352
LOSS: 1.110753059387207
LOSS: 1.1081641912460327
LOSS: 1.1056660413742065
LOSS: 1.1032228469848633
LOSS: 1.1008058786392212
LOSS: 1.0984089374542236
LOSS: 1.0960408449172974
LOSS: 1.0937094688415527
LOSS: 1.0914115905761719
LOSS: 1.0891313552856445
LOSS: 1.086847186088562
LOSS: 1.084538221359253
LOSS: 1.082192301750183
LOSS: 1.0798084735870361
LOSS: 1.0773996114730835
LOSS: 1.0749880075454712
LOSS: 1.072601556777954
LOSS: 1.070268988609314
LOSS: 1.068016529083252
LOSS: 1.0658631324768066
LOSS: 1.063819169998169
LOSS: 1.0618844032287598
LOSS: 1.060050368309021
LOSS: 1.058302640914917
LOSS: 1.0566273927688599
LOSS: 1.0550174713134766
LOSS: 1.0534738302230835
LOSS: 1.0519989728927612
LOSS: 1.0505861043930054
LOSS: 1.0492162704467773
LOSS: 1.0478613376617432
LOSS: 1.0464924573898315
LOSS: 1.045086145401001
LOSS: 1.0436314344406128
LOSS: 1.0421305894851685
LOSS: 1.0406007766723633
LOSS: 1.039068341255188
LOSS: 1.0375641584396362
LOSS: 1.0361140966415405
LOSS: 1.0347352027893066
LOSS: 1.0334333181381226
LOSS: 1.032213807106018
LOSS: 1.0310837030410767
LOSS: 1.0300424098968506
LOSS: 1.0290756225585938
LOSS: 1.028157114982605
LOSS: 1.3733500242233276
LOSS: 1.3670432567596436
LOSS: 1.3611189126968384
LOSS: 1.3556126356124878
LOSS: 1.3505581617355347
LOSS: 1.3459830284118652
LOSS: 1.3419073820114136
LOSS: 1.3383302688598633
LOSS: 1.3352285623550415
LOSS: 1.3325448036193848
LOSS: 1.3301851749420166
LOSS: 1.3280274868011475
LOSS: 1.325943946838379
LOSS: 1.323819637298584
LOSS: 1.3215711116790771
LOSS: 1.319145679473877
LOSS: 1.3165204524993896
LOSS: 1.3136907815933228
LOSS: 1.3106608390808105
LOSS: 1.3074363470077515
LOSS: 1.3040227890014648
LOSS: 1.3004237413406372
LOSS: 1.2966433763504028
LOSS: 1.2926851511001587
LOSS: 1.2885537147521973
LOSS: 1.2842535972595215
LOSS: 1.2797901630401611
LOSS: 1.2751686573028564
LOSS: 1.2703980207443237
LOSS: 1.2654871940612793
LOSS: 1.2604491710662842
LOSS: 1.2552978992462158
LOSS: 1.2500475645065308
LOSS: 1.2447121143341064
LOSS: 1.2393070459365845
LOSS: 1.2338509559631348
LOSS: 1.2283674478530884
LOSS: 1.2228851318359375
LOSS: 1.2174391746520996
LOSS: 1.2120697498321533
LOSS: 1.2068208456039429
LOSS: 1.2017396688461304
LOSS: 1.1968621015548706
LOSS: 1.1922082901000977
LOSS: 1.1877851486206055
LOSS: 1.1835994720458984
LOSS: 1.1796586513519287
LOSS: 1.1759706735610962
LOSS: 1.1725428104400635
LOSS: 1.169371485710144
LOSS: 1.166428565979004
LOSS: 1.1636558771133423
LOSS: 1.161002278327942
LOSS: 1.158431887626648
LOSS: 1.155915379524231
LOSS: 1.153432846069336
LOSS: 1.1509802341461182
LOSS: 1.1485651731491089
LOSS: 1.146201491355896
LOSS: 1.1438921689987183
LOSS: 1.1416229009628296
LOSS: 1.139377474784851
LOSS: 1.1371537446975708
LOSS: 1.1349645853042603
LOSS: 1.1328262090682983
LOSS: 1.1307514905929565
LOSS: 1.128746747970581
LOSS: 1.1268106698989868
LOSS: 1.1249334812164307
LOSS: 1.1230950355529785
LOSS: 1.1212681531906128
LOSS: 1.119428038597107
LOSS: 1.117569088935852
LOSS: 1.1157039403915405
LOSS: 1.113855004310608
LOSS: 1.1120471954345703
LOSS: 1.1103025674819946
LOSS: 1.1086369752883911
LOSS: 1.107060194015503
LOSS: 1.1055749654769897
LOSS: 1.1041721105575562
LOSS: 1.1028244495391846
LOSS: 1.101489782333374
LOSS: 1.1001307964324951
LOSS: 1.0987391471862793
LOSS: 1.0973361730575562
LOSS: 1.09595787525177
LOSS: 1.0946351289749146
LOSS: 1.0933829545974731
LOSS: 1.0921988487243652
LOSS: 1.0910727977752686
LOSS: 1.089992642402649
LOSS: 1.0889426469802856
LOSS: 1.0879038572311401
LOSS: 1.0868622064590454
LOSS: 1.0858170986175537
LOSS: 1.0847805738449097
LOSS: 1.0837713479995728
LOSS: 1.0828051567077637
LOSS: 1.081887125968933
LOSS: 1.3799479007720947
LOSS: 1.3737581968307495
LOSS: 1.3679609298706055
LOSS: 1.362593173980713
LOSS: 1.3576921224594116
LOSS: 1.3532872200012207
LOSS: 1.3493983745574951
LOSS: 1.3460288047790527
LOSS: 1.3431564569473267
LOSS: 1.3407249450683594
LOSS: 1.3386422395706177
LOSS: 1.3367856740951538
LOSS: 1.3350236415863037
LOSS: 1.3332363367080688
LOSS: 1.3313311338424683
LOSS: 1.3292491436004639
LOSS: 1.3269585371017456
LOSS: 1.324447751045227
LOSS: 1.3217154741287231
LOSS: 1.3187675476074219
LOSS: 1.3156083822250366
LOSS: 1.3122409582138062
LOSS: 1.308664321899414
LOSS: 1.3048748970031738
LOSS: 1.3008654117584229
LOSS: 1.2966281175613403
LOSS: 1.2921539545059204
LOSS: 1.2874351739883423
LOSS: 1.282469391822815
LOSS: 1.277255654335022
LOSS: 1.271798849105835
LOSS: 1.266109824180603
LOSS: 1.2602009773254395
LOSS: 1.2540905475616455
LOSS: 1.2477988004684448
LOSS: 1.2413513660430908
LOSS: 1.2347806692123413
LOSS: 1.2281246185302734
LOSS: 1.2214292287826538
LOSS: 1.2147457599639893
LOSS: 1.208128809928894
LOSS: 1.2016299962997437
LOSS: 1.1952909231185913
LOSS: 1.189150094985962
LOSS: 1.183242678642273
LOSS: 1.177596926689148
LOSS: 1.1722313165664673
LOSS: 1.1671556234359741
LOSS: 1.1623713970184326
LOSS: 1.1578739881515503
LOSS: 1.1536577939987183
LOSS: 1.1497130393981934
LOSS: 1.1460185050964355
LOSS: 1.1425271034240723
LOSS: 1.1391860246658325
LOSS: 1.135959506034851
LOSS: 1.1328253746032715
LOSS: 1.1297718286514282
LOSS: 1.1267929077148438
LOSS: 1.1238845586776733
LOSS: 1.1210367679595947
LOSS: 1.1182321310043335
LOSS: 1.115447759628296
LOSS: 1.1126669645309448
LOSS: 1.1098891496658325
LOSS: 1.1071302890777588
LOSS: 1.1044222116470337
LOSS: 1.1018074750900269
LOSS: 1.0993329286575317
LOSS: 1.0970327854156494
LOSS: 1.094915509223938
LOSS: 1.0929527282714844
LOSS: 1.0910969972610474
LOSS: 1.0892952680587769
LOSS: 1.0875087976455688
LOSS: 1.0857101678848267
LOSS: 1.083886981010437
LOSS: 1.0820374488830566
LOSS: 1.0801693201065063
LOSS: 1.0782967805862427
LOSS: 1.0764397382736206
LOSS: 1.0746171474456787
LOSS: 1.072845697402954
LOSS: 1.071134090423584
LOSS: 1.0694856643676758
LOSS: 1.067897915840149
LOSS: 1.0663647651672363
LOSS: 1.0648831129074097
LOSS: 1.0634597539901733
LOSS: 1.062111735343933
LOSS: 1.060858130455017
LOSS: 1.0597114562988281
LOSS: 1.0586622953414917
LOSS: 1.057671308517456
LOSS: 1.056681513786316
LOSS: 1.055647373199463
LOSS: 1.0545589923858643
LOSS: 1.0534310340881348
LOSS: 1.0522927045822144
LOSS: 1.051173210144043
LOSS: 1.3708332777023315
LOSS: 1.3643684387207031
LOSS: 1.358278512954712
LOSS: 1.1207289695739746
LOSS: 1.1188764572143555
LOSS: 1.1170191764831543
LOSS: 1.1151705980300903
LOSS: 1.1133527755737305
LOSS: 1.1115870475769043
LOSS: 1.1098928451538086
LOSS: 1.1082885265350342
LOSS: 1.1067813634872437
LOSS: 1.1053708791732788
LOSS: 1.104052186012268
LOSS: 1.1028183698654175
LOSS: 1.10164213180542
LOSS: 1.1004799604415894
LOSS: 1.0992945432662964
LOSS: 1.0980702638626099
LOSS: 1.0968116521835327
LOSS: 1.0955383777618408
LOSS: 1.0942742824554443
LOSS: 1.0930298566818237
LOSS: 1.0917965173721313
LOSS: 1.0905659198760986
LOSS: 1.0893391370773315
LOSS: 1.0881197452545166
LOSS: 1.0869113206863403
LOSS: 1.085720181465149
LOSS: 1.0845519304275513
LOSS: 1.0834015607833862
LOSS: 1.3691940307617188
LOSS: 1.3626631498336792
LOSS: 1.3565044403076172
LOSS: 1.350752353668213
LOSS: 1.3454428911209106
LOSS: 1.34060537815094
LOSS: 1.3362624645233154
LOSS: 1.332421898841858
LOSS: 1.3290698528289795
LOSS: 1.3261607885360718
LOSS: 1.3236174583435059
LOSS: 1.3213322162628174
LOSS: 1.319179892539978
LOSS: 1.3170417547225952
LOSS: 1.3148199319839478
LOSS: 1.3124504089355469
LOSS: 1.3098987340927124
LOSS: 1.3071540594100952
LOSS: 1.3042192459106445
LOSS: 1.3011066913604736
LOSS: 1.2978324890136719
LOSS: 1.294416069984436
LOSS: 1.2908778190612793
LOSS: 1.287237286567688
LOSS: 1.2835127115249634
LOSS: 1.2797167301177979
LOSS: 1.2758604288101196
LOSS: 1.2719488143920898
LOSS: 1.267982840538025
LOSS: 1.2639586925506592
LOSS: 1.25986647605896
LOSS: 1.2556947469711304
LOSS: 1.2514296770095825
LOSS: 1.2470561265945435
LOSS: 1.242564082145691
LOSS: 1.237947702407837
LOSS: 1.2332109212875366
LOSS: 1.2283631563186646
LOSS: 1.2234218120574951
LOSS: 1.2184098958969116
LOSS: 1.2133558988571167
LOSS: 1.208292007446289
LOSS: 1.2032487392425537
LOSS: 1.1982519626617432
LOSS: 1.193328857421875
LOSS: 1.1885106563568115
LOSS: 1.1838289499282837
LOSS: 1.1793107986450195
LOSS: 1.1749768257141113
LOSS: 1.1708407402038574
LOSS: 1.166909098625183
LOSS: 1.1631646156311035
LOSS: 1.1595561504364014
LOSS: 1.1560307741165161
LOSS: 1.1525505781173706
LOSS: 1.1490918397903442
LOSS: 1.145646333694458
LOSS: 1.1422224044799805
LOSS: 1.1388455629348755
LOSS: 1.1355522871017456
LOSS: 1.1323728561401367
LOSS: 1.1292990446090698
LOSS: 1.126279592514038
LOSS: 1.1232625246047974
LOSS: 1.1202293634414673
LOSS: 1.117192268371582
LOSS: 1.1141836643218994
LOSS: 1.1112432479858398
LOSS: 1.108411192893982
LOSS: 1.105716586112976
LOSS: 1.1031765937805176
LOSS: 1.1007909774780273
LOSS: 1.0985404253005981
LOSS: 1.0963964462280273
LOSS: 1.0943297147750854
LOSS: 1.0923153162002563
LOSS: 1.090331792831421
LOSS: 1.088361382484436
LOSS: 1.0863970518112183
LOSS: 1.0844485759735107
LOSS: 1.0825377702713013
LOSS: 1.0806888341903687
LOSS: 1.0789210796356201
LOSS: 1.077244520187378
LOSS: 1.0756632089614868
LOSS: 1.0741782188415527
LOSS: 1.0727897882461548
LOSS: 1.071494698524475
LOSS: 1.0702813863754272
LOSS: 1.069136619567871
LOSS: 1.0680521726608276
LOSS: 1.067026972770691
LOSS: 1.0660598278045654
LOSS: 1.0651379823684692
LOSS: 1.064238429069519
LOSS: 1.0633426904678345
LOSS: 1.0624455213546753
LOSS: 1.0615512132644653
LOSS: 1.060668706893921
LOSS: 1.0598053932189941
LOSS: 1.3837237358093262
LOSS: 1.3777769804000854
LOSS: 1.3722330331802368
LOSS: 1.3671293258666992
LOSS: 1.3625001907348633
LOSS: 1.3583719730377197
LOSS: 1.3547585010528564
LOSS: 1.3516526222229004
LOSS: 1.3490113019943237
LOSS: 1.346754789352417
LOSS: 1.344765305519104
LOSS: 1.342905044555664
LOSS: 1.3410425186157227
LOSS: 1.3390734195709229
LOSS: 1.3369290828704834
LOSS: 1.3345718383789062
LOSS: 1.331984281539917
LOSS: 1.3291646242141724
LOSS: 1.326112151145935
LOSS: 1.3228309154510498
LOSS: 1.3193234205245972
LOSS: 1.3155919313430786
LOSS: 1.311637043952942
LOSS: 1.3074604272842407
LOSS: 1.303061842918396
LOSS: 1.2984397411346436
LOSS: 1.2935973405838013
LOSS: 1.2885386943817139
LOSS: 1.2832720279693604
LOSS: 1.2778089046478271
LOSS: 1.2721645832061768
LOSS: 1.266357183456421
LOSS: 1.2604081630706787
LOSS: 1.2543450593948364
LOSS: 1.248199701309204
LOSS: 1.2420105934143066
LOSS: 1.2358171939849854
LOSS: 1.2296593189239502
LOSS: 1.2235811948776245
LOSS: 1.2176270484924316
LOSS: 1.2118383646011353
LOSS: 1.2062522172927856
LOSS: 1.2009007930755615
LOSS: 1.1958088874816895
LOSS: 1.190983533859253
LOSS: 1.1864248514175415
LOSS: 1.1821343898773193
LOSS: 1.1781156063079834
LOSS: 1.1743724346160889
LOSS: 1.1709076166152954
LOSS: 1.1677217483520508
LOSS: 1.1648098230361938
LOSS: 1.1621596813201904
LOSS: 1.1597574949264526
LOSS: 1.157583475112915
LOSS: 1.155604362487793
LOSS: 1.1537714004516602
LOSS: 1.152025818824768
LOSS: 1.1503117084503174
LOSS: 1.1485978364944458
LOSS: 1.146877408027649
LOSS: 1.1451547145843506
LOSS: 1.1434370279312134
LOSS: 1.1417317390441895
LOSS: 1.1400396823883057
LOSS: 1.1383588314056396
LOSS: 1.1366863250732422
LOSS: 1.1350194215774536
LOSS: 1.1333584785461426
LOSS: 1.131706953048706
LOSS: 1.1300733089447021
LOSS: 1.1284679174423218
LOSS: 1.1268985271453857
LOSS: 1.1253710985183716
LOSS: 1.123888373374939
LOSS: 1.1224489212036133
LOSS: 1.1210497617721558
LOSS: 1.1196882724761963
LOSS: 1.118364930152893
LOSS: 1.1170830726623535
LOSS: 1.1158486604690552
LOSS: 1.1146631240844727
LOSS: 1.1135238409042358
LOSS: 1.1124213933944702
LOSS: 1.1113450527191162
LOSS: 1.1102839708328247
LOSS: 1.1092338562011719
LOSS: 1.1081972122192383
LOSS: 1.1071841716766357
LOSS: 1.1062065362930298
LOSS: 1.1052724123001099
LOSS: 1.1043787002563477
LOSS: 1.1035109758377075
LOSS: 1.1026493310928345
LOSS: 1.10178542137146
LOSS: 1.1009204387664795
LOSS: 1.1000609397888184
LOSS: 1.0992138385772705
LOSS: 1.0983808040618896
LOSS: 1.0975635051727295
LOSS: 1.3691940307617188
LOSS: 1.3626631498336792
LOSS: 1.3565044403076172
LOSS: 1.350752353668213
LOSS: 1.3454428911209106
LOSS: 1.34060537815094
LOSS: 1.3362624645233154
LOSS: 1.332421898841858
LOSS: 1.3290698528289795
LOSS: 1.3261607885360718
LOSS: 1.3236174583435059
LOSS: 1.3213322162628174
LOSS: 1.319179892539978
LOSS: 1.3170417547225952
LOSS: 1.3148199319839478
LOSS: 1.3124504089355469
LOSS: 1.3098987340927124
LOSS: 1.3071540594100952
LOSS: 1.3042192459106445
LOSS: 1.3011066913604736
LOSS: 1.2978324890136719
LOSS: 1.294416069984436
LOSS: 1.2908778190612793
LOSS: 1.287237286567688
LOSS: 1.2835127115249634
LOSS: 1.2797167301177979
LOSS: 1.2758604288101196
LOSS: 1.2719488143920898
LOSS: 1.267982840538025
LOSS: 1.2639586925506592
LOSS: 1.25986647605896
LOSS: 1.2556947469711304
LOSS: 1.2514296770095825
LOSS: 1.2470561265945435
LOSS: 1.242564082145691
LOSS: 1.237947702407837
LOSS: 1.2332109212875366
LOSS: 1.2283631563186646
LOSS: 1.2234218120574951
LOSS: 1.2184098958969116
LOSS: 1.2133558988571167
LOSS: 1.208292007446289
LOSS: 1.2032487392425537
LOSS: 1.1982519626617432
LOSS: 1.193328857421875
LOSS: 1.1885106563568115
LOSS: 1.1838289499282837
LOSS: 1.1793107986450195
LOSS: 1.1749768257141113
LOSS: 1.1708407402038574
LOSS: 1.166909098625183
LOSS: 1.1631646156311035
LOSS: 1.1595561504364014
LOSS: 1.1560307741165161
LOSS: 1.1525505781173706
LOSS: 1.1490918397903442
LOSS: 1.145646333694458
LOSS: 1.1422224044799805
LOSS: 1.1388455629348755
LOSS: 1.1355522871017456
LOSS: 1.1323728561401367
LOSS: 1.1292990446090698
LOSS: 1.126279592514038
LOSS: 1.1232625246047974
LOSS: 1.1202293634414673
LOSS: 1.117192268371582
LOSS: 1.1141836643218994
LOSS: 1.1112432479858398
LOSS: 1.108411192893982
LOSS: 1.105716586112976
LOSS: 1.1031765937805176
LOSS: 1.1007909774780273
LOSS: 1.0985404253005981
LOSS: 1.0963964462280273
LOSS: 1.0943297147750854
LOSS: 1.0923153162002563
LOSS: 1.090331792831421
LOSS: 1.088361382484436
LOSS: 1.0863970518112183
LOSS: 1.0844485759735107
LOSS: 1.0825377702713013
LOSS: 1.0806888341903687
LOSS: 1.0789210796356201
LOSS: 1.077244520187378
LOSS: 1.0756632089614868
LOSS: 1.0741782188415527
LOSS: 1.0727897882461548
LOSS: 1.071494698524475
LOSS: 1.0702813863754272
LOSS: 1.069136619567871
LOSS: 1.0680521726608276
LOSS: 1.067026972770691
LOSS: 1.0660598278045654
LOSS: 1.0651379823684692
LOSS: 1.064238429069519
LOSS: 1.0633426904678345
LOSS: 1.0624455213546753
LOSS: 1.0615512132644653
LOSS: 1.060668706893921
LOSS: 1.0598053932189941
LOSS: 1.3837237358093262
LOSS: 1.3777769804000854
LOSS: 1.3524247407913208
LOSS: 1.347235918045044
LOSS: 1.3425272703170776
LOSS: 1.3383198976516724
LOSS: 1.3346220254898071
LOSS: 1.3314146995544434
LOSS: 1.3286519050598145
LOSS: 1.3262505531311035
LOSS: 1.3241002559661865
LOSS: 1.3220707178115845
LOSS: 1.320046067237854
LOSS: 1.3179296255111694
LOSS: 1.3156594038009644
LOSS: 1.3132017850875854
LOSS: 1.3105430603027344
LOSS: 1.3076847791671753
LOSS: 1.3046314716339111
LOSS: 1.3013912439346313
LOSS: 1.2979695796966553
LOSS: 1.2943724393844604
LOSS: 1.2906056642532349
LOSS: 1.2866688966751099
LOSS: 1.2825626134872437
LOSS: 1.278283953666687
LOSS: 1.2738299369812012
LOSS: 1.2691982984542847
LOSS: 1.2643914222717285
LOSS: 1.2594151496887207
LOSS: 1.2542815208435059
LOSS: 1.2490071058273315
LOSS: 1.2436140775680542
LOSS: 1.2381304502487183
LOSS: 1.2325884103775024
LOSS: 1.2270265817642212
LOSS: 1.2214899063110352
LOSS: 1.216025710105896
LOSS: 1.2106812000274658
LOSS: 1.2054963111877441
LOSS: 1.200508952140808
LOSS: 1.1957557201385498
LOSS: 1.1912659406661987
LOSS: 1.187058925628662
LOSS: 1.183140516281128
LOSS: 1.1795052289962769
LOSS: 1.1761345863342285
LOSS: 1.1730024814605713
LOSS: 1.1700797080993652
LOSS: 1.167338490486145
LOSS: 1.164757251739502
LOSS: 1.1623175144195557
LOSS: 1.1600068807601929
LOSS: 1.157822847366333
LOSS: 1.1557682752609253
LOSS: 1.1538455486297607
LOSS: 1.1520501375198364
LOSS: 1.1503586769104004
LOSS: 1.1487339735031128
LOSS: 1.1471494436264038
LOSS: 1.145592451095581
LOSS: 1.1440644264221191
LOSS: 1.1425557136535645
LOSS: 1.1410636901855469
LOSS: 1.1395819187164307
LOSS: 1.138112187385559
LOSS: 1.1366583108901978
LOSS: 1.1352251768112183
LOSS: 1.1338212490081787
LOSS: 1.1324412822723389
LOSS: 1.1310842037200928
LOSS: 1.1297420263290405
LOSS: 1.1284276247024536
LOSS: 1.127129077911377
LOSS: 1.1258538961410522
LOSS: 1.124610185623169
LOSS: 1.1233879327774048
LOSS: 1.122207760810852
LOSS: 1.1210825443267822
LOSS: 1.1199885606765747
LOSS: 1.1189494132995605
LOSS: 1.1179753541946411
LOSS: 1.1170482635498047
LOSS: 1.1161901950836182
LOSS: 1.1153532266616821
LOSS: 1.1145434379577637
LOSS: 1.1137739419937134
LOSS: 1.113013744354248
LOSS: 1.1122790575027466
LOSS: 1.1116247177124023
LOSS: 1.1109082698822021
LOSS: 1.1102756261825562
LOSS: 1.1096279621124268
LOSS: 1.109009861946106
LOSS: 1.1083685159683228
LOSS: 1.1077520847320557
LOSS: 1.1072081327438354
LOSS: 1.1065620183944702
LOSS: 1.1059578657150269
LOSS: 1.3705490827560425
LOSS: 1.364116907119751
LOSS: 1.3580635786056519
LOSS: 1.3524247407913208
LOSS: 1.347235918045044
LOSS: 1.3425272703170776
LOSS: 1.3383198976516724
LOSS: 1.3346220254898071
LOSS: 1.3314146995544434
LOSS: 1.3286519050598145
LOSS: 1.3262505531311035
LOSS: 1.3241002559661865
LOSS: 1.3220707178115845
LOSS: 1.320046067237854
LOSS: 1.3179296255111694
LOSS: 1.3156594038009644
LOSS: 1.3132017850875854
LOSS: 1.3105430603027344
LOSS: 1.3076847791671753
LOSS: 1.3046314716339111
LOSS: 1.3013912439346313
LOSS: 1.2979695796966553
LOSS: 1.2943724393844604
LOSS: 1.2906056642532349
LOSS: 1.2866688966751099
LOSS: 1.2825626134872437
LOSS: 1.278283953666687
LOSS: 1.2738299369812012
LOSS: 1.2691982984542847
LOSS: 1.2643914222717285
LOSS: 1.2594151496887207
LOSS: 1.2542815208435059
LOSS: 1.2490071058273315
LOSS: 1.2436140775680542
LOSS: 1.2381304502487183
LOSS: 1.2325884103775024
LOSS: 1.2270265817642212
LOSS: 1.2214899063110352
LOSS: 1.216025710105896
LOSS: 1.2106812000274658
LOSS: 1.2054963111877441
LOSS: 1.200508952140808
LOSS: 1.1957557201385498
LOSS: 1.1912659406661987
LOSS: 1.187058925628662
LOSS: 1.183140516281128
LOSS: 1.1795052289962769
LOSS: 1.1761345863342285
LOSS: 1.1730024814605713
LOSS: 1.1700797080993652
LOSS: 1.167338490486145
LOSS: 1.164757251739502
LOSS: 1.1623175144195557
LOSS: 1.1600068807601929
LOSS: 1.157822847366333
LOSS: 1.1557682752609253
LOSS: 1.1538455486297607
LOSS: 1.1520501375198364
LOSS: 1.1503586769104004
LOSS: 1.1487339735031128
LOSS: 1.1471494436264038
LOSS: 1.145592451095581
LOSS: 1.1440644264221191
LOSS: 1.1425557136535645
LOSS: 1.1410636901855469
LOSS: 1.1395819187164307
LOSS: 1.138112187385559
LOSS: 1.1366583108901978
LOSS: 1.1352251768112183
LOSS: 1.1338212490081787
LOSS: 1.1324412822723389
LOSS: 1.1310842037200928
LOSS: 1.1297420263290405
LOSS: 1.1284276247024536
LOSS: 1.127129077911377
LOSS: 1.1258538961410522
LOSS: 1.124610185623169
LOSS: 1.1233879327774048
LOSS: 1.122207760810852
LOSS: 1.1210825443267822
LOSS: 1.1199885606765747
LOSS: 1.1189494132995605
LOSS: 1.1179753541946411
LOSS: 1.1170482635498047
LOSS: 1.1161901950836182
LOSS: 1.1153532266616821
LOSS: 1.1145434379577637
LOSS: 1.1137739419937134
LOSS: 1.113013744354248
LOSS: 1.1122790575027466
LOSS: 1.1116247177124023
LOSS: 1.1109082698822021
LOSS: 1.1102756261825562
LOSS: 1.1096279621124268
LOSS: 1.109009861946106
LOSS: 1.1083685159683228
LOSS: 1.1077520847320557
LOSS: 1.1072081327438354
LOSS: 1.1065620183944702
LOSS: 1.1059578657150269
LOSS: 1.372121810913086
LOSS: 1.3656820058822632
LOSS: 1.359618902206421
LOSS: 1.353968858718872
LOSS: 1.3487646579742432
LOSS: 1.34403657913208
LOSS: 1.3398054838180542
LOSS: 1.3360776901245117
LOSS: 1.332837700843811
LOSS: 1.3300392627716064
LOSS: 1.327601671218872
LOSS: 1.325412392616272
LOSS: 1.3233437538146973
LOSS: 1.3212718963623047
LOSS: 1.3190957307815552
LOSS: 1.3167469501495361
LOSS: 1.3141855001449585
LOSS: 1.3113960027694702
LOSS: 1.308376669883728
LOSS: 1.3051362037658691
LOSS: 1.3016842603683472
LOSS: 1.2980320453643799
LOSS: 1.294188141822815
LOSS: 1.2901620864868164
LOSS: 1.285963773727417
LOSS: 1.2816020250320435
LOSS: 1.2770864963531494
LOSS: 1.2724242210388184
LOSS: 1.2676177024841309
LOSS: 1.2626663446426392
LOSS: 1.2575647830963135
LOSS: 1.2523043155670166
LOSS: 1.2468786239624023
LOSS: 1.241284966468811
LOSS: 1.2355279922485352
LOSS: 1.229622483253479
LOSS: 1.223595380783081
LOSS: 1.217485785484314
LOSS: 1.2113434076309204
LOSS: 1.20522141456604
LOSS: 1.1991755962371826
LOSS: 1.1932653188705444
LOSS: 1.1875553131103516
LOSS: 1.1821117401123047
LOSS: 1.1769943237304688
LOSS: 1.1722506284713745
LOSS: 1.1679050922393799
LOSS: 1.163958191871643
LOSS: 1.1603851318359375
LOSS: 1.1571446657180786
LOSS: 1.1541889905929565
LOSS: 1.1514726877212524
LOSS: 1.1489545106887817
LOSS: 1.1466028690338135
LOSS: 1.1443843841552734
LOSS: 1.1422655582427979
LOSS: 1.1402238607406616
LOSS: 1.138245701789856
LOSS: 1.1363410949707031
LOSS: 1.1345233917236328
LOSS: 1.1328070163726807
LOSS: 1.131203055381775
LOSS: 1.1297107934951782
LOSS: 1.128322720527649
LOSS: 1.1270222663879395
LOSS: 1.125784993171692
LOSS: 1.124587893486023
LOSS: 1.1234081983566284
LOSS: 1.1222376823425293
LOSS: 1.121071457862854
LOSS: 1.1199164390563965
LOSS: 1.1187795400619507
LOSS: 1.1176650524139404
LOSS: 1.116579532623291
LOSS: 1.1155234575271606
LOSS: 1.1144970655441284
LOSS: 1.1134895086288452
LOSS: 1.1125154495239258
LOSS: 1.1115479469299316
LOSS: 1.1106001138687134
LOSS: 1.1096630096435547
LOSS: 1.108730435371399
LOSS: 1.1078044176101685
LOSS: 1.106886863708496
LOSS: 1.1059858798980713
LOSS: 1.1051008701324463
LOSS: 1.1042389869689941
LOSS: 1.103388786315918
LOSS: 1.1025574207305908
LOSS: 1.101763129234314
LOSS: 1.1010128259658813
LOSS: 1.1003096103668213
LOSS: 1.099661111831665
LOSS: 1.0990509986877441
LOSS: 1.0984430313110352
LOSS: 1.0978230237960815
LOSS: 1.097178339958191
LOSS: 1.0965312719345093
LOSS: 1.095865249633789
LOSS: 1.0952032804489136
LOSS: 1.372121810913086
LOSS: 1.3656820058822632
LOSS: 1.359618902206421
LOSS: 1.353968858718872
LOSS: 1.3487646579742432
LOSS: 1.34403657913208
LOSS: 1.3398054838180542
LOSS: 1.3360776901245117
LOSS: 1.332837700843811
LOSS: 1.3300392627716064
LOSS: 1.327601671218872
LOSS: 1.325412392616272
LOSS: 1.3233437538146973
LOSS: 1.3212718963623047
LOSS: 1.3190957307815552
LOSS: 1.3167469501495361
LOSS: 1.3141855001449585
LOSS: 1.3113960027694702
LOSS: 1.308376669883728
LOSS: 1.3051362037658691
LOSS: 1.3016842603683472
LOSS: 1.2980320453643799
LOSS: 1.294188141822815
LOSS: 1.2901620864868164
LOSS: 1.285963773727417
LOSS: 1.2816020250320435
LOSS: 1.2770864963531494
LOSS: 1.2724242210388184
LOSS: 1.2676177024841309
LOSS: 1.2626663446426392
LOSS: 1.2575647830963135
LOSS: 1.2523043155670166
LOSS: 1.2468786239624023
LOSS: 1.3722330331802368
LOSS: 1.3671293258666992
LOSS: 1.3625001907348633
LOSS: 1.3583719730377197
LOSS: 1.3547585010528564
LOSS: 1.3516526222229004
LOSS: 1.3490113019943237
LOSS: 1.346754789352417
LOSS: 1.344765305519104
LOSS: 1.342905044555664
LOSS: 1.3410425186157227
LOSS: 1.3390734195709229
LOSS: 1.3369290828704834
LOSS: 1.3345718383789062
LOSS: 1.331984281539917
LOSS: 1.3291646242141724
LOSS: 1.326112151145935
LOSS: 1.3228309154510498
LOSS: 1.3193234205245972
LOSS: 1.3155919313430786
LOSS: 1.311637043952942
LOSS: 1.3074604272842407
LOSS: 1.303061842918396
LOSS: 1.2984397411346436
LOSS: 1.2935973405838013
LOSS: 1.2885386943817139
LOSS: 1.2832720279693604
LOSS: 1.2778089046478271
LOSS: 1.2721645832061768
LOSS: 1.266357183456421
LOSS: 1.2604081630706787
LOSS: 1.2543450593948364
LOSS: 1.248199701309204
LOSS: 1.2420105934143066
LOSS: 1.2358171939849854
LOSS: 1.2296593189239502
LOSS: 1.2235811948776245
LOSS: 1.2176270484924316
LOSS: 1.2118383646011353
LOSS: 1.2062522172927856
LOSS: 1.2009007930755615
LOSS: 1.1958088874816895
LOSS: 1.190983533859253
LOSS: 1.1864248514175415
LOSS: 1.1821343898773193
LOSS: 1.1781156063079834
LOSS: 1.1743724346160889
LOSS: 1.1709076166152954
LOSS: 1.1677217483520508
LOSS: 1.1648098230361938
LOSS: 1.1621596813201904
LOSS: 1.1597574949264526
LOSS: 1.157583475112915
LOSS: 1.155604362487793
LOSS: 1.1537714004516602
LOSS: 1.152025818824768
LOSS: 1.1503117084503174
LOSS: 1.1485978364944458
LOSS: 1.146877408027649
LOSS: 1.1451547145843506
LOSS: 1.1434370279312134
LOSS: 1.1417317390441895
LOSS: 1.1400396823883057
LOSS: 1.1383588314056396
LOSS: 1.1366863250732422
LOSS: 1.1350194215774536
LOSS: 1.1333584785461426
LOSS: 1.131706953048706
LOSS: 1.1300733089447021
LOSS: 1.1284679174423218
LOSS: 1.1268985271453857
LOSS: 1.1253710985183716
LOSS: 1.123888373374939
LOSS: 1.1224489212036133
LOSS: 1.1210497617721558
LOSS: 1.1196882724761963
LOSS: 1.118364930152893
LOSS: 1.1170830726623535
LOSS: 1.1158486604690552
LOSS: 1.1146631240844727
LOSS: 1.1135238409042358
LOSS: 1.1124213933944702
LOSS: 1.1113450527191162
LOSS: 1.1102839708328247
LOSS: 1.1092338562011719
LOSS: 1.1081972122192383
LOSS: 1.1071841716766357
LOSS: 1.1062065362930298
LOSS: 1.1052724123001099
LOSS: 1.1043787002563477
LOSS: 1.1035109758377075
LOSS: 1.1026493310928345
LOSS: 1.10178542137146
LOSS: 1.1009204387664795
LOSS: 1.1000609397888184
LOSS: 1.0992138385772705
LOSS: 1.0983808040618896
LOSS: 1.0975635051727295
LOSS: 1.3796838521957397
LOSS: 1.3735734224319458
LOSS: 1.3678613901138306
LOSS: 1.362586498260498
LOSS: 1.3577842712402344
LOSS: 1.353487491607666
LOSS: 1.3497130870819092
LOSS: 1.346466302871704
LOSS: 1.3437206745147705
LOSS: 1.3414160013198853
LOSS: 1.3394578695297241
LOSS: 1.3377197980880737
LOSS: 1.336074948310852
LOSS: 1.3344072103500366
LOSS: 1.3326343297958374
LOSS: 1.3307057619094849
LOSS: 1.328599452972412
LOSS: 1.3263112306594849
LOSS: 1.3238489627838135
LOSS: 1.3212250471115112
LOSS: 1.3184536695480347
LOSS: 1.3155473470687866
LOSS: 1.3125146627426147
LOSS: 1.3093582391738892
LOSS: 1.306075930595398
LOSS: 1.3026621341705322
LOSS: 1.2991055250167847
LOSS: 1.2953938245773315
LOSS: 1.2915128469467163
LOSS: 1.2874475717544556
LOSS: 1.2831871509552002
LOSS: 1.2787244319915771
LOSS: 1.2740579843521118
LOSS: 1.269194483757019
LOSS: 1.2641470432281494
LOSS: 1.2589367628097534
LOSS: 1.2535954713821411
LOSS: 1.248165488243103
LOSS: 1.2426958084106445
LOSS: 1.2372409105300903
LOSS: 1.2318552732467651
LOSS: 1.2265926599502563
LOSS: 1.2215006351470947
LOSS: 1.2166194915771484
LOSS: 1.211978554725647
LOSS: 1.2075903415679932
LOSS: 1.203453540802002
LOSS: 1.199556589126587
LOSS: 1.1958820819854736
LOSS: 1.192407488822937
LOSS: 1.189110279083252
LOSS: 1.185976505279541
LOSS: 1.183000922203064
LOSS: 1.1801786422729492
LOSS: 1.1774914264678955
LOSS: 1.1749072074890137
LOSS: 1.1723897457122803
LOSS: 1.1699111461639404
LOSS: 1.1674529314041138
LOSS: 1.1649998426437378
LOSS: 1.1625453233718872
LOSS: 1.1600948572158813
LOSS: 1.1576647758483887
LOSS: 1.1552772521972656
LOSS: 1.1529574394226074
LOSS: 1.1507233381271362
LOSS: 1.1485809087753296
LOSS: 1.1465235948562622
LOSS: 1.1445294618606567
LOSS: 1.1425728797912598
LOSS: 1.1406372785568237
LOSS: 1.1387161016464233
LOSS: 1.13681161403656
LOSS: 1.1349273920059204
LOSS: 1.1330665349960327
LOSS: 1.1312353610992432
LOSS: 1.129448413848877
LOSS: 1.1277319192886353
LOSS: 1.1261122226715088
LOSS: 1.1246058940887451
LOSS: 1.1232103109359741
LOSS: 1.121899127960205
LOSS: 1.120634913444519
LOSS: 1.1193785667419434
LOSS: 1.1181079149246216
LOSS: 1.116820216178894
LOSS: 1.1155287027359009
LOSS: 1.1142598390579224
LOSS: 1.1130436658859253
LOSS: 1.1119067668914795
LOSS: 1.1108614206314087
LOSS: 1.109902262687683
LOSS: 1.1090031862258911
LOSS: 1.108126163482666
LOSS: 1.1072399616241455
LOSS: 1.106333613395691
LOSS: 1.1054158210754395
LOSS: 1.1045050621032715
LOSS: 1.1036173105239868
LOSS: 1.102760910987854
LOSS: 1.374792456626892
LOSS: 1.3685146570205688
LOSS: 1.3626205921173096
LOSS: 1.3571455478668213
LOSS: 1.35212242603302
LOSS: 1.3475794792175293
LOSS: 1.3435356616973877
LOSS: 1.3399931192398071
LOSS: 1.3369289636611938
LOSS: 1.3342880010604858
LOSS: 1.3319779634475708
LOSS: 1.3298768997192383
LOSS: 1.3278499841690063
LOSS: 1.3257721662521362
LOSS: 1.3235461711883545
LOSS: 1.321105718612671
LOSS: 1.3184130191802979
LOSS: 1.315452218055725
LOSS: 1.3122210502624512
LOSS: 1.3087255954742432
LOSS: 1.3049776554107666
LOSS: 1.300987720489502
LOSS: 1.2967634201049805
LOSS: 1.2923091650009155
LOSS: 1.28762686252594
LOSS: 1.2827200889587402
LOSS: 1.2775932550430298
LOSS: 1.2722561359405518
LOSS: 1.2667232751846313
LOSS: 1.261014699935913
LOSS: 1.2551567554473877
LOSS: 1.2491787672042847
LOSS: 1.2431107759475708
LOSS: 1.2369818687438965
LOSS: 1.2308259010314941
LOSS: 1.2246819734573364
LOSS: 1.218593716621399
LOSS: 1.2126051187515259
LOSS: 1.2067601680755615
LOSS: 1.2010987997055054
LOSS: 1.1956568956375122
LOSS: 1.1904655694961548
LOSS: 1.1855489015579224
LOSS: 1.1809210777282715
LOSS: 1.1765857934951782
LOSS: 1.1725273132324219
LOSS: 1.1687054634094238
LOSS: 1.1650731563568115
LOSS: 1.1615878343582153
LOSS: 1.158212423324585
LOSS: 1.1549272537231445
LOSS: 1.1517280340194702
LOSS: 1.1486294269561768
LOSS: 1.1456563472747803
LOSS: 1.1428285837173462
LOSS: 1.1401598453521729
LOSS: 1.1376579999923706
LOSS: 1.1353280544281006
LOSS: 1.133157730102539
LOSS: 1.1311218738555908
LOSS: 1.1291835308074951
LOSS: 1.1273051500320435
LOSS: 1.1254559755325317
LOSS: 1.1236190795898438
LOSS: 1.1217901706695557
LOSS: 1.1199742555618286
LOSS: 1.1181844472885132
LOSS: 1.116438388824463
LOSS: 1.1147562265396118
LOSS: 1.113155722618103
LOSS: 1.111647605895996
LOSS: 1.1102311611175537
LOSS: 1.1088961362838745
LOSS: 1.1076266765594482
LOSS: 1.1064060926437378
LOSS: 1.1052237749099731
LOSS: 1.1040748357772827
LOSS: 1.1029597520828247
LOSS: 1.1018800735473633
LOSS: 1.1008347272872925
LOSS: 1.0998163223266602
LOSS: 1.098810076713562
LOSS: 1.0978004932403564
LOSS: 1.0967812538146973
LOSS: 1.0957579612731934
LOSS: 1.0947433710098267
LOSS: 1.0937516689300537
LOSS: 1.0927965641021729
LOSS: 1.0918898582458496
LOSS: 1.091037631034851
LOSS: 1.0902373790740967
LOSS: 1.0894750356674194
LOSS: 1.088727593421936
LOSS: 1.087974190711975
LOSS: 1.0872082710266113
LOSS: 1.086438536643982
LOSS: 1.085679531097412
LOSS: 1.0849452018737793
LOSS: 1.0842418670654297
LOSS: 1.0835676193237305
LOSS: 1.3796838521957397
LOSS: 1.3735734224319458
LOSS: 1.3678613901138306
LOSS: 1.362586498260498
LOSS: 1.3577842712402344
LOSS: 1.353487491607666
LOSS: 1.3497130870819092
LOSS: 1.346466302871704
LOSS: 1.3437206745147705
LOSS: 1.3414160013198853
LOSS: 1.3394578695297241
LOSS: 1.3377197980880737
LOSS: 1.336074948310852
LOSS: 1.3344072103500366
LOSS: 1.3326343297958374
LOSS: 1.3307057619094849
LOSS: 1.328599452972412
LOSS: 1.3263112306594849
LOSS: 1.3238489627838135
LOSS: 1.3212250471115112
LOSS: 1.3184536695480347
LOSS: 1.3155473470687866
LOSS: 1.3125146627426147
LOSS: 1.3093582391738892
LOSS: 1.306075930595398
LOSS: 1.3026621341705322
LOSS: 1.2991055250167847
LOSS: 1.2953938245773315
LOSS: 1.2915128469467163
LOSS: 1.2874475717544556
LOSS: 1.2831871509552002
LOSS: 1.2787244319915771
LOSS: 1.0999186038970947
LOSS: 1.0984351634979248
LOSS: 1.0969544649124146
LOSS: 1.0954949855804443
LOSS: 1.0940803289413452
LOSS: 1.0927022695541382
LOSS: 1.0913771390914917
LOSS: 1.0901068449020386
LOSS: 1.088897466659546
LOSS: 1.0877522230148315
LOSS: 1.086667537689209
LOSS: 1.085625171661377
LOSS: 1.0846301317214966
LOSS: 1.0836704969406128
LOSS: 1.0827487707138062
LOSS: 1.0818500518798828
LOSS: 1.0809693336486816
LOSS: 1.0800926685333252
LOSS: 1.079222321510315
LOSS: 1.0783615112304688
LOSS: 1.0775142908096313
LOSS: 1.0766836404800415
LOSS: 1.0758775472640991
LOSS: 1.075095295906067
LOSS: 1.0743488073349
LOSS: 1.0736260414123535
LOSS: 1.072928786277771
LOSS: 1.0722497701644897
LOSS: 1.3799479007720947
LOSS: 1.3737581968307495
LOSS: 1.3679609298706055
LOSS: 1.362593173980713
LOSS: 1.3576921224594116
LOSS: 1.3532872200012207
LOSS: 1.3493983745574951
LOSS: 1.3460288047790527
LOSS: 1.3431564569473267
LOSS: 1.3407249450683594
LOSS: 1.3386422395706177
LOSS: 1.3367856740951538
LOSS: 1.3350236415863037
LOSS: 1.3332363367080688
LOSS: 1.3313311338424683
LOSS: 1.3292491436004639
LOSS: 1.3269585371017456
LOSS: 1.324447751045227
LOSS: 1.3217154741287231
LOSS: 1.3187675476074219
LOSS: 1.3156083822250366
LOSS: 1.3122409582138062
LOSS: 1.308664321899414
LOSS: 1.3048748970031738
LOSS: 1.3008654117584229
LOSS: 1.2966281175613403
LOSS: 1.2921539545059204
LOSS: 1.2874351739883423
LOSS: 1.282469391822815
LOSS: 1.277255654335022
LOSS: 1.271798849105835
LOSS: 1.266109824180603
LOSS: 1.2602009773254395
LOSS: 1.2540905475616455
LOSS: 1.2477988004684448
LOSS: 1.2413513660430908
LOSS: 1.2347806692123413
LOSS: 1.2281246185302734
LOSS: 1.2214292287826538
LOSS: 1.2147457599639893
LOSS: 1.208128809928894
LOSS: 1.2016299962997437
LOSS: 1.1952909231185913
LOSS: 1.189150094985962
LOSS: 1.183242678642273
LOSS: 1.177596926689148
LOSS: 1.1722313165664673
LOSS: 1.1671556234359741
LOSS: 1.1623713970184326
LOSS: 1.1578739881515503
LOSS: 1.1536577939987183
LOSS: 1.1497130393981934
LOSS: 1.1460185050964355
LOSS: 1.1425271034240723
LOSS: 1.1391860246658325
LOSS: 1.135959506034851
LOSS: 1.1328253746032715
LOSS: 1.1297718286514282
LOSS: 1.1267929077148438
LOSS: 1.1238845586776733
LOSS: 1.1210367679595947
LOSS: 1.1182321310043335
LOSS: 1.115447759628296
LOSS: 1.1126669645309448
LOSS: 1.1098891496658325
LOSS: 1.1071302890777588
LOSS: 1.1044222116470337
LOSS: 1.1018074750900269
LOSS: 1.0993329286575317
LOSS: 1.0970327854156494
LOSS: 1.094915509223938
LOSS: 1.0929527282714844
LOSS: 1.0910969972610474
LOSS: 1.0892952680587769
LOSS: 1.0875087976455688
LOSS: 1.0857101678848267
LOSS: 1.083886981010437
LOSS: 1.0820374488830566
LOSS: 1.0801693201065063
LOSS: 1.0782967805862427
LOSS: 1.0764397382736206
LOSS: 1.0746171474456787
LOSS: 1.072845697402954
LOSS: 1.071134090423584
LOSS: 1.0694856643676758
LOSS: 1.067897915840149
LOSS: 1.0663647651672363
LOSS: 1.0648831129074097
LOSS: 1.0634597539901733
LOSS: 1.062111735343933
LOSS: 1.060858130455017
LOSS: 1.0597114562988281
LOSS: 1.0586622953414917
LOSS: 1.057671308517456
LOSS: 1.056681513786316
LOSS: 1.055647373199463
LOSS: 1.0545589923858643
LOSS: 1.0534310340881348
LOSS: 1.0522927045822144
LOSS: 1.051173210144043
LOSS: 1.3827296495437622
LOSS: 1.3765571117401123
LOSS: 1.3707785606384277
LOSS: 1.3654298782348633
LOSS: 1.3605473041534424
LOSS: 1.3561614751815796
LOSS: 1.3522905111312866
LOSS: 1.3489360809326172
LOSS: 1.3460723161697388
LOSS: 1.3436391353607178
LOSS: 1.3415392637252808
LOSS: 1.3396462202072144
LOSS: 1.3378283977508545
LOSS: 1.3359637260437012
LOSS: 1.3339706659317017
LOSS: 1.3317928314208984
LOSS: 1.3294031620025635
LOSS: 1.326803207397461
LOSS: 1.3239985704421997
LOSS: 1.3210041522979736
LOSS: 1.317834734916687
LOSS: 1.314503788948059
LOSS: 1.3110233545303345
LOSS: 1.3074004650115967
LOSS: 1.3036376237869263
LOSS: 1.2997310161590576
LOSS: 1.2956708669662476
LOSS: 1.2914434671401978
LOSS: 1.287031888961792
LOSS: 1.2824254035949707
LOSS: 1.2776155471801758
LOSS: 1.2726017236709595
LOSS: 1.2673892974853516
LOSS: 1.2619918584823608
LOSS: 1.2564282417297363
LOSS: 1.2507201433181763
LOSS: 1.2448982000350952
LOSS: 1.2389994859695435
LOSS: 1.2330665588378906
LOSS: 1.2271496057510376
LOSS: 1.2212997674942017
LOSS: 1.2155649662017822
LOSS: 1.2099905014038086
LOSS: 1.2046154737472534
LOSS: 1.1994682550430298
LOSS: 1.1945608854293823
LOSS: 1.1898894309997559
LOSS: 1.1854380369186401
LOSS: 1.181187629699707
LOSS: 1.1771223545074463
LOSS: 1.1732276678085327
LOSS: 1.169490933418274
LOSS: 1.1659061908721924
LOSS: 1.1624730825424194
LOSS: 1.1591954231262207
LOSS: 1.1560770273208618
LOSS: 1.1531133651733398
LOSS: 1.150286316871643
LOSS: 1.1475658416748047
LOSS: 1.1449223756790161
LOSS: 1.142336368560791
LOSS: 1.1397933959960938
LOSS: 1.1372815370559692
LOSS: 1.1347821950912476
LOSS: 1.1322689056396484
LOSS: 1.1297190189361572
LOSS: 1.127126932144165
LOSS: 1.1245049238204956
LOSS: 1.1218774318695068
LOSS: 1.1192671060562134
LOSS: 1.116679072380066
LOSS: 1.1141005754470825
LOSS: 1.111521601676941
LOSS: 1.1089484691619873
LOSS: 1.1063995361328125
LOSS: 1.1038979291915894
LOSS: 1.1014654636383057
LOSS: 1.0991135835647583
LOSS: 1.096838116645813
LOSS: 1.0946214199066162
LOSS: 1.0924408435821533
LOSS: 1.090279459953308
LOSS: 1.088133692741394
LOSS: 1.0860058069229126
LOSS: 1.0838942527770996
LOSS: 1.0817879438400269
LOSS: 1.0796740055084229
LOSS: 1.077549934387207
LOSS: 1.0754282474517822
LOSS: 1.073330283164978
LOSS: 1.0712767839431763
LOSS: 1.069284439086914
LOSS: 1.067365050315857
LOSS: 1.0655243396759033
LOSS: 1.0637649297714233
LOSS: 1.0620884895324707
LOSS: 1.0605000257492065
LOSS: 1.0590065717697144
LOSS: 1.057613730430603
LOSS: 1.0563209056854248
LOSS: 1.3665093183517456
LOSS: 1.3599706888198853
LOSS: 1.3538029193878174
LOSS: 1.348041296005249
LOSS: 1.342720627784729
LOSS: 1.3378701210021973
LOSS: 1.3335069417953491
LOSS: 1.3296350240707397
LOSS: 1.3262280225753784
LOSS: 1.3232299089431763
LOSS: 1.3205468654632568
LOSS: 1.3180592060089111
LOSS: 1.315638542175293
LOSS: 1.313170313835144
LOSS: 1.3105709552764893
LOSS: 1.3077881336212158
LOSS: 1.3047972917556763
LOSS: 1.3015893697738647
LOSS: 1.2981666326522827
LOSS: 1.2945353984832764
LOSS: 1.290704369544983
LOSS: 1.2866861820220947
LOSS: 1.282495379447937
LOSS: 1.278145432472229
LOSS: 1.2736520767211914
LOSS: 1.2690280675888062
LOSS: 1.2642838954925537
LOSS: 1.259426474571228
LOSS: 1.2544573545455933
LOSS: 1.2493739128112793
LOSS: 1.2441662549972534
LOSS: 1.238815426826477
LOSS: 1.2333005666732788
LOSS: 1.2276027202606201
LOSS: 1.2217100858688354
LOSS: 1.2156224250793457
LOSS: 1.209357738494873
LOSS: 1.2029523849487305
LOSS: 1.1964620351791382
LOSS: 1.1899607181549072
LOSS: 1.1835336685180664
LOSS: 1.1772712469100952
LOSS: 1.1712591648101807
LOSS: 1.1655677556991577
LOSS: 1.160247802734375
LOSS: 1.1553289890289307
LOSS: 1.1508209705352783
LOSS: 1.1467175483703613
LOSS: 1.1429970264434814
LOSS: 1.139623999595642
LOSS: 1.1365528106689453
LOSS: 1.1337285041809082
LOSS: 1.131087303161621
LOSS: 1.128563642501831
LOSS: 1.1261060237884521
LOSS: 1.123683214187622
LOSS: 1.1212875843048096
LOSS: 1.1189290285110474
LOSS: 1.1166259050369263
LOSS: 1.1143925189971924
LOSS: 1.1122312545776367
LOSS: 1.1101367473602295
LOSS: 1.1081011295318604
LOSS: 1.1061177253723145
LOSS: 1.1041775941848755
LOSS: 1.1022683382034302
LOSS: 1.1003754138946533
LOSS: 1.098490595817566
LOSS: 1.0966198444366455
LOSS: 1.0947821140289307
LOSS: 1.0930061340332031
LOSS: 1.091322660446167
LOSS: 1.0897514820098877
LOSS: 1.088287591934204
LOSS: 1.0868993997573853
LOSS: 1.085566759109497
LOSS: 1.0842982530593872
LOSS: 1.0831148624420166
LOSS: 1.0820320844650269
LOSS: 1.0810452699661255
LOSS: 1.0801230669021606
LOSS: 1.0792174339294434
LOSS: 1.0782948732376099
LOSS: 1.0773543119430542
LOSS: 1.0764192342758179
LOSS: 1.0755155086517334
LOSS: 1.074660062789917
LOSS: 1.0738484859466553
LOSS: 1.0730646848678589
LOSS: 1.0722899436950684
LOSS: 1.0715147256851196
LOSS: 1.0707368850708008
LOSS: 1.0699617862701416
LOSS: 1.0691955089569092
LOSS: 1.0684411525726318
LOSS: 1.0677026510238647
LOSS: 1.0669808387756348
LOSS: 1.0662823915481567
LOSS: 1.065609335899353
LOSS: 1.064965844154358
LOSS: 1.385347843170166
LOSS: 1.3794187307357788
LOSS: 1.352599859237671
LOSS: 1.3473669290542603
LOSS: 1.3426108360290527
LOSS: 1.338353157043457
LOSS: 1.3346002101898193
LOSS: 1.3313368558883667
LOSS: 1.3285174369812012
LOSS: 1.3260600566864014
LOSS: 1.323851466178894
LOSS: 1.3217604160308838
LOSS: 1.3196580410003662
LOSS: 1.3174384832382202
LOSS: 1.315024971961975
LOSS: 1.3123711347579956
LOSS: 1.3094502687454224
LOSS: 1.3062539100646973
LOSS: 1.3027797937393188
LOSS: 1.2990330457687378
LOSS: 1.2950204610824585
LOSS: 1.2907500267028809
LOSS: 1.2862318754196167
LOSS: 1.281476378440857
LOSS: 1.2764939069747925
LOSS: 1.2712972164154053
LOSS: 1.265899658203125
LOSS: 1.2603176832199097
LOSS: 1.254571557044983
LOSS: 1.2486822605133057
LOSS: 1.2426719665527344
LOSS: 1.236562728881836
LOSS: 1.2303773164749146
LOSS: 1.2241408824920654
LOSS: 1.2178809642791748
LOSS: 1.2116265296936035
LOSS: 1.205411434173584
LOSS: 1.1992743015289307
LOSS: 1.193255066871643
LOSS: 1.1873935461044312
LOSS: 1.1817275285720825
LOSS: 1.1762930154800415
LOSS: 1.1711206436157227
LOSS: 1.1662288904190063
LOSS: 1.1616296768188477
LOSS: 1.1573233604431152
LOSS: 1.1533011198043823
LOSS: 1.1495481729507446
LOSS: 1.1460477113723755
LOSS: 1.1427844762802124
LOSS: 1.139747142791748
LOSS: 1.1369308233261108
LOSS: 1.1343270540237427
LOSS: 1.1319074630737305
LOSS: 1.1296190023422241
LOSS: 1.1273964643478394
LOSS: 1.1251939535140991
LOSS: 1.1230026483535767
LOSS: 1.1208406686782837
LOSS: 1.1187361478805542
LOSS: 1.1167069673538208
LOSS: 1.1147887706756592
LOSS: 1.1129975318908691
LOSS: 1.1113399267196655
LOSS: 1.109806776046753
LOSS: 1.1083643436431885
LOSS: 1.1069883108139038
LOSS: 1.1056252717971802
LOSS: 1.1042464971542358
LOSS: 1.1028374433517456
LOSS: 1.1013939380645752
LOSS: 1.0999186038970947
LOSS: 1.0984351634979248
LOSS: 1.0969544649124146
LOSS: 1.0954949855804443
LOSS: 1.0940803289413452
LOSS: 1.0927022695541382
LOSS: 1.0913771390914917
LOSS: 1.0901068449020386
LOSS: 1.088897466659546
LOSS: 1.0877522230148315
LOSS: 1.086667537689209
LOSS: 1.085625171661377
LOSS: 1.0846301317214966
LOSS: 1.0836704969406128
LOSS: 1.0827487707138062
LOSS: 1.0818500518798828
LOSS: 1.0809693336486816
LOSS: 1.0800926685333252
LOSS: 1.079222321510315
LOSS: 1.0783615112304688
LOSS: 1.0775142908096313
LOSS: 1.0766836404800415
LOSS: 1.0758775472640991
LOSS: 1.075095295906067
LOSS: 1.0743488073349
LOSS: 1.0736260414123535
LOSS: 1.072928786277771
LOSS: 1.0722497701644897
LOSS: 1.3665093183517456
LOSS: 1.3599706888198853
LOSS: 1.3538029193878174
LOSS: 1.348041296005249
LOSS: 1.342720627784729
LOSS: 1.3378701210021973
LOSS: 1.3335069417953491
LOSS: 1.3296350240707397
LOSS: 1.3262280225753784
LOSS: 1.3232299089431763
LOSS: 1.3205468654632568
LOSS: 1.3180592060089111
LOSS: 1.315638542175293
LOSS: 1.313170313835144
LOSS: 1.3105709552764893
LOSS: 1.3077881336212158
LOSS: 1.3047972917556763
LOSS: 1.3015893697738647
LOSS: 1.2981666326522827
LOSS: 1.2945353984832764
LOSS: 1.290704369544983
LOSS: 1.2866861820220947
LOSS: 1.282495379447937
LOSS: 1.278145432472229
LOSS: 1.2736520767211914
LOSS: 1.2690280675888062
LOSS: 1.2642838954925537
LOSS: 1.259426474571228
LOSS: 1.2544573545455933
LOSS: 1.2493739128112793
LOSS: 1.2441662549972534
LOSS: 1.238815426826477
LOSS: 1.2333005666732788
LOSS: 1.2276027202606201
LOSS: 1.2217100858688354
LOSS: 1.2156224250793457
LOSS: 1.209357738494873
LOSS: 1.2029523849487305
LOSS: 1.1964620351791382
LOSS: 1.1899607181549072
LOSS: 1.1835336685180664
LOSS: 1.1772712469100952
LOSS: 1.1712591648101807
LOSS: 1.1655677556991577
LOSS: 1.160247802734375
LOSS: 1.1553289890289307
LOSS: 1.1508209705352783
LOSS: 1.1467175483703613
LOSS: 1.1429970264434814
LOSS: 1.139623999595642
LOSS: 1.1365528106689453
LOSS: 1.1337285041809082
LOSS: 1.131087303161621
LOSS: 1.128563642501831
LOSS: 1.1261060237884521
LOSS: 1.123683214187622
LOSS: 1.1212875843048096
LOSS: 1.1189290285110474
LOSS: 1.1166259050369263
LOSS: 1.1143925189971924
LOSS: 1.1122312545776367
LOSS: 1.1101367473602295
LOSS: 1.1081011295318604
LOSS: 1.1061177253723145
LOSS: 1.1041775941848755
LOSS: 1.1022683382034302
LOSS: 1.1003754138946533
LOSS: 1.098490595817566
LOSS: 1.0966198444366455
LOSS: 1.0947821140289307
LOSS: 1.0930061340332031
LOSS: 1.091322660446167
LOSS: 1.0897514820098877
LOSS: 1.088287591934204
LOSS: 1.0868993997573853
LOSS: 1.085566759109497
LOSS: 1.0842982530593872
LOSS: 1.0831148624420166
LOSS: 1.0820320844650269
LOSS: 1.0810452699661255
LOSS: 1.0801230669021606
LOSS: 1.0792174339294434
LOSS: 1.0782948732376099
LOSS: 1.0773543119430542
LOSS: 1.0764192342758179
LOSS: 1.0755155086517334
LOSS: 1.074660062789917
LOSS: 1.0738484859466553
LOSS: 1.0730646848678589
LOSS: 1.0722899436950684
LOSS: 1.0715147256851196
LOSS: 1.0707368850708008
LOSS: 1.0699617862701416
LOSS: 1.0691955089569092
LOSS: 1.0684411525726318
LOSS: 1.0677026510238647
LOSS: 1.0669808387756348
LOSS: 1.0662823915481567
LOSS: 1.065609335899353
LOSS: 1.064965844154358
LOSS: 1.3827296495437622
LOSS: 1.3765571117401123
LOSS: 1.3707785606384277
LOSS: 1.3654298782348633
LOSS: 1.3605473041534424
LOSS: 1.3561614751815796
LOSS: 1.3522905111312866
LOSS: 1.3489360809326172
LOSS: 1.3460723161697388
LOSS: 1.3436391353607178
LOSS: 1.3415392637252808
LOSS: 1.3396462202072144
LOSS: 1.3378283977508545
LOSS: 1.3359637260437012
LOSS: 1.3339706659317017
LOSS: 1.3317928314208984
LOSS: 1.3294031620025635
LOSS: 1.326803207397461
LOSS: 1.3239985704421997
LOSS: 1.3210041522979736
LOSS: 1.317834734916687
LOSS: 1.314503788948059
LOSS: 1.3110233545303345
LOSS: 1.3074004650115967
LOSS: 1.3036376237869263
LOSS: 1.2997310161590576
LOSS: 1.2956708669662476
LOSS: 1.2914434671401978
LOSS: 1.287031888961792
LOSS: 1.2824254035949707
LOSS: 1.2776155471801758
LOSS: 1.2726017236709595
LOSS: 1.2673892974853516
LOSS: 1.2619918584823608
LOSS: 1.2564282417297363
LOSS: 1.2507201433181763
LOSS: 1.2448982000350952
LOSS: 1.2389994859695435
LOSS: 1.2330665588378906
LOSS: 1.2271496057510376
LOSS: 1.2212997674942017
LOSS: 1.2155649662017822
LOSS: 1.2099905014038086
LOSS: 1.2046154737472534
LOSS: 1.1994682550430298
LOSS: 1.1945608854293823
LOSS: 1.1898894309997559
LOSS: 1.1854380369186401
LOSS: 1.181187629699707
LOSS: 1.1771223545074463
LOSS: 1.1732276678085327
LOSS: 1.169490933418274
LOSS: 1.1659061908721924
LOSS: 1.1624730825424194
LOSS: 1.1591954231262207
LOSS: 1.1560770273208618
LOSS: 1.1531133651733398
LOSS: 1.150286316871643
LOSS: 1.1475658416748047
LOSS: 1.1449223756790161
LOSS: 1.142336368560791
LOSS: 1.1397933959960938
LOSS: 1.1372815370559692
LOSS: 1.1347821950912476
LOSS: 1.1322689056396484
LOSS: 1.1297190189361572
LOSS: 1.127126932144165
LOSS: 1.1245049238204956
LOSS: 1.1218774318695068
LOSS: 1.1192671060562134
LOSS: 1.116679072380066
LOSS: 1.1141005754470825
LOSS: 1.111521601676941
LOSS: 1.1089484691619873
LOSS: 1.1063995361328125
LOSS: 1.1038979291915894
LOSS: 1.1014654636383057
LOSS: 1.0991135835647583
LOSS: 1.096838116645813
LOSS: 1.0946214199066162
LOSS: 1.0924408435821533
LOSS: 1.090279459953308
LOSS: 1.088133692741394
LOSS: 1.0860058069229126
LOSS: 1.0838942527770996
LOSS: 1.0817879438400269
LOSS: 1.0796740055084229
LOSS: 1.077549934387207
LOSS: 1.0754282474517822
LOSS: 1.073330283164978
LOSS: 1.0712767839431763
LOSS: 1.069284439086914
LOSS: 1.067365050315857
LOSS: 1.0655243396759033
LOSS: 1.0637649297714233
LOSS: 1.0620884895324707
LOSS: 1.0605000257492065
LOSS: 1.0590065717697144
LOSS: 1.057613730430603
LOSS: 1.0563209056854248
LOSS: 1.3658277988433838
LOSS: 1.359381079673767
LOSS: 1.3533141613006592
LOSS: 1.347664475440979
LOSS: 1.3424675464630127
LOSS: 1.3377548456192017
LOSS: 1.3335503339767456
LOSS: 1.3298618793487549
LOSS: 1.3266786336898804
LOSS: 1.3239585161209106
LOSS: 1.3216267824172974
LOSS: 1.3195792436599731
LOSS: 1.317694067955017
LOSS: 1.3158528804779053
LOSS: 1.313963770866394
LOSS: 1.311962366104126
LOSS: 1.309818983078003
LOSS: 1.3075250387191772
LOSS: 1.3050936460494995
LOSS: 1.302547574043274
LOSS: 1.2999176979064941
LOSS: 1.2972365617752075
LOSS: 1.2945383787155151
LOSS: 1.2918555736541748
LOSS: 1.28921639919281
LOSS: 1.2866460084915161
LOSS: 1.2841626405715942
LOSS: 1.2817777395248413
LOSS: 1.2794973850250244
LOSS: 1.2773207426071167
LOSS: 1.2752419710159302
LOSS: 1.2732508182525635
LOSS: 1.2713335752487183
LOSS: 1.2740579843521118
LOSS: 1.269194483757019
LOSS: 1.2641470432281494
LOSS: 1.2589367628097534
LOSS: 1.2535954713821411
LOSS: 1.248165488243103
LOSS: 1.2426958084106445
LOSS: 1.2372409105300903
LOSS: 1.2318552732467651
LOSS: 1.2265926599502563
LOSS: 1.2215006351470947
LOSS: 1.2166194915771484
LOSS: 1.211978554725647
LOSS: 1.2075903415679932
LOSS: 1.203453540802002
LOSS: 1.199556589126587
LOSS: 1.1958820819854736
LOSS: 1.192407488822937
LOSS: 1.189110279083252
LOSS: 1.185976505279541
LOSS: 1.183000922203064
LOSS: 1.1801786422729492
LOSS: 1.1774914264678955
LOSS: 1.1749072074890137
LOSS: 1.1723897457122803
LOSS: 1.1699111461639404
LOSS: 1.1674529314041138
LOSS: 1.1649998426437378
LOSS: 1.1625453233718872
LOSS: 1.1600948572158813
LOSS: 1.1576647758483887
LOSS: 1.1552772521972656
LOSS: 1.1529574394226074
LOSS: 1.1507233381271362
LOSS: 1.1485809087753296
LOSS: 1.1465235948562622
LOSS: 1.1445294618606567
LOSS: 1.1425728797912598
LOSS: 1.1406372785568237
LOSS: 1.1387161016464233
LOSS: 1.13681161403656
LOSS: 1.1349273920059204
LOSS: 1.1330665349960327
LOSS: 1.1312353610992432
LOSS: 1.129448413848877
LOSS: 1.1277319192886353
LOSS: 1.1261122226715088
LOSS: 1.1246058940887451
LOSS: 1.1232103109359741
LOSS: 1.121899127960205
LOSS: 1.120634913444519
LOSS: 1.1193785667419434
LOSS: 1.1181079149246216
LOSS: 1.116820216178894
LOSS: 1.1155287027359009
LOSS: 1.1142598390579224
LOSS: 1.1130436658859253
LOSS: 1.1119067668914795
LOSS: 1.1108614206314087
LOSS: 1.109902262687683
LOSS: 1.1090031862258911
LOSS: 1.108126163482666
LOSS: 1.1072399616241455
LOSS: 1.106333613395691
LOSS: 1.1054158210754395
LOSS: 1.1045050621032715
LOSS: 1.1036173105239868
LOSS: 1.102760910987854
LOSS: 1.374792456626892
LOSS: 1.3685146570205688
LOSS: 1.3626205921173096
LOSS: 1.3571455478668213
LOSS: 1.35212242603302
LOSS: 1.3475794792175293
LOSS: 1.3435356616973877
LOSS: 1.3399931192398071
LOSS: 1.3369289636611938
LOSS: 1.3342880010604858
LOSS: 1.3319779634475708
LOSS: 1.3298768997192383
LOSS: 1.3278499841690063
LOSS: 1.3257721662521362
LOSS: 1.3235461711883545
LOSS: 1.321105718612671
LOSS: 1.3184130191802979
LOSS: 1.315452218055725
LOSS: 1.3122210502624512
LOSS: 1.3087255954742432
LOSS: 1.3049776554107666
LOSS: 1.300987720489502
LOSS: 1.2967634201049805
LOSS: 1.2923091650009155
LOSS: 1.28762686252594
LOSS: 1.2827200889587402
LOSS: 1.2775932550430298
LOSS: 1.2722561359405518
LOSS: 1.2667232751846313
LOSS: 1.261014699935913
LOSS: 1.2551567554473877
LOSS: 1.2491787672042847
LOSS: 1.2431107759475708
LOSS: 1.2369818687438965
LOSS: 1.2308259010314941
LOSS: 1.2246819734573364
LOSS: 1.218593716621399
LOSS: 1.2126051187515259
LOSS: 1.2067601680755615
LOSS: 1.2010987997055054
LOSS: 1.1956568956375122
LOSS: 1.1904655694961548
LOSS: 1.1855489015579224
LOSS: 1.1809210777282715
LOSS: 1.1765857934951782
LOSS: 1.1725273132324219
LOSS: 1.1687054634094238
LOSS: 1.1650731563568115
LOSS: 1.1615878343582153
LOSS: 1.158212423324585
LOSS: 1.1549272537231445
LOSS: 1.1517280340194702
LOSS: 1.1486294269561768
LOSS: 1.1456563472747803
LOSS: 1.1428285837173462
LOSS: 1.1401598453521729
LOSS: 1.1376579999923706
LOSS: 1.1353280544281006
LOSS: 1.133157730102539
LOSS: 1.1311218738555908
LOSS: 1.1291835308074951
LOSS: 1.1273051500320435
LOSS: 1.1254559755325317
LOSS: 1.1236190795898438
LOSS: 1.1217901706695557
LOSS: 1.1199742555618286
LOSS: 1.1181844472885132
LOSS: 1.116438388824463
LOSS: 1.1147562265396118
LOSS: 1.113155722618103
LOSS: 1.111647605895996
LOSS: 1.1102311611175537
LOSS: 1.1088961362838745
LOSS: 1.1076266765594482
LOSS: 1.1064060926437378
LOSS: 1.1052237749099731
LOSS: 1.1040748357772827
LOSS: 1.1029597520828247
LOSS: 1.1018800735473633
LOSS: 1.1008347272872925
LOSS: 1.0998163223266602
LOSS: 1.098810076713562
LOSS: 1.0978004932403564
LOSS: 1.0967812538146973
LOSS: 1.0957579612731934
LOSS: 1.0947433710098267
LOSS: 1.0937516689300537
LOSS: 1.0927965641021729
LOSS: 1.0918898582458496
LOSS: 1.091037631034851
LOSS: 1.0902373790740967
LOSS: 1.0894750356674194
LOSS: 1.088727593421936
LOSS: 1.087974190711975
LOSS: 1.0872082710266113
LOSS: 1.086438536643982
LOSS: 1.085679531097412
LOSS: 1.0849452018737793
LOSS: 1.0842418670654297
LOSS: 1.0835676193237305
LOSS: 1.3669131994247437
LOSS: 1.3605185747146606
LOSS: 1.3545053005218506
LOSS: 1.3489115238189697
LOSS: 1.3437719345092773
LOSS: 1.3391156196594238
LOSS: 1.334961175918579
LOSS: 1.3313108682632446
LOSS: 1.3281400203704834
LOSS: 1.3253908157348633
LOSS: 1.3229695558547974
LOSS: 1.320754885673523
LOSS: 1.3186204433441162
LOSS: 1.3164546489715576
LOSS: 1.3141788244247437
LOSS: 1.3117468357086182
LOSS: 1.3091402053833008
LOSS: 1.3063634634017944
LOSS: 1.3034342527389526
LOSS: 1.3003777265548706
LOSS: 1.2972246408462524
LOSS: 1.2940078973770142
LOSS: 1.2907596826553345
LOSS: 1.2875103950500488
LOSS: 1.2842888832092285
LOSS: 1.281117558479309
LOSS: 1.2780144214630127
LOSS: 1.2749912738800049
LOSS: 1.2720556259155273
LOSS: 1.26920747756958
LOSS: 1.2664427757263184
LOSS: 1.2637536525726318
LOSS: 1.2611316442489624
LOSS: 1.2585654258728027
LOSS: 1.2560453414916992
LOSS: 1.2535638809204102
LOSS: 1.251116394996643
LOSS: 1.2487032413482666
LOSS: 1.2463302612304688
LOSS: 1.2440016269683838
LOSS: 1.2417290210723877
LOSS: 1.2395230531692505
LOSS: 1.2373936176300049
LOSS: 1.2353515625
LOSS: 1.2334001064300537
LOSS: 1.2315398454666138
LOSS: 1.2297669649124146
LOSS: 1.2280651330947876
LOSS: 1.2264111042022705
LOSS: 1.2247720956802368
LOSS: 1.223114252090454
LOSS: 1.2214009761810303
LOSS: 1.2196018695831299
LOSS: 1.2176756858825684
LOSS: 1.2155944108963013
LOSS: 1.213333010673523
LOSS: 1.2108715772628784
LOSS: 1.2082023620605469
LOSS: 1.205331802368164
LOSS: 1.2022714614868164
LOSS: 1.1990432739257812
LOSS: 1.1956713199615479
LOSS: 1.1921865940093994
LOSS: 1.188625693321228
LOSS: 1.185027837753296
LOSS: 1.1814402341842651
LOSS: 1.177905559539795
LOSS: 1.1744580268859863
LOSS: 1.1711328029632568
LOSS: 1.1679548025131226
LOSS: 1.1649307012557983
LOSS: 1.1620490550994873
LOSS: 1.1593023538589478
LOSS: 1.156678557395935
LOSS: 1.1541852951049805
LOSS: 1.151819109916687
LOSS: 1.1495798826217651
LOSS: 1.1474568843841553
LOSS: 1.145439624786377
LOSS: 1.1435006856918335
LOSS: 1.1416397094726562
LOSS: 1.1398632526397705
LOSS: 1.1381713151931763
LOSS: 1.1365736722946167
LOSS: 1.1350706815719604
LOSS: 1.133658528327942
LOSS: 1.1323225498199463
LOSS: 1.131056547164917
LOSS: 1.1298474073410034
LOSS: 1.1287007331848145
LOSS: 1.127609372138977
LOSS: 1.1265625953674316
LOSS: 1.1255481243133545
LOSS: 1.1245514154434204
LOSS: 1.123563528060913
LOSS: 1.1225758790969849
LOSS: 1.1215903759002686
LOSS: 1.120605707168579
LOSS: 1.1196308135986328
LOSS: 1.1186773777008057
LOSS: 1.3658277988433838
LOSS: 1.359381079673767
LOSS: 1.3533141613006592
LOSS: 1.347664475440979
LOSS: 1.3424675464630127
LOSS: 1.3377548456192017
LOSS: 1.3335503339767456
LOSS: 1.3298618793487549
LOSS: 1.3266786336898804
LOSS: 1.3239585161209106
LOSS: 1.3216267824172974
LOSS: 1.3195792436599731
LOSS: 1.317694067955017
LOSS: 1.3158528804779053
LOSS: 1.313963770866394
LOSS: 1.311962366104126
LOSS: 1.309818983078003
LOSS: 1.3075250387191772
LOSS: 1.3050936460494995
LOSS: 1.302547574043274
LOSS: 1.2999176979064941
LOSS: 1.2972365617752075
LOSS: 1.2945383787155151
LOSS: 1.2918555736541748
LOSS: 1.28921639919281
LOSS: 1.2866460084915161
LOSS: 1.2841626405715942
LOSS: 1.2817777395248413
LOSS: 1.2794973850250244
LOSS: 1.2773207426071167
LOSS: 1.2752419710159302
LOSS: 1.2732508182525635
LOSS: 1.2713335752487183
LOSS: 1.2694766521453857
LOSS: 1.2676657438278198
LOSS: 1.26589035987854
LOSS: 1.2641385793685913
LOSS: 1.2624014616012573
LOSS: 1.2606737613677979
LOSS: 1.2589519023895264
LOSS: 1.2572333812713623
LOSS: 1.255520224571228
LOSS: 1.2538115978240967
LOSS: 1.252103567123413
LOSS: 1.2503905296325684
LOSS: 1.2486635446548462
LOSS: 1.246908187866211
LOSS: 1.2451097965240479
LOSS: 1.2432442903518677
LOSS: 1.241283655166626
LOSS: 1.2391984462738037
LOSS: 1.2369558811187744
LOSS: 1.2345219850540161
LOSS: 1.2318679094314575
LOSS: 1.2289639711380005
LOSS: 1.2257943153381348
LOSS: 1.222348690032959
LOSS: 1.218632698059082
LOSS: 1.214663028717041
LOSS: 1.2104657888412476
LOSS: 1.2060754299163818
LOSS: 1.2015304565429688
LOSS: 1.1968671083450317
LOSS: 1.3738961219787598
LOSS: 1.368817687034607
LOSS: 1.364220142364502
LOSS: 1.360133171081543
LOSS: 1.3565747737884521
LOSS: 1.3535460233688354
LOSS: 1.3510184288024902
LOSS: 1.348928451538086
LOSS: 1.3471745252609253
LOSS: 1.345628023147583
LOSS: 1.3441554307937622
LOSS: 1.3426427841186523
LOSS: 1.3410102128982544
LOSS: 1.339212417602539
LOSS: 1.337233304977417
LOSS: 1.3350775241851807
LOSS: 1.3327635526657104
LOSS: 1.330316185951233
LOSS: 1.3277655839920044
LOSS: 1.3251416683197021
LOSS: 1.3224724531173706
LOSS: 1.3197838068008423
LOSS: 1.31709623336792
LOSS: 1.3144235610961914
LOSS: 1.3117748498916626
LOSS: 1.3091506958007812
LOSS: 1.306545615196228
LOSS: 1.3039463758468628
LOSS: 1.301330804824829
LOSS: 1.2986726760864258
LOSS: 1.2959399223327637
LOSS: 1.2930941581726074
LOSS: 1.2900992631912231
LOSS: 1.286919116973877
LOSS: 1.2835224866867065
LOSS: 1.279884696006775
LOSS: 1.2759889364242554
LOSS: 1.2718287706375122
LOSS: 1.2674108743667603
LOSS: 1.2627519369125366
LOSS: 1.2578787803649902
LOSS: 1.2528254985809326
LOSS: 1.2476304769515991
LOSS: 1.24233078956604
LOSS: 1.236959457397461
LOSS: 1.2315452098846436
LOSS: 1.2261115312576294
LOSS: 1.2206796407699585
LOSS: 1.2152646780014038
LOSS: 1.209877371788025
LOSS: 1.2045254707336426
LOSS: 1.19921875
LOSS: 1.1939727067947388
LOSS: 1.1888089179992676
LOSS: 1.1837533712387085
LOSS: 1.1788296699523926
LOSS: 1.1740448474884033
LOSS: 1.169384241104126
LOSS: 1.16482675075531
LOSS: 1.160351276397705
LOSS: 1.1559381484985352
LOSS: 1.1515758037567139
LOSS: 1.1472615003585815
LOSS: 1.1429998874664307
LOSS: 1.1388016939163208
LOSS: 1.1346747875213623
LOSS: 1.1306312084197998
LOSS: 1.126690149307251
LOSS: 1.122873067855835
LOSS: 1.1191929578781128
LOSS: 1.1156537532806396
LOSS: 1.1122527122497559
LOSS: 1.108977198600769
LOSS: 1.1058173179626465
LOSS: 1.1027745008468628
LOSS: 1.0998635292053223
LOSS: 1.097104787826538
LOSS: 1.0945141315460205
LOSS: 1.092099666595459
LOSS: 1.0898656845092773
LOSS: 1.087815284729004
LOSS: 1.0859431028366089
LOSS: 1.084228277206421
LOSS: 1.0826349258422852
LOSS: 1.0811246633529663
LOSS: 1.0796669721603394
LOSS: 1.078244686126709
LOSS: 1.0768516063690186
LOSS: 1.0754860639572144
LOSS: 1.0741502046585083
LOSS: 1.0728479623794556
LOSS: 1.0715851783752441
LOSS: 1.0703665018081665
LOSS: 1.069190263748169
LOSS: 1.0680466890335083
LOSS: 1.06692636013031
LOSS: 1.0658279657363892
LOSS: 1.0647577047348022
LOSS: 1.3691240549087524
LOSS: 1.3628761768341064
LOSS: 1.3570184707641602
LOSS: 1.351588249206543
LOSS: 1.3466209173202515
LOSS: 1.3421450853347778
LOSS: 1.338179111480713
LOSS: 1.3347207307815552
LOSS: 1.3317416906356812
LOSS: 1.329176664352417
LOSS: 1.3269245624542236
LOSS: 1.324861764907837
LOSS: 1.3228628635406494
LOSS: 1.3208248615264893
LOSS: 1.3186782598495483
LOSS: 1.3163871765136719
LOSS: 1.3139420747756958
LOSS: 1.3113504648208618
LOSS: 1.3086330890655518
LOSS: 1.3058183193206787
LOSS: 1.3029346466064453
LOSS: 1.3000166416168213
LOSS: 1.2970954179763794
LOSS: 1.2941968441009521
LOSS: 1.2913498878479004
LOSS: 1.288572907447815
LOSS: 1.2858812808990479
LOSS: 1.2832831144332886
LOSS: 1.2807825803756714
LOSS: 1.2783759832382202
LOSS: 1.2760539054870605
LOSS: 1.273802638053894
LOSS: 1.2716108560562134
LOSS: 1.2694648504257202
LOSS: 1.2673553228378296
LOSS: 1.2652772665023804
LOSS: 1.2632261514663696
LOSS: 1.2612062692642212
LOSS: 1.2592138051986694
LOSS: 1.257250189781189
LOSS: 1.255313754081726
LOSS: 1.2534092664718628
LOSS: 1.2515294551849365
LOSS: 1.2496676445007324
LOSS: 1.2478082180023193
LOSS: 1.2459354400634766
LOSS: 1.2440223693847656
LOSS: 1.2420495748519897
LOSS: 1.239990472793579
LOSS: 1.2378342151641846
LOSS: 1.2355693578720093
LOSS: 1.2331925630569458
LOSS: 1.2307183742523193
LOSS: 1.2281419038772583
LOSS: 1.2254807949066162
LOSS: 1.2227423191070557
LOSS: 1.2199379205703735
LOSS: 1.2170754671096802
LOSS: 1.2141865491867065
LOSS: 1.2113021612167358
LOSS: 1.2084516286849976
LOSS: 1.2056527137756348
LOSS: 1.2029424905776978
LOSS: 1.2003334760665894
LOSS: 1.1978538036346436
LOSS: 1.1955033540725708
LOSS: 1.1932915449142456
LOSS: 1.1912304162979126
LOSS: 1.1893200874328613
LOSS: 1.1875159740447998
LOSS: 1.1858323812484741
LOSS: 1.1842234134674072
LOSS: 1.1826825141906738
LOSS: 1.1811704635620117
LOSS: 1.1796773672103882
LOSS: 1.178168773651123
LOSS: 1.1766550540924072
LOSS: 1.1751033067703247
LOSS: 1.173521637916565
LOSS: 1.1719121932983398
LOSS: 1.1702804565429688
LOSS: 1.1686164140701294
LOSS: 1.1669461727142334
LOSS: 1.165259599685669
LOSS: 1.1635327339172363
LOSS: 1.1617552042007446
LOSS: 1.1599217653274536
LOSS: 1.1580398082733154
LOSS: 1.1561452150344849
LOSS: 1.1542613506317139
LOSS: 1.1524022817611694
LOSS: 1.1505987644195557
LOSS: 1.1488449573516846
LOSS: 1.1471325159072876
LOSS: 1.1454356908798218
LOSS: 1.14374577999115
LOSS: 1.1420608758926392
LOSS: 1.1403820514678955
LOSS: 1.1387091875076294
LOSS: 1.1370660066604614
LOSS: 1.3669131994247437
LOSS: 1.3605185747146606
LOSS: 1.3545053005218506
LOSS: 1.3489115238189697
LOSS: 1.3437719345092773
LOSS: 1.3391156196594238
LOSS: 1.334961175918579
LOSS: 1.3313108682632446
LOSS: 1.3281400203704834
LOSS: 1.3253908157348633
LOSS: 1.3229695558547974
LOSS: 1.320754885673523
LOSS: 1.3186204433441162
LOSS: 1.3164546489715576
LOSS: 1.3141788244247437
LOSS: 1.3117468357086182
LOSS: 1.3091402053833008
LOSS: 1.3063634634017944
LOSS: 1.3034342527389526
LOSS: 1.3003777265548706
LOSS: 1.2972246408462524
LOSS: 1.2940078973770142
LOSS: 1.2907596826553345
LOSS: 1.2875103950500488
LOSS: 1.2842888832092285
LOSS: 1.281117558479309
LOSS: 1.2780144214630127
LOSS: 1.2749912738800049
LOSS: 1.2720556259155273
LOSS: 1.26920747756958
LOSS: 1.2664427757263184
LOSS: 1.2637536525726318
LOSS: 1.2611316442489624
LOSS: 1.2585654258728027
LOSS: 1.2560453414916992
LOSS: 1.2535638809204102
LOSS: 1.251116394996643
LOSS: 1.2487032413482666
LOSS: 1.2463302612304688
LOSS: 1.2440016269683838
LOSS: 1.2417290210723877
LOSS: 1.2395230531692505
LOSS: 1.2373936176300049
LOSS: 1.2353515625
LOSS: 1.2334001064300537
LOSS: 1.2315398454666138
LOSS: 1.2297669649124146
LOSS: 1.2280651330947876
LOSS: 1.2264111042022705
LOSS: 1.2247720956802368
LOSS: 1.223114252090454
LOSS: 1.2214009761810303
LOSS: 1.2196018695831299
LOSS: 1.2176756858825684
LOSS: 1.2155944108963013
LOSS: 1.213333010673523
LOSS: 1.2108715772628784
LOSS: 1.2082023620605469
LOSS: 1.205331802368164
LOSS: 1.2022714614868164
LOSS: 1.1990432739257812
LOSS: 1.1956713199615479
LOSS: 1.1921865940093994
LOSS: 1.188625693321228
LOSS: 1.185027837753296
LOSS: 1.1814402341842651
LOSS: 1.177905559539795
LOSS: 1.1744580268859863
LOSS: 1.1711328029632568
LOSS: 1.1679548025131226
LOSS: 1.1649307012557983
LOSS: 1.1620490550994873
LOSS: 1.1593023538589478
LOSS: 1.156678557395935
LOSS: 1.1541852951049805
LOSS: 1.151819109916687
LOSS: 1.1495798826217651
LOSS: 1.1474568843841553
LOSS: 1.145439624786377
LOSS: 1.1435006856918335
LOSS: 1.1416397094726562
LOSS: 1.1398632526397705
LOSS: 1.1381713151931763
LOSS: 1.1365736722946167
LOSS: 1.1350706815719604
LOSS: 1.133658528327942
LOSS: 1.1323225498199463
LOSS: 1.131056547164917
LOSS: 1.1298474073410034
LOSS: 1.1287007331848145
LOSS: 1.127609372138977
LOSS: 1.1265625953674316
LOSS: 1.1255481243133545
LOSS: 1.1245514154434204
LOSS: 1.123563528060913
LOSS: 1.1225758790969849
LOSS: 1.1215903759002686
LOSS: 1.120605707168579
LOSS: 1.1196308135986328
LOSS: 1.1186773777008057
LOSS: 1.3785216808319092
LOSS: 1.3724493980407715
LOSS: 1.366777777671814
LOSS: 1.3615448474884033
LOSS: 1.356787085533142
LOSS: 1.3525357246398926
LOSS: 1.3488105535507202
LOSS: 1.3456144332885742
LOSS: 1.3429239988327026
LOSS: 1.3406798839569092
LOSS: 1.338787317276001
LOSS: 1.3371236324310303
LOSS: 1.3355571031570435
LOSS: 1.3339735269546509
LOSS: 1.3322882652282715
LOSS: 1.3304519653320312
LOSS: 1.3284456729888916
LOSS: 1.3262691497802734
LOSS: 1.3239394426345825
LOSS: 1.321480631828308
LOSS: 1.3189215660095215
LOSS: 1.3162921667099
LOSS: 1.313623070716858
LOSS: 1.3109383583068848
LOSS: 1.3082655668258667
LOSS: 1.3056211471557617
LOSS: 1.3030203580856323
LOSS: 1.3004711866378784
LOSS: 1.2979775667190552
LOSS: 1.2955355644226074
LOSS: 1.2931374311447144
LOSS: 1.2907698154449463
LOSS: 1.2884141206741333
LOSS: 1.2694766521453857
LOSS: 1.2676657438278198
LOSS: 1.26589035987854
LOSS: 1.2641385793685913
LOSS: 1.2624014616012573
LOSS: 1.2606737613677979
LOSS: 1.2589519023895264
LOSS: 1.2572333812713623
LOSS: 1.255520224571228
LOSS: 1.2538115978240967
LOSS: 1.252103567123413
LOSS: 1.2503905296325684
LOSS: 1.2486635446548462
LOSS: 1.246908187866211
LOSS: 1.2451097965240479
LOSS: 1.2432442903518677
LOSS: 1.241283655166626
LOSS: 1.2391984462738037
LOSS: 1.2369558811187744
LOSS: 1.2345219850540161
LOSS: 1.2318679094314575
LOSS: 1.2289639711380005
LOSS: 1.2257943153381348
LOSS: 1.222348690032959
LOSS: 1.218632698059082
LOSS: 1.214663028717041
LOSS: 1.2104657888412476
LOSS: 1.2060754299163818
LOSS: 1.2015304565429688
LOSS: 1.1968671083450317
LOSS: 1.1921277046203613
LOSS: 1.1873538494110107
LOSS: 1.1825892925262451
LOSS: 1.1778806447982788
LOSS: 1.1732730865478516
LOSS: 1.1688127517700195
LOSS: 1.1645431518554688
LOSS: 1.160499930381775
LOSS: 1.1567106246948242
LOSS: 1.1531926393508911
LOSS: 1.1499522924423218
LOSS: 1.1469885110855103
LOSS: 1.1443055868148804
LOSS: 1.1418828964233398
LOSS: 1.1396958827972412
LOSS: 1.1377058029174805
LOSS: 1.1358715295791626
LOSS: 1.134158730506897
LOSS: 1.1325452327728271
LOSS: 1.1310101747512817
LOSS: 1.1295377016067505
LOSS: 1.1281148195266724
LOSS: 1.1267224550247192
LOSS: 1.1253491640090942
LOSS: 1.1239948272705078
LOSS: 1.1226664781570435
LOSS: 1.121375560760498
LOSS: 1.1201353073120117
LOSS: 1.1189548969268799
LOSS: 1.1178369522094727
LOSS: 1.1167752742767334
LOSS: 1.1157574653625488
LOSS: 1.1147656440734863
LOSS: 1.113787293434143
LOSS: 1.1128147840499878
LOSS: 1.1118484735488892
LOSS: 1.1108958721160889
LOSS: 1.385347843170166
LOSS: 1.3794187307357788
LOSS: 1.3738961219787598
LOSS: 1.368817687034607
LOSS: 1.364220142364502
LOSS: 1.360133171081543
LOSS: 1.3565747737884521
LOSS: 1.3535460233688354
LOSS: 1.3510184288024902
LOSS: 1.348928451538086
LOSS: 1.3471745252609253
LOSS: 1.345628023147583
LOSS: 1.3441554307937622
LOSS: 1.3426427841186523
LOSS: 1.3410102128982544
LOSS: 1.339212417602539
LOSS: 1.337233304977417
LOSS: 1.3350775241851807
LOSS: 1.3327635526657104
LOSS: 1.330316185951233
LOSS: 1.3277655839920044
LOSS: 1.3251416683197021
LOSS: 1.3224724531173706
LOSS: 1.3197838068008423
LOSS: 1.31709623336792
LOSS: 1.3144235610961914
LOSS: 1.3117748498916626
LOSS: 1.3091506958007812
LOSS: 1.306545615196228
LOSS: 1.3039463758468628
LOSS: 1.301330804824829
LOSS: 1.2986726760864258
LOSS: 1.2959399223327637
LOSS: 1.2930941581726074
LOSS: 1.2900992631912231
LOSS: 1.286919116973877
LOSS: 1.2835224866867065
LOSS: 1.279884696006775
LOSS: 1.2759889364242554
LOSS: 1.2718287706375122
LOSS: 1.2674108743667603
LOSS: 1.2627519369125366
LOSS: 1.2578787803649902
LOSS: 1.2528254985809326
LOSS: 1.2476304769515991
LOSS: 1.24233078956604
LOSS: 1.236959457397461
LOSS: 1.2315452098846436
LOSS: 1.2261115312576294
LOSS: 1.2206796407699585
LOSS: 1.2152646780014038
LOSS: 1.209877371788025
LOSS: 1.2045254707336426
LOSS: 1.19921875
LOSS: 1.1939727067947388
LOSS: 1.1888089179992676
LOSS: 1.1837533712387085
LOSS: 1.1788296699523926
LOSS: 1.1740448474884033
LOSS: 1.169384241104126
LOSS: 1.16482675075531
LOSS: 1.160351276397705
LOSS: 1.1559381484985352
LOSS: 1.1515758037567139
LOSS: 1.1472615003585815
LOSS: 1.1429998874664307
LOSS: 1.1388016939163208
LOSS: 1.1346747875213623
LOSS: 1.1306312084197998
LOSS: 1.126690149307251
LOSS: 1.122873067855835
LOSS: 1.1191929578781128
LOSS: 1.1156537532806396
LOSS: 1.1122527122497559
LOSS: 1.108977198600769
LOSS: 1.1058173179626465
LOSS: 1.1027745008468628
LOSS: 1.0998635292053223
LOSS: 1.097104787826538
LOSS: 1.0945141315460205
LOSS: 1.092099666595459
LOSS: 1.0898656845092773
LOSS: 1.087815284729004
LOSS: 1.0859431028366089
LOSS: 1.084228277206421
LOSS: 1.0826349258422852
LOSS: 1.0811246633529663
LOSS: 1.0796669721603394
LOSS: 1.078244686126709
LOSS: 1.0768516063690186
LOSS: 1.0754860639572144
LOSS: 1.0741502046585083
LOSS: 1.0728479623794556
LOSS: 1.0715851783752441
LOSS: 1.0703665018081665
LOSS: 1.069190263748169
LOSS: 1.0680466890335083
LOSS: 1.06692636013031
LOSS: 1.0658279657363892
LOSS: 1.0647577047348022
LOSS: 1.3691240549087524
LOSS: 1.3628761768341064
LOSS: 1.3570184707641602
LOSS: 1.351588249206543
LOSS: 1.3466209173202515
LOSS: 1.3421450853347778
LOSS: 1.338179111480713
LOSS: 1.3347207307815552
LOSS: 1.3317416906356812
LOSS: 1.329176664352417
LOSS: 1.3269245624542236
LOSS: 1.324861764907837
LOSS: 1.3228628635406494
LOSS: 1.3208248615264893
LOSS: 1.3186782598495483
LOSS: 1.3163871765136719
LOSS: 1.3139420747756958
LOSS: 1.3113504648208618
LOSS: 1.3086330890655518
LOSS: 1.3058183193206787
LOSS: 1.3029346466064453
LOSS: 1.3000166416168213
LOSS: 1.2970954179763794
LOSS: 1.2941968441009521
LOSS: 1.2913498878479004
LOSS: 1.288572907447815
LOSS: 1.2858812808990479
LOSS: 1.2832831144332886
LOSS: 1.2807825803756714
LOSS: 1.2783759832382202
LOSS: 1.2760539054870605
LOSS: 1.273802638053894
LOSS: 1.2716108560562134
LOSS: 1.2694648504257202
LOSS: 1.2673553228378296
LOSS: 1.2652772665023804
LOSS: 1.2632261514663696
LOSS: 1.2612062692642212
LOSS: 1.2592138051986694
LOSS: 1.257250189781189
LOSS: 1.255313754081726
LOSS: 1.2534092664718628
LOSS: 1.2515294551849365
LOSS: 1.2496676445007324
LOSS: 1.2478082180023193
LOSS: 1.2459354400634766
LOSS: 1.2440223693847656
LOSS: 1.2420495748519897
LOSS: 1.239990472793579
LOSS: 1.2378342151641846
LOSS: 1.2355693578720093
LOSS: 1.2331925630569458
LOSS: 1.2307183742523193
LOSS: 1.2281419038772583
LOSS: 1.2254807949066162
LOSS: 1.2227423191070557
LOSS: 1.2199379205703735
LOSS: 1.2170754671096802
LOSS: 1.2141865491867065
LOSS: 1.2113021612167358
LOSS: 1.2084516286849976
LOSS: 1.2056527137756348
LOSS: 1.2029424905776978
LOSS: 1.2003334760665894
LOSS: 1.1978538036346436
LOSS: 1.1955033540725708
LOSS: 1.1932915449142456
LOSS: 1.1912304162979126
LOSS: 1.1893200874328613
LOSS: 1.1875159740447998
LOSS: 1.1858323812484741
LOSS: 1.1842234134674072
LOSS: 1.1826825141906738
LOSS: 1.1811704635620117
LOSS: 1.1796773672103882
LOSS: 1.178168773651123
LOSS: 1.1766550540924072
LOSS: 1.1751033067703247
LOSS: 1.173521637916565
LOSS: 1.1719121932983398
LOSS: 1.1702804565429688
LOSS: 1.1686164140701294
LOSS: 1.1669461727142334
LOSS: 1.165259599685669
LOSS: 1.1635327339172363
LOSS: 1.1617552042007446
LOSS: 1.1599217653274536
LOSS: 1.1580398082733154
LOSS: 1.1561452150344849
LOSS: 1.1542613506317139
LOSS: 1.1524022817611694
LOSS: 1.1505987644195557
LOSS: 1.1488449573516846
LOSS: 1.1471325159072876
LOSS: 1.1454356908798218
LOSS: 1.14374577999115
LOSS: 1.1420608758926392
LOSS: 1.1403820514678955
LOSS: 1.1387091875076294
LOSS: 1.1370660066604614
LOSS: 1.3751647472381592
LOSS: 1.369048833847046
LOSS: 1.363328218460083
LOSS: 1.3580409288406372
LOSS: 1.353222370147705
LOSS: 1.348902940750122
LOSS: 1.3451013565063477
LOSS: 1.3418200016021729
LOSS: 1.3390330076217651
LOSS: 1.3366820812225342
LOSS: 1.3346706628799438
LOSS: 1.3328750133514404
LOSS: 1.3311659097671509
LOSS: 1.3294318914413452
LOSS: 1.327592372894287
LOSS: 1.3256020545959473
LOSS: 1.3234429359436035
LOSS: 1.321119785308838
LOSS: 1.318650722503662
LOSS: 1.3160614967346191
LOSS: 1.3133823871612549
LOSS: 1.3106458187103271
LOSS: 1.3078840970993042
LOSS: 1.3051257133483887
LOSS: 1.3023974895477295
LOSS: 1.2997208833694458
LOSS: 1.297110915184021
LOSS: 1.2945786714553833
LOSS: 1.2921288013458252
LOSS: 1.2897578477859497
LOSS: 1.287458062171936
LOSS: 1.28521728515625
LOSS: 1.2830169200897217
LOSS: 1.2808371782302856
LOSS: 1.2786610126495361
LOSS: 1.2764689922332764
LOSS: 1.2742455005645752
LOSS: 1.2719736099243164
LOSS: 1.2696415185928345
LOSS: 1.2672337293624878
LOSS: 1.2647383213043213
LOSS: 1.2621407508850098
LOSS: 1.2594231367111206
LOSS: 1.256568431854248
LOSS: 1.2535622119903564
LOSS: 1.2503918409347534
LOSS: 1.2470519542694092
LOSS: 1.2435369491577148
LOSS: 1.2398511171340942
LOSS: 1.2360111474990845
LOSS: 1.2320314645767212
LOSS: 1.22794508934021
LOSS: 1.2237941026687622
LOSS: 1.2196283340454102
LOSS: 1.2155050039291382
LOSS: 1.2114834785461426
LOSS: 1.2076128721237183
LOSS: 1.2039343118667603
LOSS: 1.2004698514938354
LOSS: 1.1972239017486572
LOSS: 1.194183349609375
LOSS: 1.1913193464279175
LOSS: 1.1885923147201538
LOSS: 1.1859593391418457
LOSS: 1.241284966468811
LOSS: 1.2355279922485352
LOSS: 1.229622483253479
LOSS: 1.223595380783081
LOSS: 1.217485785484314
LOSS: 1.2113434076309204
LOSS: 1.20522141456604
LOSS: 1.1991755962371826
LOSS: 1.1932653188705444
LOSS: 1.1875553131103516
LOSS: 1.1821117401123047
LOSS: 1.1769943237304688
LOSS: 1.1722506284713745
LOSS: 1.1679050922393799
LOSS: 1.163958191871643
LOSS: 1.1603851318359375
LOSS: 1.1571446657180786
LOSS: 1.1541889905929565
LOSS: 1.1514726877212524
LOSS: 1.1489545106887817
LOSS: 1.1466028690338135
LOSS: 1.1443843841552734
LOSS: 1.1422655582427979
LOSS: 1.1402238607406616
LOSS: 1.138245701789856
LOSS: 1.1363410949707031
LOSS: 1.1345233917236328
LOSS: 1.1328070163726807
LOSS: 1.131203055381775
LOSS: 1.1297107934951782
LOSS: 1.128322720527649
LOSS: 1.1270222663879395
LOSS: 1.125784993171692
LOSS: 1.124587893486023
LOSS: 1.1234081983566284
LOSS: 1.1222376823425293
LOSS: 1.121071457862854
LOSS: 1.1199164390563965
LOSS: 1.1187795400619507
LOSS: 1.1176650524139404
LOSS: 1.116579532623291
LOSS: 1.1155234575271606
LOSS: 1.1144970655441284
LOSS: 1.1134895086288452
LOSS: 1.1125154495239258
LOSS: 1.1115479469299316
LOSS: 1.1106001138687134
LOSS: 1.1096630096435547
LOSS: 1.108730435371399
LOSS: 1.1078044176101685
LOSS: 1.106886863708496
LOSS: 1.1059858798980713
LOSS: 1.1051008701324463
LOSS: 1.1042389869689941
LOSS: 1.103388786315918
LOSS: 1.1025574207305908
LOSS: 1.101763129234314
LOSS: 1.1010128259658813
LOSS: 1.1003096103668213
LOSS: 1.099661111831665
LOSS: 1.0990509986877441
LOSS: 1.0984430313110352
LOSS: 1.0978230237960815
LOSS: 1.097178339958191
LOSS: 1.0965312719345093
LOSS: 1.095865249633789
LOSS: 1.0952032804489136
LOSS: 1.3773622512817383
LOSS: 1.3712270259857178
LOSS: 1.3654886484146118
LOSS: 1.3601844310760498
LOSS: 1.355351448059082
LOSS: 1.3510198593139648
LOSS: 1.347213625907898
LOSS: 1.343937635421753
LOSS: 1.3411762714385986
LOSS: 1.338877558708191
LOSS: 1.3369593620300293
LOSS: 1.3353052139282227
LOSS: 1.3337880373001099
LOSS: 1.3322863578796387
LOSS: 1.3307093381881714
LOSS: 1.3289998769760132
LOSS: 1.3271276950836182
LOSS: 1.3250935077667236
LOSS: 1.3229109048843384
LOSS: 1.3206062316894531
LOSS: 1.3182100057601929
LOSS: 1.3157570362091064
LOSS: 1.3132812976837158
LOSS: 1.3108142614364624
LOSS: 1.3083837032318115
LOSS: 1.306012511253357
LOSS: 1.3037188053131104
LOSS: 1.3015155792236328
LOSS: 1.2994040250778198
LOSS: 1.2973878383636475
LOSS: 1.295459270477295
LOSS: 1.2936104536056519
LOSS: 1.291830062866211
LOSS: 1.2901053428649902
LOSS: 1.288426160812378
LOSS: 1.2867852449417114
LOSS: 1.2851759195327759
LOSS: 1.2835968732833862
LOSS: 1.2820508480072021
LOSS: 1.2805416584014893
LOSS: 1.2790790796279907
LOSS: 1.277672290802002
LOSS: 1.2763311862945557
LOSS: 1.2750661373138428
LOSS: 1.2738866806030273
LOSS: 1.272798776626587
LOSS: 1.2718055248260498
LOSS: 1.270907998085022
LOSS: 1.2701036930084229
LOSS: 1.2693870067596436
LOSS: 1.2687466144561768
LOSS: 1.2681682109832764
LOSS: 1.2676457166671753
LOSS: 1.26716148853302
LOSS: 1.26670503616333
LOSS: 1.266265630722046
LOSS: 1.2658331394195557
LOSS: 1.2654026746749878
LOSS: 1.2649643421173096
LOSS: 1.2645162343978882
LOSS: 1.2640602588653564
LOSS: 1.2635858058929443
LOSS: 1.2631059885025024
LOSS: 1.2626203298568726
LOSS: 1.262131929397583
LOSS: 1.261643648147583
LOSS: 1.2611545324325562
LOSS: 1.260658621788025
LOSS: 1.2601467370986938
LOSS: 1.259604811668396
LOSS: 1.2590049505233765
LOSS: 1.2583160400390625
LOSS: 1.2575023174285889
LOSS: 1.256526231765747
LOSS: 1.2553601264953613
LOSS: 1.2539643049240112
LOSS: 1.2523258924484253
LOSS: 1.2504328489303589
LOSS: 1.2483007907867432
LOSS: 1.2459487915039062
LOSS: 1.2434102296829224
LOSS: 1.2407375574111938
LOSS: 1.2379848957061768
LOSS: 1.235205888748169
LOSS: 1.2324622869491577
LOSS: 1.2298097610473633
LOSS: 1.2272977828979492
LOSS: 1.2249531745910645
LOSS: 1.2228062152862549
LOSS: 1.2208620309829712
LOSS: 1.2191094160079956
LOSS: 1.2175315618515015
LOSS: 1.2160991430282593
LOSS: 1.2147893905639648
LOSS: 1.213576316833496
LOSS: 1.2124313116073608
LOSS: 1.2113184928894043
LOSS: 1.2102168798446655
LOSS: 1.209113597869873
LOSS: 1.2080014944076538
LOSS: 1.3773622512817383
LOSS: 1.3712270259857178
LOSS: 1.3654886484146118
LOSS: 1.3601844310760498
LOSS: 1.355351448059082
LOSS: 1.3510198593139648
LOSS: 1.347213625907898
LOSS: 1.343937635421753
LOSS: 1.3411762714385986
LOSS: 1.338877558708191
LOSS: 1.3369593620300293
LOSS: 1.3353052139282227
LOSS: 1.3337880373001099
LOSS: 1.3322863578796387
LOSS: 1.3307093381881714
LOSS: 1.3289998769760132
LOSS: 1.3271276950836182
LOSS: 1.3250935077667236
LOSS: 1.3229109048843384
LOSS: 1.3206062316894531
LOSS: 1.3182100057601929
LOSS: 1.3157570362091064
LOSS: 1.3132812976837158
LOSS: 1.3108142614364624
LOSS: 1.3083837032318115
LOSS: 1.306012511253357
LOSS: 1.3037188053131104
LOSS: 1.3015155792236328
LOSS: 1.2994040250778198
LOSS: 1.2973878383636475
LOSS: 1.295459270477295
LOSS: 1.2936104536056519
LOSS: 1.291830062866211
LOSS: 1.2901053428649902
LOSS: 1.288426160812378
LOSS: 1.2867852449417114
LOSS: 1.2851759195327759
LOSS: 1.2835968732833862
LOSS: 1.2820508480072021
LOSS: 1.2805416584014893
LOSS: 1.2790790796279907
LOSS: 1.277672290802002
LOSS: 1.2763311862945557
LOSS: 1.2750661373138428
LOSS: 1.2738866806030273
LOSS: 1.272798776626587
LOSS: 1.2718055248260498
LOSS: 1.270907998085022
LOSS: 1.2701036930084229
LOSS: 1.2693870067596436
LOSS: 1.2687466144561768
LOSS: 1.2681682109832764
LOSS: 1.2676457166671753
LOSS: 1.26716148853302
LOSS: 1.26670503616333
LOSS: 1.266265630722046
LOSS: 1.2658331394195557
LOSS: 1.2654026746749878
LOSS: 1.2649643421173096
LOSS: 1.2645162343978882
LOSS: 1.2640602588653564
LOSS: 1.2635858058929443
LOSS: 1.2631059885025024
LOSS: 1.2626203298568726
LOSS: 1.262131929397583
LOSS: 1.261643648147583
LOSS: 1.2611545324325562
LOSS: 1.260658621788025
LOSS: 1.2601467370986938
LOSS: 1.259604811668396
LOSS: 1.2590049505233765
LOSS: 1.2583160400390625
LOSS: 1.2575023174285889
LOSS: 1.256526231765747
LOSS: 1.2553601264953613
LOSS: 1.2539643049240112
LOSS: 1.2523258924484253
LOSS: 1.2504328489303589
LOSS: 1.2483007907867432
LOSS: 1.2459487915039062
LOSS: 1.2434102296829224
LOSS: 1.2407375574111938
LOSS: 1.2379848957061768
LOSS: 1.235205888748169
LOSS: 1.2324622869491577
LOSS: 1.2298097610473633
LOSS: 1.2272977828979492
LOSS: 1.2249531745910645
LOSS: 1.2228062152862549
LOSS: 1.2208620309829712
LOSS: 1.2191094160079956
LOSS: 1.2175315618515015
LOSS: 1.2160991430282593
LOSS: 1.2147893905639648
LOSS: 1.213576316833496
LOSS: 1.2124313116073608
LOSS: 1.2113184928894043
LOSS: 1.2102168798446655
LOSS: 1.209113597869873
LOSS: 1.2080014944076538
LOSS: 1.366773009300232
LOSS: 1.3603516817092896
LOSS: 1.3543106317520142
LOSS: 1.348686695098877
LOSS: 1.3435159921646118
LOSS: 1.3388280868530273
LOSS: 1.3346445560455322
LOSS: 1.3309717178344727
LOSS: 1.3277919292449951
LOSS: 1.3250572681427002
LOSS: 1.3226861953735352
LOSS: 1.3205662965774536
LOSS: 1.3185734748840332
LOSS: 1.3165934085845947
LOSS: 1.3145369291305542
LOSS: 1.312347173690796
LOSS: 1.310001015663147
LOSS: 1.3074942827224731
LOSS: 1.3048417568206787
LOSS: 1.302068829536438
LOSS: 1.2992058992385864
LOSS: 1.2962861061096191
LOSS: 1.2933440208435059
LOSS: 1.2904109954833984
LOSS: 1.287514090538025
LOSS: 1.2846791744232178
LOSS: 1.2819236516952515
LOSS: 1.2792580127716064
LOSS: 1.2766882181167603
LOSS: 1.274213194847107
LOSS: 1.2718257904052734
LOSS: 1.2695159912109375
LOSS: 1.2672680616378784
LOSS: 1.2650688886642456
LOSS: 1.2629066705703735
LOSS: 1.260772466659546
LOSS: 1.258655071258545
LOSS: 1.256546974182129
LOSS: 1.2544420957565308
LOSS: 1.252337098121643
LOSS: 1.2502297163009644
LOSS: 1.2481156587600708
LOSS: 1.2459921836853027
LOSS: 1.2438501119613647
LOSS: 1.2416844367980957
LOSS: 1.2394828796386719
LOSS: 1.2372349500656128
LOSS: 1.2349355220794678
LOSS: 1.232581615447998
LOSS: 1.2301795482635498
LOSS: 1.2277419567108154
LOSS: 1.225284218788147
LOSS: 1.2228193283081055
LOSS: 1.2203667163848877
LOSS: 1.2179350852966309
LOSS: 1.215542197227478
LOSS: 1.2131903171539307
LOSS: 1.2108937501907349
LOSS: 1.2086634635925293
LOSS: 1.2065081596374512
LOSS: 1.20443856716156
LOSS: 1.2024537324905396
LOSS: 1.2005615234375
LOSS: 1.198759913444519
LOSS: 1.1921277046203613
LOSS: 1.1873538494110107
LOSS: 1.1825892925262451
LOSS: 1.1778806447982788
LOSS: 1.1732730865478516
LOSS: 1.1688127517700195
LOSS: 1.1645431518554688
LOSS: 1.160499930381775
LOSS: 1.1567106246948242
LOSS: 1.1531926393508911
LOSS: 1.1499522924423218
LOSS: 1.1469885110855103
LOSS: 1.1443055868148804
LOSS: 1.1418828964233398
LOSS: 1.1396958827972412
LOSS: 1.1377058029174805
LOSS: 1.1358715295791626
LOSS: 1.134158730506897
LOSS: 1.1325452327728271
LOSS: 1.1310101747512817
LOSS: 1.1295377016067505
LOSS: 1.1281148195266724
LOSS: 1.1267224550247192
LOSS: 1.1253491640090942
LOSS: 1.1239948272705078
LOSS: 1.1226664781570435
LOSS: 1.121375560760498
LOSS: 1.1201353073120117
LOSS: 1.1189548969268799
LOSS: 1.1178369522094727
LOSS: 1.1167752742767334
LOSS: 1.1157574653625488
LOSS: 1.1147656440734863
LOSS: 1.113787293434143
LOSS: 1.1128147840499878
LOSS: 1.1118484735488892
LOSS: 1.1108958721160889
LOSS: 1.3662248849868774
LOSS: 1.359713077545166
LOSS: 1.353579044342041
LOSS: 1.3478598594665527
LOSS: 1.3425897359848022
LOSS: 1.3378019332885742
LOSS: 1.333518385887146
LOSS: 1.329748272895813
LOSS: 1.3264800310134888
LOSS: 1.323671817779541
LOSS: 1.3212494850158691
LOSS: 1.3191076517105103
LOSS: 1.3171250820159912
LOSS: 1.3151824474334717
LOSS: 1.3131837844848633
LOSS: 1.3110663890838623
LOSS: 1.3087944984436035
LOSS: 1.3063626289367676
LOSS: 1.3037810325622559
LOSS: 1.3010722398757935
LOSS: 1.2982667684555054
LOSS: 1.295399785041809
LOSS: 1.2925074100494385
LOSS: 1.2896230220794678
LOSS: 1.2867779731750488
LOSS: 1.2839981317520142
LOSS: 1.2813061475753784
LOSS: 1.2787151336669922
LOSS: 1.2762352228164673
LOSS: 1.2738666534423828
LOSS: 1.2716072797775269
LOSS: 1.2694463729858398
LOSS: 1.2673742771148682
LOSS: 1.2653778791427612
LOSS: 1.263444185256958
LOSS: 1.2615635395050049
LOSS: 1.2597274780273438
LOSS: 1.2579327821731567
LOSS: 1.2561774253845215
LOSS: 1.254463791847229
LOSS: 1.252794623374939
LOSS: 1.251171350479126
LOSS: 1.2495996952056885
LOSS: 1.2480772733688354
LOSS: 1.2466052770614624
LOSS: 1.2451761960983276
LOSS: 1.2437814474105835
LOSS: 1.242408275604248
LOSS: 1.2410426139831543
LOSS: 1.23966383934021
LOSS: 1.238254189491272
LOSS: 1.2367942333221436
LOSS: 1.2352631092071533
LOSS: 1.233644723892212
LOSS: 1.2319293022155762
LOSS: 1.2301069498062134
LOSS: 1.2281798124313354
LOSS: 1.2261433601379395
LOSS: 1.2240060567855835
LOSS: 1.2217639684677124
LOSS: 1.2194111347198486
LOSS: 1.216945767402649
LOSS: 1.2143654823303223
LOSS: 1.2116678953170776
LOSS: 1.2088550329208374
LOSS: 1.2059240341186523
LOSS: 1.2028800249099731
LOSS: 1.199725866317749
LOSS: 1.19647216796875
LOSS: 1.1931281089782715
LOSS: 1.1897163391113281
LOSS: 1.1862664222717285
LOSS: 1.1828222274780273
LOSS: 1.1794315576553345
LOSS: 1.1761436462402344
LOSS: 1.1729958057403564
LOSS: 1.1700137853622437
LOSS: 1.1672133207321167
LOSS: 1.1645954847335815
LOSS: 1.1621567010879517
LOSS: 1.1598888635635376
LOSS: 1.1577805280685425
LOSS: 1.1558220386505127
LOSS: 1.1540002822875977
LOSS: 1.1523038148880005
LOSS: 1.1507130861282349
LOSS: 1.1492141485214233
LOSS: 1.147798776626587
LOSS: 1.1464533805847168
LOSS: 1.1451690196990967
LOSS: 1.143930435180664
LOSS: 1.1427323818206787
LOSS: 1.141570806503296
LOSS: 1.1404492855072021
LOSS: 1.1393673419952393
LOSS: 1.1383310556411743
LOSS: 1.1373437643051147
LOSS: 1.136397123336792
LOSS: 1.1354823112487793
LOSS: 1.1345994472503662
LOSS: 1.3751647472381592
LOSS: 1.369048833847046
LOSS: 1.363328218460083
LOSS: 1.3580409288406372
LOSS: 1.353222370147705
LOSS: 1.348902940750122
LOSS: 1.3451013565063477
LOSS: 1.3418200016021729
LOSS: 1.3390330076217651
LOSS: 1.3366820812225342
LOSS: 1.3346706628799438
LOSS: 1.3328750133514404
LOSS: 1.3311659097671509
LOSS: 1.3294318914413452
LOSS: 1.327592372894287
LOSS: 1.3256020545959473
LOSS: 1.3234429359436035
LOSS: 1.321119785308838
LOSS: 1.318650722503662
LOSS: 1.3160614967346191
LOSS: 1.3133823871612549
LOSS: 1.3106458187103271
LOSS: 1.3078840970993042
LOSS: 1.3051257133483887
LOSS: 1.3023974895477295
LOSS: 1.2997208833694458
LOSS: 1.297110915184021
LOSS: 1.2945786714553833
LOSS: 1.2921288013458252
LOSS: 1.2897578477859497
LOSS: 1.287458062171936
LOSS: 1.28521728515625
LOSS: 1.2830169200897217
LOSS: 1.2808371782302856
LOSS: 1.2786610126495361
LOSS: 1.2764689922332764
LOSS: 1.2742455005645752
LOSS: 1.2719736099243164
LOSS: 1.2696415185928345
LOSS: 1.2672337293624878
LOSS: 1.2647383213043213
LOSS: 1.2621407508850098
LOSS: 1.2594231367111206
LOSS: 1.256568431854248
LOSS: 1.2535622119903564
LOSS: 1.2503918409347534
LOSS: 1.2470519542694092
LOSS: 1.2435369491577148
LOSS: 1.2398511171340942
LOSS: 1.2360111474990845
LOSS: 1.2320314645767212
LOSS: 1.22794508934021
LOSS: 1.2237941026687622
LOSS: 1.2196283340454102
LOSS: 1.2155050039291382
LOSS: 1.2114834785461426
LOSS: 1.2076128721237183
LOSS: 1.2039343118667603
LOSS: 1.2004698514938354
LOSS: 1.1972239017486572
LOSS: 1.194183349609375
LOSS: 1.1913193464279175
LOSS: 1.1885923147201538
LOSS: 1.1859593391418457
LOSS: 1.183377742767334
LOSS: 1.1808068752288818
LOSS: 1.1782015562057495
LOSS: 1.1755229234695435
LOSS: 1.172736644744873
LOSS: 1.1698054075241089
LOSS: 1.1666977405548096
LOSS: 1.1633857488632202
LOSS: 1.1598519086837769
LOSS: 1.15609610080719
LOSS: 1.1521375179290771
LOSS: 1.1480141878128052
LOSS: 1.1437731981277466
LOSS: 1.1394635438919067
LOSS: 1.135135293006897
LOSS: 1.1308410167694092
LOSS: 1.1266316175460815
LOSS: 1.1225552558898926
LOSS: 1.1186554431915283
LOSS: 1.1149672269821167
LOSS: 1.111506700515747
LOSS: 1.1082627773284912
LOSS: 1.1051896810531616
LOSS: 1.1022284030914307
LOSS: 1.099347472190857
LOSS: 1.0965474843978882
LOSS: 1.0938433408737183
LOSS: 1.0912381410598755
LOSS: 1.0887196063995361
LOSS: 1.0862716436386108
LOSS: 1.0838847160339355
LOSS: 1.0815528631210327
LOSS: 1.0792772769927979
LOSS: 1.0770710706710815
LOSS: 1.0749598741531372
LOSS: 1.0729767084121704
LOSS: 1.3756234645843506
LOSS: 1.3695030212402344
LOSS: 1.3637809753417969
LOSS: 1.3584952354431152
LOSS: 1.3536818027496338
LOSS: 1.3493692874908447
LOSS: 1.3455756902694702
LOSS: 1.3422973155975342
LOSS: 1.339500904083252
LOSS: 1.3371167182922363
LOSS: 1.335037112236023
LOSS: 1.333133578300476
LOSS: 1.3312803506851196
LOSS: 1.3293761014938354
LOSS: 1.3273556232452393
LOSS: 1.3251858949661255
LOSS: 1.3228598833084106
LOSS: 1.320388674736023
LOSS: 1.3177942037582397
LOSS: 1.3151042461395264
LOSS: 1.3123505115509033
LOSS: 1.3095614910125732
LOSS: 1.3067700862884521
LOSS: 1.3040004968643188
LOSS: 1.301278829574585
LOSS: 1.298621654510498
LOSS: 1.2960423231124878
LOSS: 1.29354989528656
LOSS: 1.2911432981491089
LOSS: 1.2888240814208984
LOSS: 1.286582350730896
LOSS: 1.284411072731018
LOSS: 1.2822999954223633
LOSS: 1.2802393436431885
LOSS: 1.2782196998596191
LOSS: 1.276235818862915
LOSS: 1.2742842435836792
LOSS: 1.2723664045333862
LOSS: 1.2704851627349854
LOSS: 1.2686461210250854
LOSS: 1.266855239868164
LOSS: 1.2651207447052002
LOSS: 1.2634490728378296
LOSS: 1.2618460655212402
LOSS: 1.260318398475647
LOSS: 1.2588648796081543
LOSS: 1.257487416267395
LOSS: 1.2561769485473633
LOSS: 1.254917860031128
LOSS: 1.2536932229995728
LOSS: 1.2524902820587158
LOSS: 1.2512891292572021
LOSS: 1.2500704526901245
LOSS: 1.2488183975219727
LOSS: 1.2475171089172363
LOSS: 1.2461580038070679
LOSS: 1.2447386980056763
LOSS: 1.2432613372802734
LOSS: 1.2417269945144653
LOSS: 1.2401386499404907
LOSS: 1.2384843826293945
LOSS: 1.2367355823516846
LOSS: 1.234852910041809
LOSS: 1.232787847518921
LOSS: 1.2304846048355103
LOSS: 1.2278831005096436
LOSS: 1.2249269485473633
LOSS: 1.2215673923492432
LOSS: 1.2177677154541016
LOSS: 1.2135062217712402
LOSS: 1.2087775468826294
LOSS: 1.2035927772521973
LOSS: 1.1979877948760986
LOSS: 1.1920350790023804
LOSS: 1.1858370304107666
LOSS: 1.1795178651809692
LOSS: 1.1732101440429688
LOSS: 1.16704261302948
LOSS: 1.1611322164535522
LOSS: 1.1555755138397217
LOSS: 1.1504101753234863
LOSS: 1.1455845832824707
LOSS: 1.1410346031188965
LOSS: 1.136728048324585
LOSS: 1.1326487064361572
LOSS: 1.128785490989685
LOSS: 1.1251282691955566
LOSS: 1.121670126914978
LOSS: 1.1184114217758179
LOSS: 1.1153515577316284
LOSS: 1.1124671697616577
LOSS: 1.1096973419189453
LOSS: 1.1069724559783936
LOSS: 1.183377742767334
LOSS: 1.1808068752288818
LOSS: 1.1782015562057495
LOSS: 1.1755229234695435
LOSS: 1.172736644744873
LOSS: 1.1698054075241089
LOSS: 1.1666977405548096
LOSS: 1.1633857488632202
LOSS: 1.1598519086837769
LOSS: 1.15609610080719
LOSS: 1.1521375179290771
LOSS: 1.1480141878128052
LOSS: 1.1437731981277466
LOSS: 1.1394635438919067
LOSS: 1.135135293006897
LOSS: 1.1308410167694092
LOSS: 1.1266316175460815
LOSS: 1.1225552558898926
LOSS: 1.1186554431915283
LOSS: 1.1149672269821167
LOSS: 1.111506700515747
LOSS: 1.1082627773284912
LOSS: 1.1051896810531616
LOSS: 1.1022284030914307
LOSS: 1.099347472190857
LOSS: 1.0965474843978882
LOSS: 1.0938433408737183
LOSS: 1.0912381410598755
LOSS: 1.0887196063995361
LOSS: 1.0862716436386108
LOSS: 1.0838847160339355
LOSS: 1.0815528631210327
LOSS: 1.0792772769927979
LOSS: 1.0770710706710815
LOSS: 1.0749598741531372
LOSS: 1.0729767084121704
LOSS: 1.3756234645843506
LOSS: 1.3695030212402344
LOSS: 1.3637809753417969
LOSS: 1.3584952354431152
LOSS: 1.3536818027496338
LOSS: 1.3493692874908447
LOSS: 1.3455756902694702
LOSS: 1.3422973155975342
LOSS: 1.339500904083252
LOSS: 1.3371167182922363
LOSS: 1.335037112236023
LOSS: 1.333133578300476
LOSS: 1.3312803506851196
LOSS: 1.3293761014938354
LOSS: 1.3273556232452393
LOSS: 1.3251858949661255
LOSS: 1.3228598833084106
LOSS: 1.320388674736023
LOSS: 1.3177942037582397
LOSS: 1.3151042461395264
LOSS: 1.3123505115509033
LOSS: 1.3095614910125732
LOSS: 1.3067700862884521
LOSS: 1.3040004968643188
LOSS: 1.301278829574585
LOSS: 1.298621654510498
LOSS: 1.2960423231124878
LOSS: 1.29354989528656
LOSS: 1.2911432981491089
LOSS: 1.2888240814208984
LOSS: 1.286582350730896
LOSS: 1.284411072731018
LOSS: 1.2822999954223633
LOSS: 1.2802393436431885
LOSS: 1.2782196998596191
LOSS: 1.276235818862915
LOSS: 1.2742842435836792
LOSS: 1.2723664045333862
LOSS: 1.2704851627349854
LOSS: 1.2686461210250854
LOSS: 1.266855239868164
LOSS: 1.2651207447052002
LOSS: 1.2634490728378296
LOSS: 1.2618460655212402
LOSS: 1.260318398475647
LOSS: 1.2588648796081543
LOSS: 1.257487416267395
LOSS: 1.2561769485473633
LOSS: 1.254917860031128
LOSS: 1.2536932229995728
LOSS: 1.2524902820587158
LOSS: 1.2512891292572021
LOSS: 1.2500704526901245
LOSS: 1.2488183975219727
LOSS: 1.2475171089172363
LOSS: 1.2461580038070679
LOSS: 1.2447386980056763
LOSS: 1.2432613372802734
LOSS: 1.2417269945144653
LOSS: 1.2401386499404907
LOSS: 1.2384843826293945
LOSS: 1.2367355823516846
LOSS: 1.234852910041809
LOSS: 1.232787847518921
LOSS: 1.2304846048355103
LOSS: 1.2278831005096436
LOSS: 1.2249269485473633
LOSS: 1.2215673923492432
LOSS: 1.2177677154541016
LOSS: 1.2135062217712402
LOSS: 1.2087775468826294
LOSS: 1.2035927772521973
LOSS: 1.1979877948760986
LOSS: 1.1920350790023804
LOSS: 1.1858370304107666
LOSS: 1.1795178651809692
LOSS: 1.1732101440429688
LOSS: 1.16704261302948
LOSS: 1.1611322164535522
LOSS: 1.1555755138397217
LOSS: 1.1504101753234863
LOSS: 1.1455845832824707
LOSS: 1.1410346031188965
LOSS: 1.136728048324585
LOSS: 1.1326487064361572
LOSS: 1.128785490989685
LOSS: 1.1251282691955566
LOSS: 1.121670126914978
LOSS: 1.1184114217758179
LOSS: 1.1153515577316284
LOSS: 1.1124671697616577
LOSS: 1.1096973419189453
LOSS: 1.1069724559783936
LOSS: 1.1042546033859253
LOSS: 1.1015400886535645
LOSS: 1.0988450050354004
LOSS: 1.0961902141571045
LOSS: 1.0935921669006348
LOSS: 1.0910594463348389
LOSS: 1.0885930061340332
LOSS: 1.3785216808319092
LOSS: 1.3724493980407715
LOSS: 1.366777777671814
LOSS: 1.3615448474884033
LOSS: 1.356787085533142
LOSS: 1.3525357246398926
LOSS: 1.3488105535507202
LOSS: 1.3456144332885742
LOSS: 1.3429239988327026
LOSS: 1.3406798839569092
LOSS: 1.338787317276001
LOSS: 1.3371236324310303
LOSS: 1.3355571031570435
LOSS: 1.3339735269546509
LOSS: 1.3322882652282715
LOSS: 1.3304519653320312
LOSS: 1.3284456729888916
LOSS: 1.3262691497802734
LOSS: 1.3239394426345825
LOSS: 1.321480631828308
LOSS: 1.3189215660095215
LOSS: 1.3162921667099
LOSS: 1.313623070716858
LOSS: 1.3109383583068848
LOSS: 1.3082655668258667
LOSS: 1.3056211471557617
LOSS: 1.3030203580856323
LOSS: 1.3004711866378784
LOSS: 1.2979775667190552
LOSS: 1.2955355644226074
LOSS: 1.2931374311447144
LOSS: 1.2907698154449463
LOSS: 1.2884141206741333
LOSS: 1.2860513925552368
LOSS: 1.2836593389511108
LOSS: 1.2812124490737915
LOSS: 1.278687596321106
LOSS: 1.276057481765747
LOSS: 1.273293137550354
LOSS: 1.2703644037246704
LOSS: 1.26724112033844
LOSS: 1.2638936042785645
LOSS: 1.2602965831756592
LOSS: 1.2564326524734497
LOSS: 1.2522952556610107
LOSS: 1.247890591621399
LOSS: 1.2432421445846558
LOSS: 1.2383882999420166
LOSS: 1.23337984085083
LOSS: 1.2282769680023193
LOSS: 1.223144769668579
LOSS: 1.2180471420288086
LOSS: 1.2130435705184937
LOSS: 1.2081844806671143
LOSS: 1.203508973121643
LOSS: 1.1990387439727783
LOSS: 1.1947859525680542
LOSS: 1.1907472610473633
LOSS: 1.18691885471344
LOSS: 1.1832869052886963
LOSS: 1.1798417568206787
LOSS: 1.1765629053115845
LOSS: 1.1734322309494019
LOSS: 1.170427680015564
LOSS: 1.1675338745117188
LOSS: 1.1647332906723022
LOSS: 1.1620014905929565
LOSS: 1.1593198776245117
LOSS: 1.156682014465332
LOSS: 1.1540964841842651
LOSS: 1.1515743732452393
LOSS: 1.149125337600708
LOSS: 1.146755576133728
LOSS: 1.1444686651229858
LOSS: 1.1422690153121948
LOSS: 1.1401623487472534
LOSS: 1.138152003288269
LOSS: 1.1362369060516357
LOSS: 1.1344037055969238
LOSS: 1.1326292753219604
LOSS: 1.1308778524398804
LOSS: 1.1291126012802124
LOSS: 1.1273107528686523
LOSS: 1.1254699230194092
LOSS: 1.1236153841018677
LOSS: 1.1217875480651855
LOSS: 1.1200250387191772
LOSS: 1.118356466293335
LOSS: 1.1168006658554077
LOSS: 1.1153656244277954
LOSS: 1.1140515804290771
LOSS: 1.112847924232483
LOSS: 1.111740231513977
LOSS: 1.1107078790664673
LOSS: 1.1097415685653687
LOSS: 1.1088274717330933
LOSS: 1.107956051826477
LOSS: 1.1071186065673828
LOSS: 1.10630464553833
LOSS: 1.1055009365081787
LOSS: 1.3696554899215698
LOSS: 1.36332368850708
LOSS: 1.3573781251907349
LOSS: 1.3518567085266113
LOSS: 1.3467943668365479
LOSS: 1.3422224521636963
LOSS: 1.338159441947937
LOSS: 1.3346092700958252
LOSS: 1.3315483331680298
LOSS: 1.3289192914962769
LOSS: 1.3266292810440063
LOSS: 1.3245582580566406
LOSS: 1.3225775957107544
LOSS: 1.3205782175064087
LOSS: 1.3184797763824463
LOSS: 1.3162357807159424
LOSS: 1.3138293027877808
LOSS: 1.3112635612487793
LOSS: 1.3085561990737915
LOSS: 1.3057328462600708
LOSS: 1.3028233051300049
LOSS: 1.2998608350753784
LOSS: 1.2968757152557373
LOSS: 1.2938992977142334
LOSS: 1.2909586429595947
LOSS: 1.2880730628967285
LOSS: 1.2852615118026733
LOSS: 1.2825337648391724
LOSS: 1.2798943519592285
LOSS: 1.2773418426513672
LOSS: 1.2748700380325317
LOSS: 1.2724698781967163
LOSS: 1.2701287269592285
LOSS: 1.2678362131118774
LOSS: 1.2655807733535767
LOSS: 1.263355016708374
LOSS: 1.2611552476882935
LOSS: 1.2589797973632812
LOSS: 1.2568272352218628
LOSS: 1.2547017335891724
LOSS: 1.2526072263717651
LOSS: 1.250548005104065
LOSS: 1.248524785041809
LOSS: 1.2465368509292603
LOSS: 1.244577407836914
LOSS: 1.2426379919052124
LOSS: 1.2407032251358032
LOSS: 1.2387498617172241
LOSS: 1.23675537109375
LOSS: 1.234693169593811
LOSS: 1.2325371503829956
LOSS: 1.230269193649292
LOSS: 1.2278748750686646
LOSS: 1.2253623008728027
LOSS: 1.2227420806884766
LOSS: 1.2200394868850708
LOSS: 1.217283010482788
LOSS: 1.2145024538040161
LOSS: 1.2117372751235962
LOSS: 1.2090108394622803
LOSS: 1.2063508033752441
LOSS: 1.20377779006958
LOSS: 1.2013146877288818
LOSS: 1.198974609375
LOSS: 1.1967692375183105
LOSS: 1.1947059631347656
LOSS: 1.192779779434204
LOSS: 1.1909829378128052
LOSS: 1.1892966032028198
LOSS: 1.1876988410949707
LOSS: 1.1861610412597656
LOSS: 1.1846574544906616
LOSS: 1.183166742324829
LOSS: 1.1816723346710205
LOSS: 1.1801657676696777
LOSS: 1.1786471605300903
LOSS: 1.1771100759506226
LOSS: 1.175547480583191
LOSS: 1.173924207687378
LOSS: 1.1722073554992676
LOSS: 1.1703652143478394
LOSS: 1.1683852672576904
LOSS: 1.1662477254867554
LOSS: 1.1639339923858643
LOSS: 1.1614280939102173
LOSS: 1.158717393875122
LOSS: 1.1557953357696533
LOSS: 1.1526635885238647
LOSS: 1.14933180809021
LOSS: 1.1458250284194946
LOSS: 1.1421750783920288
LOSS: 1.1384148597717285
LOSS: 1.1345760822296143
LOSS: 1.130688190460205
LOSS: 1.1267971992492676
LOSS: 1.1970564126968384
LOSS: 1.1954528093338013
LOSS: 1.1939501762390137
LOSS: 1.1925435066223145
LOSS: 1.1912317276000977
LOSS: 1.1900168657302856
LOSS: 1.188889741897583
LOSS: 1.1878434419631958
LOSS: 1.1868540048599243
LOSS: 1.1859034299850464
LOSS: 1.1849637031555176
LOSS: 1.1840252876281738
LOSS: 1.1830852031707764
LOSS: 1.182141661643982
LOSS: 1.1811999082565308
LOSS: 1.180281162261963
LOSS: 1.1793838739395142
LOSS: 1.1785157918930054
LOSS: 1.1776800155639648
LOSS: 1.1768689155578613
LOSS: 1.1761057376861572
LOSS: 1.1753754615783691
LOSS: 1.1746788024902344
LOSS: 1.174009084701538
LOSS: 1.1733405590057373
LOSS: 1.1726874113082886
LOSS: 1.1720255613327026
LOSS: 1.1713755130767822
LOSS: 1.1707338094711304
LOSS: 1.1701157093048096
LOSS: 1.169496774673462
LOSS: 1.1688963174819946
LOSS: 1.1682939529418945
LOSS: 1.1676909923553467
LOSS: 1.1670655012130737
LOSS: 1.1664310693740845
LOSS: 1.366773009300232
LOSS: 1.3603516817092896
LOSS: 1.3543106317520142
LOSS: 1.348686695098877
LOSS: 1.3435159921646118
LOSS: 1.3388280868530273
LOSS: 1.3346445560455322
LOSS: 1.3309717178344727
LOSS: 1.3277919292449951
LOSS: 1.3250572681427002
LOSS: 1.3226861953735352
LOSS: 1.3205662965774536
LOSS: 1.3185734748840332
LOSS: 1.3165934085845947
LOSS: 1.3145369291305542
LOSS: 1.312347173690796
LOSS: 1.310001015663147
LOSS: 1.3074942827224731
LOSS: 1.3048417568206787
LOSS: 1.302068829536438
LOSS: 1.2992058992385864
LOSS: 1.2962861061096191
LOSS: 1.2933440208435059
LOSS: 1.2904109954833984
LOSS: 1.287514090538025
LOSS: 1.2846791744232178
LOSS: 1.2819236516952515
LOSS: 1.2792580127716064
LOSS: 1.2766882181167603
LOSS: 1.274213194847107
LOSS: 1.2718257904052734
LOSS: 1.2695159912109375
LOSS: 1.2672680616378784
LOSS: 1.2650688886642456
LOSS: 1.2629066705703735
LOSS: 1.260772466659546
LOSS: 1.258655071258545
LOSS: 1.256546974182129
LOSS: 1.2544420957565308
LOSS: 1.252337098121643
LOSS: 1.2502297163009644
LOSS: 1.2481156587600708
LOSS: 1.2459921836853027
LOSS: 1.2438501119613647
LOSS: 1.2416844367980957
LOSS: 1.2394828796386719
LOSS: 1.2372349500656128
LOSS: 1.2349355220794678
LOSS: 1.232581615447998
LOSS: 1.2301795482635498
LOSS: 1.2277419567108154
LOSS: 1.225284218788147
LOSS: 1.2228193283081055
LOSS: 1.2203667163848877
LOSS: 1.2179350852966309
LOSS: 1.215542197227478
LOSS: 1.2131903171539307
LOSS: 1.2108937501907349
LOSS: 1.2086634635925293
LOSS: 1.2065081596374512
LOSS: 1.20443856716156
LOSS: 1.2024537324905396
LOSS: 1.2005615234375
LOSS: 1.198759913444519
LOSS: 1.1970564126968384
LOSS: 1.1954528093338013
LOSS: 1.1939501762390137
LOSS: 1.1925435066223145
LOSS: 1.1912317276000977
LOSS: 1.1900168657302856
LOSS: 1.188889741897583
LOSS: 1.1878434419631958
LOSS: 1.1868540048599243
LOSS: 1.1859034299850464
LOSS: 1.1849637031555176
LOSS: 1.1840252876281738
LOSS: 1.1830852031707764
LOSS: 1.182141661643982
LOSS: 1.1811999082565308
LOSS: 1.180281162261963
LOSS: 1.1793838739395142
LOSS: 1.1785157918930054
LOSS: 1.1776800155639648
LOSS: 1.1768689155578613
LOSS: 1.1761057376861572
LOSS: 1.1753754615783691
LOSS: 1.1746788024902344
LOSS: 1.174009084701538
LOSS: 1.1733405590057373
LOSS: 1.1726874113082886
LOSS: 1.1720255613327026
LOSS: 1.1713755130767822
LOSS: 1.1707338094711304
LOSS: 1.1701157093048096
LOSS: 1.169496774673462
LOSS: 1.1688963174819946
LOSS: 1.1682939529418945
LOSS: 1.1676909923553467
LOSS: 1.1670655012130737
LOSS: 1.1664310693740845
LOSS: 1.3742773532867432
LOSS: 1.3681102991104126
LOSS: 1.3623374700546265
LOSS: 1.3569961786270142
LOSS: 1.3521231412887573
LOSS: 1.3477485179901123
LOSS: 1.343893051147461
LOSS: 1.3405601978302002
LOSS: 1.33772611618042
LOSS: 1.335335373878479
LOSS: 1.33329439163208
LOSS: 1.3314834833145142
LOSS: 1.3297737836837769
LOSS: 1.3280507326126099
LOSS: 1.3262310028076172
LOSS: 1.3242663145065308
LOSS: 1.3221378326416016
LOSS: 1.3198474645614624
LOSS: 1.3174145221710205
LOSS: 1.3148643970489502
LOSS: 1.312231421470642
LOSS: 1.30954909324646
LOSS: 1.3068506717681885
LOSS: 1.3041678667068481
LOSS: 1.3015276193618774
LOSS: 1.2989531755447388
LOSS: 1.2964606285095215
LOSS: 1.2940610647201538
LOSS: 1.29175865650177
LOSS: 1.2895514965057373
LOSS: 1.2874327898025513
LOSS: 1.2853918075561523
LOSS: 1.2834161520004272
LOSS: 1.2814931869506836
LOSS: 1.2796117067337036
LOSS: 1.2777634859085083
LOSS: 1.2759430408477783
LOSS: 1.274148941040039
LOSS: 1.2723801136016846
LOSS: 1.2706345319747925
LOSS: 1.2689052820205688
LOSS: 1.2671858072280884
LOSS: 1.2654699087142944
LOSS: 1.2637444734573364
LOSS: 1.2619925737380981
LOSS: 1.2601890563964844
LOSS: 1.2583060264587402
LOSS: 1.25630784034729
LOSS: 1.254158854484558
LOSS: 1.2518264055252075
LOSS: 1.2492833137512207
LOSS: 1.246508002281189
LOSS: 1.2434930801391602
LOSS: 1.2402381896972656
LOSS: 1.2367602586746216
LOSS: 1.2330783605575562
LOSS: 1.229217767715454
LOSS: 1.225203514099121
LOSS: 1.2210646867752075
LOSS: 1.2168382406234741
LOSS: 1.2125693559646606
LOSS: 1.2083029747009277
LOSS: 1.204077124595642
LOSS: 1.1999218463897705
LOSS: 1.1958563327789307
LOSS: 1.1918879747390747
LOSS: 1.1880117654800415
LOSS: 1.1842161417007446
LOSS: 1.1804864406585693
LOSS: 1.1768051385879517
LOSS: 1.1731594800949097
LOSS: 1.169546127319336
LOSS: 1.165973424911499
LOSS: 1.1624547243118286
LOSS: 1.159006118774414
LOSS: 1.1556419134140015
LOSS: 1.1523805856704712
LOSS: 1.1492472887039185
LOSS: 1.146269679069519
LOSS: 1.1434684991836548
LOSS: 1.1408615112304688
LOSS: 1.1384532451629639
LOSS: 1.136216640472412
LOSS: 1.1341074705123901
LOSS: 1.132086992263794
LOSS: 1.1301268339157104
LOSS: 1.1282025575637817
LOSS: 1.1262931823730469
LOSS: 1.1243840456008911
LOSS: 1.1224690675735474
LOSS: 1.1205500364303589
LOSS: 1.118635654449463
LOSS: 1.1167374849319458
LOSS: 1.1148654222488403
LOSS: 1.11302649974823
LOSS: 1.111222505569458
LOSS: 1.1094502210617065
LOSS: 1.1077085733413696
LOSS: 1.1060047149658203
LOSS: 1.1043529510498047
LOSS: 1.3699754476547241
LOSS: 1.3637111186981201
LOSS: 1.3578366041183472
LOSS: 1.3523892164230347
LOSS: 1.3474037647247314
LOSS: 1.342909574508667
LOSS: 1.3389254808425903
LOSS: 1.3354523181915283
LOSS: 1.3324638605117798
LOSS: 1.329899787902832
LOSS: 1.3276655673980713
LOSS: 1.325638771057129
LOSS: 1.323696255683899
LOSS: 1.3217319250106812
LOSS: 1.3196731805801392
LOSS: 1.3174811601638794
LOSS: 1.3151440620422363
LOSS: 1.3126689195632935
LOSS: 1.3100765943527222
LOSS: 1.3073952198028564
LOSS: 1.3046561479568481
LOSS: 1.3018920421600342
LOSS: 1.2991359233856201
LOSS: 1.2964175939559937
LOSS: 1.2937629222869873
LOSS: 1.2911922931671143
LOSS: 1.288723349571228
LOSS: 1.2863653898239136
LOSS: 1.284122109413147
LOSS: 1.281991720199585
LOSS: 1.2799686193466187
LOSS: 1.2780431509017944
LOSS: 1.2762037515640259
LOSS: 1.2744383811950684
LOSS: 1.2727364301681519
LOSS: 1.27108895778656
LOSS: 1.2694884538650513
LOSS: 1.2679312229156494
LOSS: 1.2664140462875366
LOSS: 1.2649341821670532
LOSS: 1.263485312461853
LOSS: 1.2620526552200317
LOSS: 1.2606091499328613
LOSS: 1.259122371673584
LOSS: 1.2575535774230957
LOSS: 1.2558585405349731
LOSS: 1.2539904117584229
LOSS: 1.2519060373306274
LOSS: 1.2495673894882202
LOSS: 1.2469518184661865
LOSS: 1.2440540790557861
LOSS: 1.2408909797668457
LOSS: 1.2374978065490723
LOSS: 1.2339290380477905
LOSS: 1.2302519083023071
LOSS: 1.226535677909851
LOSS: 1.2228507995605469
LOSS: 1.219262719154358
LOSS: 1.215813159942627
LOSS: 1.2125396728515625
LOSS: 1.209437608718872
LOSS: 1.2065088748931885
LOSS: 1.203749656677246
LOSS: 1.2011524438858032
LOSS: 1.1987050771713257
LOSS: 1.1963999271392822
LOSS: 1.1942288875579834
LOSS: 1.1921676397323608
LOSS: 1.1902062892913818
LOSS: 1.188328504562378
LOSS: 1.1865131855010986
LOSS: 1.184751272201538
LOSS: 1.1830264329910278
LOSS: 1.1813342571258545
LOSS: 1.1796696186065674
LOSS: 1.178005337715149
LOSS: 1.1763279438018799
LOSS: 1.1745938062667847
LOSS: 1.1727681159973145
LOSS: 1.1708345413208008
LOSS: 1.1687719821929932
LOSS: 1.1666008234024048
LOSS: 1.1643320322036743
LOSS: 1.1619915962219238
LOSS: 1.1595919132232666
LOSS: 1.157147765159607
LOSS: 1.154650092124939
LOSS: 1.1520836353302002
LOSS: 1.1494523286819458
LOSS: 1.1467735767364502
LOSS: 1.144088864326477
LOSS: 1.1414459943771362
LOSS: 1.1388957500457764
LOSS: 1.1364805698394775
LOSS: 1.134202241897583LOSS: 1.2860513925552368
LOSS: 1.2836593389511108
LOSS: 1.2812124490737915
LOSS: 1.278687596321106
LOSS: 1.276057481765747
LOSS: 1.273293137550354
LOSS: 1.2703644037246704
LOSS: 1.26724112033844
LOSS: 1.2638936042785645
LOSS: 1.2602965831756592
LOSS: 1.2564326524734497
LOSS: 1.2522952556610107
LOSS: 1.247890591621399
LOSS: 1.2432421445846558
LOSS: 1.2383882999420166
LOSS: 1.23337984085083
LOSS: 1.2282769680023193
LOSS: 1.223144769668579
LOSS: 1.2180471420288086
LOSS: 1.2130435705184937
LOSS: 1.2081844806671143
LOSS: 1.203508973121643
LOSS: 1.1990387439727783
LOSS: 1.1947859525680542
LOSS: 1.1907472610473633
LOSS: 1.18691885471344
LOSS: 1.1832869052886963
LOSS: 1.1798417568206787
LOSS: 1.1765629053115845
LOSS: 1.1734322309494019
LOSS: 1.170427680015564
LOSS: 1.1675338745117188
LOSS: 1.1647332906723022
LOSS: 1.1620014905929565
LOSS: 1.1593198776245117
LOSS: 1.156682014465332
LOSS: 1.1540964841842651
LOSS: 1.1515743732452393
LOSS: 1.149125337600708
LOSS: 1.146755576133728
LOSS: 1.1444686651229858
LOSS: 1.1422690153121948
LOSS: 1.1401623487472534
LOSS: 1.138152003288269
LOSS: 1.1362369060516357
LOSS: 1.1344037055969238
LOSS: 1.1326292753219604
LOSS: 1.1308778524398804
LOSS: 1.1291126012802124
LOSS: 1.1273107528686523
LOSS: 1.1254699230194092
LOSS: 1.1236153841018677
LOSS: 1.1217875480651855
LOSS: 1.1200250387191772
LOSS: 1.118356466293335
LOSS: 1.1168006658554077
LOSS: 1.1153656244277954
LOSS: 1.1140515804290771
LOSS: 1.112847924232483
LOSS: 1.111740231513977
LOSS: 1.1107078790664673
LOSS: 1.1097415685653687
LOSS: 1.1088274717330933
LOSS: 1.107956051826477
LOSS: 1.1071186065673828
LOSS: 1.10630464553833
LOSS: 1.1055009365081787
LOSS: 1.3662248849868774
LOSS: 1.359713077545166
LOSS: 1.353579044342041
LOSS: 1.3478598594665527
LOSS: 1.3425897359848022
LOSS: 1.3378019332885742
LOSS: 1.333518385887146
LOSS: 1.329748272895813
LOSS: 1.3264800310134888
LOSS: 1.323671817779541
LOSS: 1.3212494850158691
LOSS: 1.3191076517105103
LOSS: 1.3171250820159912
LOSS: 1.3151824474334717
LOSS: 1.3131837844848633
LOSS: 1.3110663890838623
LOSS: 1.3087944984436035
LOSS: 1.3063626289367676
LOSS: 1.3037810325622559
LOSS: 1.3010722398757935
LOSS: 1.2982667684555054
LOSS: 1.295399785041809
LOSS: 1.2925074100494385
LOSS: 1.2896230220794678
LOSS: 1.2867779731750488
LOSS: 1.2839981317520142
LOSS: 1.2813061475753784
LOSS: 1.2787151336669922
LOSS: 1.2762352228164673
LOSS: 1.2738666534423828
LOSS: 1.2716072797775269
LOSS: 1.2694463729858398
LOSS: 1.2673742771148682
LOSS: 1.2653778791427612
LOSS: 1.263444185256958
LOSS: 1.2615635395050049
LOSS: 1.2597274780273438
LOSS: 1.2579327821731567
LOSS: 1.2561774253845215
LOSS: 1.254463791847229
LOSS: 1.252794623374939
LOSS: 1.251171350479126
LOSS: 1.2495996952056885
LOSS: 1.2480772733688354
LOSS: 1.2466052770614624
LOSS: 1.2451761960983276
LOSS: 1.2437814474105835
LOSS: 1.242408275604248
LOSS: 1.2410426139831543
LOSS: 1.23966383934021
LOSS: 1.238254189491272
LOSS: 1.2367942333221436
LOSS: 1.2352631092071533
LOSS: 1.233644723892212
LOSS: 1.2319293022155762
LOSS: 1.2301069498062134
LOSS: 1.2281798124313354
LOSS: 1.2261433601379395
LOSS: 1.2240060567855835
LOSS: 1.2217639684677124
LOSS: 1.2194111347198486
LOSS: 1.216945767402649
LOSS: 1.2143654823303223
LOSS: 1.2116678953170776
LOSS: 1.2088550329208374
LOSS: 1.2059240341186523
LOSS: 1.2028800249099731
LOSS: 1.199725866317749
LOSS: 1.19647216796875
LOSS: 1.1931281089782715
LOSS: 1.1897163391113281
LOSS: 1.1862664222717285
LOSS: 1.1828222274780273
LOSS: 1.1794315576553345
LOSS: 1.1761436462402344
LOSS: 1.1729958057403564
LOSS: 1.1700137853622437
LOSS: 1.1672133207321167
LOSS: 1.1645954847335815
LOSS: 1.1621567010879517
LOSS: 1.1598888635635376
LOSS: 1.1577805280685425
LOSS: 1.1558220386505127
LOSS: 1.1540002822875977
LOSS: 1.1523038148880005
LOSS: 1.1507130861282349
LOSS: 1.1492141485214233
LOSS: 1.147798776626587
LOSS: 1.1464533805847168
LOSS: 1.1451690196990967
LOSS: 1.143930435180664
LOSS: 1.1427323818206787
LOSS: 1.141570806503296
LOSS: 1.1404492855072021
LOSS: 1.1393673419952393
LOSS: 1.1383310556411743
LOSS: 1.1373437643051147
LOSS: 1.136397123336792
LOSS: 1.1354823112487793
LOSS: 1.1345994472503662
LOSS: 1.3663958311080933
LOSS: 1.3599871397018433
LOSS: 1.3539584875106812
LOSS: 1.3483473062515259
LOSS: 1.343187928199768
LOSS: 1.3385119438171387
LOSS: 1.3343411684036255
LOSS: 1.330681324005127
LOSS: 1.327516794204712
LOSS: 1.3247992992401123
LOSS: 1.3224467039108276
LOSS: 1.3203479051589966
LOSS: 1.3183767795562744
LOSS: 1.3164188861846924
LOSS: 1.3143830299377441
LOSS: 1.312212586402893
LOSS: 1.3098808526992798
LOSS: 1.3073853254318237
LOSS: 1.3047370910644531
LOSS: 1.3019616603851318
LOSS: 1.2990883588790894
LOSS: 1.2961498498916626
LOSS: 1.2931793928146362
LOSS: 1.2902084589004517
LOSS: 1.2872662544250488
LOSS: 1.2843759059906006
LOSS: 1.2815568447113037
LOSS: 1.2788193225860596
LOSS: 1.2761709690093994
LOSS: 1.2736101150512695
LOSS: 1.2711312770843506
LOSS: 1.268722414970398
LOSS: 1.2663719654083252
LOSS: 1.264062762260437
LOSS: 1.261780858039856
LOSS: 1.2595120668411255
LOSS: 1.25724458694458
LOSS: 1.2549687623977661
LOSS: 1.2526788711547852
LOSS: 1.250370740890503
LOSS: 1.2480400800704956
LOSS: 1.2456824779510498
LOSS: 1.2432905435562134
LOSS: 1.240852952003479
LOSS: 1.238351821899414
LOSS: 1.2357631921768188
LOSS: 1.2330623865127563
LOSS: 1.230224370956421
LOSS: 1.2272322177886963
LOSS: 1.2240724563598633
LOSS: 1.2207412719726562
LOSS: 1.2172445058822632
LOSS: 1.2135931253433228
LOSS: 1.2098029851913452
LOSS: 1.2059049606323242
LOSS: 1.2019239664077759
LOSS: 1.1978938579559326
LOSS: 1.1938406229019165
LOSS: 1.1897798776626587
LOSS: 1.1857212781906128
LOSS: 1.1816751956939697
LOSS: 1.1776604652404785
LOSS: 1.1736969947814941
LOSS: 1.169808030128479
LOSS: 1.1660139560699463
LOSS: 1.1623265743255615
LOSS: 1.1587576866149902
LOSS: 1.1553184986114502
LOSS: 1.1520215272903442
LOSS: 1.1488759517669678
LOSS: 1.1458921432495117
LOSS: 1.143074870109558
LOSS: 1.1404235363006592
LOSS: 1.1379181146621704
LOSS: 1.1355270147323608
LOSS: 1.1332193613052368
LOSS: 1.1309758424758911
LOSS: 1.128787875175476
LOSS: 1.1266655921936035
LOSS: 1.12461519241333
LOSS: 1.1226472854614258
LOSS: 1.1207526922225952
LOSS: 1.1189268827438354
LOSS: 1.1171634197235107
LOSS: 1.1154543161392212
LOSS: 1.1138023138046265
LOSS: 1.112207293510437
LOSS: 1.1106750965118408
LOSS: 1.1091969013214111
LOSS: 1.1077651977539062
LOSS: 1.1063666343688965
LOSS: 1.1049833297729492
LOSS: 1.1036036014556885
LOSS: 1.1022206544876099
LOSS: 1.1008318662643433
LOSS: 1.0994495153427124
LOSS: 1.098097324371338
LOSS: 1.0967921018600464
LOSS: 1.0955321788787842
LOSS: 1.0942888259887695
LOSS: 1.3696554899215698
LOSS: 1.36332368850708
LOSS: 1.3573781251907349
LOSS: 1.3518567085266113
LOSS: 1.3467943668365479
LOSS: 1.3422224521636963
LOSS: 1.338159441947937
LOSS: 1.3346092700958252
LOSS: 1.3315483331680298
LOSS: 1.3289192914962769
LOSS: 1.3266292810440063
LOSS: 1.3245582580566406
LOSS: 1.3225775957107544
LOSS: 1.3205782175064087
LOSS: 1.3184797763824463
LOSS: 1.3162357807159424
LOSS: 1.3138293027877808
LOSS: 1.3112635612487793
LOSS: 1.3085561990737915
LOSS: 1.3057328462600708
LOSS: 1.3028233051300049
LOSS: 1.2998608350753784
LOSS: 1.2968757152557373
LOSS: 1.2938992977142334
LOSS: 1.2909586429595947
LOSS: 1.2880730628967285
LOSS: 1.2852615118026733
LOSS: 1.2825337648391724
LOSS: 1.2798943519592285
LOSS: 1.2773418426513672
LOSS: 1.2748700380325317
LOSS: 1.2724698781967163
LOSS: 1.2701287269592285
LOSS: 1.2678362131118774
LOSS: 1.2655807733535767
LOSS: 1.263355016708374
LOSS: 1.2611552476882935
LOSS: 1.2589797973632812
LOSS: 1.2568272352218628
LOSS: 1.2547017335891724
LOSS: 1.2526072263717651
LOSS: 1.250548005104065
LOSS: 1.248524785041809
LOSS: 1.2465368509292603
LOSS: 1.244577407836914
LOSS: 1.2426379919052124
LOSS: 1.2407032251358032
LOSS: 1.2387498617172241
LOSS: 1.23675537109375
LOSS: 1.234693169593811
LOSS: 1.2325371503829956
LOSS: 1.230269193649292
LOSS: 1.2278748750686646
LOSS: 1.2253623008728027
LOSS: 1.2227420806884766
LOSS: 1.2200394868850708
LOSS: 1.217283010482788
LOSS: 1.2145024538040161
LOSS: 1.2117372751235962
LOSS: 1.2090108394622803
LOSS: 1.2063508033752441
LOSS: 1.20377779006958
LOSS: 1.2013146877288818
LOSS: 1.198974609375

LOSS: 1.1320282220840454
LOSS: 1.1299080848693848
LOSS: 1.1278027296066284
LOSS: 1.125688076019287
LOSS: 1.1235687732696533
LOSS: 1.3742773532867432
LOSS: 1.3681102991104126
LOSS: 1.3623374700546265
LOSS: 1.3569961786270142
LOSS: 1.3521231412887573
LOSS: 1.3477485179901123
LOSS: 1.343893051147461
LOSS: 1.3405601978302002
LOSS: 1.33772611618042
LOSS: 1.335335373878479
LOSS: 1.33329439163208
LOSS: 1.3314834833145142
LOSS: 1.3297737836837769
LOSS: 1.3280507326126099
LOSS: 1.3262310028076172
LOSS: 1.3242663145065308
LOSS: 1.3221378326416016
LOSS: 1.3198474645614624
LOSS: 1.3174145221710205
LOSS: 1.3148643970489502
LOSS: 1.312231421470642
LOSS: 1.30954909324646
LOSS: 1.3068506717681885
LOSS: 1.3041678667068481
LOSS: 1.3015276193618774
LOSS: 1.2989531755447388
LOSS: 1.2964606285095215
LOSS: 1.2940610647201538
LOSS: 1.29175865650177
LOSS: 1.2895514965057373
LOSS: 1.2874327898025513
LOSS: 1.2853918075561523
LOSS: 1.2834161520004272
LOSS: 1.2814931869506836
LOSS: 1.2796117067337036
LOSS: 1.2777634859085083
LOSS: 1.2759430408477783
LOSS: 1.274148941040039
LOSS: 1.2723801136016846
LOSS: 1.2706345319747925
LOSS: 1.2689052820205688
LOSS: 1.2671858072280884
LOSS: 1.2654699087142944
LOSS: 1.2637444734573364
LOSS: 1.2619925737380981
LOSS: 1.2601890563964844
LOSS: 1.2583060264587402
LOSS: 1.25630784034729
LOSS: 1.254158854484558
LOSS: 1.2518264055252075
LOSS: 1.2492833137512207
LOSS: 1.246508002281189
LOSS: 1.2434930801391602
LOSS: 1.2402381896972656
LOSS: 1.2367602586746216
LOSS: 1.2330783605575562
LOSS: 1.229217767715454
LOSS: 1.225203514099121
LOSS: 1.2210646867752075
LOSS: 1.2168382406234741
LOSS: 1.2125693559646606
LOSS: 1.2083029747009277
LOSS: 1.204077124595642
LOSS: 1.1999218463897705
LOSS: 1.1958563327789307
LOSS: 1.1918879747390747
LOSS: 1.1880117654800415
LOSS: 1.1842161417007446
LOSS: 1.1804864406585693
LOSS: 1.1768051385879517
LOSS: 1.1731594800949097
LOSS: 1.169546127319336
LOSS: 1.165973424911499
LOSS: 1.1624547243118286
LOSS: 1.159006118774414
LOSS: 1.1556419134140015
LOSS: 1.1523805856704712
LOSS: 1.1492472887039185
LOSS: 1.146269679069519
LOSS: 1.1434684991836548
LOSS: 1.1408615112304688
LOSS: 1.1384532451629639
LOSS: 1.136216640472412
LOSS: 1.1341074705123901
LOSS: 1.132086992263794
LOSS: 1.1301268339157104
LOSS: 1.1282025575637817
LOSS: 1.1262931823730469
LOSS: 1.1243840456008911
LOSS: 1.1224690675735474
LOSS: 1.1205500364303589
LOSS: 1.118635654449463
LOSS: 1.1167374849319458
LOSS: 1.1148654222488403
LOSS: 1.11302649974823
LOSS: 1.111222505569458
LOSS: 1.1094502210617065
LOSS: 1.1077085733413696
LOSS: 1.1060047149658203
LOSS: 1.1043529510498047
LOSS: 1.3699754476547241
LOSS: 1.3637111186981201
LOSS: 1.3578366041183472
LOSS: 1.3523892164230347
LOSS: 1.3474037647247314
LOSS: 1.342909574508667
LOSS: 1.3389254808425903
LOSS: 1.3354523181915283
LOSS: 1.3324638605117798
LOSS: 1.329899787902832
LOSS: 1.3276655673980713
LOSS: 1.325638771057129
LOSS: 1.323696255683899
LOSS: 1.3217319250106812
LOSS: 1.3196731805801392
LOSS: 1.3174811601638794
LOSS: 1.3151440620422363
LOSS: 1.3126689195632935
LOSS: 1.3100765943527222
LOSS: 1.3073952198028564
LOSS: 1.3046561479568481
LOSS: 1.3018920421600342
LOSS: 1.2991359233856201
LOSS: 1.2964175939559937
LOSS: 1.2937629222869873
LOSS: 1.2911922931671143
LOSS: 1.288723349571228
LOSS: 1.2863653898239136
LOSS: 1.284122109413147
LOSS: 1.281991720199585
LOSS: 1.2799686193466187
LOSS: 1.2780431509017944
LOSS: 1.2762037515640259
LOSS: 1.2744383811950684
LOSS: 1.2727364301681519
LOSS: 1.27108895778656
LOSS: 1.2694884538650513
LOSS: 1.2679312229156494
LOSS: 1.2664140462875366
LOSS: 1.2649341821670532
LOSS: 1.263485312461853
LOSS: 1.2620526552200317
LOSS: 1.2606091499328613
LOSS: 1.259122371673584
LOSS: 1.2575535774230957
LOSS: 1.2558585405349731
LOSS: 1.2539904117584229
LOSS: 1.2519060373306274
LOSS: 1.2495673894882202
LOSS: 1.2469518184661865
LOSS: 1.2440540790557861
LOSS: 1.2408909797668457
LOSS: 1.2374978065490723
LOSS: 1.2339290380477905
LOSS: 1.2302519083023071
LOSS: 1.226535677909851
LOSS: 1.2228507995605469
LOSS: 1.219262719154358
LOSS: 1.215813159942627
LOSS: 1.2125396728515625
LOSS: 1.209437608718872
LOSS: 1.2065088748931885
LOSS: 1.203749656677246
LOSS: 1.2011524438858032
LOSS: 1.1987050771713257
LOSS: 1.1963999271392822
LOSS: 1.1942288875579834
LOSS: 1.1921676397323608
LOSS: 1.1902062892913818
LOSS: 1.188328504562378
LOSS: 1.1865131855010986
LOSS: 1.184751272201538
LOSS: 1.1830264329910278
LOSS: 1.1813342571258545
LOSS: 1.1796696186065674
LOSS: 1.178005337715149
LOSS: 1.1763279438018799
LOSS: 1.1745938062667847
LOSS: 1.1727681159973145
LOSS: 1.1708345413208008
LOSS: 1.1687719821929932
LOSS: 1.1666008234024048
LOSS: 1.1643320322036743
LOSS: 1.1619915962219238
LOSS: 1.1595919132232666
LOSS: 1.157147765159607
LOSS: 1.154650092124939
LOSS: 1.1520836353302002
LOSS: 1.1494523286819458
LOSS: 1.1467735767364502
LOSS: 1.144088864326477
LOSS: 1.1414459943771362
LOSS: 1.1388957500457764
LOSS: 1.1364805698394775
LOSS: 1.134202241897583
LOSS: 1.1320282220840454
LOSS: 1.1299080848693848
LOSS: 1.1278027296066284
LOSS: 1.125688076019287
LOSS: 1.1235687732696533
LOSS: 1.382778286933899
LOSS: 1.3768092393875122
LOSS: 1.371245265007019
LOSS: 1.3661246299743652
LOSS: 1.3614822626113892
LOSS: 1.357348918914795
LOSS: 1.3537434339523315
LOSS: 1.3506659269332886
LOSS: 1.3480890989303589
LOSS: 1.345950722694397
LOSS: 1.3441510200500488
LOSS: 1.3425627946853638
LOSS: 1.3410577774047852
LOSS: 1.3395237922668457
LOSS: 1.3378840684890747
LOSS: 1.3360960483551025
LOSS: 1.334145188331604
LOSS: 1.3320400714874268
LOSS: 1.3298004865646362
LOSS: 1.3274552822113037
LOSS: 1.3250364065170288
LOSS: 1.3225764036178589
LOSS: 1.3201072216033936
LOSS: 1.317657709121704
LOSS: 1.3152509927749634
LOSS: 1.3129066228866577
LOSS: 1.3106385469436646
LOSS: 1.308454155921936
LOSS: 1.3063557147979736
LOSS: 1.3043394088745117
LOSS: 1.30239737033844
LOSS: 1.30051851272583
LOSS: 1.2986897230148315
LOSS: 1.2968987226486206
LOSS: 1.2951321601867676
LOSS: 1.2933799028396606
LOSS: 1.291635274887085
LOSS: 1.2898859977722168
LOSS: 1.2881181240081787
LOSS: 1.2863072156906128
LOSS: 1.284425139427185
LOSS: 1.2824417352676392
LOSS: 1.2803267240524292
LOSS: 1.2780474424362183
LOSS: 1.2755727767944336
LOSS: 1.2728774547576904
LOSS: 1.269944429397583
LOSS: 1.2667704820632935
LOSS: 1.2633657455444336
LOSS: 1.2597527503967285
LOSS: 1.255963921546936
LOSS: 1.2520434856414795
LOSS: 1.2480376958847046
LOSS: 1.243988275527954
LOSS: 1.2399272918701172
LOSS: 1.2358824014663696
LOSS: 1.231868863105774
LOSS: 1.2278989553451538
LOSS: 1.2239733934402466
LOSS: 1.220085859298706
LOSS: 1.2162173986434937
LOSS: 1.2123132944107056
LOSS: 1.2083159685134888
LOSS: 1.2041784524917603
LOSS: 1.1998705863952637
LOSS: 1.1953845024108887
LOSS: 1.1907439231872559
LOSS: 1.1860049962997437
LOSS: 1.1812490224838257
LOSS: 1.1765637397766113
LOSS: 1.172013759613037
LOSS: 1.1676141023635864
LOSS: 1.163338303565979
LOSS: 1.1591485738754272
LOSS: 1.1550242900848389
LOSS: 1.1509652137756348
LOSS: 1.1469846963882446
LOSS: 1.1431007385253906
LOSS: 1.1393285989761353
LOSS: 1.1356784105300903
LOSS: 1.1321572065353394
LOSS: 1.1287719011306763
LOSS: 1.1255264282226562
LOSS: 1.1224204301834106
LOSS: 1.1194590330123901
LOSS: 1.116646647453308
LOSS: 1.1139781475067139
LOSS: 1.1114387512207031
LOSS: 1.109009027481079
LOSS: 1.1066675186157227
LOSS: 1.1043930053710938
LOSS: 1.1021677255630493
LOSS: 1.099976897239685
LOSS: 1.097821593284607
LOSS: 1.0957237482070923
LOSS: 1.0937180519104004
LOSS: 1.0918388366699219
LOSS: 1.0901048183441162
LOSS: 1.0885132551193237
LOSS: 1.0870474576950073
LOSS: 1.3691104650497437
LOSS: 1.3628044128417969
LOSS: 1.3568841218948364
LOSS: 1.3513875007629395
LOSS: 1.346348524093628
LOSS: 1.3417969942092896
LOSS: 1.3377512693405151
LOSS: 1.3342100381851196
LOSS: 1.3311465978622437
LOSS: 1.3284976482391357
LOSS: 1.3261650800704956
LOSS: 1.324023723602295
LOSS: 1.3219494819641113
LOSS: 1.3198355436325073
LOSS: 1.3176106214523315
LOSS: 1.31523597240448
LOSS: 1.3126989603042603
LOSS: 1.3100072145462036
LOSS: 1.3071815967559814
LOSS: 1.3042486906051636
LOSS: 1.301240086555481
LOSS: 1.2981877326965332
LOSS: 1.2951231002807617
LOSS: 1.2920770645141602
LOSS: 1.2890719175338745
LOSS: 1.2861329317092896LOSS: 1.1042546033859253
LOSS: 1.1015400886535645
LOSS: 1.0988450050354004
LOSS: 1.0961902141571045
LOSS: 1.0935921669006348
LOSS: 1.0910594463348389
LOSS: 1.0885930061340332
LOSS: 1.3772188425064087
LOSS: 1.37103271484375
LOSS: 1.3652411699295044
LOSS: 1.3598830699920654
LOSS: 1.3549946546554565
LOSS: 1.3506065607070923
LOSS: 1.3467414379119873
LOSS: 1.3434034585952759
LOSS: 1.340573787689209
LOSS: 1.3382010459899902
LOSS: 1.3361974954605103
LOSS: 1.3344464302062988
LOSS: 1.3328181505203247
LOSS: 1.3311949968338013
LOSS: 1.329486608505249
LOSS: 1.3276374340057373
LOSS: 1.32562255859375
LOSS: 1.3234400749206543
LOSS: 1.3211053609848022
LOSS: 1.3186429738998413
LOSS: 1.3160812854766846
LOSS: 1.3134527206420898
LOSS: 1.3107876777648926
LOSS: 1.3081141710281372
LOSS: 1.3054581880569458
LOSS: 1.302838683128357
LOSS: 1.3002715110778809
LOSS: 1.2977654933929443
LOSS: 1.2953240871429443
LOSS: 1.2929476499557495
LOSS: 1.2906274795532227
LOSS: 1.288355827331543
LOSS: 1.2861171960830688
LOSS: 1.2838984727859497
LOSS: 1.2816829681396484
LOSS: 1.279457688331604
LOSS: 1.2772080898284912
LOSS: 1.2749196290969849
LOSS: 1.272577166557312
LOSS: 1.270169734954834
LOSS: 1.2676786184310913
LOSS: 1.2650874853134155
LOSS: 1.2623786926269531
LOSS: 1.2595335245132446
LOSS: 1.2565360069274902
LOSS: 1.2533714771270752
LOSS: 1.2500338554382324
LOSS: 1.246523380279541
LOSS: 1.2428466081619263
LOSS: 1.239031434059143
LOSS: 1.235101580619812
LOSS: 1.2310858964920044
LOSS: 1.2270190715789795
LOSS: 1.2229326963424683
LOSS: 1.2188646793365479
LOSS: 1.2148486375808716
LOSS: 1.2109224796295166
LOSS: 1.2071309089660645
LOSS: 1.2035115957260132
LOSS: 1.2001068592071533
LOSS: 1.1969255208969116
LOSS: 1.1939724683761597
LOSS: 1.191221833229065
LOSS: 1.1886372566223145
LOSS: 1.1862059831619263
LOSS: 1.183915615081787
LOSS: 1.181772232055664
LOSS: 1.1797666549682617
LOSS: 1.1778912544250488
LOSS: 1.1761218309402466
LOSS: 1.174439787864685
LOSS: 1.172821044921875
LOSS: 1.171265959739685
LOSS: 1.169774055480957
LOSS: 1.1683454513549805
LOSS: 1.1669822931289673
LOSS: 1.1656737327575684
LOSS: 1.1644147634506226
LOSS: 1.1631813049316406
LOSS: 1.1619592905044556
LOSS: 1.160740852355957
LOSS: 1.159530520439148
LOSS: 1.1583356857299805
LOSS: 1.1571540832519531
LOSS: 1.1559960842132568
LOSS: 1.1548409461975098
LOSS: 1.1536900997161865
LOSS: 1.1525540351867676
LOSS: 1.1514443159103394
LOSS: 1.1503708362579346
LOSS: 1.1493306159973145
LOSS: 1.148328423500061
LOSS: 1.1473479270935059
LOSS: 1.1463913917541504
LOSS: 1.145450234413147
LOSS: 1.1445205211639404
LOSS: 1.1436095237731934
LOSS: 1.142715573310852
LOSS: 1.1418362855911255
LOSS: 1.1409721374511719
LOSS: 1.3663958311080933
LOSS: 1.3599871397018433
LOSS: 1.3539584875106812
LOSS: 1.3483473062515259
LOSS: 1.343187928199768
LOSS: 1.3385119438171387
LOSS: 1.3343411684036255
LOSS: 1.330681324005127
LOSS: 1.327516794204712
LOSS: 1.3247992992401123
LOSS: 1.3224467039108276
LOSS: 1.3203479051589966
LOSS: 1.3183767795562744
LOSS: 1.3164188861846924
LOSS: 1.3143830299377441
LOSS: 1.312212586402893
LOSS: 1.3098808526992798
LOSS: 1.3073853254318237
LOSS: 1.3047370910644531
LOSS: 1.3019616603851318
LOSS: 1.2990883588790894
LOSS: 1.2961498498916626
LOSS: 1.2931793928146362
LOSS: 1.2902084589004517
LOSS: 1.2872662544250488
LOSS: 1.2843759059906006
LOSS: 1.2815568447113037
LOSS: 1.2788193225860596
LOSS: 1.2761709690093994
LOSS: 1.2736101150512695
LOSS: 1.2711312770843506
LOSS: 1.268722414970398
LOSS: 1.2663719654083252
LOSS: 1.264062762260437
LOSS: 1.261780858039856
LOSS: 1.2595120668411255
LOSS: 1.25724458694458
LOSS: 1.2549687623977661
LOSS: 1.2526788711547852
LOSS: 1.250370740890503
LOSS: 1.2480400800704956
LOSS: 1.2456824779510498
LOSS: 1.2432905435562134
LOSS: 1.240852952003479
LOSS: 1.238351821899414
LOSS: 1.2357631921768188
LOSS: 1.2330623865127563
LOSS: 1.230224370956421
LOSS: 1.2272322177886963
LOSS: 1.2240724563598633
LOSS: 1.2207412719726562
LOSS: 1.2172445058822632
LOSS: 1.2135931253433228
LOSS: 1.2098029851913452
LOSS: 1.2059049606323242
LOSS: 1.2019239664077759
LOSS: 1.1978938579559326
LOSS: 1.1938406229019165
LOSS: 1.1897798776626587
LOSS: 1.1857212781906128
LOSS: 1.1816751956939697
LOSS: 1.1776604652404785
LOSS: 1.1736969947814941
LOSS: 1.169808030128479
LOSS: 1.1660139560699463
LOSS: 1.1623265743255615
LOSS: 1.1587576866149902
LOSS: 1.1553184986114502
LOSS: 1.1520215272903442
LOSS: 1.1488759517669678
LOSS: 1.1458921432495117
LOSS: 1.143074870109558
LOSS: 1.1404235363006592
LOSS: 1.1379181146621704
LOSS: 1.1355270147323608
LOSS: 1.1332193613052368
LOSS: 1.1309758424758911
LOSS: 1.128787875175476
LOSS: 1.1266655921936035
LOSS: 1.12461519241333
LOSS: 1.1226472854614258
LOSS: 1.1207526922225952
LOSS: 1.1189268827438354
LOSS: 1.1171634197235107
LOSS: 1.1154543161392212
LOSS: 1.1138023138046265
LOSS: 1.112207293510437
LOSS: 1.1106750965118408
LOSS: 1.1091969013214111
LOSS: 1.1077651977539062
LOSS: 1.1063666343688965
LOSS: 1.1049833297729492
LOSS: 1.1036036014556885
LOSS: 1.1022206544876099
LOSS: 1.1008318662643433
LOSS: 1.0994495153427124
LOSS: 1.098097324371338
LOSS: 1.0967921018600464
LOSS: 1.0955321788787842
LOSS: 1.0942888259887695
LOSS: 1.3713003396987915
LOSS: 1.3650542497634888
LOSS: 1.3591984510421753
LOSS: 1.3537718057632446
LOSS: 1.348811149597168
LOSS: 1.3443466424942017
LOSS: 1.3404020071029663
LOSS: 1.3369842767715454
LOSS: 1.3340768814086914
LOSS: 1.3316309452056885
LOSS: 1.329565405845642
LOSS: 1.327767252922058
LOSS: 1.326109528541565
LOSS: 1.3244763612747192
LOSS: 1.3227769136428833
LOSS: 1.3209525346755981
LOSS: 1.318977952003479
LOSS: 1.3168472051620483
LOSS: 1.314575433731079
LOSS: 1.3121838569641113
LOSS: 1.3097025156021118
LOSS: 1.3071613311767578
LOSS: 1.3045920133590698
LOSS: 1.3020237684249878
LOSS: 1.2994811534881592
LOSS: 1.2969857454299927
LOSS: 1.2945516109466553
LOSS: 1.2921867370605469
LOSS: 1.2898918390274048
LOSS: 1.2876577377319336
LOSS: 1.2854688167572021
LOSS: 1.2833032608032227
LOSS: 1.2811329364776611
LOSS: 1.2789270877838135
LOSS: 1.2766512632369995
LOSS: 1.2742682695388794
LOSS: 1.271742343902588
LOSS: 1.269040584564209
LOSS: 1.2661360502243042
LOSS: 1.2630078792572021
LOSS: 1.2596462965011597
LOSS: 1.2560534477233887
LOSS: 1.252241611480713
LOSS: 1.248237133026123
LOSS: 1.2440776824951172
LOSS: 1.239806890487671
LOSS: 1.2354680299758911
LOSS: 1.231091022491455
LOSS: 1.2267073392868042
LOSS: 1.222346305847168
LOSS: 1.2180356979370117
LOSS: 1.2137970924377441
LOSS: 1.2096500396728516
LOSS: 1.2056145668029785
LOSS: 1.201711654663086
LOSS: 1.1979540586471558
LOSS: 1.1943507194519043
LOSS: 1.1909114122390747
LOSS: 1.187630534172058
LOSS: 1.1844958066940308
LOSS: 1.1814841032028198
LOSS: 1.1785705089569092
LOSS: 1.1757285594940186
LOSS: 1.1729387044906616
LOSS: 1.1701844930648804
LOSS: 1.167445182800293
LOSS: 1.1647063493728638
LOSS: 1.1619597673416138
LOSS: 1.159206748008728
LOSS: 1.1564542055130005
LOSS: 1.1537141799926758
LOSS: 1.150999903678894
LOSS: 1.1483228206634521
LOSS: 1.1456975936889648
LOSS: 1.1431336402893066
LOSS: 1.1406347751617432
LOSS: 1.138214111328125
LOSS: 1.1358883380889893
LOSS: 1.133675217628479
LOSS: 1.131585955619812
LOSS: 1.129618763923645
LOSS: 1.1277589797973633
LOSS: 1.1259782314300537
LOSS: 1.1242358684539795
LOSS: 1.122499704360962
LOSS: 1.1207568645477295
LOSS: 1.119010090827942
LOSS: 1.1172819137573242
LOSS: 1.1155996322631836
LOSS: 1.1139806509017944
LOSS: 1.112427830696106
LOSS: 1.11094331741333
LOSS: 1.1095283031463623
LOSS: 1.1081933975219727
LOSS: 1.1069390773773193
LOSS: 1.1057589054107666
LOSS: 1.1046313047409058
LOSS: 1.1035311222076416
LOSS: 1.102439045906067
LOSS: 1.101356029510498
LOSS: 1.3713003396987915
LOSS: 1.3650542497634888
LOSS: 1.3591984510421753
LOSS: 1.3537718057632446
LOSS: 1.348811149597168
LOSS: 1.3443466424942017
LOSS: 1.3404020071029663
LOSS: 1.3369842767715454
LOSS: 1.3340768814086914
LOSS: 1.3316309452056885
LOSS: 1.329565405845642
LOSS: 1.327767252922058
LOSS: 1.326109528541565
LOSS: 1.3244763612747192
LOSS: 1.3227769136428833
LOSS: 1.3209525346755981
LOSS: 1.318977952003479
LOSS: 1.3168472051620483
LOSS: 1.314575433731079
LOSS: 1.3121838569641113
LOSS: 1.3097025156021118
LOSS: 1.3071613311767578
LOSS: 1.3045920133590698
LOSS: 1.1967692375183105
LOSS: 1.1947059631347656
LOSS: 1.192779779434204
LOSS: 1.1909829378128052
LOSS: 1.1892966032028198
LOSS: 1.1876988410949707
LOSS: 1.1861610412597656
LOSS: 1.1846574544906616
LOSS: 1.183166742324829
LOSS: 1.1816723346710205
LOSS: 1.1801657676696777
LOSS: 1.1786471605300903
LOSS: 1.1771100759506226
LOSS: 1.175547480583191
LOSS: 1.173924207687378
LOSS: 1.1722073554992676
LOSS: 1.1703652143478394
LOSS: 1.1683852672576904
LOSS: 1.1662477254867554
LOSS: 1.1639339923858643
LOSS: 1.1614280939102173
LOSS: 1.158717393875122
LOSS: 1.1557953357696533
LOSS: 1.1526635885238647
LOSS: 1.14933180809021
LOSS: 1.1458250284194946
LOSS: 1.1421750783920288
LOSS: 1.1384148597717285
LOSS: 1.1345760822296143
LOSS: 1.130688190460205
LOSS: 1.1267971992492676
LOSS: 1.122942566871643
LOSS: 1.1191647052764893
LOSS: 1.1154917478561401
LOSS: 1.1119225025177002
LOSS: 1.1084421873092651
LOSS: 1.3726768493652344
LOSS: 1.3664487600326538
LOSS: 1.3606116771697998
LOSS: 1.3552030324935913
LOSS: 1.35025954246521
LOSS: 1.3458125591278076
LOSS: 1.3418831825256348
LOSS: 1.3384772539138794
LOSS: 1.335576057434082
LOSS: 1.333129644393921
LOSS: 1.3310508728027344
LOSS: 1.3292245864868164
LOSS: 1.327523946762085
LOSS: 1.325832486152649
LOSS: 1.324060320854187
LOSS: 1.322152853012085
LOSS: 1.320085883140564
LOSS: 1.3178582191467285
LOSS: 1.3154860734939575
LOSS: 1.3129937648773193
LOSS: 1.3104140758514404
LOSS: 1.3077808618545532
LOSS: 1.3051254749298096
LOSS: 1.3024826049804688
LOSS: 1.2998805046081543
LOSS: 1.2973427772521973
LOSS: 1.294891595840454
LOSS: 1.29253351688385
LOSS: 1.290282964706421
LOSS: 1.2881405353546143
LOSS: 1.286101222038269
LOSS: 1.2841590642929077
LOSS: 1.2823067903518677
LOSS: 1.2805298566818237
LOSS: 1.2788236141204834
LOSS: 1.2771738767623901
LOSS: 1.2755804061889648
LOSS: 1.2740375995635986
LOSS: 1.2725474834442139
LOSS: 1.2711106538772583
LOSS: 1.2697346210479736
LOSS: 1.2684258222579956
LOSS: 1.267186164855957
LOSS: 1.2660257816314697
LOSS: 1.2649394273757935
LOSS: 1.2639269828796387
LOSS: 1.262969970703125
LOSS: 1.2620569467544556
LOSS: 1.2611688375473022
LOSS: 1.2602860927581787
LOSS: 1.2593871355056763
LOSS: 1.2584505081176758
LOSS: 1.2574570178985596
LOSS: 1.256386160850525
LOSS: 1.2552202939987183
LOSS: 1.2539476156234741
LOSS: 1.2525533437728882
LOSS: 1.25103759765625
LOSS: 1.2494043111801147
LOSS: 1.247658610343933
LOSS: 1.245818853378296
LOSS: 1.2438983917236328
LOSS: 1.2419099807739258
LOSS: 1.2398759126663208
LOSS: 1.2378000020980835
LOSS: 1.2357051372528076
LOSS: 1.2336066961288452
LOSS: 1.2315210103988647
LOSS: 1.229459285736084
LOSS: 1.2274385690689087
LOSS: 1.225473165512085
LOSS: 1.2235617637634277
LOSS: 1.2217081785202026
LOSS: 1.2199041843414307
LOSS: 1.218132495880127
LOSS: 1.2163742780685425
LOSS: 1.2146110534667969
LOSS: 1.2128289937973022
LOSS: 1.211010456085205
LOSS: 1.2091498374938965
LOSS: 1.2072359323501587
LOSS: 1.2052638530731201
LOSS: 1.2032421827316284
LOSS: 1.2011891603469849
LOSS: 1.1991263628005981
LOSS: 1.1970754861831665
LOSS: 1.1950465440750122
LOSS: 1.1930301189422607
LOSS: 1.1910172700881958
LOSS: 1.1889878511428833
LOSS: 1.1869407892227173
LOSS: 1.1848785877227783
LOSS: 1.1828142404556274
LOSS: 1.1807663440704346
LOSS: 1.1787585020065308
LOSS: 1.1768074035644531
LOSS: 1.1749144792556763
LOSS: 1.17307710647583
LOSS: 1.1712868213653564
LOSS: 1.16952645778656
LOSS: 1.382778286933899
LOSS: 1.3768092393875122
LOSS: 1.371245265007019
LOSS: 1.3661246299743652
LOSS: 1.3614822626113892
LOSS: 1.357348918914795
LOSS: 1.3537434339523315
LOSS: 1.3506659269332886
LOSS: 1.3480890989303589
LOSS: 1.345950722694397
LOSS: 1.3441510200500488
LOSS: 1.3425627946853638
LOSS: 1.3410577774047852
LOSS: 1.3395237922668457
LOSS: 1.3378840684890747
LOSS: 1.3360960483551025
LOSS: 1.334145188331604
LOSS: 1.3320400714874268
LOSS: 1.3298004865646362
LOSS: 1.3274552822113037
LOSS: 1.3250364065170288
LOSS: 1.3225764036178589
LOSS: 1.3201072216033936
LOSS: 1.317657709121704
LOSS: 1.3152509927749634
LOSS: 1.3129066228866577
LOSS: 1.3106385469436646
LOSS: 1.308454155921936
LOSS: 1.3063557147979736
LOSS: 1.3043394088745117
LOSS: 1.30239737033844
LOSS: 1.30051851272583
LOSS: 1.2986897230148315
LOSS: 1.2968987226486206
LOSS: 1.2951321601867676
LOSS: 1.2933799028396606
LOSS: 1.291635274887085
LOSS: 1.2898859977722168
LOSS: 1.2881181240081787
LOSS: 1.2863072156906128
LOSS: 1.284425139427185
LOSS: 1.2824417352676392
LOSS: 1.2803267240524292
LOSS: 1.2780474424362183
LOSS: 1.2755727767944336
LOSS: 1.2728774547576904
LOSS: 1.269944429397583
LOSS: 1.2667704820632935
LOSS: 1.2633657455444336
LOSS: 1.2597527503967285
LOSS: 1.255963921546936
LOSS: 1.2520434856414795
LOSS: 1.2480376958847046
LOSS: 1.243988275527954
LOSS: 1.2399272918701172
LOSS: 1.2358824014663696
LOSS: 1.231868863105774
LOSS: 1.2278989553451538
LOSS: 1.2239733934402466
LOSS: 1.220085859298706
LOSS: 1.2162173986434937
LOSS: 1.2123132944107056
LOSS: 1.2083159685134888
LOSS: 1.2041784524917603
LOSS: 1.1998705863952637
LOSS: 1.1953845024108887
LOSS: 1.1907439231872559
LOSS: 1.1860049962997437
LOSS: 1.1812490224838257
LOSS: 1.1765637397766113
LOSS: 1.172013759613037
LOSS: 1.1676141023635864
LOSS: 1.163338303565979
LOSS: 1.1591485738754272
LOSS: 1.1550242900848389
LOSS: 1.1509652137756348
LOSS: 1.1469846963882446
LOSS: 1.1431007385253906
LOSS: 1.1393285989761353
LOSS: 1.1356784105300903
LOSS: 1.1321572065353394
LOSS: 1.1287719011306763
LOSS: 1.1255264282226562
LOSS: 1.1224204301834106
LOSS: 1.1194590330123901
LOSS: 1.116646647453308
LOSS: 1.1139781475067139
LOSS: 1.1114387512207031
LOSS: 1.109009027481079
LOSS: 1.1066675186157227
LOSS: 1.1043930053710938
LOSS: 1.1021677255630493
LOSS: 1.099976897239685
LOSS: 1.097821593284607
LOSS: 1.0957237482070923
LOSS: 1.0937180519104004
LOSS: 1.0918388366699219
LOSS: 1.0901048183441162
LOSS: 1.0885132551193237
LOSS: 1.0870474576950073
LOSS: 1.3691104650497437
LOSS: 1.3628044128417969
LOSS: 1.3568841218948364
LOSS: 1.3513875007629395
LOSS: 1.346348524093628
LOSS: 1.3417969942092896
LOSS: 1.3377512693405151
LOSS: 1.3342100381851196
LOSS: 1.3311465978622437
LOSS: 1.3284976482391357
LOSS: 1.3261650800704956
LOSS: 1.324023723602295
LOSS: 1.3219494819641113
LOSS: 1.3198355436325073
LOSS: 1.3176106214523315
LOSS: 1.31523597240448
LOSS: 1.3126989603042603
LOSS: 1.3100072145462036
LOSS: 1.3071815967559814
LOSS: 1.3042486906051636
LOSS: 1.301240086555481
LOSS: 1.2981877326965332
LOSS: 1.2951231002807617
LOSS: 1.2920770645141602
LOSS: 1.2890719175338745
LOSS: 1.2861329317092896
LOSS: 1.2832722663879395
LOSS: 1.280500888824463
LOSS: 1.2778234481811523
LOSS: 1.2752373218536377
LOSS: 1.2727348804473877
LOSS: 1.2703064680099487
LOSS: 1.2679399251937866
LOSS: 1.2656223773956299
LOSS: 1.263342022895813
LOSS: 1.261089563369751
LOSS: 1.258859634399414
LOSS: 1.2566474676132202
LOSS: 1.254454255104065
LOSS: 1.2522814273834229
LOSS: 1.2501307725906372
LOSS: 1.248005747795105
LOSS: 1.2459070682525635
LOSS: 1.2438313961029053
LOSS: 1.2417699098587036
LOSS: 1.239707589149475
LOSS: 1.2376281023025513
LOSS: 1.2355053424835205
LOSS: 1.2333112955093384
LOSS: 1.2310125827789307
LOSS: 1.2285730838775635
LOSS: 1.2259546518325806
LOSS: 1.2231242656707764
LOSS: 1.22005295753479
LOSS: 1.2167261838912964
LOSS: 1.2131385803222656
LOSS: 1.2092983722686768
LOSS: 1.2052216529846191
LOSS: 1.2009313106536865
LOSS: 1.1964468955993652
LOSS: 1.1917846202850342
LOSS: 1.1869581937789917
LOSS: 1.1819911003112793
LOSS: 1.176912546157837
LOSS: 1.1717594861984253
LOSS: 1.1665802001953125
LOSS: 1.1614296436309814
LOSS: 1.1563564538955688
LOSS: 1.1513909101486206
LOSS: 1.1465518474578857
LOSS: 1.1418501138687134
LOSS: 1.1372896432876587
LOSS: 1.132869005203247
LOSS: 1.1285901069641113
LOSS: 1.1244556903839111
LOSS: 1.1204698085784912
LOSS: 1.1166372299194336
LOSS: 1.1129623651504517
LOSS: 1.1094472408294678
LOSS: 1.1060909032821655
LOSS: 1.1028823852539062
LOSS: 1.0998040437698364
LOSS: 1.0968329906463623
LOSS: 1.0939520597457886
LOSS: 1.0911555290222168
LOSS: 1.0884499549865723
LOSS: 1.085848093032837
LOSS: 1.0833661556243896
LOSS: 1.081018328666687
LOSS: 1.0788148641586304
LOSS: 1.0767604112625122
LOSS: 1.074853777885437
LOSS: 1.0730851888656616
LOSS: 1.0714315176010132
LOSS: 1.122942566871643
LOSS: 1.1191647052764893
LOSS: 1.1154917478561401
LOSS: 1.1119225025177002
LOSS: 1.1084421873092651
LOSS: 1.3772188425064087
LOSS: 1.37103271484375
LOSS: 1.3652411699295044
LOSS: 1.3598830699920654
LOSS: 1.3549946546554565
LOSS: 1.3506065607070923
LOSS: 1.3467414379119873
LOSS: 1.3434034585952759
LOSS: 1.340573787689209
LOSS: 1.3382010459899902
LOSS: 1.3361974954605103
LOSS: 1.3344464302062988
LOSS: 1.3328181505203247
LOSS: 1.3311949968338013
LOSS: 1.329486608505249
LOSS: 1.3276374340057373
LOSS: 1.32562255859375
LOSS: 1.3234400749206543
LOSS: 1.3211053609848022
LOSS: 1.3186429738998413
LOSS: 1.3160812854766846
LOSS: 1.3134527206420898
LOSS: 1.3107876777648926
LOSS: 1.3081141710281372
LOSS: 1.3054581880569458
LOSS: 1.302838683128357
LOSS: 1.3002715110778809
LOSS: 1.2977654933929443
LOSS: 1.2953240871429443
LOSS: 1.2929476499557495
LOSS: 1.2906274795532227
LOSS: 1.288355827331543
LOSS: 1.2861171960830688
LOSS: 1.2838984727859497
LOSS: 1.2816829681396484
LOSS: 1.279457688331604
LOSS: 1.2772080898284912
LOSS: 1.2749196290969849
LOSS: 1.272577166557312
LOSS: 1.270169734954834
LOSS: 1.2676786184310913
LOSS: 1.2650874853134155
LOSS: 1.2623786926269531
LOSS: 1.2595335245132446
LOSS: 1.2565360069274902
LOSS: 1.2533714771270752
LOSS: 1.2500338554382324
LOSS: 1.246523380279541
LOSS: 1.2428466081619263
LOSS: 1.239031434059143
LOSS: 1.235101580619812
LOSS: 1.2310858964920044
LOSS: 1.2270190715789795
LOSS: 1.2229326963424683
LOSS: 1.2188646793365479
LOSS: 1.2148486375808716
LOSS: 1.2109224796295166
LOSS: 1.2071309089660645
LOSS: 1.2035115957260132
LOSS: 1.2001068592071533
LOSS: 1.1969255208969116
LOSS: 1.1939724683761597
LOSS: 1.191221833229065
LOSS: 1.1886372566223145
LOSS: 1.1862059831619263
LOSS: 1.183915615081787
LOSS: 1.181772232055664
LOSS: 1.1797666549682617
LOSS: 1.1778912544250488
LOSS: 1.1761218309402466
LOSS: 1.174439787864685
LOSS: 1.172821044921875
LOSS: 1.171265959739685
LOSS: 1.169774055480957
LOSS: 1.1683454513549805
LOSS: 1.1669822931289673
LOSS: 1.1656737327575684
LOSS: 1.1644147634506226
LOSS: 1.1631813049316406
LOSS: 1.1619592905044556
LOSS: 1.160740852355957
LOSS: 1.159530520439148
LOSS: 1.1583356857299805
LOSS: 1.1571540832519531
LOSS: 1.1559960842132568
LOSS: 1.1548409461975098
LOSS: 1.1536900997161865
LOSS: 1.1525540351867676
LOSS: 1.1514443159103394
LOSS: 1.1503708362579346
LOSS: 1.1493306159973145
LOSS: 1.148328423500061
LOSS: 1.1473479270935059
LOSS: 1.1463913917541504
LOSS: 1.145450234413147
LOSS: 1.1445205211639404
LOSS: 1.1436095237731934
LOSS: 1.142715573310852
LOSS: 1.1418362855911255
LOSS: 1.1409721374511719
LOSS: 1.3626775741577148
LOSS: 1.3560960292816162
LOSS: 1.3498893976211548
LOSS: 1.3440946340560913
LOSS: 1.338746428489685
LOSS: 1.3338754177093506
LOSS: 1.3295027017593384
LOSS: 1.3256330490112305
LOSS: 1.3222486972808838
LOSS: 1.3192983865737915
LOSS: 1.316698670387268
LOSS: 1.3143353462219238
LOSS: 1.3120828866958618
LOSS: 1.309828519821167
LOSS: 1.3074846267700195
LOSS: 1.3049981594085693
LOSS: 1.3023450374603271
LOSS: 1.299524188041687
LOSS: 1.2965506315231323
LOSS: 1.293446660041809
LOSS: 1.2902429103851318
LOSS: 1.2869725227355957
LOSS: 1.2836689949035645
LOSS: 1.2803632020950317
LOSS: 1.277082085609436
LOSS: 1.2738521099090576
LOSS: 1.2706917524337769
LOSS: 1.2676122188568115
LOSS: 1.2646191120147705
LOSS: 1.2617141008377075
LOSS: 1.2588869333267212
LOSS: 1.256131649017334
LOSS: 1.253433346748352
LOSS: 1.2507805824279785
LOSS: 1.248160719871521
LOSS: 1.2455650568008423
LOSS: 1.24298894405365
LOSS: 1.2404292821884155
LOSS: 1.2378871440887451
LOSS: 1.2353659868240356
LOSS: 1.2328718900680542
LOSS: 1.230412483215332
LOSS: 1.2279964685440063
LOSS: 1.225630283355713
LOSS: 1.2233186960220337
LOSS: 1.2210662364959717
LOSS: 1.2188702821731567
LOSS: 1.2167322635650635
LOSS: 1.214641809463501
LOSS: 1.2125906944274902
LOSS: 1.2105640172958374
LOSS: 1.2085440158843994
LOSS: 1.2065112590789795
LOSS: 1.2044463157653809
LOSS: 1.2023323774337769
LOSS: 1.2001621723175049
LOSS: 1.1979304552078247
LOSS: 1.1956361532211304
LOSS: 1.1932817697525024
LOSS: 1.1908682584762573
LOSS: 1.1884009838104248
LOSS: 1.185878038406372
LOSS: 1.183294653892517
LOSS: 1.180638074874878
LOSS: 1.177881121635437
LOSS: 1.1749987602233887
LOSS: 1.171972393989563
LOSS: 1.1688058376312256
LOSS: 1.165500521659851
LOSS: 1.1620748043060303
LOSS: 1.158552885055542
LOSS: 1.1549646854400635
LOSS: 1.151330828666687
LOSS: 1.1476811170578003
LOSS: 1.1440314054489136
LOSS: 1.1404017210006714
LOSS: 1.136817216873169
LOSS: 1.1332908868789673
LOSS: 1.1298307180404663
LOSS: 1.1264429092407227
LOSS: 1.12313711643219
LOSS: 1.1199376583099365
LOSS: 1.1168642044067383
LOSS: 1.1139202117919922
LOSS: 1.1110988855361938
LOSS: 1.108397126197815
LOSS: 1.1058146953582764
LOSS: 1.1033389568328857
LOSS: 1.1009540557861328
LOSS: 1.0986249446868896
LOSS: 1.096331000328064
LOSS: 1.094064474105835
LOSS: 1.091831922531128
LOSS: 1.0896633863449097
LOSS: 1.0875310897827148
LOSS: 1.085500717163086
LOSS: 1.0835663080215454
LOSS: 1.0817369222640991
LOSS: 1.0800399780273438
LOSS: 1.0784558057785034
LOSS: 1.3626775741577148
LOSS: 1.3560960292816162
LOSS: 1.3498893976211548
LOSS: 1.3440946340560913
LOSS: 1.338746428489685
LOSS: 1.3338754177093506
LOSS: 1.3295027017593384
LOSS: 1.3256330490112305
LOSS: 1.3222486972808838
LOSS: 1.3192983865737915
LOSS: 1.316698670387268
LOSS: 1.3143353462219238
LOSS: 1.3120828866958618
LOSS: 1.309828519821167
LOSS: 1.3074846267700195
LOSS: 1.3049981594085693
LOSS: 1.3023450374603271
LOSS: 1.299524188041687
LOSS: 1.2965506315231323
LOSS: 1.293446660041809
LOSS: 1.2902429103851318
LOSS: 1.2869725227355957
LOSS: 1.2836689949035645
LOSS: 1.2803632020950317
LOSS: 1.277082085609436
LOSS: 1.2738521099090576
LOSS: 1.2706917524337769
LOSS: 1.2676122188568115
LOSS: 1.2646191120147705
LOSS: 1.2617141008377075
LOSS: 1.2588869333267212
LOSS: 1.256131649017334
LOSS: 1.253433346748352
LOSS: 1.2507805824279785
LOSS: 1.248160719871521
LOSS: 1.2455650568008423
LOSS: 1.24298894405365
LOSS: 1.2404292821884155
LOSS: 1.2378871440887451
LOSS: 1.2353659868240356
LOSS: 1.2328718900680542
LOSS: 1.230412483215332
LOSS: 1.2279964685440063
LOSS: 1.225630283355713
LOSS: 1.2233186960220337
LOSS: 1.2210662364959717
LOSS: 1.2188702821731567
LOSS: 1.2167322635650635
LOSS: 1.214641809463501
LOSS: 1.2125906944274902
LOSS: 1.2105640172958374
LOSS: 1.2085440158843994
LOSS: 1.2065112590789795
LOSS: 1.2044463157653809
LOSS: 1.2023323774337769
LOSS: 1.2001621723175049
LOSS: 1.1979304552078247
LOSS: 1.1956361532211304
LOSS: 1.1932817697525024
LOSS: 1.1908682584762573
LOSS: 1.1884009838104248
LOSS: 1.185878038406372
LOSS: 1.183294653892517
LOSS: 1.180638074874878
LOSS: 1.177881121635437
LOSS: 1.1749987602233887
LOSS: 1.171972393989563
LOSS: 1.1688058376312256
LOSS: 1.165500521659851
LOSS: 1.1620748043060303
LOSS: 1.158552885055542
LOSS: 1.1549646854400635
LOSS: 1.151330828666687
LOSS: 1.1476811170578003
LOSS: 1.1440314054489136
LOSS: 1.1404017210006714
LOSS: 1.136817216873169
LOSS: 1.1332908868789673
LOSS: 1.1298307180404663
LOSS: 1.1264429092407227
LOSS: 1.12313711643219
LOSS: 1.1199376583099365
LOSS: 1.1168642044067383
LOSS: 1.1139202117919922
LOSS: 1.1110988855361938
LOSS: 1.108397126197815
LOSS: 1.1058146953582764
LOSS: 1.1033389568328857
LOSS: 1.1009540557861328
LOSS: 1.0986249446868896
LOSS: 1.096331000328064
LOSS: 1.094064474105835
LOSS: 1.091831922531128
LOSS: 1.0896633863449097
LOSS: 1.0875310897827148
LOSS: 1.085500717163086
LOSS: 1.0835663080215454
LOSS: 1.0817369222640991
LOSS: 1.0800399780273438
LOSS: 1.0784558057785034
LOSS: 1.3751704692840576
LOSS: 1.3690059185028076
LOSS: 1.363236904144287
LOSS: 1.3579018115997314
LOSS: 1.353036642074585
LOSS: 1.348671555519104
LOSS: 1.344826579093933
LOSS: 1.3415037393569946
LOSS: 1.3386778831481934
LOSS: 1.3362916707992554
LOSS: 1.3342485427856445
LOSS: 1.3324278593063354
LOSS: 1.3306999206542969
LOSS: 1.3289532661437988
LOSS: 1.3271068334579468
LOSS: 1.3251147270202637
LOSS: 1.3229575157165527
LOSS: 1.3206415176391602
LOSS: 1.3181829452514648
LOSS: 1.315609097480774
LOSS: 1.3129507303237915
LOSS: 1.3102387189865112
LOSS: 1.3075041770935059
LOSS: 1.3047763109207153
LOSS: 1.302079439163208
LOSS: 1.2994351387023926

LOSS: 1.2832722663879395
LOSS: 1.280500888824463
LOSS: 1.2778234481811523
LOSS: 1.2752373218536377
LOSS: 1.2727348804473877
LOSS: 1.2703064680099487
LOSS: 1.2679399251937866
LOSS: 1.2656223773956299
LOSS: 1.263342022895813
LOSS: 1.261089563369751
LOSS: 1.258859634399414
LOSS: 1.2566474676132202
LOSS: 1.254454255104065
LOSS: 1.2522814273834229
LOSS: 1.2501307725906372
LOSS: 1.248005747795105
LOSS: 1.2459070682525635
LOSS: 1.2438313961029053
LOSS: 1.2417699098587036
LOSS: 1.239707589149475
LOSS: 1.2376281023025513
LOSS: 1.2355053424835205
LOSS: 1.2333112955093384
LOSS: 1.2310125827789307
LOSS: 1.2285730838775635
LOSS: 1.2259546518325806
LOSS: 1.2231242656707764
LOSS: 1.22005295753479
LOSS: 1.2167261838912964
LOSS: 1.2131385803222656
LOSS: 1.2092983722686768
LOSS: 1.2052216529846191
LOSS: 1.2009313106536865
LOSS: 1.1964468955993652
LOSS: 1.1917846202850342
LOSS: 1.1869581937789917
LOSS: 1.1819911003112793
LOSS: 1.176912546157837
LOSS: 1.1717594861984253
LOSS: 1.1665802001953125
LOSS: 1.1614296436309814
LOSS: 1.1563564538955688
LOSS: 1.1513909101486206
LOSS: 1.1465518474578857
LOSS: 1.1418501138687134
LOSS: 1.1372896432876587
LOSS: 1.132869005203247
LOSS: 1.1285901069641113
LOSS: 1.1244556903839111
LOSS: 1.1204698085784912
LOSS: 1.1166372299194336
LOSS: 1.1129623651504517
LOSS: 1.1094472408294678
LOSS: 1.1060909032821655
LOSS: 1.1028823852539062
LOSS: 1.0998040437698364
LOSS: 1.0968329906463623
LOSS: 1.0939520597457886
LOSS: 1.0911555290222168
LOSS: 1.0884499549865723
LOSS: 1.085848093032837
LOSS: 1.0833661556243896
LOSS: 1.081018328666687
LOSS: 1.0788148641586304
LOSS: 1.0767604112625122
LOSS: 1.074853777885437
LOSS: 1.0730851888656616
LOSS: 1.0714315176010132
LOSS: 1.0698559284210205
LOSS: 1.0683199167251587
LOSS: 1.066793441772461
LOSS: 1.06526517868042
LOSS: 1.0637379884719849
LOSS: 1.0622236728668213
LOSS: 1.3726768493652344
LOSS: 1.3664487600326538
LOSS: 1.3606116771697998
LOSS: 1.3552030324935913
LOSS: 1.35025954246521
LOSS: 1.3458125591278076
LOSS: 1.3418831825256348
LOSS: 1.3384772539138794
LOSS: 1.335576057434082
LOSS: 1.333129644393921
LOSS: 1.3310508728027344
LOSS: 1.3292245864868164
LOSS: 1.327523946762085
LOSS: 1.325832486152649
LOSS: 1.324060320854187
LOSS: 1.322152853012085
LOSS: 1.320085883140564
LOSS: 1.3178582191467285
LOSS: 1.3154860734939575
LOSS: 1.3129937648773193
LOSS: 1.3104140758514404
LOSS: 1.3077808618545532
LOSS: 1.3051254749298096
LOSS: 1.3024826049804688
LOSS: 1.2998805046081543
LOSS: 1.2973427772521973
LOSS: 1.294891595840454
LOSS: 1.29253351688385
LOSS: 1.290282964706421
LOSS: 1.2881405353546143
LOSS: 1.286101222038269
LOSS: 1.2841590642929077
LOSS: 1.2823067903518677
LOSS: 1.2805298566818237
LOSS: 1.2788236141204834
LOSS: 1.2771738767623901
LOSS: 1.2755804061889648
LOSS: 1.2740375995635986
LOSS: 1.2725474834442139
LOSS: 1.2711106538772583
LOSS: 1.2697346210479736
LOSS: 1.2684258222579956
LOSS: 1.267186164855957
LOSS: 1.2660257816314697
LOSS: 1.2649394273757935
LOSS: 1.2639269828796387
LOSS: 1.262969970703125
LOSS: 1.2620569467544556
LOSS: 1.2611688375473022
LOSS: 1.2602860927581787
LOSS: 1.2593871355056763
LOSS: 1.2584505081176758
LOSS: 1.2574570178985596
LOSS: 1.256386160850525
LOSS: 1.2552202939987183
LOSS: 1.2539476156234741
LOSS: 1.2525533437728882
LOSS: 1.25103759765625
LOSS: 1.2494043111801147
LOSS: 1.247658610343933
LOSS: 1.245818853378296
LOSS: 1.2438983917236328
LOSS: 1.2419099807739258
LOSS: 1.2398759126663208
LOSS: 1.2378000020980835
LOSS: 1.2357051372528076
LOSS: 1.2336066961288452
LOSS: 1.2315210103988647
LOSS: 1.229459285736084
LOSS: 1.2274385690689087
LOSS: 1.225473165512085
LOSS: 1.2235617637634277
LOSS: 1.2217081785202026
LOSS: 1.2199041843414307
LOSS: 1.218132495880127
LOSS: 1.2163742780685425
LOSS: 1.2146110534667969
LOSS: 1.2128289937973022
LOSS: 1.211010456085205
LOSS: 1.2091498374938965
LOSS: 1.2072359323501587
LOSS: 1.2052638530731201
LOSS: 1.2032421827316284
LOSS: 1.2011891603469849
LOSS: 1.1991263628005981
LOSS: 1.1970754861831665
LOSS: 1.1950465440750122
LOSS: 1.1930301189422607
LOSS: 1.1910172700881958
LOSS: 1.1889878511428833
LOSS: 1.1869407892227173
LOSS: 1.1848785877227783
LOSS: 1.1828142404556274
LOSS: 1.1807663440704346
LOSS: 1.1787585020065308
LOSS: 1.1768074035644531
LOSS: 1.1749144792556763
LOSS: 1.17307710647583
LOSS: 1.1712868213653564
LOSS: 1.16952645778656
LOSS: 1.3701744079589844
LOSS: 1.3638073205947876
LOSS: 1.357824444770813
LOSS: 1.352263331413269
LOSS: 1.3471596240997314
LOSS: 1.342545986175537
LOSS: 1.3384451866149902
LOSS: 1.3348661661148071
LOSS: 1.3317961692810059
LOSS: 1.3291923999786377
LOSS: 1.326978087425232
LOSS: 1.3250452280044556
LOSS: 1.323269009590149
LOSS: 1.3215289115905762
LOSS: 1.319728136062622
LOSS: 1.317801594734192
LOSS: 1.3157142400741577
LOSS: 1.3134599924087524
LOSS: 1.3110469579696655
LOSS: 1.3084981441497803
LOSS: 1.3058432340621948
LOSS: 1.3031139373779297
LOSS: 1.3003437519073486
LOSS: 1.2975654602050781
LOSS: 1.2948075532913208
LOSS: 1.29209566116333
LOSS: 1.2894493341445923
LOSS: 1.2868845462799072
LOSS: 1.2844078540802002
LOSS: 1.2820239067077637
LOSS: 1.279728889465332
LOSS: 1.2775157690048218
LOSS: 1.2753762006759644
LOSS: 1.2732971906661987
LOSS: 1.2712681293487549
LOSS: 1.2692779302597046
LOSS: 1.2673193216323853
LOSS: 1.2653868198394775
LOSS: 1.2634766101837158
LOSS: 1.261587142944336
LOSS: 1.2597192525863647
LOSS: 1.2578706741333008
LOSS: 1.2560393810272217
LOSS: 1.2542256116867065
LOSS: 1.2524234056472778
LOSS: 1.2506242990493774
LOSS: 1.2488207817077637
LOSS: 1.2469990253448486
LOSS: 1.245144009590149
LOSS: 1.24324631690979
LOSS: 1.2412948608398438
LOSS: 1.2392826080322266
LOSS: 1.2372167110443115
LOSS: 1.235100269317627
LOSS: 1.2329483032226562
LOSS: 1.2307770252227783
LOSS: 1.228602409362793
LOSS: 1.2264395952224731
LOSS: 1.2243001461029053
LOSS: 1.222192645072937
LOSS: 1.2201162576675415
LOSS: 1.2180770635604858
LOSS: 1.2160682678222656
LOSS: 1.214086651802063
LOSS: 1.2121379375457764
LOSS: 1.2102255821228027
LOSS: 1.2083654403686523
LOSS: 1.206552505493164
LOSS: 1.2047854661941528
LOSS: 1.2030479907989502
LOSS: 1.2013025283813477
LOSS: 1.1995304822921753
LOSS: 1.197701334953308
LOSS: 1.1958067417144775
LOSS: 1.1938523054122925
LOSS: 1.1918126344680786
LOSS: 1.189680576324463
LOSS: 1.1874558925628662
LOSS: 1.185146450996399
LOSS: 1.182776927947998
LOSS: 1.180374264717102
LOSS: 1.177972674369812
LOSS: 1.1755971908569336
LOSS: 1.1732585430145264
LOSS: 1.1709500551223755
LOSS: 1.168668270111084
LOSS: 1.1664092540740967
LOSS: 1.1641483306884766
LOSS: 1.1618614196777344
LOSS: 1.1595515012741089
LOSS: 1.1572548151016235
LOSS: 1.1550172567367554
LOSS: 1.15288245677948
LOSS: 1.1508740186691284
LOSS: 1.1489940881729126
LOSS: 1.147222638130188
LOSS: 1.1455247402191162
LOSS: 1.1438636779785156
LOSS: 1.1422135829925537
LOSS: 1.1405459642410278
LOSS: 1.3722448348999023
LOSS: 1.3660136461257935
LOSS: 1.3601750135421753
LOSS: 1.354766845703125
LOSS: 1.3498239517211914
LOSS: 1.3453774452209473
LOSS: 1.3414454460144043
LOSS: 1.3380287885665894
LOSS: 1.335101842880249
LOSS: 1.3326029777526855
LOSS: 1.3304333686828613
LOSS: 1.3284701108932495
LOSS: 1.3265849351882935
LOSS: 1.3246690034866333
LOSS: 1.3226450681686401
LOSS: 1.3204694986343384
LOSS: 1.3181281089782715
LOSS: 1.3156254291534424
LOSS: 1.3129793405532837
LOSS: 1.3102151155471802
LOSS: 1.3073630332946777
LOSS: 1.3044534921646118
LOSS: 1.3015165328979492
LOSS: 1.2985790967941284
LOSS: 1.2956663370132446
LOSS: 1.2927963733673096
LOSS: 1.2899842262268066
LOSS: 1.2872388362884521
LOSS: 1.28456449508667
LOSS: 1.2819600105285645
LOSS: 1.2794162034988403
LOSS: 1.276926875114441
LOSS: 1.274477243423462
LOSS: 1.2720528841018677
LOSS: 1.269636869430542
LOSS: 1.267212152481079
LOSS: 1.2647546529769897
LOSS: 1.2622385025024414
LOSS: 1.2596355676651
LOSS: 1.256913423538208
LOSS: 1.2540346384048462
LOSS: 1.2509543895721436
LOSS: 1.247623324394226
LOSS: 1.243990421295166
LOSS: 1.2400047779083252
LOSS: 1.2356208562850952
LOSS: 1.2308048009872437
LOSS: 1.2255454063415527
LOSS: 1.2198593616485596
LOSS: 1.213797926902771
LOSS: 1.2074391841888428
LOSS: 1.2008863687515259
LOSS: 1.194258451461792
LOSS: 1.1876757144927979
LOSS: 1.1812478303909302
LOSS: 1.1750611066818237
LOSS: 1.1691735982894897
LOSS: 1.0698559284210205
LOSS: 1.0683199167251587
LOSS: 1.066793441772461
LOSS: 1.06526517868042
LOSS: 1.0637379884719849
LOSS: 1.0622236728668213
LOSS: 1.3722448348999023
LOSS: 1.3660136461257935
LOSS: 1.3601750135421753
LOSS: 1.354766845703125
LOSS: 1.3498239517211914
LOSS: 1.3453774452209473
LOSS: 1.3414454460144043
LOSS: 1.3380287885665894
LOSS: 1.335101842880249
LOSS: 1.3326029777526855
LOSS: 1.3304333686828613
LOSS: 1.3284701108932495
LOSS: 1.3265849351882935
LOSS: 1.3246690034866333
LOSS: 1.3226450681686401
LOSS: 1.3204694986343384
LOSS: 1.3181281089782715
LOSS: 1.3156254291534424
LOSS: 1.3129793405532837
LOSS: 1.3102151155471802
LOSS: 1.3073630332946777
LOSS: 1.3044534921646118
LOSS: 1.3015165328979492
LOSS: 1.2985790967941284
LOSS: 1.2956663370132446
LOSS: 1.2927963733673096
LOSS: 1.2899842262268066
LOSS: 1.2872388362884521
LOSS: 1.28456449508667
LOSS: 1.2819600105285645
LOSS: 1.2794162034988403
LOSS: 1.276926875114441
LOSS: 1.274477243423462
LOSS: 1.2720528841018677
LOSS: 1.269636869430542
LOSS: 1.267212152481079
LOSS: 1.2647546529769897
LOSS: 1.2622385025024414
LOSS: 1.2596355676651
LOSS: 1.256913423538208
LOSS: 1.2540346384048462
LOSS: 1.2509543895721436
LOSS: 1.247623324394226
LOSS: 1.243990421295166
LOSS: 1.2400047779083252
LOSS: 1.2356208562850952
LOSS: 1.2308048009872437
LOSS: 1.2255454063415527
LOSS: 1.2198593616485596
LOSS: 1.213797926902771
LOSS: 1.2074391841888428
LOSS: 1.2008863687515259
LOSS: 1.194258451461792
LOSS: 1.1876757144927979
LOSS: 1.1812478303909302
LOSS: 1.1750611066818237
LOSS: 1.1691735982894897
LOSS: 1.1636091470718384
LOSS: 1.1583662033081055
LOSS: 1.153427004814148
LOSS: 1.1487666368484497
LOSS: 1.1443549394607544
LOSS: 1.1401615142822266
LOSS: 1.136157512664795
LOSS: 1.1323198080062866
LOSS: 1.1286321878433228
LOSS: 1.12507164478302
LOSS: 1.1216148138046265
LOSS: 1.1182526350021362
LOSS: 1.114991307258606
LOSS: 1.111843228340149
LOSS: 1.1088181734085083
LOSS: 1.1059200763702393
LOSS: 1.1031399965286255
LOSS: 1.1004654169082642
LOSS: 1.0978819131851196
LOSS: 1.0953733921051025
LOSS: 1.0929299592971802
LOSS: 1.090546727180481
LOSS: 1.0882225036621094
LOSS: 1.0859565734863281
LOSS: 1.0837467908859253
LOSS: 1.0815902948379517
LOSS: 1.0794907808303833
LOSS: 1.077460765838623
LOSS: 1.0755234956741333
LOSS: 1.0737029314041138
LOSS: 1.072016954421997
LOSS: 1.070469617843628
LOSS: 1.0690491199493408
LOSS: 1.0677297115325928
LOSS: 1.0664796829223633
LOSS: 1.065267562866211
LOSS: 1.064064860343933
LOSS: 1.062850832939148
LOSS: 1.0616141557693481
LOSS: 1.0603543519973755
LOSS: 1.0590828657150269
LOSS: 1.0578150749206543
LOSS: 1.0565674304962158
LOSS: 1.3709468841552734
LOSS: 1.3646938800811768
LOSS: 1.3588311672210693
LOSS: 1.3533953428268433
LOSS: 1.3484214544296265
LOSS: 1.343940258026123
LOSS: 1.3399707078933716
LOSS: 1.3365164995193481
LOSS: 1.3335548639297485
LOSS: 1.3310285806655884
LOSS: 1.3288476467132568
LOSS: 1.326892375946045
LOSS: 1.3250377178192139
LOSS: 1.3231719732284546
LOSS: 1.32121741771698
LOSS: 1.319125771522522
LOSS: 1.3168821334838867
LOSS: 1.314490795135498
LOSS: 1.3119693994522095
LOSS: 1.309346079826355
LOSS: 1.3066524267196655
LOSS: 1.3039222955703735
LOSS: 1.3011891841888428
LOSS: 1.2984851598739624
LOSS: 1.2958366870880127
LOSS: 1.2932677268981934
LOSS: 1.2907919883728027
LOSS: 1.2884231805801392
LOSS: 1.2861655950546265
LOSS: 1.2840170860290527
LOSS: 1.281973123550415
LOSS: 1.2800229787826538
LOSS: 1.2781537771224976
LOSS: 1.2763559818267822
LOSS: 1.2746168375015259
LOSS: 1.2729285955429077
LOSS: 1.2712849378585815
LOSS: 1.2696846723556519
LOSS: 1.2681262493133545
LOSS: 1.2666144371032715
LOSS: 1.2651517391204834
LOSS: 1.2637420892715454
LOSS: 1.262386441230774
LOSS: 1.2610849142074585
LOSS: 1.2598319053649902
LOSS: 1.2586164474487305
LOSS: 1.257423758506775
LOSS: 1.2562333345413208
LOSS: 1.25502347946167
LOSS: 1.2537676095962524
LOSS: 1.2524280548095703
LOSS: 1.2509592771530151
LOSS: 1.249316692352295
LOSS: 1.2474591732025146
LOSS: 1.2453482151031494
LOSS: 1.2429487705230713
LOSS: 1.2402199506759644
LOSS: 1.237112283706665
LOSS: 1.2335797548294067
LOSS: 1.2296046018600464
LOSS: 1.2251917123794556
LOSS: 1.2203691005706787
LOSS: 1.215190052986145
LOSS: 1.2097234725952148
LOSS: 1.2040528059005737
LOSS: 1.1982665061950684
LOSS: 1.1924551725387573
LOSS: 1.1867088079452515
LOSS: 1.1811168193817139
LOSS: 1.1757622957229614
LOSS: 1.1707206964492798
LOSS: 1.1660494804382324
LOSS: 1.1617813110351562
LOSS: 1.1579084396362305
LOSS: 1.154383897781372
LOSS: 1.1511505842208862
LOSS: 1.1481447219848633
LOSS: 1.145296573638916
LOSS: 1.1425327062606812
LOSS: 1.139790415763855
LOSS: 1.1370227336883545
LOSS: 1.1342021226882935
LOSS: 1.1313281059265137
LOSS: 1.1284215450286865
LOSS: 1.1255104541778564
LOSS: 1.1226252317428589
LOSS: 1.1197941303253174
LOSS: 1.1170393228530884
LOSS: 1.1143790483474731
LOSS: 1.1118276119232178
LOSS: 1.109399676322937
LOSS: 1.1071062088012695
LOSS: 1.1049526929855347
LOSS: 1.1029341220855713
LOSS: 1.1010353565216064
LOSS: 1.0992388725280762
LOSS: 1.0975292921066284
LOSS: 1.0958987474441528
LOSS: 1.0943448543548584
LOSS: 1.0928674936294556
LOSS: 1.3751704692840576
LOSS: 1.3690059185028076
LOSS: 1.363236904144287
LOSS: 1.3579018115997314
LOSS: 1.353036642074585
LOSS: 1.348671555519104
LOSS: 1.344826579093933
LOSS: 1.3415037393569946
LOSS: 1.3386778831481934
LOSS: 1.3362916707992554
LOSS: 1.3342485427856445
LOSS: 1.3324278593063354
LOSS: 1.3306999206542969
LOSS: 1.3289532661437988
LOSS: 1.3271068334579468
LOSS: 1.3251147270202637
LOSS: 1.3229575157165527
LOSS: 1.3206415176391602
LOSS: 1.3181829452514648
LOSS: 1.315609097480774
LOSS: 1.3129507303237915
LOSS: 1.3102387189865112
LOSS: 1.3075041770935059
LOSS: 1.3047763109207153
LOSS: 1.302079439163208
LOSS: 1.2994351387023926
LOSS: 1.2968581914901733
LOSS: 1.2943588495254517
LOSS: 1.291939616203308
LOSS: 1.2896032333374023
LOSS: 1.2873417139053345
LOSS: 1.2851442098617554
LOSS: 1.2830005884170532
LOSS: 1.2808946371078491
LOSS: 1.278814435005188
LOSS: 1.276741862297058
LOSS: 1.2746654748916626
LOSS: 1.2725709676742554
LOSS: 1.2704434394836426
LOSS: 1.268270492553711
LOSS: 1.26603364944458
LOSS: 1.2637156248092651
LOSS: 1.2612998485565186
LOSS: 1.2587671279907227
LOSS: 1.256108283996582
LOSS: 1.2533141374588013
LOSS: 1.2503904104232788
LOSS: 1.2473551034927368
LOSS: 1.244235634803772
LOSS: 1.2410743236541748
LOSS: 1.2379117012023926
LOSS: 1.2348016500473022
LOSS: 1.231794834136963
LOSS: 1.2289270162582397
LOSS: 1.226240873336792
LOSS: 1.223746418952942
LOSS: 1.2214514017105103
LOSS: 1.2193444967269897
LOSS: 1.217397689819336
LOSS: 1.2155860662460327
LOSS: 1.2138921022415161
LOSS: 1.2122912406921387
LOSS: 1.2107607126235962
LOSS: 1.2092763185501099
LOSS: 1.2077971696853638
LOSS: 1.2062865495681763
LOSS: 1.2047120332717896
LOSS: 1.2030439376831055
LOSS: 1.201264500617981
LOSS: 1.199358344078064
LOSS: 1.1973230838775635
LOSS: 1.1951712369918823
LOSS: 1.1929278373718262
LOSS: 1.1906181573867798
LOSS: 1.188280463218689
LOSS: 1.1859452724456787
LOSS: 1.1836440563201904
LOSS: 1.1813957691192627
LOSS: 1.1792280673980713
LOSS: 1.177160382270813
LOSS: 1.1751964092254639
LOSS: 1.1733413934707642
LOSS: 1.1715770959854126
LOSS: 1.1698949337005615
LOSS: 1.168290615081787
LOSS: 1.166759967803955
LOSS: 1.1653001308441162
LOSS: 1.163921594619751
LOSS: 1.1626113653182983
LOSS: 1.161352276802063
LOSS: 1.1601369380950928
LOSS: 1.158957600593567
LOSS: 1.157798409461975
LOSS: 1.1566513776779175
LOSS: 1.1555110216140747
LOSS: 1.1543569564819336
LOSS: 1.153196096420288
LOSS: 1.1520296335220337
LOSS: 1.1508512496948242
LOSS: 1.149675965309143
LOSS: 1.3682305812835693
LOSS: 1.3618581295013428
LOSS: 1.3558694124221802
LOSS: 1.3503022193908691
LOSS: 1.3451911211013794
LOSS: 1.340567946434021
LOSS: 1.3364548683166504
LOSS: 1.3328595161437988
LOSS: 1.3297643661499023
LOSS: 1.3271234035491943
LOSS: 1.324855089187622
LOSS: 1.3228470087051392
LOSS: 1.3209751844406128
LOSS: 1.3191235065460205
LOSS: 1.3172022104263306
LOSS: 1.315152883529663
LOSS: 1.312950849533081
LOSS: 1.3105911016464233
LOSS: 1.3080898523330688
LOSS: 1.3054699897766113
LOSS: 1.302761435508728
LOSS: 1.2999985218048096
LOSS: 1.2972142696380615
LOSS: 1.2944413423538208
LOSS: 1.2917073965072632
LOSS: 1.3020237684249878
LOSS: 1.2994811534881592
LOSS: 1.2969857454299927
LOSS: 1.2945516109466553
LOSS: 1.2921867370605469
LOSS: 1.2898918390274048
LOSS: 1.2876577377319336
LOSS: 1.2854688167572021
LOSS: 1.2833032608032227
LOSS: 1.2811329364776611
LOSS: 1.2789270877838135
LOSS: 1.2766512632369995
LOSS: 1.2742682695388794
LOSS: 1.271742343902588
LOSS: 1.269040584564209
LOSS: 1.2661360502243042
LOSS: 1.2630078792572021
LOSS: 1.2596462965011597
LOSS: 1.2560534477233887
LOSS: 1.252241611480713
LOSS: 1.248237133026123
LOSS: 1.2440776824951172
LOSS: 1.239806890487671
LOSS: 1.2354680299758911
LOSS: 1.231091022491455
LOSS: 1.2267073392868042
LOSS: 1.222346305847168
LOSS: 1.2180356979370117
LOSS: 1.2137970924377441
LOSS: 1.2096500396728516
LOSS: 1.2056145668029785
LOSS: 1.201711654663086
LOSS: 1.1979540586471558
LOSS: 1.1943507194519043
LOSS: 1.1909114122390747
LOSS: 1.187630534172058
LOSS: 1.1844958066940308
LOSS: 1.1814841032028198
LOSS: 1.1785705089569092
LOSS: 1.1757285594940186
LOSS: 1.1729387044906616
LOSS: 1.1701844930648804
LOSS: 1.167445182800293
LOSS: 1.1647063493728638
LOSS: 1.1619597673416138
LOSS: 1.159206748008728
LOSS: 1.1564542055130005
LOSS: 1.1537141799926758
LOSS: 1.150999903678894
LOSS: 1.1483228206634521
LOSS: 1.1456975936889648
LOSS: 1.1431336402893066
LOSS: 1.1406347751617432
LOSS: 1.138214111328125
LOSS: 1.1358883380889893
LOSS: 1.133675217628479
LOSS: 1.131585955619812
LOSS: 1.129618763923645
LOSS: 1.1277589797973633
LOSS: 1.1259782314300537
LOSS: 1.1242358684539795
LOSS: 1.122499704360962
LOSS: 1.1207568645477295
LOSS: 1.119010090827942
LOSS: 1.1172819137573242
LOSS: 1.1155996322631836
LOSS: 1.1139806509017944
LOSS: 1.112427830696106
LOSS: 1.11094331741333
LOSS: 1.1095283031463623
LOSS: 1.1081933975219727
LOSS: 1.1069390773773193
LOSS: 1.1057589054107666
LOSS: 1.1046313047409058
LOSS: 1.1035311222076416
LOSS: 1.102439045906067
LOSS: 1.101356029510498
LOSS: 1.3736768960952759
LOSS: 1.3674671649932861
LOSS: 1.3616496324539185
LOSS: 1.3562613725662231
LOSS: 1.3513380289077759
LOSS: 1.3469092845916748
LOSS: 1.3429948091506958
LOSS: 1.3395962715148926
LOSS: 1.3366886377334595
LOSS: 1.3342149257659912
LOSS: 1.3320801258087158
LOSS: 1.3301626443862915
LOSS: 1.3283360004425049
LOSS: 1.3264877796173096
LOSS: 1.3245422840118408
LOSS: 1.3224523067474365
LOSS: 1.320202350616455
LOSS: 1.3177958726882935
LOSS: 1.3152531385421753
LOSS: 1.3126007318496704
LOSS: 1.3098704814910889
LOSS: 1.307094931602478
LOSS: 1.3043066263198853
LOSS: 1.3015360832214355
LOSS: 1.2988104820251465
LOSS: 1.296150803565979
LOSS: 1.2935725450515747
LOSS: 1.2910867929458618
LOSS: 1.2886978387832642
LOSS: 1.2864031791687012
LOSS: 1.284196138381958
LOSS: 1.2820667028427124
LOSS: 1.280002236366272
LOSS: 1.277988076210022
LOSS: 1.2760111093521118
LOSS: 1.2740609645843506
LOSS: 1.272128939628601
LOSS: 1.2702091932296753
LOSS: 1.2682960033416748
LOSS: 1.2663873434066772
LOSS: 1.264474868774414
LOSS: 1.2625504732131958
LOSS: 1.2606052160263062
LOSS: 1.2586321830749512
LOSS: 1.2566273212432861
LOSS: 1.254579782485962
LOSS: 1.2524774074554443
LOSS: 1.2503079175949097
LOSS: 1.2480589151382446
LOSS: 1.2457188367843628
LOSS: 1.243283748626709
LOSS: 1.240752100944519
LOSS: 1.2381219863891602
LOSS: 1.2353826761245728
LOSS: 1.2325224876403809
LOSS: 1.2295280694961548
LOSS: 1.2263926267623901
LOSS: 1.2231097221374512
LOSS: 1.2196812629699707
LOSS: 1.2161155939102173
LOSS: 1.2124356031417847
LOSS: 1.2086737155914307
LOSS: 1.2048616409301758
LOSS: 1.201034426689148
LOSS: 1.1972168684005737
LOSS: 1.1934313774108887
LOSS: 1.189690113067627
LOSS: 1.1860140562057495
LOSS: 1.1824146509170532
LOSS: 1.1789138317108154
LOSS: 1.175533652305603
LOSS: 1.1722948551177979
LOSS: 1.1692174673080444
LOSS: 1.166316270828247
LOSS: 1.1635946035385132
LOSS: 1.1610406637191772
LOSS: 1.158638834953308
LOSS: 1.1563711166381836
LOSS: 1.15421462059021
LOSS: 1.1521559953689575
LOSS: 1.1501675844192505
LOSS: 1.1482325792312622
LOSS: 1.1463178396224976
LOSS: 1.144405722618103
LOSS: 1.1425118446350098
LOSS: 1.140637993812561
LOSS: 1.1387680768966675
LOSS: 1.136933445930481
LOSS: 1.1351206302642822
LOSS: 1.133339762687683
LOSS: 1.131602168083191
LOSS: 1.1299078464508057
LOSS: 1.128282070159912
LOSS: 1.126659870147705
LOSS: 1.1251386404037476
LOSS: 1.1235367059707642
LOSS: 1.121860146522522
LOSS: 1.120117425918579
LOSS: 1.1183260679244995
LOSS: 1.1164577007293701
LOSS: 1.3736768960952759
LOSS: 1.3674671649932861
LOSS: 1.3616496324539185
LOSS: 1.3562613725662231
LOSS: 1.3513380289077759
LOSS: 1.3469092845916748
LOSS: 1.3429948091506958
LOSS: 1.3395962715148926
LOSS: 1.3366886377334595
LOSS: 1.3342149257659912
LOSS: 1.3320801258087158
LOSS: 1.3301626443862915
LOSS: 1.3283360004425049
LOSS: 1.3264877796173096
LOSS: 1.3245422840118408
LOSS: 1.3224523067474365
LOSS: 1.320202350616455
LOSS: 1.3177958726882935
LOSS: 1.3152531385421753
LOSS: 1.3126007318496704
LOSS: 1.3098704814910889
LOSS: 1.307094931602478
LOSS: 1.3043066263198853
LOSS: 1.3015360832214355
LOSS: 1.2988104820251465
LOSS: 1.296150803565979
LOSS: 1.2935725450515747
LOSS: 1.2910867929458618
LOSS: 1.2886978387832642
LOSS: 1.2864031791687012
LOSS: 1.284196138381958
LOSS: 1.2820667028427124
LOSS: 1.280002236366272
LOSS: 1.277988076210022
LOSS: 1.2760111093521118
LOSS: 1.2740609645843506
LOSS: 1.272128939628601
LOSS: 1.2702091932296753
LOSS: 1.2682960033416748
LOSS: 1.2663873434066772
LOSS: 1.264474868774414
LOSS: 1.2625504732131958
LOSS: 1.2606052160263062
LOSS: 1.2586321830749512
LOSS: 1.2566273212432861
LOSS: 1.254579782485962
LOSS: 1.2524774074554443
LOSS: 1.2503079175949097
LOSS: 1.2480589151382446
LOSS: 1.2457188367843628
LOSS: 1.243283748626709
LOSS: 1.240752100944519
LOSS: 1.2381219863891602
LOSS: 1.2353826761245728
LOSS: 1.2325224876403809
LOSS: 1.2295280694961548
LOSS: 1.2263926267623901
LOSS: 1.2231097221374512
LOSS: 1.2196812629699707
LOSS: 1.2161155939102173
LOSS: 1.2124356031417847
LOSS: 1.2086737155914307
LOSS: 1.2048616409301758
LOSS: 1.201034426689148
LOSS: 1.1972168684005737
LOSS: 1.1934313774108887
LOSS: 1.189690113067627
LOSS: 1.1860140562057495
LOSS: 1.1824146509170532
LOSS: 1.1789138317108154
LOSS: 1.175533652305603
LOSS: 1.1722948551177979
LOSS: 1.1692174673080444
LOSS: 1.166316270828247
LOSS: 1.1635946035385132
LOSS: 1.1610406637191772
LOSS: 1.158638834953308
LOSS: 1.1563711166381836
LOSS: 1.15421462059021
LOSS: 1.1521559953689575
LOSS: 1.1501675844192505
LOSS: 1.1482325792312622
LOSS: 1.1463178396224976
LOSS: 1.144405722618103
LOSS: 1.1425118446350098
LOSS: 1.140637993812561
LOSS: 1.1387680768966675
LOSS: 1.136933445930481
LOSS: 1.1351206302642822
LOSS: 1.133339762687683
LOSS: 1.131602168083191
LOSS: 1.1299078464508057
LOSS: 1.128282070159912
LOSS: 1.126659870147705
LOSS: 1.1251386404037476
LOSS: 1.1235367059707642
LOSS: 1.121860146522522
LOSS: 1.120117425918579
LOSS: 1.1183260679244995
LOSS: 1.1164577007293701
LOSS: 1.3681843280792236
LOSS: 1.3617751598358154
LOSS: 1.3557476997375488
LOSS: 1.3501386642456055
LOSS: 1.3449838161468506
LOSS: 1.340314269065857
LOSS: 1.3361514806747437
LOSS: 1.3325022459030151
LOSS: 1.3293497562408447
LOSS: 1.3266479969024658
LOSS: 1.324313998222351
LOSS: 1.3222358226776123
LOSS: 1.320287823677063
LOSS: 1.3183516263961792
LOSS: 1.3163357973098755
LOSS: 1.3141800165176392
LOSS: 1.3118574619293213
LOSS: 1.3093640804290771
LOSS: 1.3067103624343872
LOSS: 1.3039195537567139
LOSS: 1.301023244857788
LOSS: 1.2980529069900513
LOSS: 1.295041561126709
LOSS: 1.2920198440551758
LOSS: 1.2890173196792603
LOSS: 1.286056399345398
LOSS: 1.2831571102142334
LOSS: 1.2803316116333008
LOSS: 1.277585744857788
LOSS: 1.274921178817749
LOSS: 1.2723318338394165
LOSS: 1.2698084115982056
LOSS: 1.2673375606536865
LOSS: 1.2649071216583252
LOSS: 1.2625048160552979
LOSS: 1.2601162195205688
LOSS: 1.2577327489852905
LOSS: 1.2553436756134033
LOSS: 1.2529447078704834
LOSS: 1.2505297660827637
LOSS: 1.2480961084365845
LOSS: 1.2456413507461548
LOSS: 1.2431634664535522
LOSS: 1.240661859512329
LOSS: 1.2381367683410645
LOSS: 1.2355917692184448
LOSS: 1.2330323457717896
LOSS: 1.2304621934890747
LOSS: 1.2278790473937988
LOSS: 1.2252801656723022
LOSS: 1.222663402557373
LOSS: 1.22002375125885
LOSS: 1.217356562614441
LOSS: 1.2146543264389038
LOSS: 1.2968581914901733
LOSS: 1.2943588495254517
LOSS: 1.291939616203308
LOSS: 1.2896032333374023
LOSS: 1.2873417139053345
LOSS: 1.2851442098617554
LOSS: 1.2830005884170532
LOSS: 1.2808946371078491
LOSS: 1.278814435005188
LOSS: 1.276741862297058
LOSS: 1.2746654748916626
LOSS: 1.2725709676742554
LOSS: 1.2704434394836426
LOSS: 1.268270492553711
LOSS: 1.26603364944458
LOSS: 1.2637156248092651
LOSS: 1.2612998485565186
LOSS: 1.2587671279907227
LOSS: 1.256108283996582
LOSS: 1.2533141374588013
LOSS: 1.2503904104232788
LOSS: 1.2473551034927368
LOSS: 1.244235634803772
LOSS: 1.2410743236541748
LOSS: 1.2379117012023926
LOSS: 1.2348016500473022
LOSS: 1.231794834136963
LOSS: 1.2289270162582397
LOSS: 1.226240873336792
LOSS: 1.223746418952942
LOSS: 1.2214514017105103
LOSS: 1.2193444967269897
LOSS: 1.217397689819336
LOSS: 1.2155860662460327
LOSS: 1.2138921022415161
LOSS: 1.2122912406921387
LOSS: 1.2107607126235962
LOSS: 1.2092763185501099
LOSS: 1.2077971696853638
LOSS: 1.2062865495681763
LOSS: 1.2047120332717896
LOSS: 1.2030439376831055
LOSS: 1.201264500617981
LOSS: 1.199358344078064
LOSS: 1.1973230838775635
LOSS: 1.1951712369918823
LOSS: 1.1929278373718262
LOSS: 1.1906181573867798
LOSS: 1.188280463218689
LOSS: 1.1859452724456787
LOSS: 1.1836440563201904
LOSS: 1.1813957691192627
LOSS: 1.1792280673980713
LOSS: 1.177160382270813
LOSS: 1.1751964092254639
LOSS: 1.1733413934707642
LOSS: 1.1715770959854126
LOSS: 1.1698949337005615
LOSS: 1.168290615081787
LOSS: 1.166759967803955
LOSS: 1.1653001308441162
LOSS: 1.163921594619751
LOSS: 1.1626113653182983
LOSS: 1.161352276802063
LOSS: 1.1601369380950928
LOSS: 1.158957600593567
LOSS: 1.157798409461975
LOSS: 1.1566513776779175
LOSS: 1.1555110216140747
LOSS: 1.1543569564819336
LOSS: 1.153196096420288
LOSS: 1.1520296335220337
LOSS: 1.1508512496948242
LOSS: 1.149675965309143
LOSS: 1.3701744079589844
LOSS: 1.3638073205947876
LOSS: 1.357824444770813
LOSS: 1.352263331413269
LOSS: 1.3471596240997314
LOSS: 1.342545986175537
LOSS: 1.3384451866149902
LOSS: 1.3348661661148071
LOSS: 1.3317961692810059
LOSS: 1.3291923999786377
LOSS: 1.326978087425232
LOSS: 1.3250452280044556
LOSS: 1.323269009590149
LOSS: 1.3215289115905762
LOSS: 1.319728136062622
LOSS: 1.317801594734192
LOSS: 1.3157142400741577
LOSS: 1.3134599924087524
LOSS: 1.3110469579696655
LOSS: 1.3084981441497803
LOSS: 1.3058432340621948
LOSS: 1.3031139373779297
LOSS: 1.3003437519073486
LOSS: 1.2975654602050781
LOSS: 1.2948075532913208
LOSS: 1.29209566116333
LOSS: 1.2894493341445923
LOSS: 1.2868845462799072
LOSS: 1.2844078540802002
LOSS: 1.2820239067077637
LOSS: 1.279728889465332
LOSS: 1.2775157690048218
LOSS: 1.2753762006759644
LOSS: 1.2732971906661987
LOSS: 1.2712681293487549
LOSS: 1.2692779302597046
LOSS: 1.2673193216323853
LOSS: 1.2653868198394775
LOSS: 1.2634766101837158
LOSS: 1.261587142944336
LOSS: 1.2597192525863647
LOSS: 1.2578706741333008
LOSS: 1.2560393810272217
LOSS: 1.2542256116867065
LOSS: 1.2524234056472778
LOSS: 1.2506242990493774
LOSS: 1.2488207817077637
LOSS: 1.2469990253448486
LOSS: 1.245144009590149
LOSS: 1.24324631690979
LOSS: 1.2412948608398438
LOSS: 1.2392826080322266
LOSS: 1.2372167110443115
LOSS: 1.235100269317627
LOSS: 1.2329483032226562
LOSS: 1.2307770252227783
LOSS: 1.228602409362793
LOSS: 1.2264395952224731
LOSS: 1.2243001461029053
LOSS: 1.222192645072937
LOSS: 1.2201162576675415
LOSS: 1.2180770635604858
LOSS: 1.2160682678222656
LOSS: 1.214086651802063
LOSS: 1.2121379375457764
LOSS: 1.2102255821228027
LOSS: 1.2083654403686523
LOSS: 1.206552505493164
LOSS: 1.2047854661941528
LOSS: 1.2030479907989502
LOSS: 1.2013025283813477
LOSS: 1.1995304822921753
LOSS: 1.197701334953308
LOSS: 1.1958067417144775
LOSS: 1.1938523054122925
LOSS: 1.1918126344680786
LOSS: 1.189680576324463
LOSS: 1.1874558925628662
LOSS: 1.185146450996399
LOSS: 1.182776927947998
LOSS: 1.180374264717102
LOSS: 1.177972674369812
LOSS: 1.1755971908569336
LOSS: 1.1732585430145264
LOSS: 1.1709500551223755
LOSS: 1.168668270111084
LOSS: 1.1664092540740967
LOSS: 1.1641483306884766
LOSS: 1.1618614196777344
LOSS: 1.1595515012741089
LOSS: 1.1572548151016235
LOSS: 1.1550172567367554
LOSS: 1.15288245677948
LOSS: 1.1508740186691284
LOSS: 1.1489940881729126
LOSS: 1.147222638130188
LOSS: 1.1455247402191162
LOSS: 1.1438636779785156
LOSS: 1.1422135829925537
LOSS: 1.1405459642410278
LOSS: 1.366335391998291
LOSS: 1.3598778247833252
LOSS: 1.3538007736206055
LOSS: 1.3481404781341553
LOSS: 1.3429323434829712
LOSS: 1.3382084369659424
LOSS: 1.3339905738830566
LOSS: 1.330287218093872
LOSS: 1.3270854949951172
LOSS: 1.324340581893921
LOSS: 1.3219764232635498
LOSS: 1.319885015487671
LOSS: 1.3179426193237305
LOSS: 1.3160325288772583
LOSS: 1.314059853553772
LOSS: 1.311963677406311
LOSS: 1.3097121715545654
LOSS: 1.3072993755340576
LOSS: 1.3047374486923218
LOSS: 1.3020495176315308
LOSS: 1.2992660999298096
LOSS: 1.2964192628860474
LOSS: 1.293543815612793
LOSS: 1.2906724214553833
LOSS: 1.287833333015442
LOSS: 1.2850526571273804
LOSS: 1.2823486328125
LOSS: 1.279734492301941
LOSS: 1.2772164344787598
LOSS: 1.274794340133667
LOSS: 1.2724629640579224
LOSS: 1.2702113389968872
LOSS: 1.2680273056030273
LOSS: 1.2658963203430176
LOSS: 1.2638046741485596
LOSS: 1.2617379426956177
LOSS: 1.2596839666366577
LOSS: 1.2576364278793335
LOSS: 1.2555902004241943
LOSS: 1.2535463571548462
LOSS: 1.251502513885498
LOSS: 1.2494641542434692
LOSS: 1.2474339008331299
LOSS: 1.2454147338867188
LOSS: 1.2434074878692627
LOSS: 1.241412878036499
LOSS: 1.239425778388977
LOSS: 1.2374389171600342
LOSS: 1.235445261001587
LOSS: 1.233435034751892
LOSS: 1.2314034700393677
LOSS: 1.229344129562378
LOSS: 1.2272554636001587
LOSS: 1.2251349687576294
LOSS: 1.2229819297790527
LOSS: 1.2207930088043213
LOSS: 1.2185603380203247
LOSS: 1.2162675857543945
LOSS: 1.2139005661010742
LOSS: 1.211443305015564
LOSS: 1.2088818550109863
LOSS: 1.2062000036239624
LOSS: 1.2033847570419312
LOSS: 1.2004284858703613
LOSS: 1.1973347663879395
LOSS: 1.1941142082214355
LOSS: 1.1907848119735718
LOSS: 1.1873743534088135
LOSS: 1.183914303779602
LOSS: 1.1804405450820923
LOSS: 1.1769951581954956
LOSS: 1.1736277341842651
LOSS: 1.1703888177871704
LOSS: 1.1673239469528198
LOSS: 1.1644710302352905
LOSS: 1.1618506908416748
LOSS: 1.159477710723877
LOSS: 1.157361626625061
LOSS: 1.1554877758026123
LOSS: 1.1537981033325195
LOSS: 1.1522055864334106
LOSS: 1.1506329774856567
LOSS: 1.149049162864685
LOSS: 1.147458791732788
LOSS: 1.145883560180664
LOSS: 1.1443464756011963
LOSS: 1.1428741216659546
LOSS: 1.141479253768921
LOSS: 1.1401711702346802
LOSS: 1.1389530897140503
LOSS: 1.1378145217895508
LOSS: 1.136732816696167
LOSS: 1.1356778144836426
LOSS: 1.1346349716186523
LOSS: 1.133603811264038
LOSS: 1.1325962543487549
LOSS: 1.1316250562667847
LOSS: 1.1306970119476318
LOSS: 1.129813551902771
LOSS: 1.12896728515625
LOSS: 1.3812003135681152
LOSS: 1.375308632850647
LOSS: 1.369825839996338
LOSS: 1.3647899627685547
LOSS: 1.3602361679077148
LOSS: 1.3561917543411255
LOSS: 1.352670431137085
LOSS: 1.3496618270874023
LOSS: 1.3471232652664185
LOSS: 1.3449724912643433
LOSS: 1.3430896997451782
LOSS: 1.3413399457931519
LOSS: 1.3395980596542358
LOSS: 1.3377695083618164
LOSS: 1.3357971906661987
LOSS: 1.3336575031280518
LOSS: 1.3313498497009277
LOSS: 1.3288893699645996
LOSS: 1.326301097869873
LOSS: 1.323614239692688
LOSS: 1.3208625316619873
LOSS: 1.3180768489837646
LOSS: 1.3152881860733032
LOSS: 1.3125240802764893
LOSS: 1.3098087310791016
LOSS: 1.3071606159210205
LOSS: 1.3045932054519653
LOSS: 1.3021152019500732
LOSS: 1.299728512763977
LOSS: 1.297430157661438
LOSS: 1.2952138185501099
LOSS: 1.2930700778961182
LOSS: 1.2909901142120361
LOSS: 1.2889630794525146
LOSS: 1.2869834899902344
LOSS: 1.2850497961044312
LOSS: 1.283154845237732
LOSS: 1.2813061475753784
LOSS: 1.279505968093872
LOSS: 1.2777642011642456
LOSS: 1.2760872840881348
LOSS: 1.2744816541671753
LOSS: 1.2729535102844238
LOSS: 1.271499514579773
LOSS: 1.2701162099838257
LOSS: 1.2688008546829224
LOSS: 1.267539381980896
LOSS: 1.2663140296936035
LOSS: 1.2651013135910034
LOSS: 1.263892650604248
LOSS: 1.2626677751541138
LOSS: 1.2614024877548218
LOSS: 1.2600913047790527
LOSS: 1.2587283849716187
LOSS: 1.2573007345199585
LOSS: 1.2558200359344482
LOSS: 1.2542864084243774
LOSS: 1.1636091470718384
LOSS: 1.1583662033081055
LOSS: 1.153427004814148
LOSS: 1.1487666368484497
LOSS: 1.1443549394607544
LOSS: 1.1401615142822266
LOSS: 1.136157512664795
LOSS: 1.1323198080062866
LOSS: 1.1286321878433228
LOSS: 1.12507164478302
LOSS: 1.1216148138046265
LOSS: 1.1182526350021362
LOSS: 1.114991307258606
LOSS: 1.111843228340149
LOSS: 1.1088181734085083
LOSS: 1.1059200763702393
LOSS: 1.1031399965286255
LOSS: 1.1004654169082642
LOSS: 1.0978819131851196
LOSS: 1.0953733921051025
LOSS: 1.0929299592971802
LOSS: 1.090546727180481
LOSS: 1.0882225036621094
LOSS: 1.0859565734863281
LOSS: 1.0837467908859253
LOSS: 1.0815902948379517
LOSS: 1.0794907808303833
LOSS: 1.077460765838623
LOSS: 1.0755234956741333
LOSS: 1.0737029314041138
LOSS: 1.072016954421997
LOSS: 1.070469617843628
LOSS: 1.0690491199493408
LOSS: 1.0677297115325928
LOSS: 1.0664796829223633
LOSS: 1.065267562866211
LOSS: 1.064064860343933
LOSS: 1.062850832939148
LOSS: 1.0616141557693481
LOSS: 1.0603543519973755
LOSS: 1.0590828657150269
LOSS: 1.0578150749206543
LOSS: 1.0565674304962158
LOSS: 1.3709468841552734
LOSS: 1.3646938800811768
LOSS: 1.3588311672210693
LOSS: 1.3533953428268433
LOSS: 1.3484214544296265
LOSS: 1.343940258026123
LOSS: 1.3399707078933716
LOSS: 1.3365164995193481
LOSS: 1.3335548639297485
LOSS: 1.3310285806655884
LOSS: 1.3288476467132568
LOSS: 1.326892375946045
LOSS: 1.3250377178192139
LOSS: 1.3231719732284546
LOSS: 1.32121741771698
LOSS: 1.319125771522522
LOSS: 1.3168821334838867
LOSS: 1.314490795135498
LOSS: 1.3119693994522095
LOSS: 1.309346079826355
LOSS: 1.3066524267196655
LOSS: 1.3039222955703735
LOSS: 1.3011891841888428
LOSS: 1.2984851598739624
LOSS: 1.2958366870880127
LOSS: 1.2932677268981934
LOSS: 1.2907919883728027
LOSS: 1.2884231805801392
LOSS: 1.2861655950546265
LOSS: 1.2840170860290527
LOSS: 1.281973123550415
LOSS: 1.2800229787826538
LOSS: 1.2781537771224976
LOSS: 1.2763559818267822
LOSS: 1.2746168375015259
LOSS: 1.2729285955429077
LOSS: 1.2712849378585815
LOSS: 1.2696846723556519
LOSS: 1.2681262493133545
LOSS: 1.2666144371032715
LOSS: 1.2651517391204834
LOSS: 1.2637420892715454
LOSS: 1.262386441230774
LOSS: 1.2610849142074585
LOSS: 1.2598319053649902
LOSS: 1.2586164474487305
LOSS: 1.257423758506775
LOSS: 1.2562333345413208
LOSS: 1.25502347946167
LOSS: 1.2537676095962524
LOSS: 1.2524280548095703
LOSS: 1.2509592771530151
LOSS: 1.249316692352295
LOSS: 1.2474591732025146
LOSS: 1.2453482151031494
LOSS: 1.2429487705230713
LOSS: 1.2402199506759644
LOSS: 1.237112283706665
LOSS: 1.2335797548294067
LOSS: 1.2296046018600464
LOSS: 1.2251917123794556
LOSS: 1.2203691005706787
LOSS: 1.215190052986145
LOSS: 1.2097234725952148
LOSS: 1.2040528059005737
LOSS: 1.1982665061950684
LOSS: 1.1924551725387573
LOSS: 1.1867088079452515
LOSS: 1.1811168193817139
LOSS: 1.1757622957229614
LOSS: 1.1707206964492798
LOSS: 1.1660494804382324
LOSS: 1.1617813110351562
LOSS: 1.1579084396362305
LOSS: 1.154383897781372
LOSS: 1.1511505842208862
LOSS: 1.1481447219848633
LOSS: 1.145296573638916
LOSS: 1.1425327062606812
LOSS: 1.139790415763855
LOSS: 1.1370227336883545
LOSS: 1.1342021226882935
LOSS: 1.1313281059265137
LOSS: 1.1284215450286865
LOSS: 1.1255104541778564
LOSS: 1.1226252317428589
LOSS: 1.1197941303253174
LOSS: 1.1170393228530884
LOSS: 1.1143790483474731
LOSS: 1.1118276119232178
LOSS: 1.109399676322937
LOSS: 1.1071062088012695
LOSS: 1.1049526929855347
LOSS: 1.1029341220855713
LOSS: 1.1010353565216064
LOSS: 1.0992388725280762
LOSS: 1.0975292921066284
LOSS: 1.0958987474441528
LOSS: 1.0943448543548584
LOSS: 1.0928674936294556
LOSS: 1.3774969577789307
LOSS: 1.3714007139205933
LOSS: 1.3657021522521973
LOSS: 1.360439658164978
LOSS: 1.3556488752365112
LOSS: 1.3513612747192383
LOSS: 1.3475968837738037
LOSS: 1.3443572521209717
LOSS: 1.3416202068328857
LOSS: 1.3393278121948242
LOSS: 1.3373881578445435
LOSS: 1.335680603981018
LOSS: 1.3340741395950317
LOSS: 1.332459568977356
LOSS: 1.3307538032531738
LOSS: 1.3289082050323486
LOSS: 1.3269041776657104
LOSS: 1.324746012687683
LOSS: 1.322450041770935
LOSS: 1.3200432062149048
LOSS: 1.317555546760559
LOSS: 1.315019130706787
LOSS: 1.3124637603759766
LOSS: 1.3099168539047241
LOSS: 1.3074026107788086
LOSS: 1.3049383163452148
LOSS: 1.3025392293930054
LOSS: 1.3002122640609741
LOSS: 1.297959566116333
LOSS: 1.2957779169082642
LOSS: 1.2936584949493408
LOSS: 1.2915898561477661
LOSS: 1.2895548343658447
LOSS: 1.2875374555587769
LOSS: 1.2855159044265747
LOSS: 1.2834633588790894
LOSS: 1.281353235244751
LOSS: 1.279158115386963
LOSS: 1.2768515348434448
LOSS: 1.2744059562683105
LOSS: 1.271795392036438
LOSS: 1.269000768661499
LOSS: 1.2660080194473267
LOSS: 1.2628142833709717
LOSS: 1.2594178915023804
LOSS: 1.2558202743530273
LOSS: 1.25203275680542
LOSS: 1.2480701208114624
LOSS: 1.2439550161361694
LOSS: 1.2397124767303467
LOSS: 1.235372543334961
LOSS: 1.2309705018997192
LOSS: 1.2265450954437256
LOSS: 1.2221378087997437
LOSS: 1.2177857160568237
LOSS: 1.2135130167007446
LOSS: 1.209340214729309
LOSS: 1.205272912979126
LOSS: 1.2013001441955566
LOSS: 1.1974064111709595
LOSS: 1.193564534187317
LOSS: 1.1897484064102173
LOSS: 1.185928225517273
LOSS: 1.182085394859314
LOSS: 1.178196907043457
LOSS: 1.174257516860962
LOSS: 1.1702677011489868
LOSS: 1.16623055934906
LOSS: 1.1621580123901367
LOSS: 1.158068060874939
LOSS: 1.1539803743362427
LOSS: 1.1499110460281372
LOSS: 1.1458699703216553
LOSS: 1.1418627500534058
LOSS: 1.1378871202468872
LOSS: 1.1339433193206787
LOSS: 1.1300380229949951
LOSS: 1.1261942386627197
LOSS: 1.1224571466445923
LOSS: 1.1188867092132568
LOSS: 1.1155383586883545
LOSS: 1.1124449968338013
LOSS: 1.109604835510254
LOSS: 1.1069865226745605
LOSS: 1.1045418977737427
LOSS: 1.1022189855575562
LOSS: 1.0999683141708374
LOSS: 1.0977567434310913
LOSS: 1.095578670501709
LOSS: 1.0934512615203857
LOSS: 1.091402530670166
LOSS: 1.0894588232040405
LOSS: 1.087634563446045
LOSS: 1.0859253406524658
LOSS: 1.0843095779418945
LOSS: 1.0827581882476807
LOSS: 1.0812432765960693
LOSS: 1.0797452926635742
LOSS: 1.0782537460327148
LOSS: 1.0767698287963867
LOSS: 1.3681843280792236
LOSS: 1.3617751598358154
LOSS: 1.3557476997375488
LOSS: 1.3501386642456055
LOSS: 1.3449838161468506
LOSS: 1.340314269065857
LOSS: 1.3361514806747437
LOSS: 1.3325022459030151
LOSS: 1.3293497562408447
LOSS: 1.3266479969024658
LOSS: 1.324313998222351
LOSS: 1.3222358226776123
LOSS: 1.320287823677063
LOSS: 1.3183516263961792
LOSS: 1.3163357973098755
LOSS: 1.3141800165176392
LOSS: 1.3118574619293213
LOSS: 1.3093640804290771
LOSS: 1.3067103624343872
LOSS: 1.3039195537567139
LOSS: 1.301023244857788
LOSS: 1.2980529069900513
LOSS: 1.295041561126709
LOSS: 1.2920198440551758
LOSS: 1.2890173196792603
LOSS: 1.286056399345398
LOSS: 1.2831571102142334
LOSS: 1.2803316116333008
LOSS: 1.277585744857788
LOSS: 1.274921178817749
LOSS: 1.2723318338394165
LOSS: 1.2698084115982056
LOSS: 1.2673375606536865
LOSS: 1.2649071216583252
LOSS: 1.2625048160552979
LOSS: 1.2601162195205688
LOSS: 1.2577327489852905
LOSS: 1.2553436756134033
LOSS: 1.2529447078704834
LOSS: 1.2505297660827637
LOSS: 1.2480961084365845
LOSS: 1.2456413507461548
LOSS: 1.2431634664535522
LOSS: 1.240661859512329
LOSS: 1.2381367683410645
LOSS: 1.2355917692184448
LOSS: 1.2330323457717896
LOSS: 1.2304621934890747
LOSS: 1.2278790473937988
LOSS: 1.2252801656723022
LOSS: 1.222663402557373
LOSS: 1.22002375125885
LOSS: 1.217356562614441
LOSS: 1.2146543264389038
LOSS: 1.211905598640442
LOSS: 1.2090957164764404
LOSS: 1.2062026262283325
LOSS: 1.2032045125961304
LOSS: 1.2000880241394043
LOSS: 1.196844220161438
LOSS: 1.1934741735458374
LOSS: 1.1899791955947876
LOSS: 1.1863656044006348
LOSS: 1.182645559310913
LOSS: 1.1788318157196045
LOSS: 1.1749430894851685
LOSS: 1.1710129976272583
LOSS: 1.167075753211975
LOSS: 1.1631629467010498
LOSS: 1.159292459487915
LOSS: 1.155470371246338
LOSS: 1.1516979932785034
LOSS: 1.14798104763031
LOSS: 1.1443382501602173
LOSS: 1.1408064365386963
LOSS: 1.1374415159225464
LOSS: 1.1342931985855103
LOSS: 1.1314126253128052
LOSS: 1.1288241147994995
LOSS: 1.1265264749526978
LOSS: 1.1244924068450928
LOSS: 1.1226803064346313
LOSS: 1.1210542917251587
LOSS: 1.119575023651123
LOSS: 1.1182069778442383
LOSS: 1.1169025897979736
LOSS: 1.1156097650527954
LOSS: 1.1142947673797607
LOSS: 1.211905598640442
LOSS: 1.2090957164764404
LOSS: 1.2062026262283325
LOSS: 1.2032045125961304
LOSS: 1.2000880241394043
LOSS: 1.196844220161438
LOSS: 1.1934741735458374
LOSS: 1.1899791955947876
LOSS: 1.1863656044006348
LOSS: 1.182645559310913
LOSS: 1.1788318157196045
LOSS: 1.1749430894851685
LOSS: 1.1710129976272583
LOSS: 1.167075753211975
LOSS: 1.1631629467010498
LOSS: 1.159292459487915
LOSS: 1.155470371246338
LOSS: 1.1516979932785034
LOSS: 1.14798104763031
LOSS: 1.1443382501602173
LOSS: 1.1408064365386963
LOSS: 1.1374415159225464
LOSS: 1.1342931985855103
LOSS: 1.1314126253128052
LOSS: 1.1288241147994995
LOSS: 1.1265264749526978
LOSS: 1.1244924068450928
LOSS: 1.1226803064346313
LOSS: 1.1210542917251587
LOSS: 1.119575023651123
LOSS: 1.1182069778442383
LOSS: 1.1169025897979736
LOSS: 1.1156097650527954
LOSS: 1.1142947673797607
LOSS: 1.1129316091537476
LOSS: 1.1115202903747559
LOSS: 1.1100878715515137
LOSS: 1.1086623668670654
LOSS: 1.1072934865951538
LOSS: 1.1060123443603516
LOSS: 1.1048475503921509
LOSS: 1.1038177013397217
LOSS: 1.1029249429702759
LOSS: 1.1021543741226196
LOSS: 1.1014735698699951
LOSS: 1.1008412837982178
LOSS: 1.366335391998291
LOSS: 1.3598778247833252
LOSS: 1.3538007736206055
LOSS: 1.3481404781341553
LOSS: 1.3429323434829712
LOSS: 1.3382084369659424
LOSS: 1.3339905738830566
LOSS: 1.330287218093872
LOSS: 1.3270854949951172
LOSS: 1.324340581893921
LOSS: 1.3219764232635498
LOSS: 1.319885015487671
LOSS: 1.3179426193237305
LOSS: 1.3160325288772583
LOSS: 1.314059853553772
LOSS: 1.311963677406311
LOSS: 1.3097121715545654
LOSS: 1.3072993755340576
LOSS: 1.3047374486923218
LOSS: 1.3020495176315308
LOSS: 1.2992660999298096
LOSS: 1.2964192628860474
LOSS: 1.293543815612793
LOSS: 1.2906724214553833
LOSS: 1.287833333015442
LOSS: 1.2850526571273804
LOSS: 1.2823486328125
LOSS: 1.279734492301941
LOSS: 1.2772164344787598
LOSS: 1.274794340133667
LOSS: 1.2724629640579224
LOSS: 1.2702113389968872
LOSS: 1.2680273056030273
LOSS: 1.2658963203430176
LOSS: 1.2638046741485596
LOSS: 1.2617379426956177
LOSS: 1.2596839666366577
LOSS: 1.2576364278793335
LOSS: 1.2555902004241943
LOSS: 1.2535463571548462
LOSS: 1.251502513885498
LOSS: 1.2494641542434692
LOSS: 1.2474339008331299
LOSS: 1.2454147338867188
LOSS: 1.2434074878692627
LOSS: 1.241412878036499
LOSS: 1.239425778388977
LOSS: 1.2374389171600342
LOSS: 1.235445261001587
LOSS: 1.233435034751892
LOSS: 1.2314034700393677
LOSS: 1.229344129562378
LOSS: 1.2272554636001587
LOSS: 1.2251349687576294
LOSS: 1.2229819297790527
LOSS: 1.2207930088043213
LOSS: 1.2185603380203247
LOSS: 1.2162675857543945
LOSS: 1.2139005661010742
LOSS: 1.211443305015564
LOSS: 1.2088818550109863
LOSS: 1.2062000036239624
LOSS: 1.2033847570419312
LOSS: 1.2004284858703613
LOSS: 1.1973347663879395
LOSS: 1.1941142082214355
LOSS: 1.1907848119735718
LOSS: 1.1873743534088135
LOSS: 1.183914303779602
LOSS: 1.1804405450820923
LOSS: 1.1769951581954956
LOSS: 1.1736277341842651
LOSS: 1.1703888177871704
LOSS: 1.1673239469528198
LOSS: 1.1644710302352905
LOSS: 1.1618506908416748
LOSS: 1.159477710723877
LOSS: 1.157361626625061
LOSS: 1.1554877758026123
LOSS: 1.1537981033325195
LOSS: 1.1522055864334106
LOSS: 1.1506329774856567
LOSS: 1.149049162864685
LOSS: 1.147458791732788
LOSS: 1.145883560180664
LOSS: 1.1443464756011963
LOSS: 1.1428741216659546
LOSS: 1.141479253768921
LOSS: 1.1401711702346802
LOSS: 1.1389530897140503
LOSS: 1.1378145217895508
LOSS: 1.136732816696167
LOSS: 1.1356778144836426
LOSS: 1.1346349716186523
LOSS: 1.133603811264038
LOSS: 1.1325962543487549
LOSS: 1.1316250562667847
LOSS: 1.1306970119476318
LOSS: 1.129813551902771
LOSS: 1.12896728515625
LOSS: 1.3812003135681152
LOSS: 1.375308632850647
LOSS: 1.369825839996338
LOSS: 1.3647899627685547
LOSS: 1.3602361679077148
LOSS: 1.3561917543411255
LOSS: 1.352670431137085
LOSS: 1.3496618270874023
LOSS: 1.3471232652664185
LOSS: 1.3449724912643433
LOSS: 1.3430896997451782
LOSS: 1.3413399457931519
LOSS: 1.3395980596542358
LOSS: 1.3377695083618164
LOSS: 1.3357971906661987
LOSS: 1.3336575031280518
LOSS: 1.3313498497009277
LOSS: 1.3288893699645996
LOSS: 1.326301097869873
LOSS: 1.323614239692688
LOSS: 1.3208625316619873
LOSS: 1.3180768489837646
LOSS: 1.3152881860733032
LOSS: 1.3125240802764893
LOSS: 1.3098087310791016
LOSS: 1.3071606159210205
LOSS: 1.3045932054519653
LOSS: 1.3021152019500732
LOSS: 1.299728512763977
LOSS: 1.297430157661438
LOSS: 1.2952138185501099
LOSS: 1.2930700778961182
LOSS: 1.2909901142120361
LOSS: 1.2889630794525146
LOSS: 1.2869834899902344
LOSS: 1.2850497961044312
LOSS: 1.283154845237732
LOSS: 1.2813061475753784
LOSS: 1.279505968093872
LOSS: 1.2777642011642456
LOSS: 1.2760872840881348
LOSS: 1.2744816541671753
LOSS: 1.2729535102844238
LOSS: 1.271499514579773
LOSS: 1.2701162099838257
LOSS: 1.2688008546829224
LOSS: 1.267539381980896
LOSS: 1.2663140296936035
LOSS: 1.2651013135910034
LOSS: 1.263892650604248
LOSS: 1.2626677751541138
LOSS: 1.2614024877548218
LOSS: 1.2600913047790527
LOSS: 1.2587283849716187
LOSS: 1.2573007345199585
LOSS: 1.2558200359344482
LOSS: 1.2542864084243774
LOSS: 1.2527040243148804
LOSS: 1.2510783672332764
LOSS: 1.2494127750396729
LOSS: 1.2477104663848877
LOSS: 1.2459765672683716
LOSS: 1.2442114353179932
LOSS: 1.2424075603485107
LOSS: 1.2405600547790527
LOSS: 1.2386711835861206
LOSS: 1.2367218732833862
LOSS: 1.2347221374511719
LOSS: 1.2326854467391968
LOSS: 1.2305964231491089
LOSS: 1.2285089492797852
LOSS: 1.226389765739441
LOSS: 1.2242786884307861
LOSS: 1.2221930027008057
LOSS: 1.2201334238052368
LOSS: 1.2181366682052612
LOSS: 1.2161844968795776
LOSS: 1.2142966985702515
LOSS: 1.2124818563461304
LOSS: 1.210749864578247
LOSS: 1.2091175317764282
LOSS: 1.2075973749160767
LOSS: 1.2061823606491089
LOSS: 1.2048561573028564
LOSS: 1.203617811203003
LOSS: 1.2024961709976196
LOSS: 1.2014747858047485
LOSS: 1.2005558013916016
LOSS: 1.1997261047363281
LOSS: 1.1989718675613403
LOSS: 1.1982532739639282
LOSS: 1.1975690126419067
LOSS: 1.1968882083892822
LOSS: 1.1962014436721802
LOSS: 1.195502519607544
LOSS: 1.1947872638702393
LOSS: 1.19405996799469
LOSS: 1.193332552909851
LOSS: 1.1926149129867554
LOSS: 1.1919065713882446
LOSS: 1.369425892829895
LOSS: 1.3630605936050415
LOSS: 1.357079267501831
LOSS: 1.3515183925628662
LOSS: 1.3464149236679077
LOSS: 1.341800570487976
LOSS: 1.3376970291137695
LOSS: 1.3341127634048462
LOSS: 1.3310333490371704
LOSS: 1.3284144401550293
LOSS: 1.3261756896972656
LOSS: 1.3242075443267822
LOSS: 1.3223849534988403
LOSS: 1.3205883502960205
LOSS: 1.3187216520309448
LOSS: 1.3167243003845215
LOSS: 1.3145647048950195
LOSS: 1.3122351169586182
LOSS: 1.309748649597168
LOSS: 1.3071260452270508
LOSS: 1.3043988943099976
LOSS: 1.3015989065170288
LOSS: 1.2987611293792725
LOSS: 1.2959156036376953
LOSS: 1.2930935621261597
LOSS: 1.2903176546096802
LOSS: 1.2876096963882446
LOSS: 1.2849812507629395
LOSS: 1.2824394702911377
LOSS: 1.2799848318099976
LOSS: 1.2776119709014893
LOSS: 1.2753119468688965
LOSS: 1.273073434829712
LOSS: 1.2708827257156372
LOSS: 1.2687289714813232
LOSS: 1.2666014432907104
LOSS: 1.2644931077957153
LOSS: 1.2623995542526245
LOSS: 1.2603187561035156
LOSS: 1.2582529783248901
LOSS: 1.2562024593353271
LOSS: 1.2541683912277222
LOSS: 1.2521474361419678
LOSS: 1.250139832496643
LOSS: 1.2481392621994019
LOSS: 1.246140718460083
LOSS: 1.2441356182098389
LOSS: 1.2421115636825562
LOSS: 1.2400528192520142
LOSS: 1.2379364967346191
LOSS: 1.2357395887374878
LOSS: 1.2334492206573486
LOSS: 1.231064796447754
LOSS: 1.2285964488983154
LOSS: 1.2260435819625854
LOSS: 1.2234106063842773
LOSS: 1.2207109928131104
LOSS: 1.2179601192474365
LOSS: 1.2151716947555542
LOSS: 1.212348461151123
LOSS: 1.2094790935516357
LOSS: 1.2065402269363403
LOSS: 1.203506350517273
LOSS: 1.2003533840179443
LOSS: 1.197074294090271
LOSS: 1.1936559677124023
LOSS: 1.1901086568832397
LOSS: 1.186429500579834
LOSS: 1.182637095451355
LOSS: 1.1787629127502441
LOSS: 1.1748374700546265
LOSS: 1.170897364616394
LOSS: 1.1669697761535645
LOSS: 1.163088083267212
LOSS: 1.159280776977539
LOSS: 1.155583381652832
LOSS: 1.1520267724990845
LOSS: 1.1486480236053467
LOSS: 1.1454836130142212
LOSS: 1.1425648927688599
LOSS: 1.1399091482162476
LOSS: 1.1375186443328857
LOSS: 1.1353671550750732
LOSS: 1.1334011554718018
LOSS: 1.2527040243148804
LOSS: 1.2510783672332764
LOSS: 1.2494127750396729
LOSS: 1.2477104663848877
LOSS: 1.2459765672683716
LOSS: 1.2442114353179932
LOSS: 1.2424075603485107
LOSS: 1.2405600547790527
LOSS: 1.2386711835861206
LOSS: 1.2367218732833862
LOSS: 1.2347221374511719
LOSS: 1.2326854467391968
LOSS: 1.2305964231491089
LOSS: 1.2285089492797852
LOSS: 1.226389765739441
LOSS: 1.2242786884307861
LOSS: 1.2221930027008057
LOSS: 1.2201334238052368
LOSS: 1.2181366682052612
LOSS: 1.2161844968795776
LOSS: 1.2142966985702515
LOSS: 1.2124818563461304
LOSS: 1.210749864578247
LOSS: 1.2091175317764282
LOSS: 1.2075973749160767
LOSS: 1.2061823606491089
LOSS: 1.2048561573028564
LOSS: 1.203617811203003
LOSS: 1.2024961709976196
LOSS: 1.2014747858047485
LOSS: 1.2005558013916016
LOSS: 1.1997261047363281
LOSS: 1.1989718675613403
LOSS: 1.1982532739639282
LOSS: 1.1975690126419067
LOSS: 1.1968882083892822
LOSS: 1.1962014436721802
LOSS: 1.195502519607544
LOSS: 1.1947872638702393
LOSS: 1.19405996799469
LOSS: 1.193332552909851
LOSS: 1.1926149129867554
LOSS: 1.1919065713882446
LOSS: 1.3682305812835693
LOSS: 1.3618581295013428
LOSS: 1.3558694124221802
LOSS: 1.3503022193908691
LOSS: 1.3451911211013794
LOSS: 1.340567946434021
LOSS: 1.3364548683166504
LOSS: 1.3328595161437988
LOSS: 1.3297643661499023
LOSS: 1.3271234035491943
LOSS: 1.324855089187622
LOSS: 1.3228470087051392
LOSS: 1.3209751844406128
LOSS: 1.3191235065460205
LOSS: 1.3172022104263306
LOSS: 1.315152883529663
LOSS: 1.312950849533081
LOSS: 1.3105911016464233
LOSS: 1.3080898523330688
LOSS: 1.3054699897766113
LOSS: 1.302761435508728
LOSS: 1.2999985218048096
LOSS: 1.2972142696380615
LOSS: 1.2944413423538208
LOSS: 1.2917073965072632
LOSS: 1.289037823677063
LOSS: 1.2864511013031006
LOSS: 1.283961296081543
LOSS: 1.2815757989883423
LOSS: 1.2792987823486328
LOSS: 1.2771238088607788
LOSS: 1.2750458717346191
LOSS: 1.2730523347854614
LOSS: 1.2711315155029297
LOSS: 1.2692720890045166
LOSS: 1.2674634456634521
LOSS: 1.265699028968811
LOSS: 1.2639744281768799
LOSS: 1.2622908353805542
LOSS: 1.260653018951416
LOSS: 1.2590668201446533
LOSS: 1.2575395107269287
LOSS: 1.2560800313949585
LOSS: 1.2546957731246948
LOSS: 1.253393292427063
LOSS: 1.2521744966506958
LOSS: 1.2510396242141724
LOSS: 1.2499886751174927
LOSS: 1.2490133047103882
LOSS: 1.2481039762496948
LOSS: 1.2472422122955322
LOSS: 1.2463985681533813
LOSS: 1.2455377578735352
LOSS: 1.2446198463439941
LOSS: 1.243603229522705
LOSS: 1.2424482107162476
LOSS: 1.241114854812622
LOSS: 1.2395646572113037
LOSS: 1.2377675771713257
LOSS: 1.2356975078582764
LOSS: 1.2333457469940186
LOSS: 1.2307066917419434
LOSS: 1.2277889251708984
LOSS: 1.224608302116394
LOSS: 1.221195101737976
LOSS: 1.217585802078247
LOSS: 1.2138254642486572
LOSS: 1.2099641561508179
LOSS: 1.2060506343841553
LOSS: 1.2021243572235107
LOSS: 1.1982152462005615
LOSS: 1.1943491697311401
LOSS: 1.1905479431152344
LOSS: 1.1868233680725098
LOSS: 1.1831835508346558
LOSS: 1.1796314716339111
LOSS: 1.1761739253997803
LOSS: 1.1728194952011108
LOSS: 1.169578194618225
LOSS: 1.1664661169052124
LOSS: 1.1635029315948486
LOSS: 1.1607033014297485
LOSS: 1.1580671072006226
LOSS: 1.1555935144424438
LOSS: 1.1532790660858154
LOSS: 1.151121973991394
LOSS: 1.1491237878799438
LOSS: 1.1472665071487427
LOSS: 1.1455363035202026
LOSS: 1.1438935995101929
LOSS: 1.1422888040542603
LOSS: 1.140721082687378
LOSS: 1.1391704082489014
LOSS: 1.1376439332962036
LOSS: 1.1361335515975952
LOSS: 1.1346869468688965
LOSS: 1.1332825422286987
LOSS: 1.1319470405578613
LOSS: 1.130689263343811
LOSS: 1.1295242309570312
LOSS: 1.3782734870910645
LOSS: 1.3721815347671509
LOSS: 1.3664886951446533
LOSS: 1.361233115196228
LOSS: 1.3564509153366089
LOSS: 1.3521728515625
LOSS: 1.3484193086624146
LOSS: 1.345191478729248
LOSS: 1.3424652814865112
LOSS: 1.340180516242981
LOSS: 1.3382418155670166
LOSS: 1.3365254402160645
LOSS: 1.3349039554595947
LOSS: 1.3332679271697998
LOSS: 1.331538200378418
LOSS: 1.3296709060668945
LOSS: 1.3276512622833252
LOSS: 1.3254848718643188
LOSS: 1.3231925964355469
LOSS: 1.320801019668579
LOSS: 1.3183432817459106
LOSS: 1.3158514499664307
LOSS: 1.3133585453033447
LOSS: 1.3108938932418823
LOSS: 1.3084838390350342
LOSS: 1.3061468601226807
LOSS: 1.3039008378982544
LOSS: 1.3017542362213135
LOSS: 1.2997106313705444
LOSS: 1.2977700233459473
LOSS: 1.2959251403808594
LOSS: 1.2941683530807495
LOSS: 1.2924879789352417
LOSS: 1.2908738851547241
LOSS: 1.2893155813217163
LOSS: 1.2878073453903198
LOSS: 1.2863414287567139
LOSS: 1.2849178314208984
LOSS: 1.2835345268249512
LOSS: 1.2821953296661377
LOSS: 1.2809014320373535
LOSS: 1.279651165008545
LOSS: 1.278438687324524
LOSS: 1.2772430181503296
LOSS: 1.2760354280471802
LOSS: 1.2747832536697388
LOSS: 1.2734439373016357
LOSS: 1.2719756364822388
LOSS: 1.270341396331787
LOSS: 1.2685117721557617
LOSS: 1.2664624452590942
LOSS: 1.264195442199707
LOSS: 1.2617292404174805
LOSS: 1.259101390838623
LOSS: 1.2563612461090088
LOSS: 1.253565788269043
LOSS: 1.2507706880569458
LOSS: 1.2480238676071167
LOSS: 1.245363712310791
LOSS: 1.2428133487701416
LOSS: 1.2403812408447266
LOSS: 1.2380645275115967
LOSS: 1.2358466386795044
LOSS: 1.2337112426757812
LOSS: 1.2316299676895142
LOSS: 1.229578971862793
LOSS: 1.2275291681289673
LOSS: 1.225448727607727
LOSS: 1.223311185836792
LOSS: 1.2210942506790161
LOSS: 1.2187851667404175
LOSS: 1.2163779735565186
LOSS: 1.213876485824585
LOSS: 1.2112922668457031
LOSS: 1.2086451053619385
LOSS: 1.205967664718628
LOSS: 1.2032890319824219
LOSS: 1.2006369829177856
LOSS: 1.1979984045028687
LOSS: 1.195330262184143
LOSS: 1.1926138401031494
LOSS: 1.189857840538025
LOSS: 1.1870765686035156
LOSS: 1.184286117553711
LOSS: 1.1814972162246704
LOSS: 1.1787140369415283
LOSS: 1.175931692123413
LOSS: 1.1731460094451904
LOSS: 1.1703603267669678
LOSS: 1.167587161064148
LOSS: 1.164847493171692
LOSS: 1.1621705293655396
LOSS: 1.159584641456604
LOSS: 1.1570936441421509
LOSS: 1.1546504497528076
LOSS: 1.1522022485733032
LOSS: 1.149721622467041
LOSS: 1.1471960544586182
LOSS: 1.1446326971054077
LOSS: 1.1420567035675049
LOSS: 1.3727697134017944
LOSS: 1.3665549755096436
LOSS: 1.3607321977615356
LOSS: 1.355338454246521
LOSS: 1.350409984588623
LOSS: 1.3459765911102295
LOSS: 1.3420604467391968
LOSS: 1.3386650085449219
LOSS: 1.3357707262039185
LOSS: 1.333324909210205
LOSS: 1.331239938735962
LOSS: 1.3293992280960083
LOSS: 1.3276735544204712
LOSS: 1.3259496688842773
LOSS: 1.3241405487060547
LOSS: 1.3221925497055054
LOSS: 1.3200839757919312
LOSS: 1.317813754081726
LOSS: 1.3153983354568481
LOSS: 1.3128626346588135
LOSS: 1.310238003730774
LOSS: 1.3075571060180664
LOSS: 1.3048524856567383
LOSS: 1.302153468132019
LOSS: 1.2994879484176636
LOSS: 1.296875238418579
LOSS: 1.2943322658538818
LOSS: 1.2918665409088135
LOSS: 1.2894794940948486
LOSS: 1.2871630191802979
LOSS: 1.2849043607711792
LOSS: 1.2826868295669556
LOSS: 1.2804902791976929
LOSS: 1.278293251991272
LOSS: 1.276075005531311
LOSS: 1.2738155126571655
LOSS: 1.2714978456497192
LOSS: 1.269108533859253
LOSS: 1.2666335105895996
LOSS: 1.2640624046325684
LOSS: 1.2613848447799683
LOSS: 1.258593201637268
LOSS: 1.2556816339492798
LOSS: 1.252646803855896
LOSS: 1.2494869232177734
LOSS: 1.2461997270584106
LOSS: 1.242783546447754
LOSS: 1.239236831665039
LOSS: 1.2355600595474243
LOSS: 1.2317590713500977
LOSS: 1.227846384048462
LOSS: 1.2238441705703735
LOSS: 1.219783902168274
LOSS: 1.2157050371170044
LOSS: 1.2116526365280151
LOSS: 1.2076672315597534
LOSS: 1.2037811279296875
LOSS: 1.2000148296356201
LOSS: 1.196382761001587
LOSS: 1.1928927898406982
LOSS: 1.1895519495010376
LOSS: 1.18636155128479
LOSS: 1.1833208799362183
LOSS: 1.1804264783859253
LOSS: 1.177677869796753
LOSS: 1.1750695705413818
LOSS: 1.1725856065750122
LOSS: 1.1702044010162354
LOSS: 1.1678876876831055
LOSS: 1.1656036376953125
LOSS: 1.1633381843566895
LOSS: 1.161097526550293
LOSS: 1.158901572227478
LOSS: 1.1567730903625488
LOSS: 1.1547309160232544
LOSS: 1.1527867317199707
LOSS: 1.1509431600570679
LOSS: 1.1491917371749878
LOSS: 1.1475085020065308
LOSS: 1.1458643674850464
LOSS: 1.1442339420318604
LOSS: 1.1426056623458862
LOSS: 1.140976071357727
LOSS: 1.1393511295318604
LOSS: 1.1377431154251099
LOSS: 1.136157751083374
LOSS: 1.1346012353897095
LOSS: 1.289037823677063
LOSS: 1.2864511013031006
LOSS: 1.283961296081543
LOSS: 1.2815757989883423
LOSS: 1.2792987823486328
LOSS: 1.2771238088607788
LOSS: 1.2750458717346191
LOSS: 1.2730523347854614
LOSS: 1.2711315155029297
LOSS: 1.2692720890045166
LOSS: 1.2674634456634521
LOSS: 1.265699028968811
LOSS: 1.2639744281768799
LOSS: 1.2622908353805542
LOSS: 1.260653018951416
LOSS: 1.2590668201446533
LOSS: 1.2575395107269287
LOSS: 1.2560800313949585
LOSS: 1.2546957731246948
LOSS: 1.253393292427063
LOSS: 1.2521744966506958
LOSS: 1.2510396242141724
LOSS: 1.2499886751174927
LOSS: 1.2490133047103882
LOSS: 1.2481039762496948
LOSS: 1.2472422122955322
LOSS: 1.2463985681533813
LOSS: 1.2455377578735352
LOSS: 1.2446198463439941
LOSS: 1.243603229522705
LOSS: 1.2424482107162476
LOSS: 1.241114854812622
LOSS: 1.2395646572113037
LOSS: 1.2377675771713257
LOSS: 1.2356975078582764
LOSS: 1.2333457469940186
LOSS: 1.2307066917419434
LOSS: 1.2277889251708984
LOSS: 1.224608302116394
LOSS: 1.221195101737976
LOSS: 1.217585802078247
LOSS: 1.2138254642486572
LOSS: 1.2099641561508179
LOSS: 1.2060506343841553
LOSS: 1.2021243572235107
LOSS: 1.1982152462005615
LOSS: 1.1943491697311401
LOSS: 1.1905479431152344
LOSS: 1.1868233680725098
LOSS: 1.1831835508346558
LOSS: 1.1796314716339111
LOSS: 1.1761739253997803
LOSS: 1.1728194952011108
LOSS: 1.169578194618225
LOSS: 1.1664661169052124
LOSS: 1.1635029315948486
LOSS: 1.1607033014297485
LOSS: 1.1580671072006226
LOSS: 1.1555935144424438
LOSS: 1.1532790660858154
LOSS: 1.151121973991394
LOSS: 1.1491237878799438
LOSS: 1.1472665071487427
LOSS: 1.1455363035202026
LOSS: 1.1438935995101929
LOSS: 1.1422888040542603
LOSS: 1.140721082687378
LOSS: 1.1391704082489014
LOSS: 1.1376439332962036
LOSS: 1.1361335515975952
LOSS: 1.1346869468688965
LOSS: 1.1332825422286987
LOSS: 1.1319470405578613
LOSS: 1.130689263343811
LOSS: 1.1295242309570312
LOSS: 1.3774969577789307
LOSS: 1.3714007139205933
LOSS: 1.3657021522521973
LOSS: 1.360439658164978
LOSS: 1.3556488752365112
LOSS: 1.3513612747192383
LOSS: 1.3475968837738037
LOSS: 1.3443572521209717
LOSS: 1.3416202068328857
LOSS: 1.3393278121948242
LOSS: 1.3373881578445435
LOSS: 1.335680603981018
LOSS: 1.3340741395950317
LOSS: 1.332459568977356
LOSS: 1.3307538032531738
LOSS: 1.3289082050323486
LOSS: 1.3269041776657104
LOSS: 1.324746012687683
LOSS: 1.322450041770935
LOSS: 1.3200432062149048
LOSS: 1.317555546760559
LOSS: 1.315019130706787
LOSS: 1.3124637603759766
LOSS: 1.3099168539047241
LOSS: 1.3074026107788086
LOSS: 1.3049383163452148
LOSS: 1.3025392293930054
LOSS: 1.3002122640609741
LOSS: 1.297959566116333
LOSS: 1.2957779169082642
LOSS: 1.2936584949493408
LOSS: 1.2915898561477661
LOSS: 1.2895548343658447
LOSS: 1.2875374555587769
LOSS: 1.2855159044265747
LOSS: 1.2834633588790894
LOSS: 1.281353235244751
LOSS: 1.279158115386963
LOSS: 1.2768515348434448
LOSS: 1.2744059562683105
LOSS: 1.271795392036438
LOSS: 1.269000768661499
LOSS: 1.2660080194473267
LOSS: 1.2628142833709717
LOSS: 1.2594178915023804
LOSS: 1.2558202743530273
LOSS: 1.25203275680542
LOSS: 1.2480701208114624
LOSS: 1.2439550161361694
LOSS: 1.2397124767303467
LOSS: 1.235372543334961
LOSS: 1.2309705018997192
LOSS: 1.2265450954437256
LOSS: 1.2221378087997437
LOSS: 1.2177857160568237
LOSS: 1.2135130167007446
LOSS: 1.209340214729309
LOSS: 1.205272912979126
LOSS: 1.2013001441955566
LOSS: 1.1974064111709595
LOSS: 1.193564534187317
LOSS: 1.1897484064102173
LOSS: 1.185928225517273
LOSS: 1.182085394859314
LOSS: 1.178196907043457
LOSS: 1.174257516860962
LOSS: 1.1702677011489868
LOSS: 1.16623055934906
LOSS: 1.1621580123901367
LOSS: 1.158068060874939
LOSS: 1.1539803743362427
LOSS: 1.1499110460281372
LOSS: 1.1458699703216553
LOSS: 1.1418627500534058
LOSS: 1.1378871202468872
LOSS: 1.1339433193206787
LOSS: 1.1300380229949951
LOSS: 1.1261942386627197
LOSS: 1.1224571466445923
LOSS: 1.1188867092132568
LOSS: 1.1155383586883545
LOSS: 1.1124449968338013
LOSS: 1.109604835510254
LOSS: 1.1069865226745605
LOSS: 1.1045418977737427
LOSS: 1.1022189855575562
LOSS: 1.0999683141708374
LOSS: 1.0977567434310913
LOSS: 1.095578670501709
LOSS: 1.0934512615203857
LOSS: 1.091402530670166
LOSS: 1.0894588232040405
LOSS: 1.087634563446045
LOSS: 1.0859253406524658
LOSS: 1.0843095779418945
LOSS: 1.0827581882476807
LOSS: 1.0812432765960693
LOSS: 1.0797452926635742
LOSS: 1.0782537460327148
LOSS: 1.0767698287963867
LOSS: 1.3800324201583862
LOSS: 1.373968243598938
LOSS: 1.3683037757873535
LOSS: 1.3630770444869995
LOSS: 1.358324646949768
LOSS: 1.3540769815444946
LOSS: 1.3503525257110596
LOSS: 1.3471524715423584
LOSS: 1.3444509506225586
LOSS: 1.3421858549118042
LOSS: 1.3402588367462158
LOSS: 1.3385449647903442
LOSS: 1.3369132280349731
LOSS: 1.3352519273757935
LOSS: 1.3334801197052002
LOSS: 1.3315551280975342
LOSS: 1.3294612169265747
LOSS: 1.3272044658660889
LOSS: 1.3248035907745361
LOSS: 1.3222875595092773
LOSS: 1.319688081741333
LOSS: 1.3170361518859863
LOSS: 1.3143656253814697
LOSS: 1.3117055892944336
LOSS: 1.309081792831421
LOSS: 1.3065152168273926
LOSS: 1.304021954536438
LOSS: 1.3016122579574585
LOSS: 1.2992911338806152
LOSS: 1.2970582246780396
LOSS: 1.2949092388153076
LOSS: 1.292837142944336
LOSS: 1.2908306121826172
LOSS: 1.288880705833435
LOSS: 1.286978840827942
LOSS: 1.285117745399475
LOSS: 1.2832931280136108
LOSS: 1.2815070152282715
LOSS: 1.279759407043457
LOSS: 1.2780601978302002
LOSS: 1.2764099836349487
LOSS: 1.2748225927352905
LOSS: 1.2733031511306763
LOSS: 1.2718596458435059
LOSS: 1.2704932689666748
LOSS: 1.2692086696624756
LOSS: 1.267999291419983
LOSS: 1.2668648958206177
LOSS: 1.2657921314239502
LOSS: 1.264768123626709
LOSS: 1.2637746334075928
LOSS: 1.2627911567687988
LOSS: 1.2617981433868408
LOSS: 1.2607802152633667
LOSS: 1.2597206830978394
LOSS: 1.2586010694503784
LOSS: 1.2573966979980469
LOSS: 1.2560827732086182
LOSS: 1.2546263933181763
LOSS: 1.2529932260513306
LOSS: 1.2511497735977173
LOSS: 1.2490696907043457
LOSS: 1.2467389106750488
LOSS: 1.2441455125808716
LOSS: 1.241286039352417
LOSS: 1.2381610870361328
LOSS: 1.2347737550735474
LOSS: 1.2311286926269531
LOSS: 1.2272264957427979
LOSS: 1.223068356513977
LOSS: 1.2186509370803833
LOSS: 1.2139737606048584
LOSS: 1.2090455293655396
LOSS: 1.2038838863372803
LOSS: 1.1985180377960205
LOSS: 1.1929959058761597
LOSS: 1.1873862743377686
LOSS: 1.1817657947540283
LOSS: 1.1762088537216187
LOSS: 1.170774221420288
LOSS: 1.1655086278915405
LOSS: 1.1604557037353516
LOSS: 1.1556519269943237
LOSS: 1.151125431060791
LOSS: 1.1468994617462158
LOSS: 1.1429922580718994
LOSS: 1.1394075155258179
LOSS: 1.136136770248413
LOSS: 1.1331425905227661
LOSS: 1.1303811073303223
LOSS: 1.127808928489685
LOSS: 1.1253896951675415
LOSS: 1.1230928897857666
LOSS: 1.1208925247192383
LOSS: 1.1187573671340942
LOSS: 1.116665244102478
LOSS: 1.1145970821380615
LOSS: 1.1125460863113403
LOSS: 1.110518217086792
LOSS: 1.1085225343704224
LOSS: 1.3782734870910645
LOSS: 1.3721815347671509
LOSS: 1.3664886951446533
LOSS: 1.361233115196228
LOSS: 1.3564509153366089
LOSS: 1.3521728515625
LOSS: 1.3484193086624146
LOSS: 1.345191478729248
LOSS: 1.3424652814865112
LOSS: 1.340180516242981
LOSS: 1.3382418155670166
LOSS: 1.3365254402160645
LOSS: 1.3349039554595947
LOSS: 1.3332679271697998
LOSS: 1.331538200378418
LOSS: 1.3296709060668945
LOSS: 1.3276512622833252
LOSS: 1.3254848718643188
LOSS: 1.3231925964355469
LOSS: 1.320801019668579
LOSS: 1.3183432817459106
LOSS: 1.3158514499664307
LOSS: 1.3133585453033447
LOSS: 1.3108938932418823
LOSS: 1.3084838390350342
LOSS: 1.3061468601226807
LOSS: 1.3039008378982544
LOSS: 1.3017542362213135
LOSS: 1.2997106313705444
LOSS: 1.2977700233459473
LOSS: 1.2959251403808594
LOSS: 1.2941683530807495
LOSS: 1.2924879789352417
LOSS: 1.2908738851547241
LOSS: 1.2893155813217163
LOSS: 1.2878073453903198
LOSS: 1.2863414287567139
LOSS: 1.2849178314208984
LOSS: 1.2835345268249512
LOSS: 1.2821953296661377
LOSS: 1.2809014320373535
LOSS: 1.279651165008545
LOSS: 1.278438687324524
LOSS: 1.2772430181503296
LOSS: 1.2760354280471802
LOSS: 1.2747832536697388
LOSS: 1.2734439373016357
LOSS: 1.2719756364822388
LOSS: 1.270341396331787
LOSS: 1.2685117721557617
LOSS: 1.2664624452590942
LOSS: 1.264195442199707
LOSS: 1.2617292404174805
LOSS: 1.259101390838623
LOSS: 1.2563612461090088
LOSS: 1.1315516233444214
LOSS: 1.1297634840011597
LOSS: 1.127997636795044
LOSS: 1.1262496709823608
LOSS: 1.1245073080062866
LOSS: 1.1227864027023315
LOSS: 1.121101975440979
LOSS: 1.1194778680801392
LOSS: 1.1179227828979492
LOSS: 1.1164294481277466
LOSS: 1.1150109767913818
LOSS: 1.113666296005249
LOSS: 1.1124038696289062
LOSS: 1.1111705303192139
LOSS: 1.1100133657455444
LOSS: 1.1089340448379517
LOSS: 1.3627771139144897
LOSS: 1.3563035726547241
LOSS: 1.3502098321914673
LOSS: 1.3445327281951904
LOSS: 1.339306116104126
LOSS: 1.3345592021942139
LOSS: 1.3303115367889404
LOSS: 1.3265646696090698
LOSS: 1.323294997215271
LOSS: 1.3204456567764282
LOSS: 1.3179227113723755
LOSS: 1.3156083822250366
LOSS: 1.3133784532546997
LOSS: 1.311125636100769
LOSS: 1.3087742328643799
LOSS: 1.3062812089920044
LOSS: 1.3036307096481323
LOSS: 1.3008285760879517
LOSS: 1.297890305519104
LOSS: 1.2948435544967651
LOSS: 1.2917158603668213
LOSS: 1.288540244102478
LOSS: 1.285347819328308
LOSS: 1.2821682691574097
LOSS: 1.2790260314941406
LOSS: 1.2759438753128052
LOSS: 1.272938847541809
LOSS: 1.2700196504592896
LOSS: 1.2671940326690674
LOSS: 1.2644588947296143
LOSS: 1.2618108987808228
LOSS: 1.259240984916687
LOSS: 1.2567354440689087
LOSS: 1.254282832145691
LOSS: 1.2518688440322876
LOSS: 1.249482274055481
LOSS: 1.2471126317977905
LOSS: 1.244749903678894
LOSS: 1.2423866987228394
LOSS: 1.240016222000122
LOSS: 1.237628698348999
LOSS: 1.2352168560028076
LOSS: 1.232775092124939
LOSS: 1.2302930355072021
LOSS: 1.2277687788009644
LOSS: 1.2252002954483032
LOSS: 1.222586989402771
LOSS: 1.2199373245239258
LOSS: 1.2172672748565674
LOSS: 1.214587926864624
LOSS: 1.2119296789169312
LOSS: 1.209306240081787
LOSS: 1.206743836402893
LOSS: 1.2042665481567383
LOSS: 1.2018868923187256
LOSS: 1.1996240615844727
LOSS: 1.1974796056747437
LOSS: 1.1954514980316162
LOSS: 1.1935278177261353
LOSS: 1.191690444946289
LOSS: 1.189937710762024
LOSS: 1.1882567405700684
LOSS: 1.1866282224655151
LOSS: 1.1850566864013672
LOSS: 1.1835122108459473
LOSS: 1.1819874048233032
LOSS: 1.180467128753662
LOSS: 1.1789493560791016
LOSS: 1.1774225234985352
LOSS: 1.1758793592453003
LOSS: 1.1743472814559937
LOSS: 1.172813057899475
LOSS: 1.171280026435852
LOSS: 1.1697423458099365
LOSS: 1.168196439743042
LOSS: 1.1666110754013062
LOSS: 1.1649457216262817
LOSS: 1.163167119026184
LOSS: 1.1612554788589478
LOSS: 1.1592278480529785
LOSS: 1.1570593118667603
LOSS: 1.1547600030899048
LOSS: 1.1523338556289673
LOSS: 1.1497933864593506
LOSS: 1.1471548080444336
LOSS: 1.1444647312164307
LOSS: 1.1417557001113892
LOSS: 1.1390511989593506
LOSS: 1.1364072561264038
LOSS: 1.1338573694229126
LOSS: 1.1314280033111572
LOSS: 1.129135251045227
LOSS: 1.126977562904358
LOSS: 1.1249275207519531
LOSS: 1.1229231357574463
LOSS: 1.1209243535995483
LOSS: 1.1189180612564087
LOSS: 1.1169019937515259
LOSS: 1.1148892641067505
LOSS: 1.112886667251587
LOSS: 1.2991641759872437
LOSS: 1.2886391878128052
LOSS: 1.2785677909851074
LOSS: 1.2690095901489258
LOSS: 1.2600221633911133
LOSS: 1.251657247543335
LOSS: 1.2439576387405396
LOSS: 1.2369519472122192
LOSS: 1.2306511402130127
LOSS: 1.2250466346740723
LOSS: 1.2201108932495117
LOSS: 1.215803861618042
LOSS: 1.2120797634124756
LOSS: 1.208890438079834
LOSS: 1.2061859369277954
LOSS: 1.2039107084274292
LOSS: 1.2020010948181152
LOSS: 1.2003846168518066
LOSS: 1.198983073234558
LOSS: 1.1977183818817139
LOSS: 1.1965203285217285
LOSS: 1.1953328847885132
LOSS: 1.1941196918487549
LOSS: 1.1928642988204956
LOSS: 1.1915695667266846
LOSS: 1.1902515888214111
LOSS: 1.188936710357666
LOSS: 1.187652587890625
LOSS: 1.186427116394043
LOSS: 1.1852823495864868
LOSS: 1.1842334270477295
LOSS: 1.1832878589630127
LOSS: 1.182446002960205
LOSS: 1.1817017793655396
LOSS: 1.1810452938079834
LOSS: 1.1804641485214233
LOSS: 1.179944396018982
LOSS: 1.1794735193252563
LOSS: 1.1790401935577393
LOSS: 1.1786357164382935
LOSS: 1.1782532930374146
LOSS: 1.1778897047042847
LOSS: 1.1775435209274292
LOSS: 1.177215337753296
LOSS: 1.1769073009490967
LOSS: 1.1766210794448853
LOSS: 1.1763601303100586
LOSS: 1.1761255264282227
LOSS: 1.1759183406829834
LOSS: 1.1757378578186035
LOSS: 1.175582766532898
LOSS: 1.1754499673843384
LOSS: 1.17533540725708
LOSS: 1.1752349138259888
LOSS: 1.175144076347351
LOSS: 1.1750586032867432
LOSS: 1.1749753952026367
LOSS: 1.174891471862793
LOSS: 1.1748055219650269
LOSS: 1.1747170686721802
LOSS: 1.1746268272399902
LOSS: 1.1745356321334839
LOSS: 1.1744451522827148
LOSS: 1.1743571758270264
LOSS: 1.1742730140686035
LOSS: 1.174194097518921
LOSS: 1.1741206645965576
LOSS: 1.1740529537200928
LOSS: 1.173991084098816
LOSS: 1.1739341020584106
LOSS: 1.173880696296692
LOSS: 1.1738301515579224
LOSS: 1.1737817525863647
LOSS: 1.1737339496612549
LOSS: 1.173687219619751
LOSS: 1.1736407279968262
LOSS: 1.1735941171646118
LOSS: 1.1735484600067139
LOSS: 1.1735029220581055
LOSS: 1.1734588146209717
LOSS: 1.1734154224395752
LOSS: 1.1733736991882324
LOSS: 1.1733334064483643
LOSS: 1.1732947826385498
LOSS: 1.1732574701309204
LOSS: 1.1732213497161865
LOSS: 1.1731864213943481
LOSS: 1.1731526851654053
LOSS: 1.1731195449829102
LOSS: 1.173087239265442
LOSS: 1.1730554103851318
LOSS: 1.173024296760559
LOSS: 1.1729940176010132
LOSS: 1.1729644536972046
LOSS: 1.1729356050491333
LOSS: 1.1729077100753784
LOSS: 1.1728804111480713
LOSS: 1.1728541851043701
LOSS: 1.1728285551071167
LOSS: 1.1728036403656006
LOSS: 1.2917648553848267
LOSS: 1.2809864282608032
LOSS: 1.270636796951294
LOSS: 1.2607777118682861
LOSS: 1.2514712810516357
LOSS: 1.2427774667739868
LOSS: 1.234750509262085
LOSS: 1.2274341583251953
LOSS: 1.2208563089370728
LOSS: 1.2150253057479858
LOSS: 1.2099300622940063
LOSS: 1.2055432796478271
LOSS: 1.2018275260925293
LOSS: 1.198740839958191
LOSS: 1.1962368488311768
LOSS: 1.1942616701126099
LOSS: 1.192750334739685
LOSS: 1.191624402999878
LOSS: 1.1907933950424194
LOSS: 1.1901617050170898
LOSS: 1.1896361112594604
LOSS: 1.1891363859176636
LOSS: 1.1886017322540283
LOSS: 1.1879957914352417
LOSS: 1.18730628490448
LOSS: 1.1865413188934326
LOSS: 1.1857246160507202
LOSS: 1.184888482093811
LOSS: 1.1840667724609375
LOSS: 1.1832915544509888
LOSS: 1.18258798122406
LOSS: 1.1819722652435303
LOSS: 1.1814513206481934
LOSS: 1.1810239553451538
LOSS: 1.1806803941726685
LOSS: 1.1804064512252808
LOSS: 1.1801835298538208
LOSS: 1.1799923181533813
LOSS: 1.1798136234283447
LOSS: 1.1796308755874634
LOSS: 1.1794300079345703
LOSS: 1.1792010068893433
LOSS: 1.178938388824463
LOSS: 1.178640365600586
LOSS: 1.178309440612793
LOSS: 1.1779519319534302
LOSS: 1.1775754690170288
LOSS: 1.177189826965332
LOSS: 1.176804780960083
LOSS: 1.1764289140701294
LOSS: 1.1760693788528442
LOSS: 1.1757307052612305
LOSS: 1.175415277481079
LOSS: 1.1751223802566528
LOSS: 1.1748501062393188
LOSS: 1.1745944023132324
LOSS: 1.1743513345718384
LOSS: 1.1741162538528442
LOSS: 1.1738852262496948
LOSS: 1.1736561059951782
LOSS: 1.173427700996399
LOSS: 1.1731996536254883
LOSS: 1.1729730367660522
LOSS: 1.172749400138855
LOSS: 1.1725306510925293
LOSS: 1.1723181009292603
LOSS: 1.172113299369812
LOSS: 1.1719163656234741
LOSS: 1.1717272996902466
LOSS: 1.1715447902679443
LOSS: 1.1713682413101196
LOSS: 1.1711958646774292
LOSS: 1.1710267066955566
LOSS: 1.170859932899475
LOSS: 1.170694351196289
LOSS: 1.170530080795288
LOSS: 1.1703662872314453
LOSS: 1.1702035665512085
LOSS: 1.1700420379638672
LOSS: 1.1698813438415527
LOSS: 1.1697221994400024
LOSS: 1.1695642471313477
LOSS: 1.1694082021713257
LOSS: 1.1692537069320679
LOSS: 1.1691009998321533
LOSS: 1.1689499616622925
LOSS: 1.1688013076782227
LOSS: 1.1686545610427856
LOSS: 1.168509602546692
LOSS: 1.1683666706085205
LOSS: 1.1682254076004028
LOSS: 1.168086290359497
LOSS: 1.1679487228393555
LOSS: 1.1678129434585571
LOSS: 1.167678713798523
LOSS: 1.1675461530685425
LOSS: 1.167415738105774
LOSS: 1.1672871112823486
LOSS: 1.167160987854004
LOSS: 1.1670374870300293
LOSS: 1.2914098501205444
LOSS: 1.280484914779663
LOSS: 1.2699750661849976
LOSS: 1.2599382400512695
LOSS: 1.2504332065582275
LOSS: 1.24151611328125
LOSS: 1.2332375049591064
LOSS: 1.2256394624710083
LOSS: 1.21875
LOSS: 1.2125790119171143
LOSS: 1.207116723060608
LOSS: 1.2023344039916992
LOSS: 1.1981892585754395
LOSS: 1.1946306228637695
LOSS: 1.1916042566299438
LOSS: 1.1129316091537476
LOSS: 1.1115202903747559
LOSS: 1.1100878715515137
LOSS: 1.1086623668670654
LOSS: 1.1072934865951538
LOSS: 1.1060123443603516
LOSS: 1.1048475503921509
LOSS: 1.1038177013397217
LOSS: 1.1029249429702759
LOSS: 1.1021543741226196
LOSS: 1.1014735698699951
LOSS: 1.1008412837982178
LOSS: 1.3627771139144897
LOSS: 1.3563035726547241
LOSS: 1.3502098321914673
LOSS: 1.3445327281951904
LOSS: 1.339306116104126
LOSS: 1.3345592021942139
LOSS: 1.3303115367889404
LOSS: 1.3265646696090698
LOSS: 1.323294997215271
LOSS: 1.3204456567764282
LOSS: 1.3179227113723755
LOSS: 1.3156083822250366
LOSS: 1.3133784532546997
LOSS: 1.311125636100769
LOSS: 1.3087742328643799
LOSS: 1.3062812089920044
LOSS: 1.3036307096481323
LOSS: 1.3008285760879517
LOSS: 1.297890305519104
LOSS: 1.2948435544967651
LOSS: 1.2917158603668213
LOSS: 1.288540244102478
LOSS: 1.285347819328308
LOSS: 1.2821682691574097
LOSS: 1.2790260314941406
LOSS: 1.2759438753128052
LOSS: 1.272938847541809
LOSS: 1.2700196504592896
LOSS: 1.2671940326690674
LOSS: 1.2644588947296143
LOSS: 1.2618108987808228
LOSS: 1.259240984916687
LOSS: 1.2567354440689087
LOSS: 1.254282832145691
LOSS: 1.2518688440322876
LOSS: 1.249482274055481
LOSS: 1.2471126317977905
LOSS: 1.244749903678894
LOSS: 1.2423866987228394
LOSS: 1.240016222000122
LOSS: 1.237628698348999
LOSS: 1.2352168560028076
LOSS: 1.232775092124939
LOSS: 1.2302930355072021
LOSS: 1.2277687788009644
LOSS: 1.2252002954483032
LOSS: 1.222586989402771
LOSS: 1.2199373245239258
LOSS: 1.2172672748565674
LOSS: 1.214587926864624
LOSS: 1.2119296789169312
LOSS: 1.209306240081787
LOSS: 1.206743836402893
LOSS: 1.2042665481567383
LOSS: 1.2018868923187256
LOSS: 1.1996240615844727
LOSS: 1.1974796056747437
LOSS: 1.1954514980316162
LOSS: 1.1935278177261353
LOSS: 1.191690444946289
LOSS: 1.189937710762024
LOSS: 1.1882567405700684
LOSS: 1.1866282224655151
LOSS: 1.1850566864013672
LOSS: 1.1835122108459473
LOSS: 1.1819874048233032
LOSS: 1.180467128753662
LOSS: 1.1789493560791016
LOSS: 1.1774225234985352
LOSS: 1.1758793592453003
LOSS: 1.1743472814559937
LOSS: 1.172813057899475
LOSS: 1.171280026435852
LOSS: 1.1697423458099365
LOSS: 1.168196439743042
LOSS: 1.1666110754013062
LOSS: 1.1649457216262817
LOSS: 1.163167119026184
LOSS: 1.1612554788589478
LOSS: 1.1592278480529785
LOSS: 1.1570593118667603
LOSS: 1.1547600030899048
LOSS: 1.1523338556289673
LOSS: 1.1497933864593506
LOSS: 1.1471548080444336
LOSS: 1.1444647312164307
LOSS: 1.1417557001113892
LOSS: 1.1390511989593506
LOSS: 1.1364072561264038
LOSS: 1.1338573694229126
LOSS: 1.1314280033111572
LOSS: 1.129135251045227
LOSS: 1.126977562904358
LOSS: 1.1249275207519531
LOSS: 1.1229231357574463
LOSS: 1.1209243535995483
LOSS: 1.1189180612564087
LOSS: 1.1169019937515259
LOSS: 1.1148892641067505
LOSS: 1.112886667251587
LOSS: 1.3800324201583862
LOSS: 1.373968243598938
LOSS: 1.3683037757873535
LOSS: 1.3630770444869995
LOSS: 1.358324646949768
LOSS: 1.3540769815444946
LOSS: 1.3503525257110596
LOSS: 1.3471524715423584
LOSS: 1.3444509506225586
LOSS: 1.3421858549118042
LOSS: 1.3402588367462158
LOSS: 1.3385449647903442
LOSS: 1.3369132280349731
LOSS: 1.3352519273757935
LOSS: 1.3334801197052002
LOSS: 1.3315551280975342
LOSS: 1.3294612169265747
LOSS: 1.3272044658660889
LOSS: 1.3248035907745361
LOSS: 1.3222875595092773
LOSS: 1.319688081741333
LOSS: 1.3170361518859863
LOSS: 1.3143656253814697
LOSS: 1.3117055892944336
LOSS: 1.309081792831421
LOSS: 1.3065152168273926
LOSS: 1.304021954536438
LOSS: 1.3016122579574585
LOSS: 1.2992911338806152
LOSS: 1.2970582246780396
LOSS: 1.2949092388153076
LOSS: 1.292837142944336
LOSS: 1.2908306121826172
LOSS: 1.288880705833435
LOSS: 1.286978840827942
LOSS: 1.285117745399475
LOSS: 1.2832931280136108
LOSS: 1.2815070152282715
LOSS: 1.279759407043457
LOSS: 1.2780601978302002
LOSS: 1.2764099836349487
LOSS: 1.2748225927352905
LOSS: 1.2733031511306763
LOSS: 1.2718596458435059
LOSS: 1.2704932689666748
LOSS: 1.2692086696624756
LOSS: 1.267999291419983
LOSS: 1.2668648958206177
LOSS: 1.2657921314239502
LOSS: 1.264768123626709
LOSS: 1.2637746334075928
LOSS: 1.2627911567687988
LOSS: 1.2617981433868408
LOSS: 1.2607802152633667
LOSS: 1.2597206830978394
LOSS: 1.2586010694503784
LOSS: 1.2573966979980469
LOSS: 1.2560827732086182
LOSS: 1.2546263933181763
LOSS: 1.2529932260513306
LOSS: 1.2511497735977173
LOSS: 1.2490696907043457
LOSS: 1.2467389106750488
LOSS: 1.2441455125808716
LOSS: 1.241286039352417
LOSS: 1.2381610870361328
LOSS: 1.2347737550735474
LOSS: 1.2311286926269531
LOSS: 1.2272264957427979
LOSS: 1.223068356513977
LOSS: 1.2186509370803833
LOSS: 1.2139737606048584
LOSS: 1.2090455293655396
LOSS: 1.2038838863372803
LOSS: 1.1985180377960205
LOSS: 1.1929959058761597
LOSS: 1.1873862743377686
LOSS: 1.1817657947540283
LOSS: 1.1762088537216187
LOSS: 1.170774221420288
LOSS: 1.1655086278915405
LOSS: 1.1604557037353516
LOSS: 1.1556519269943237
LOSS: 1.151125431060791
LOSS: 1.1468994617462158
LOSS: 1.1429922580718994
LOSS: 1.1394075155258179
LOSS: 1.136136770248413
LOSS: 1.1331425905227661
LOSS: 1.1303811073303223
LOSS: 1.127808928489685
LOSS: 1.1253896951675415
LOSS: 1.1230928897857666
LOSS: 1.1208925247192383
LOSS: 1.1187573671340942
LOSS: 1.116665244102478
LOSS: 1.1145970821380615
LOSS: 1.1125460863113403
LOSS: 1.110518217086792
LOSS: 1.1085225343704224
LOSS: 1.302241325378418
LOSS: 1.2918519973754883
LOSS: 1.2818917036056519
LOSS: 1.2724295854568481
LOSS: 1.2635341882705688
LOSS: 1.2552692890167236
LOSS: 1.247689962387085
LOSS: 1.2408366203308105
LOSS: 1.2347290515899658
LOSS: 1.229364037513733
LOSS: 1.224717617034912
LOSS: 1.2207518815994263
LOSS: 1.2174222469329834
LOSS: 1.214682936668396
LOSS: 1.2124834060668945
LOSS: 1.210763931274414
LOSS: 1.2094521522521973
LOSS: 1.208461046218872
LOSS: 1.2076956033706665
LOSS: 1.207059621810913
LOSS: 1.2064653635025024
LOSS: 1.205844759941101
LOSS: 1.2051528692245483
LOSS: 1.2043715715408325
LOSS: 1.2035057544708252
LOSS: 1.2025786638259888
LOSS: 1.201623797416687
LOSS: 1.200679063796997
LOSS: 1.1997801065444946
LOSS: 1.19895601272583
LOSS: 1.1982265710830688
LOSS: 1.197601079940796
LOSS: 1.197079062461853
LOSS: 1.1966519355773926
LOSS: 1.1963039636611938
LOSS: 1.1960169076919556
LOSS: 1.195770502090454
LOSS: 1.1955456733703613
LOSS: 1.1953264474868774
LOSS: 1.1951000690460205
LOSS: 1.1948591470718384
LOSS: 1.1945998668670654
LOSS: 1.1943235397338867
LOSS: 1.1940332651138306
LOSS: 1.1937353610992432
LOSS: 1.1934369802474976
LOSS: 1.1931449174880981
LOSS: 1.1928656101226807
LOSS: 1.1926037073135376
LOSS: 1.192362666130066
LOSS: 1.192143440246582
LOSS: 1.1919453144073486
LOSS: 1.1917667388916016
LOSS: 1.1916043758392334
LOSS: 1.1914559602737427
LOSS: 1.1913176774978638
LOSS: 1.1911876201629639
LOSS: 1.1910632848739624
LOSS: 1.1909444332122803
LOSS: 1.1908299922943115
LOSS: 1.190719723701477
LOSS: 1.1906139850616455
LOSS: 1.1905128955841064
LOSS: 1.1904152631759644
LOSS: 1.190321683883667
LOSS: 1.1902308464050293
LOSS: 1.190142035484314
LOSS: 1.1900547742843628
LOSS: 1.1899690628051758
LOSS: 1.1898844242095947
LOSS: 1.1898019313812256
LOSS: 1.1897213459014893
LOSS: 1.1896438598632812
LOSS: 1.1895694732666016
LOSS: 1.1894986629486084
LOSS: 1.1894315481185913
LOSS: 1.189367413520813
LOSS: 1.189306378364563
LOSS: 1.1892476081848145
LOSS: 1.1891906261444092
LOSS: 1.1891353130340576
LOSS: 1.1890811920166016
LOSS: 1.189028024673462
LOSS: 1.1889760494232178
LOSS: 1.1889253854751587
LOSS: 1.188875675201416
LOSS: 1.1888279914855957
LOSS: 1.1887818574905396
LOSS: 1.188737392425537
LOSS: 1.188694953918457
LOSS: 1.188653826713562
LOSS: 1.1886141300201416
LOSS: 1.1885755062103271
LOSS: 1.188537836074829
LOSS: 1.1885008811950684
LOSS: 1.188464879989624
LOSS: 1.1884292364120483
LOSS: 1.1883941888809204
LOSS: 1.1883599758148193
LOSS: 1.1883265972137451
LOSS: 1.302241325378418
LOSS: 1.2918519973754883
LOSS: 1.2818917036056519
LOSS: 1.2724295854568481
LOSS: 1.2635341882705688
LOSS: 1.2552692890167236
LOSS: 1.247689962387085
LOSS: 1.2408366203308105
LOSS: 1.2347290515899658
LOSS: 1.229364037513733
LOSS: 1.224717617034912
LOSS: 1.2207518815994263
LOSS: 1.2174222469329834
LOSS: 1.214682936668396
LOSS: 1.2124834060668945
LOSS: 1.210763931274414
LOSS: 1.2094521522521973
LOSS: 1.208461046218872
LOSS: 1.2076956033706665
LOSS: 1.253565788269043
LOSS: 1.2507706880569458
LOSS: 1.2480238676071167
LOSS: 1.245363712310791
LOSS: 1.2428133487701416
LOSS: 1.2403812408447266
LOSS: 1.2380645275115967
LOSS: 1.2358466386795044
LOSS: 1.2337112426757812
LOSS: 1.2316299676895142
LOSS: 1.229578971862793
LOSS: 1.2275291681289673
LOSS: 1.225448727607727
LOSS: 1.223311185836792
LOSS: 1.2210942506790161
LOSS: 1.2187851667404175
LOSS: 1.2163779735565186
LOSS: 1.213876485824585
LOSS: 1.2112922668457031
LOSS: 1.2086451053619385
LOSS: 1.205967664718628
LOSS: 1.2032890319824219
LOSS: 1.2006369829177856
LOSS: 1.1979984045028687
LOSS: 1.195330262184143
LOSS: 1.1926138401031494
LOSS: 1.189857840538025
LOSS: 1.1870765686035156
LOSS: 1.184286117553711
LOSS: 1.1814972162246704
LOSS: 1.1787140369415283
LOSS: 1.175931692123413
LOSS: 1.1731460094451904
LOSS: 1.1703603267669678
LOSS: 1.167587161064148
LOSS: 1.164847493171692
LOSS: 1.1621705293655396
LOSS: 1.159584641456604
LOSS: 1.1570936441421509
LOSS: 1.1546504497528076
LOSS: 1.1522022485733032
LOSS: 1.149721622467041
LOSS: 1.1471960544586182
LOSS: 1.1446326971054077
LOSS: 1.1420567035675049
LOSS: 1.3727697134017944
LOSS: 1.3665549755096436
LOSS: 1.3607321977615356
LOSS: 1.355338454246521
LOSS: 1.350409984588623
LOSS: 1.3459765911102295
LOSS: 1.3420604467391968
LOSS: 1.3386650085449219
LOSS: 1.3357707262039185
LOSS: 1.333324909210205
LOSS: 1.331239938735962
LOSS: 1.3293992280960083
LOSS: 1.3276735544204712
LOSS: 1.3259496688842773
LOSS: 1.3241405487060547
LOSS: 1.3221925497055054
LOSS: 1.3200839757919312
LOSS: 1.317813754081726
LOSS: 1.3153983354568481
LOSS: 1.3128626346588135
LOSS: 1.310238003730774
LOSS: 1.3075571060180664
LOSS: 1.3048524856567383
LOSS: 1.302153468132019
LOSS: 1.2994879484176636
LOSS: 1.296875238418579
LOSS: 1.2943322658538818
LOSS: 1.2918665409088135
LOSS: 1.2894794940948486
LOSS: 1.2871630191802979
LOSS: 1.2849043607711792
LOSS: 1.2826868295669556
LOSS: 1.2804902791976929
LOSS: 1.278293251991272
LOSS: 1.276075005531311
LOSS: 1.2738155126571655
LOSS: 1.2714978456497192
LOSS: 1.269108533859253
LOSS: 1.2666335105895996
LOSS: 1.2640624046325684
LOSS: 1.2613848447799683
LOSS: 1.258593201637268
LOSS: 1.2556816339492798
LOSS: 1.252646803855896
LOSS: 1.2494869232177734
LOSS: 1.2461997270584106
LOSS: 1.242783546447754
LOSS: 1.239236831665039
LOSS: 1.2355600595474243
LOSS: 1.2317590713500977
LOSS: 1.227846384048462
LOSS: 1.2238441705703735
LOSS: 1.219783902168274
LOSS: 1.2157050371170044
LOSS: 1.2116526365280151
LOSS: 1.2076672315597534
LOSS: 1.2037811279296875
LOSS: 1.2000148296356201
LOSS: 1.196382761001587
LOSS: 1.1928927898406982
LOSS: 1.1895519495010376
LOSS: 1.18636155128479
LOSS: 1.1833208799362183
LOSS: 1.1804264783859253
LOSS: 1.177677869796753
LOSS: 1.1750695705413818
LOSS: 1.1725856065750122
LOSS: 1.1702044010162354
LOSS: 1.1678876876831055
LOSS: 1.1656036376953125
LOSS: 1.1633381843566895
LOSS: 1.161097526550293
LOSS: 1.158901572227478
LOSS: 1.1567730903625488
LOSS: 1.1547309160232544
LOSS: 1.1527867317199707
LOSS: 1.1509431600570679
LOSS: 1.1491917371749878
LOSS: 1.1475085020065308
LOSS: 1.1458643674850464
LOSS: 1.1442339420318604
LOSS: 1.1426056623458862
LOSS: 1.140976071357727
LOSS: 1.1393511295318604
LOSS: 1.1377431154251099
LOSS: 1.136157751083374
LOSS: 1.1346012353897095
LOSS: 1.1330764293670654
LOSS: 1.1315850019454956
LOSS: 1.130130410194397
LOSS: 1.1287143230438232
LOSS: 1.12733793258667
LOSS: 1.1259980201721191
LOSS: 1.1246875524520874
LOSS: 1.123413324356079
LOSS: 1.1221791505813599
LOSS: 1.120996356010437
LOSS: 1.1198674440383911
LOSS: 1.1187915802001953
LOSS: 1.1177705526351929
LOSS: 1.2914098501205444
LOSS: 1.280484914779663
LOSS: 1.2699750661849976
LOSS: 1.2599382400512695
LOSS: 1.2504332065582275
LOSS: 1.24151611328125
LOSS: 1.2332375049591064
LOSS: 1.2256394624710083
LOSS: 1.21875
LOSS: 1.2125790119171143
LOSS: 1.207116723060608
LOSS: 1.2023344039916992
LOSS: 1.1981892585754395
LOSS: 1.1946306228637695
LOSS: 1.1916042566299438
LOSS: 1.1890522241592407
LOSS: 1.1869122982025146
LOSS: 1.1851164102554321
LOSS: 1.1835918426513672
LOSS: 1.1822638511657715
LOSS: 1.1810615062713623
LOSS: 1.1799242496490479
LOSS: 1.178804874420166
LOSS: 1.1776753664016724
LOSS: 1.1765249967575073
LOSS: 1.1753596067428589
LOSS: 1.1741971969604492
LOSS: 1.1730620861053467
LOSS: 1.1719810962677002
LOSS: 1.1709773540496826
LOSS: 1.1700680255889893
LOSS: 1.169263243675232
LOSS: 1.1685646772384644
LOSS: 1.1679680347442627
LOSS: 1.1674633026123047
LOSS: 1.1670382022857666
LOSS: 1.1666786670684814
LOSS: 1.1663715839385986
LOSS: 1.1661044359207153
LOSS: 1.1658669710159302
LOSS: 1.1656509637832642
LOSS: 1.165450096130371
LOSS: 1.1652610301971436
LOSS: 1.1650813817977905
LOSS: 1.1649106740951538
LOSS: 1.1647485494613647
LOSS: 1.1645959615707397
LOSS: 1.1644537448883057
LOSS: 1.1643227338790894
LOSS: 1.1642032861709595
LOSS: 1.1640948057174683
LOSS: 1.163996934890747
LOSS: 1.1639082431793213
LOSS: 1.1638270616531372
LOSS: 1.1637519598007202
LOSS: 1.1636810302734375
LOSS: 1.1636124849319458
LOSS: 1.1635457277297974
LOSS: 1.1634804010391235
LOSS: 1.163415789604187
LOSS: 1.1633527278900146
LOSS: 1.163292407989502
LOSS: 1.1632353067398071
LOSS: 1.1631823778152466
LOSS: 1.163134217262268
LOSS: 1.1630913019180298
LOSS: 1.1630536317825317
LOSS: 1.163020133972168
LOSS: 1.162990689277649
LOSS: 1.1629637479782104
LOSS: 1.1629388332366943
LOSS: 1.1629141569137573
LOSS: 1.1628899574279785
LOSS: 1.162865161895752
LOSS: 1.1628397703170776
LOSS: 1.162813425064087
LOSS: 1.1627874374389648
LOSS: 1.1627614498138428
LOSS: 1.1627360582351685
LOSS: 1.1627113819122314
LOSS: 1.1626877784729004
LOSS: 1.162665605545044
LOSS: 1.162644624710083
LOSS: 1.162624716758728
LOSS: 1.1626054048538208
LOSS: 1.1625866889953613
LOSS: 1.1625688076019287
LOSS: 1.1625510454177856
LOSS: 1.162533164024353
LOSS: 1.1625157594680786
LOSS: 1.162498116493225
LOSS: 1.1624805927276611
LOSS: 1.1624635457992554
LOSS: 1.1624466180801392
LOSS: 1.1624302864074707
LOSS: 1.1624138355255127
LOSS: 1.1623977422714233
LOSS: 1.1623828411102295
LOSS: 1.1623677015304565
LOSS: 1.1623526811599731
LOSS: 1.2991641759872437
LOSS: 1.2886391878128052
LOSS: 1.2785677909851074
LOSS: 1.2690095901489258
LOSS: 1.2600221633911133
LOSS: 1.251657247543335
LOSS: 1.2439576387405396
LOSS: 1.2369519472122192
LOSS: 1.2306511402130127
LOSS: 1.2250466346740723
LOSS: 1.2201108932495117
LOSS: 1.215803861618042
LOSS: 1.2120797634124756
LOSS: 1.208890438079834
LOSS: 1.2061859369277954
LOSS: 1.2039107084274292
LOSS: 1.2020010948181152
LOSS: 1.2003846168518066
LOSS: 1.198983073234558
LOSS: 1.1977183818817139
LOSS: 1.1965203285217285
LOSS: 1.1953328847885132
LOSS: 1.1941196918487549
LOSS: 1.1928642988204956
LOSS: 1.1915695667266846
LOSS: 1.1902515888214111
LOSS: 1.188936710357666
LOSS: 1.187652587890625
LOSS: 1.186427116394043
LOSS: 1.1852823495864868
LOSS: 1.1842334270477295
LOSS: 1.1832878589630127
LOSS: 1.182446002960205
LOSS: 1.1817017793655396
LOSS: 1.1810452938079834
LOSS: 1.1804641485214233
LOSS: 1.179944396018982
LOSS: 1.1794735193252563
LOSS: 1.1790401935577393
LOSS: 1.1786357164382935
LOSS: 1.1782532930374146
LOSS: 1.1778897047042847
LOSS: 1.1775435209274292
LOSS: 1.177215337753296
LOSS: 1.1769073009490967
LOSS: 1.1766210794448853
LOSS: 1.1763601303100586
LOSS: 1.1761255264282227
LOSS: 1.1759183406829834
LOSS: 1.1757378578186035
LOSS: 1.175582766532898
LOSS: 1.1754499673843384
LOSS: 1.17533540725708
LOSS: 1.1752349138259888
LOSS: 1.175144076347351
LOSS: 1.1750586032867432
LOSS: 1.1749753952026367
LOSS: 1.174891471862793
LOSS: 1.1748055219650269
LOSS: 1.1747170686721802
LOSS: 1.1746268272399902
LOSS: 1.1745356321334839
LOSS: 1.1744451522827148
LOSS: 1.1743571758270264
LOSS: 1.1742730140686035
LOSS: 1.174194097518921
LOSS: 1.1741206645965576
LOSS: 1.1740529537200928
LOSS: 1.173991084098816
LOSS: 1.1739341020584106
LOSS: 1.173880696296692
LOSS: 1.1738301515579224
LOSS: 1.1737817525863647
LOSS: 1.1737339496612549
LOSS: 1.173687219619751
LOSS: 1.1736407279968262
LOSS: 1.1735941171646118
LOSS: 1.1735484600067139
LOSS: 1.1735029220581055
LOSS: 1.1734588146209717
LOSS: 1.1734154224395752
LOSS: 1.1733736991882324
LOSS: 1.1733334064483643
LOSS: 1.1732947826385498
LOSS: 1.1732574701309204
LOSS: 1.1732213497161865
LOSS: 1.1330764293670654
LOSS: 1.1315850019454956
LOSS: 1.130130410194397
LOSS: 1.1287143230438232
LOSS: 1.12733793258667
LOSS: 1.1259980201721191
LOSS: 1.1246875524520874
LOSS: 1.123413324356079
LOSS: 1.1221791505813599
LOSS: 1.120996356010437
LOSS: 1.1198674440383911
LOSS: 1.1187915802001953
LOSS: 1.1177705526351929
LOSS: 1.369425892829895
LOSS: 1.3630605936050415
LOSS: 1.357079267501831
LOSS: 1.3515183925628662
LOSS: 1.3464149236679077
LOSS: 1.341800570487976
LOSS: 1.3376970291137695
LOSS: 1.3341127634048462
LOSS: 1.3310333490371704
LOSS: 1.3284144401550293
LOSS: 1.3261756896972656
LOSS: 1.3242075443267822
LOSS: 1.3223849534988403
LOSS: 1.3205883502960205
LOSS: 1.3187216520309448
LOSS: 1.3167243003845215
LOSS: 1.3145647048950195
LOSS: 1.3122351169586182
LOSS: 1.309748649597168
LOSS: 1.3071260452270508
LOSS: 1.3043988943099976
LOSS: 1.3015989065170288
LOSS: 1.2987611293792725
LOSS: 1.2959156036376953
LOSS: 1.2930935621261597
LOSS: 1.2903176546096802
LOSS: 1.2876096963882446
LOSS: 1.2849812507629395
LOSS: 1.2824394702911377
LOSS: 1.2799848318099976
LOSS: 1.2776119709014893
LOSS: 1.2753119468688965
LOSS: 1.273073434829712
LOSS: 1.2708827257156372
LOSS: 1.2687289714813232
LOSS: 1.2666014432907104
LOSS: 1.2644931077957153
LOSS: 1.2623995542526245
LOSS: 1.2603187561035156
LOSS: 1.2582529783248901
LOSS: 1.2562024593353271
LOSS: 1.2541683912277222
LOSS: 1.2521474361419678
LOSS: 1.250139832496643
LOSS: 1.2481392621994019
LOSS: 1.246140718460083
LOSS: 1.2441356182098389
LOSS: 1.2421115636825562
LOSS: 1.2400528192520142
LOSS: 1.2379364967346191
LOSS: 1.2357395887374878
LOSS: 1.2334492206573486
LOSS: 1.231064796447754
LOSS: 1.2285964488983154
LOSS: 1.2260435819625854
LOSS: 1.2234106063842773
LOSS: 1.2207109928131104
LOSS: 1.2179601192474365
LOSS: 1.2151716947555542
LOSS: 1.212348461151123
LOSS: 1.2094790935516357
LOSS: 1.2065402269363403
LOSS: 1.203506350517273
LOSS: 1.2003533840179443
LOSS: 1.197074294090271
LOSS: 1.1936559677124023
LOSS: 1.1901086568832397
LOSS: 1.186429500579834
LOSS: 1.182637095451355
LOSS: 1.1787629127502441
LOSS: 1.1748374700546265
LOSS: 1.170897364616394
LOSS: 1.1669697761535645
LOSS: 1.163088083267212
LOSS: 1.159280776977539
LOSS: 1.155583381652832
LOSS: 1.1520267724990845
LOSS: 1.1486480236053467
LOSS: 1.1454836130142212
LOSS: 1.1425648927688599
LOSS: 1.1399091482162476
LOSS: 1.1375186443328857
LOSS: 1.1353671550750732
LOSS: 1.1334011554718018
LOSS: 1.1315516233444214
LOSS: 1.1297634840011597
LOSS: 1.127997636795044
LOSS: 1.1262496709823608
LOSS: 1.1245073080062866
LOSS: 1.1227864027023315
LOSS: 1.121101975440979
LOSS: 1.1194778680801392
LOSS: 1.1179227828979492
LOSS: 1.1164294481277466
LOSS: 1.1150109767913818
LOSS: 1.113666296005249
LOSS: 1.1124038696289062
LOSS: 1.1111705303192139
LOSS: 1.1100133657455444
LOSS: 1.1089340448379517
LOSS: 1.2930500507354736
LOSS: 1.282517910003662
LOSS: 1.2724016904830933
LOSS: 1.2627626657485962
LOSS: 1.2536628246307373
LOSS: 1.2451624870300293
LOSS: 1.237317681312561
LOSS: 1.2301747798919678
LOSS: 1.2237666845321655
LOSS: 1.2181072235107422
LOSS: 1.2131903171539307
LOSS: 1.2089898586273193
LOSS: 1.2054646015167236
LOSS: 1.202561855316162
LOSS: 1.2002215385437012
LOSS: 1.1983747482299805
LOSS: 1.196943759918213
LOSS: 1.195841670036316
LOSS: 1.1949760913848877
LOSS: 1.1942565441131592
LOSS: 1.1936007738113403
LOSS: 1.1929447650909424
LOSS: 1.1922467947006226
LOSS: 1.1914894580841064
LOSS: 1.1906777620315552
LOSS: 1.189834475517273
LOSS: 1.1889935731887817
LOSS: 1.1881933212280273
LOSS: 1.1874704360961914
LOSS: 1.1868547201156616
LOSS: 1.186366319656372
LOSS: 1.1860119104385376
LOSS: 1.185787558555603
LOSS: 1.1856772899627686
LOSS: 1.1856566667556763
LOSS: 1.1856967210769653
LOSS: 1.1857672929763794
LOSS: 1.1858406066894531
LOSS: 1.1858952045440674
LOSS: 1.1859164237976074
LOSS: 1.1858971118927002
LOSS: 1.1858381032943726
LOSS: 1.1857458353042603
LOSS: 1.185630202293396
LOSS: 1.1855024099349976
LOSS: 1.1853744983673096
LOSS: 1.1852561235427856
LOSS: 1.1851539611816406
LOSS: 1.1850727796554565
LOSS: 1.1850130558013916
LOSS: 1.1849733591079712
LOSS: 1.1849493980407715
LOSS: 1.184936761856079
LOSS: 1.1849297285079956
LOSS: 1.1849236488342285
LOSS: 1.1849145889282227
LOSS: 1.1849004030227661
LOSS: 1.1848794221878052
LOSS: 1.1848530769348145
LOSS: 1.1848220825195312
LOSS: 1.1847889423370361
LOSS: 1.184755802154541
LOSS: 1.1847245693206787
LOSS: 1.184696912765503
LOSS: 1.18467378616333
LOSS: 1.1846550703048706
LOSS: 1.1846404075622559
LOSS: 1.1846288442611694
LOSS: 1.1846191883087158
LOSS: 1.1846100091934204
LOSS: 1.1846004724502563
LOSS: 1.1845895051956177
LOSS: 1.1845769882202148
LOSS: 1.1845629215240479
LOSS: 1.1845479011535645
LOSS: 1.1845322847366333
LOSS: 1.1845166683197021
LOSS: 1.1845018863677979
LOSS: 1.1844886541366577
LOSS: 1.1844762563705444
LOSS: 1.184465765953064
LOSS: 1.1844562292099
LOSS: 1.1844474077224731
LOSS: 1.1844393014907837
LOSS: 1.1844311952590942
LOSS: 1.1844226121902466
LOSS: 1.1844137907028198
LOSS: 1.184403896331787
LOSS: 1.1843934059143066
LOSS: 1.1843819618225098
LOSS: 1.1843701601028442
LOSS: 1.1843574047088623
LOSS: 1.184343934059143
LOSS: 1.1843297481536865
LOSS: 1.184314489364624
LOSS: 1.1842982769012451
LOSS: 1.1842801570892334
LOSS: 1.184260606765747
LOSS: 1.1842389106750488
LOSS: 1.184214472770691
LOSS: 1.2930500507354736
LOSS: 1.282517910003662
LOSS: 1.2724016904830933
LOSS: 1.2627626657485962
LOSS: 1.2536628246307373
LOSS: 1.2451624870300293
LOSS: 1.237317681312561
LOSS: 1.2301747798919678
LOSS: 1.2237666845321655
LOSS: 1.2181072235107422
LOSS: 1.2131903171539307
LOSS: 1.2089898586273193
LOSS: 1.2054646015167236
LOSS: 1.202561855316162
LOSS: 1.2002215385437012
LOSS: 1.1983747482299805
LOSS: 1.196943759918213
LOSS: 1.195841670036316
LOSS: 1.1949760913848877
LOSS: 1.1942565441131592
LOSS: 1.1936007738113403
LOSS: 1.1929447650909424
LOSS: 1.1922467947006226
LOSS: 1.1914894580841064
LOSS: 1.1906777620315552
LOSS: 1.189834475517273
LOSS: 1.1889935731887817
LOSS: 1.1881933212280273
LOSS: 1.1874704360961914
LOSS: 1.1868547201156616
LOSS: 1.186366319656372
LOSS: 1.1860119104385376
LOSS: 1.185787558555603
LOSS: 1.1856772899627686
LOSS: 1.1856566667556763
LOSS: 1.1856967210769653
LOSS: 1.1857672929763794
LOSS: 1.1858406066894531
LOSS: 1.1858952045440674
LOSS: 1.1859164237976074
LOSS: 1.1858971118927002
LOSS: 1.1858381032943726
LOSS: 1.1857458353042603
LOSS: 1.185630202293396
LOSS: 1.1855024099349976
LOSS: 1.1853744983673096
LOSS: 1.1852561235427856
LOSS: 1.1851539611816406
LOSS: 1.1850727796554565
LOSS: 1.1850130558013916
LOSS: 1.1849733591079712
LOSS: 1.1849493980407715
LOSS: 1.184936761856079
LOSS: 1.1849297285079956
LOSS: 1.1849236488342285
LOSS: 1.1849145889282227
LOSS: 1.1849004030227661
LOSS: 1.1848794221878052
LOSS: 1.1848530769348145
LOSS: 1.1848220825195312
LOSS: 1.1847889423370361
LOSS: 1.184755802154541
LOSS: 1.1847245693206787
LOSS: 1.184696912765503
LOSS: 1.18467378616333
LOSS: 1.1846550703048706
LOSS: 1.1846404075622559
LOSS: 1.1846288442611694
LOSS: 1.1846191883087158
LOSS: 1.1846100091934204
LOSS: 1.1846004724502563
LOSS: 1.1845895051956177
LOSS: 1.1845769882202148
LOSS: 1.1845629215240479
LOSS: 1.1845479011535645
LOSS: 1.1845322847366333
LOSS: 1.1845166683197021
LOSS: 1.1845018863677979
LOSS: 1.1844886541366577
LOSS: 1.1844762563705444
LOSS: 1.184465765953064
LOSS: 1.1844562292099
LOSS: 1.1844474077224731
LOSS: 1.1844393014907837
LOSS: 1.1844311952590942
LOSS: 1.1844226121902466
LOSS: 1.1844137907028198
LOSS: 1.184403896331787
LOSS: 1.1843934059143066
LOSS: 1.1843819618225098
LOSS: 1.1843701601028442
LOSS: 1.1843574047088623
LOSS: 1.184343934059143
LOSS: 1.1843297481536865
LOSS: 1.184314489364624
LOSS: 1.1842982769012451
LOSS: 1.1842801570892334
LOSS: 1.184260606765747
LOSS: 1.1842389106750488
LOSS: 1.184214472770691
LOSS: 1.2947858572006226
LOSS: 1.2840628623962402
LOSS: 1.273758053779602
LOSS: 1.2639329433441162
LOSS: 1.2546504735946655
LOSS: 1.2459698915481567
LOSS: 1.2379446029663086
LOSS: 1.2306163311004639
LOSS: 1.2240098714828491
LOSS: 1.2181289196014404
LOSS: 1.212956428527832
LOSS: 1.2084592580795288
LOSS: 1.2045965194702148
LOSS: 1.2013245820999146
LOSS: 1.198598027229309
LOSS: 1.1963673830032349
LOSS: 1.194574236869812
LOSS: 1.1890522241592407
LOSS: 1.1869122982025146
LOSS: 1.1851164102554321
LOSS: 1.1835918426513672
LOSS: 1.1822638511657715
LOSS: 1.1810615062713623
LOSS: 1.1799242496490479
LOSS: 1.178804874420166
LOSS: 1.1776753664016724
LOSS: 1.1765249967575073
LOSS: 1.1753596067428589
LOSS: 1.1741971969604492
LOSS: 1.1730620861053467
LOSS: 1.1719810962677002
LOSS: 1.1709773540496826
LOSS: 1.1700680255889893
LOSS: 1.169263243675232
LOSS: 1.1685646772384644
LOSS: 1.1679680347442627
LOSS: 1.1674633026123047
LOSS: 1.1670382022857666
LOSS: 1.1666786670684814
LOSS: 1.1663715839385986
LOSS: 1.1661044359207153
LOSS: 1.1658669710159302
LOSS: 1.1656509637832642
LOSS: 1.165450096130371
LOSS: 1.1652610301971436
LOSS: 1.1650813817977905
LOSS: 1.1649106740951538
LOSS: 1.1647485494613647
LOSS: 1.1645959615707397
LOSS: 1.1644537448883057
LOSS: 1.1643227338790894
LOSS: 1.1642032861709595
LOSS: 1.1640948057174683
LOSS: 1.163996934890747
LOSS: 1.1639082431793213
LOSS: 1.1638270616531372
LOSS: 1.1637519598007202
LOSS: 1.1636810302734375
LOSS: 1.1636124849319458
LOSS: 1.1635457277297974
LOSS: 1.1634804010391235
LOSS: 1.163415789604187
LOSS: 1.1633527278900146
LOSS: 1.163292407989502
LOSS: 1.1632353067398071
LOSS: 1.1631823778152466
LOSS: 1.163134217262268
LOSS: 1.1630913019180298
LOSS: 1.1630536317825317
LOSS: 1.163020133972168
LOSS: 1.162990689277649
LOSS: 1.1629637479782104
LOSS: 1.1629388332366943
LOSS: 1.1629141569137573
LOSS: 1.1628899574279785
LOSS: 1.162865161895752
LOSS: 1.1628397703170776
LOSS: 1.162813425064087
LOSS: 1.1627874374389648
LOSS: 1.1627614498138428
LOSS: 1.1627360582351685
LOSS: 1.1627113819122314
LOSS: 1.1626877784729004
LOSS: 1.162665605545044
LOSS: 1.162644624710083
LOSS: 1.162624716758728
LOSS: 1.1626054048538208
LOSS: 1.1625866889953613
LOSS: 1.1625688076019287
LOSS: 1.1625510454177856
LOSS: 1.162533164024353
LOSS: 1.1625157594680786
LOSS: 1.162498116493225
LOSS: 1.1624805927276611
LOSS: 1.1624635457992554
LOSS: 1.1624466180801392
LOSS: 1.1624302864074707
LOSS: 1.1624138355255127
LOSS: 1.1623977422714233
LOSS: 1.1623828411102295
LOSS: 1.1623677015304565
LOSS: 1.1623526811599731
LOSS: 1.2917648553848267
LOSS: 1.2809864282608032
LOSS: 1.270636796951294
LOSS: 1.2607777118682861
LOSS: 1.2514712810516357
LOSS: 1.2427774667739868
LOSS: 1.234750509262085
LOSS: 1.2274341583251953
LOSS: 1.2208563089370728
LOSS: 1.2150253057479858
LOSS: 1.2099300622940063
LOSS: 1.2055432796478271
LOSS: 1.2018275260925293
LOSS: 1.198740839958191
LOSS: 1.1962368488311768
LOSS: 1.1942616701126099
LOSS: 1.192750334739685
LOSS: 1.191624402999878
LOSS: 1.1907933950424194
LOSS: 1.1901617050170898
LOSS: 1.1896361112594604
LOSS: 1.1891363859176636
LOSS: 1.1886017322540283
LOSS: 1.1879957914352417
LOSS: 1.18730628490448
LOSS: 1.1865413188934326
LOSS: 1.1857246160507202
LOSS: 1.184888482093811
LOSS: 1.1840667724609375
LOSS: 1.1832915544509888
LOSS: 1.18258798122406
LOSS: 1.1819722652435303
LOSS: 1.1814513206481934
LOSS: 1.1810239553451538
LOSS: 1.1806803941726685
LOSS: 1.1804064512252808
LOSS: 1.1801835298538208
LOSS: 1.1799923181533813
LOSS: 1.1798136234283447
LOSS: 1.1796308755874634
LOSS: 1.1794300079345703
LOSS: 1.1792010068893433
LOSS: 1.178938388824463
LOSS: 1.178640365600586
LOSS: 1.178309440612793
LOSS: 1.1779519319534302
LOSS: 1.1775754690170288
LOSS: 1.177189826965332
LOSS: 1.176804780960083
LOSS: 1.1764289140701294
LOSS: 1.1760693788528442
LOSS: 1.1757307052612305
LOSS: 1.175415277481079
LOSS: 1.1751223802566528
LOSS: 1.1748501062393188
LOSS: 1.1745944023132324
LOSS: 1.1743513345718384
LOSS: 1.1741162538528442
LOSS: 1.1738852262496948
LOSS: 1.1736561059951782
LOSS: 1.173427700996399
LOSS: 1.1731996536254883
LOSS: 1.1729730367660522
LOSS: 1.172749400138855
LOSS: 1.1725306510925293
LOSS: 1.1723181009292603
LOSS: 1.172113299369812
LOSS: 1.1719163656234741
LOSS: 1.1717272996902466
LOSS: 1.1715447902679443
LOSS: 1.1713682413101196
LOSS: 1.1711958646774292
LOSS: 1.1710267066955566
LOSS: 1.170859932899475
LOSS: 1.170694351196289
LOSS: 1.170530080795288
LOSS: 1.1703662872314453
LOSS: 1.1702035665512085
LOSS: 1.1700420379638672
LOSS: 1.1698813438415527
LOSS: 1.1697221994400024
LOSS: 1.1695642471313477
LOSS: 1.1694082021713257
LOSS: 1.1692537069320679
LOSS: 1.1691009998321533
LOSS: 1.1689499616622925
LOSS: 1.1688013076782227
LOSS: 1.1686545610427856
LOSS: 1.168509602546692
LOSS: 1.1683666706085205
LOSS: 1.1682254076004028
LOSS: 1.168086290359497
LOSS: 1.1679487228393555
LOSS: 1.1678129434585571
LOSS: 1.167678713798523
LOSS: 1.1675461530685425
LOSS: 1.167415738105774
LOSS: 1.1672871112823486
LOSS: 1.167160987854004
LOSS: 1.1670374870300293
LOSS: 1.30367910861969
LOSS: 1.2934958934783936
LOSS: 1.2837711572647095
LOSS: 1.274563193321228
LOSS: 1.265929102897644
LOSS: 1.2579203844070435
LOSS: 1.2505794763565063
LOSS: 1.243933916091919
LOSS: 1.2379913330078125
LOSS: 1.232737421989441
LOSS: 1.22813880443573
LOSS: 1.2241510152816772
LOSS: 1.2207266092300415
LOSS: 1.2178195714950562
LOSS: 1.2153828144073486
LOSS: 1.2133625745773315
LOSS: 1.211695909500122
LOSS: 1.21030855178833
LOSS: 1.2091211080551147
LOSS: 1.208054542541504
LOSS: 1.207039475440979
LOSS: 1.2060235738754272
LOSS: 1.204973816871643
LOSS: 1.2038788795471191
LOSS: 1.202745795249939
LOSS: 1.2015950679779053
LOSS: 1.2004550695419312
LOSS: 1.199356198310852
LOSS: 1.1983267068862915
LOSS: 1.1973886489868164
LOSS: 1.1965560913085938
LOSS: 1.1958339214324951
LOSS: 1.1952182054519653
LOSS: 1.1946964263916016
LOSS: 1.194250464439392
LOSS: 1.1938577890396118
LOSS: 1.1934958696365356
LOSS: 1.1931452751159668
LOSS: 1.1927907466888428
LOSS: 1.1924240589141846
LOSS: 1.192043662071228
LOSS: 1.1916524171829224
LOSS: 1.191257357597351
LOSS: 1.1908669471740723
LOSS: 1.1904895305633545
LOSS: 1.1901313066482544
LOSS: 1.189797043800354
LOSS: 1.1894885301589966
LOSS: 1.1892046928405762
LOSS: 1.1889432668685913
LOSS: 1.1887000799179077
LOSS: 1.1884709596633911
LOSS: 1.1882522106170654
LOSS: 1.188040852546692
LOSS: 1.187835931777954
LOSS: 1.1876373291015625
LOSS: 1.187447190284729
LOSS: 1.1872680187225342
LOSS: 1.1871027946472168
LOSS: 1.186951994895935
LOSS: 1.186815619468689
LOSS: 1.1866899728775024
LOSS: 1.1865705251693726
LOSS: 1.1864521503448486
LOSS: 1.186332106590271
LOSS: 1.186208963394165
LOSS: 1.1860840320587158
LOSS: 1.1859607696533203
LOSS: 1.18584144115448
LOSS: 1.1857291460037231
LOSS: 1.1856244802474976
LOSS: 1.1855270862579346
LOSS: 1.185435175895691
LOSS: 1.1853476762771606
LOSS: 1.1852620840072632
LOSS: 1.1851778030395508
LOSS: 1.1850945949554443
LOSS: 1.1850131750106812
LOSS: 1.1849344968795776
LOSS: 1.1848595142364502
LOSS: 1.184788703918457
LOSS: 1.184721827507019
LOSS: 1.184658408164978
LOSS: 1.1845978498458862
LOSS: 1.1845389604568481
LOSS: 1.1844810247421265
LOSS: 1.1844247579574585
LOSS: 1.1843700408935547
LOSS: 1.1843173503875732
LOSS: 1.1842671632766724
LOSS: 1.184219479560852
LOSS: 1.1841742992401123
LOSS: 1.1841306686401367
LOSS: 1.1840879917144775
LOSS: 1.1840471029281616
LOSS: 1.1840063333511353
LOSS: 1.183966875076294
LOSS: 1.1839289665222168
LOSS: 1.1838923692703247
LOSS: 1.1838568449020386
LOSS: 1.286154866218567
LOSS: 1.274694800376892
LOSS: 1.2636194229125977
LOSS: 1.2529900074005127
LOSS: 1.2428679466247559
LOSS: 1.2333124876022339
LOSS: 1.224377989768982
LOSS: 1.2161091566085815
LOSS: 1.2085371017456055
LOSS: 1.2016748189926147
LOSS: 1.1955174207687378
LOSS: 1.190045952796936
LOSS: 1.1852341890335083
LOSS: 1.1810542345046997
LOSS: 1.1774801015853882
LOSS: 1.1744831800460815
LOSS: 1.1720285415649414
LOSS: 1.1700682640075684
LOSS: 1.1685408353805542
LOSS: 1.1673696041107178
LOSS: 1.1664685010910034
LOSS: 1.1657477617263794
LOSS: 1.165122628211975
LOSS: 1.164521336555481
LOSS: 1.1638902425765991
LOSS: 1.1631975173950195
LOSS: 1.1624324321746826
LOSS: 1.1616027355194092
LOSS: 1.1607294082641602
LOSS: 1.1598411798477173
LOSS: 1.1589691638946533
LOSS: 1.1581430435180664
LOSS: 1.1573861837387085
LOSS: 1.1567145586013794
LOSS: 1.156136155128479
LOSS: 1.1556508541107178
LOSS: 1.1552515029907227
LOSS: 1.1549255847930908
LOSS: 1.1546576023101807
LOSS: 1.1544301509857178
LOSS: 1.1542266607284546
LOSS: 1.1540319919586182
LOSS: 1.1538338661193848
LOSS: 1.1536237001419067
LOSS: 1.153396725654602
LOSS: 1.153150200843811
LOSS: 1.1731864213943481
LOSS: 1.1731526851654053
LOSS: 1.1731195449829102
LOSS: 1.173087239265442
LOSS: 1.1730554103851318
LOSS: 1.173024296760559
LOSS: 1.1729940176010132
LOSS: 1.1729644536972046
LOSS: 1.1729356050491333
LOSS: 1.1729077100753784
LOSS: 1.1728804111480713
LOSS: 1.1728541851043701
LOSS: 1.1728285551071167
LOSS: 1.1728036403656006
LOSS: 1.286154866218567
LOSS: 1.274694800376892
LOSS: 1.2636194229125977
LOSS: 1.2529900074005127
LOSS: 1.2428679466247559
LOSS: 1.2333124876022339
LOSS: 1.224377989768982
LOSS: 1.2161091566085815
LOSS: 1.2085371017456055
LOSS: 1.2016748189926147
LOSS: 1.1955174207687378
LOSS: 1.190045952796936
LOSS: 1.1852341890335083
LOSS: 1.1810542345046997
LOSS: 1.1774801015853882
LOSS: 1.1744831800460815
LOSS: 1.1720285415649414
LOSS: 1.1700682640075684
LOSS: 1.1685408353805542
LOSS: 1.1673696041107178
LOSS: 1.1664685010910034
LOSS: 1.1657477617263794
LOSS: 1.165122628211975
LOSS: 1.164521336555481
LOSS: 1.1638902425765991
LOSS: 1.1631975173950195
LOSS: 1.1624324321746826
LOSS: 1.1616027355194092
LOSS: 1.1607294082641602
LOSS: 1.1598411798477173
LOSS: 1.1589691638946533
LOSS: 1.1581430435180664
LOSS: 1.1573861837387085
LOSS: 1.1567145586013794
LOSS: 1.156136155128479
LOSS: 1.1556508541107178
LOSS: 1.1552515029907227
LOSS: 1.1549255847930908
LOSS: 1.1546576023101807
LOSS: 1.1544301509857178
LOSS: 1.1542266607284546
LOSS: 1.1540319919586182
LOSS: 1.1538338661193848
LOSS: 1.1536237001419067
LOSS: 1.153396725654602
LOSS: 1.153150200843811
LOSS: 1.1528860330581665
LOSS: 1.1526075601577759
LOSS: 1.1523199081420898
LOSS: 1.1520284414291382
LOSS: 1.1517397165298462
LOSS: 1.1514586210250854
LOSS: 1.1511887311935425
LOSS: 1.1509335041046143
LOSS: 1.1506941318511963
LOSS: 1.1504697799682617
LOSS: 1.1502591371536255
LOSS: 1.150059700012207
LOSS: 1.1498682498931885
LOSS: 1.1496814489364624
LOSS: 1.1494966745376587
LOSS: 1.1493103504180908
LOSS: 1.149121642112732
LOSS: 1.1489291191101074
LOSS: 1.1487324237823486
LOSS: 1.1485321521759033
LOSS: 1.1483299732208252
LOSS: 1.148126482963562
LOSS: 1.147923469543457
LOSS: 1.1477223634719849
LOSS: 1.1475245952606201
LOSS: 1.147330641746521
LOSS: 1.1471413373947144
LOSS: 1.1469563245773315
LOSS: 1.1467759609222412
LOSS: 1.1465996503829956
LOSS: 1.1464266777038574
LOSS: 1.1462568044662476
LOSS: 1.1460895538330078
LOSS: 1.1459239721298218
LOSS: 1.145760178565979
LOSS: 1.14559805393219
LOSS: 1.1454373598098755
LOSS: 1.1452783346176147
LOSS: 1.1451213359832764
LOSS: 1.14496648311615
LOSS: 1.144813895225525
LOSS: 1.144663691520691
LOSS: 1.1445165872573853
LOSS: 1.1443718671798706
LOSS: 1.144229769706726
LOSS: 1.1440904140472412
LOSS: 1.1439539194107056
LOSS: 1.1438194513320923
LOSS: 1.14368736743927
LOSS: 1.1435575485229492
LOSS: 1.1434298753738403
LOSS: 1.143304467201233
LOSS: 1.1431810855865479
LOSS: 1.1430600881576538
LOSS: 1.295655369758606
LOSS: 1.2850128412246704
LOSS: 1.2748113870620728
LOSS: 1.2651125192642212
LOSS: 1.2559771537780762
LOSS: 1.2474638223648071
LOSS: 1.2396256923675537
LOSS: 1.2325067520141602
LOSS: 1.2261378765106201
LOSS: 1.220532774925232
LOSS: 1.2156842947006226
LOSS: 1.211565375328064
LOSS: 1.2081282138824463
LOSS: 1.2053091526031494
LOSS: 1.2030316591262817
LOSS: 1.2012083530426025
LOSS: 1.1997443437576294
LOSS: 1.1985405683517456
LOSS: 1.1974999904632568
LOSS: 1.1965348720550537
LOSS: 1.1955746412277222
LOSS: 1.194571614265442
LOSS: 1.19350266456604
LOSS: 1.192367672920227
LOSS: 1.1911848783493042
LOSS: 1.1899844408035278
LOSS: 1.1888011693954468
LOSS: 1.1876691579818726
LOSS: 1.1866170167922974
LOSS: 1.1856648921966553
LOSS: 1.1848236322402954
LOSS: 1.184094786643982
LOSS: 1.1834717988967896
LOSS: 1.1829421520233154
LOSS: 1.1824897527694702
LOSS: 1.1820963621139526
LOSS: 1.18174409866333
LOSS: 1.1814171075820923
LOSS: 1.18110191822052
LOSS: 1.1807889938354492
LOSS: 1.1804721355438232
LOSS: 1.1801488399505615
LOSS: 1.1798200607299805
LOSS: 1.1794884204864502
LOSS: 1.179159164428711
LOSS: 1.1788378953933716
LOSS: 1.1785309314727783
LOSS: 1.1782435178756714
LOSS: 1.177980661392212
LOSS: 1.177744746208191
LOSS: 1.1775373220443726
LOSS: 1.177357792854309
LOSS: 1.1772041320800781
LOSS: 1.1770732402801514
LOSS: 1.1769604682922363
LOSS: 1.1768617630004883
LOSS: 1.1767722368240356
LOSS: 1.1766880750656128
LOSS: 1.1766060590744019
LOSS: 1.1765234470367432
LOSS: 1.1764394044876099
LOSS: 1.1763532161712646
LOSS: 1.176265001296997
LOSS: 1.1761753559112549
LOSS: 1.1760854721069336
LOSS: 1.1759966611862183
LOSS: 1.1759096384048462
LOSS: 1.1758249998092651
LOSS: 1.1757433414459229
LOSS: 1.1756649017333984
LOSS: 1.1755889654159546
LOSS: 1.175516128540039
LOSS: 1.175445318222046
LOSS: 1.175376296043396
LOSS: 1.1753088235855103
LOSS: 1.1752426624298096
LOSS: 1.1751774549484253
LOSS: 1.1751136779785156
LOSS: 1.1750510931015015
LOSS: 1.1749902963638306
LOSS: 1.174931287765503
LOSS: 1.1748743057250977
LOSS: 1.174819827079773
LOSS: 1.1747673749923706
LOSS: 1.174717664718628
LOSS: 1.1746701002120972
LOSS: 1.1746245622634888
LOSS: 1.1745808124542236
LOSS: 1.1745387315750122
LOSS: 1.1744976043701172
LOSS: 1.1744575500488281
LOSS: 1.1744182109832764
LOSS: 1.1743793487548828
LOSS: 1.174340844154358
LOSS: 1.1743024587631226
LOSS: 1.1742644309997559
LOSS: 1.1742265224456787
LOSS: 1.1741890907287598
LOSS: 1.1741520166397095
LOSS: 1.1741153001785278
LOSS: 1.30367910861969
LOSS: 1.2934958934783936
LOSS: 1.2837711572647095
LOSS: 1.274563193321228
LOSS: 1.265929102897644
LOSS: 1.2579203844070435
LOSS: 1.2505794763565063
LOSS: 1.243933916091919
LOSS: 1.2379913330078125
LOSS: 1.232737421989441
LOSS: 1.22813880443573
LOSS: 1.2241510152816772
LOSS: 1.2207266092300415
LOSS: 1.2178195714950562
LOSS: 1.2153828144073486
LOSS: 1.2133625745773315
LOSS: 1.211695909500122
LOSS: 1.21030855178833
LOSS: 1.2091211080551147
LOSS: 1.208054542541504
LOSS: 1.207039475440979
LOSS: 1.2060235738754272
LOSS: 1.204973816871643
LOSS: 1.2038788795471191
LOSS: 1.202745795249939
LOSS: 1.2015950679779053
LOSS: 1.2004550695419312
LOSS: 1.199356198310852
LOSS: 1.1983267068862915
LOSS: 1.1973886489868164
LOSS: 1.1965560913085938
LOSS: 1.1958339214324951
LOSS: 1.1952182054519653
LOSS: 1.1946964263916016
LOSS: 1.194250464439392
LOSS: 1.1938577890396118
LOSS: 1.1934958696365356
LOSS: 1.1931452751159668
LOSS: 1.1927907466888428
LOSS: 1.1924240589141846
LOSS: 1.192043662071228
LOSS: 1.1916524171829224
LOSS: 1.191257357597351
LOSS: 1.1908669471740723
LOSS: 1.1904895305633545
LOSS: 1.1901313066482544
LOSS: 1.189797043800354
LOSS: 1.1894885301589966
LOSS: 1.1892046928405762
LOSS: 1.1889432668685913
LOSS: 1.1887000799179077
LOSS: 1.1884709596633911
LOSS: 1.1882522106170654
LOSS: 1.188040852546692
LOSS: 1.187835931777954
LOSS: 1.1876373291015625
LOSS: 1.187447190284729
LOSS: 1.1872680187225342
LOSS: 1.1871027946472168
LOSS: 1.186951994895935
LOSS: 1.186815619468689
LOSS: 1.1866899728775024
LOSS: 1.1865705251693726
LOSS: 1.1864521503448486
LOSS: 1.186332106590271
LOSS: 1.186208963394165
LOSS: 1.1860840320587158
LOSS: 1.1859607696533203
LOSS: 1.18584144115448
LOSS: 1.1857291460037231
LOSS: 1.1856244802474976
LOSS: 1.1855270862579346
LOSS: 1.185435175895691
LOSS: 1.1853476762771606
LOSS: 1.1852620840072632
LOSS: 1.1851778030395508
LOSS: 1.1850945949554443
LOSS: 1.1850131750106812
LOSS: 1.1849344968795776
LOSS: 1.1848595142364502
LOSS: 1.184788703918457
LOSS: 1.184721827507019
LOSS: 1.184658408164978
LOSS: 1.1845978498458862
LOSS: 1.1845389604568481
LOSS: 1.1844810247421265
LOSS: 1.1844247579574585
LOSS: 1.1843700408935547
LOSS: 1.1843173503875732
LOSS: 1.1842671632766724
LOSS: 1.184219479560852
LOSS: 1.1841742992401123
LOSS: 1.1841306686401367
LOSS: 1.1840879917144775
LOSS: 1.1840471029281616
LOSS: 1.1840063333511353
LOSS: 1.183966875076294
LOSS: 1.1839289665222168
LOSS: 1.1838923692703247
LOSS: 1.1838568449020386
LOSS: 1.3057231903076172
LOSS: 1.2957597970962524
LOSS: 1.286278247833252
LOSS: 1.2773466110229492
LOSS: 1.2690315246582031
LOSS: 1.2613952159881592
LOSS: 1.254490613937378
LOSS: 1.248355746269226
LOSS: 1.2430078983306885
LOSS: 1.2384401559829712
LOSS: 1.234621286392212
LOSS: 1.231500506401062
LOSS: 1.2290124893188477
LOSS: 1.227083683013916
LOSS: 1.2256308794021606
LOSS: 1.2245618104934692
LOSS: 1.2237753868103027
LOSS: 1.207059621810913
LOSS: 1.2064653635025024
LOSS: 1.205844759941101
LOSS: 1.2051528692245483
LOSS: 1.2043715715408325
LOSS: 1.2035057544708252
LOSS: 1.2025786638259888
LOSS: 1.201623797416687
LOSS: 1.200679063796997
LOSS: 1.1997801065444946
LOSS: 1.19895601272583
LOSS: 1.1982265710830688
LOSS: 1.197601079940796
LOSS: 1.197079062461853
LOSS: 1.1966519355773926
LOSS: 1.1963039636611938
LOSS: 1.1960169076919556
LOSS: 1.195770502090454
LOSS: 1.1955456733703613
LOSS: 1.1953264474868774
LOSS: 1.1951000690460205
LOSS: 1.1948591470718384
LOSS: 1.1945998668670654
LOSS: 1.1943235397338867
LOSS: 1.1940332651138306
LOSS: 1.1937353610992432
LOSS: 1.1934369802474976
LOSS: 1.1931449174880981
LOSS: 1.1928656101226807
LOSS: 1.1926037073135376
LOSS: 1.192362666130066
LOSS: 1.192143440246582
LOSS: 1.1919453144073486
LOSS: 1.1917667388916016
LOSS: 1.1916043758392334
LOSS: 1.1914559602737427
LOSS: 1.1913176774978638
LOSS: 1.1911876201629639
LOSS: 1.1910632848739624
LOSS: 1.1909444332122803
LOSS: 1.1908299922943115
LOSS: 1.190719723701477
LOSS: 1.1906139850616455
LOSS: 1.1905128955841064
LOSS: 1.1904152631759644
LOSS: 1.190321683883667
LOSS: 1.1902308464050293
LOSS: 1.190142035484314
LOSS: 1.1900547742843628
LOSS: 1.1899690628051758
LOSS: 1.1898844242095947
LOSS: 1.1898019313812256
LOSS: 1.1897213459014893
LOSS: 1.1896438598632812
LOSS: 1.1895694732666016
LOSS: 1.1894986629486084
LOSS: 1.1894315481185913
LOSS: 1.189367413520813
LOSS: 1.189306378364563
LOSS: 1.1892476081848145
LOSS: 1.1891906261444092
LOSS: 1.1891353130340576
LOSS: 1.1890811920166016
LOSS: 1.189028024673462
LOSS: 1.1889760494232178
LOSS: 1.1889253854751587
LOSS: 1.188875675201416
LOSS: 1.1888279914855957
LOSS: 1.1887818574905396
LOSS: 1.188737392425537
LOSS: 1.188694953918457
LOSS: 1.188653826713562
LOSS: 1.1886141300201416
LOSS: 1.1885755062103271
LOSS: 1.188537836074829
LOSS: 1.1885008811950684
LOSS: 1.188464879989624
LOSS: 1.1884292364120483
LOSS: 1.1883941888809204
LOSS: 1.1883599758148193
LOSS: 1.1883265972137451
LOSS: 1.293160319328308
LOSS: 1.2828575372695923
LOSS: 1.272998571395874
LOSS: 1.2636468410491943
LOSS: 1.2548645734786987
LOSS: 1.2467093467712402
LOSS: 1.2392282485961914
LOSS: 1.2324525117874146
LOSS: 1.2263916730880737
LOSS: 1.221031665802002
LOSS: 1.216338872909546
LOSS: 1.2122691869735718
LOSS: 1.208776593208313
LOSS: 1.2058173418045044
LOSS: 1.2033478021621704
LOSS: 1.2013189792633057
LOSS: 1.1996737718582153
LOSS: 1.1983451843261719
LOSS: 1.1972581148147583
LOSS: 1.1963353157043457
LOSS: 1.1955041885375977
LOSS: 1.194704294204712
LOSS: 1.1938929557800293
LOSS: 1.1930474042892456
LOSS: 1.1921656131744385
LOSS: 1.1912627220153809
LOSS: 1.1903645992279053
LOSS: 1.1895031929016113
LOSS: 1.188709020614624
LOSS: 1.1880061626434326
LOSS: 1.1874094009399414
LOSS: 1.1869219541549683
LOSS: 1.1865369081497192
LOSS: 1.1862387657165527
LOSS: 1.1860074996948242
LOSS: 1.1858221292495728
LOSS: 1.1856626272201538
LOSS: 1.1855130195617676
LOSS: 1.185361623764038
LOSS: 1.1852012872695923
LOSS: 1.1850296258926392
LOSS: 1.18484628200531
LOSS: 1.1846551895141602
LOSS: 1.1844593286514282
LOSS: 1.1842631101608276
LOSS: 1.1840709447860718
LOSS: 1.1838854551315308
LOSS: 1.1837092638015747
LOSS: 1.183543086051941
LOSS: 1.183387279510498
LOSS: 1.1832404136657715
LOSS: 1.1831018924713135
LOSS: 1.1829701662063599
LOSS: 1.1828436851501465
LOSS: 1.1827211380004883
LOSS: 1.1826024055480957
LOSS: 1.1824873685836792
LOSS: 1.1823761463165283
LOSS: 1.1822704076766968
LOSS: 1.1821705102920532
LOSS: 1.1820783615112305
LOSS: 1.1819945573806763
LOSS: 1.181920051574707
LOSS: 1.1818549633026123
LOSS: 1.1817985773086548
LOSS: 1.1817504167556763
LOSS: 1.1817090511322021
LOSS: 1.181673288345337
LOSS: 1.1816414594650269
LOSS: 1.1816117763519287
LOSS: 1.1815836429595947
LOSS: 1.1815561056137085
LOSS: 1.1815283298492432
LOSS: 1.1815004348754883
LOSS: 1.1814724206924438
LOSS: 1.181444764137268
LOSS: 1.1814175844192505
LOSS: 1.1813915967941284
LOSS: 1.1813669204711914
LOSS: 1.181343913078308
LOSS: 1.181322455406189
LOSS: 1.1813029050827026
LOSS: 1.181284785270691
LOSS: 1.1812680959701538
LOSS: 1.1812528371810913
LOSS: 1.1812385320663452
LOSS: 1.1812254190444946
LOSS: 1.1812126636505127
LOSS: 1.1812012195587158
LOSS: 1.1811904907226562
LOSS: 1.1811808347702026
LOSS: 1.1811721324920654
LOSS: 1.181164264678955
LOSS: 1.1811573505401611
LOSS: 1.1811513900756836
LOSS: 1.1811459064483643
LOSS: 1.1811412572860718
LOSS: 1.1811368465423584
LOSS: 1.1811331510543823
LOSS: 1.1811294555664062
LOSS: 1.2947858572006226
LOSS: 1.2840628623962402
LOSS: 1.273758053779602
LOSS: 1.2639329433441162
LOSS: 1.2546504735946655
LOSS: 1.2459698915481567
LOSS: 1.2379446029663086
LOSS: 1.2306163311004639
LOSS: 1.2240098714828491
LOSS: 1.2181289196014404
LOSS: 1.212956428527832
LOSS: 1.2084592580795288
LOSS: 1.2045965194702148
LOSS: 1.2013245820999146
LOSS: 1.198598027229309
LOSS: 1.1963673830032349
LOSS: 1.194574236869812
LOSS: 1.1931511163711548
LOSS: 1.1920216083526611
LOSS: 1.1911048889160156
LOSS: 1.190322756767273
LOSS: 1.1896064281463623
LOSS: 1.188903570175171
LOSS: 1.1881804466247559
LOSS: 1.1874250173568726
LOSS: 1.1866408586502075
LOSS: 1.1858443021774292
LOSS: 1.1850568056106567
LOSS: 1.184300422668457
LOSS: 1.183592677116394
LOSS: 1.1829434633255005
LOSS: 1.1823564767837524
LOSS: 1.1818289756774902
LOSS: 1.1813523769378662
LOSS: 1.180917501449585
LOSS: 1.1805135011672974
LOSS: 1.1801307201385498
LOSS: 1.1797606945037842
LOSS: 1.1793971061706543
LOSS: 1.179036259651184
LOSS: 1.1786766052246094
LOSS: 1.178318977355957
LOSS: 1.1779656410217285
LOSS: 1.1776198148727417
LOSS: 1.1772855520248413
LOSS: 1.1769660711288452
LOSS: 1.1766642332077026
LOSS: 1.1763817071914673
LOSS: 1.1761195659637451
LOSS: 1.175876498222351
LOSS: 1.1756515502929688
LOSS: 1.1754409074783325
LOSS: 1.1752424240112305
LOSS: 1.1750527620315552
LOSS: 1.1748690605163574
LOSS: 1.1746891736984253
LOSS: 1.1745116710662842
LOSS: 1.1743358373641968
LOSS: 1.174161672592163
LOSS: 1.1739904880523682
LOSS: 1.173823595046997
LOSS: 1.1736623048782349
LOSS: 1.1735082864761353
LOSS: 1.173363208770752
LOSS: 1.1732275485992432
LOSS: 1.1731013059616089
LOSS: 1.1729844808578491
LOSS: 1.1728758811950684
LOSS: 1.172774076461792
LOSS: 1.1726778745651245
LOSS: 1.1725860834121704
LOSS: 1.172497272491455
LOSS: 1.1724103689193726
LOSS: 1.1723254919052124
LOSS: 1.1722419261932373
LOSS: 1.1721605062484741
LOSS: 1.172080636024475
LOSS: 1.172003149986267
LOSS: 1.1719285249710083
LOSS: 1.1718568801879883
LOSS: 1.171788215637207
LOSS: 1.1717222929000854
LOSS: 1.171658992767334
LOSS: 1.171598196029663
LOSS: 1.171539306640625
LOSS: 1.1714822053909302
LOSS: 1.171426773071289
LOSS: 1.171372652053833
LOSS: 1.171319842338562
LOSS: 1.1712679862976074
LOSS: 1.171217679977417
LOSS: 1.1711686849594116
LOSS: 1.1711210012435913
LOSS: 1.1710748672485352
LOSS: 1.171030044555664
LOSS: 1.1709868907928467
LOSS: 1.1709450483322144
LOSS: 1.170904517173767
LOSS: 1.1708650588989258
LOSS: 1.1708263158798218
LOSS: 1.289784550666809
LOSS: 1.2793811559677124
LOSS: 1.2694400548934937
LOSS: 1.260021448135376
LOSS: 1.25118088722229
LOSS: 1.242963194847107
LOSS: 1.2353971004486084
LOSS: 1.2284964323043823
LOSS: 1.222264289855957
LOSS: 1.216693639755249
LOSS: 1.2117657661437988
LOSS: 1.207450032234192
LOSS: 1.2037094831466675
LOSS: 1.2005045413970947
LOSS: 1.1977956295013428
LOSS: 1.1955409049987793
LOSS: 1.1936947107315063
LOSS: 1.1922050714492798
LOSS: 1.191014051437378
LOSS: 1.190058708190918
LOSS: 1.1892729997634888
LOSS: 1.188591480255127
LOSS: 1.187953233718872
LOSS: 1.1873100996017456
LOSS: 1.1866319179534912
LOSS: 1.185907006263733
LOSS: 1.185139775276184
LOSS: 1.1843442916870117
LOSS: 1.183542251586914
LOSS: 1.1827569007873535
LOSS: 1.1820098161697388
LOSS: 1.1813206672668457
LOSS: 1.1807032823562622
LOSS: 1.180167317390442
LOSS: 1.179716944694519
LOSS: 1.1793510913848877
LOSS: 1.179064393043518
LOSS: 1.178848147392273
LOSS: 1.1786904335021973
LOSS: 1.1785781383514404
LOSS: 1.1784974336624146
LOSS: 1.178436279296875
LOSS: 1.1783827543258667
LOSS: 1.1783287525177002
LOSS: 1.1782678365707397
LOSS: 1.1781965494155884
LOSS: 1.1781134605407715
LOSS: 1.1780195236206055
LOSS: 1.177917242050171
LOSS: 1.1778091192245483
LOSS: 1.1931511163711548
LOSS: 1.1920216083526611
LOSS: 1.1911048889160156
LOSS: 1.190322756767273
LOSS: 1.1896064281463623
LOSS: 1.188903570175171
LOSS: 1.1881804466247559
LOSS: 1.1874250173568726
LOSS: 1.1866408586502075
LOSS: 1.1858443021774292
LOSS: 1.1850568056106567
LOSS: 1.184300422668457
LOSS: 1.183592677116394
LOSS: 1.1829434633255005
LOSS: 1.1823564767837524
LOSS: 1.1818289756774902
LOSS: 1.1813523769378662
LOSS: 1.180917501449585
LOSS: 1.1805135011672974
LOSS: 1.1801307201385498
LOSS: 1.1797606945037842
LOSS: 1.1793971061706543
LOSS: 1.179036259651184
LOSS: 1.1786766052246094
LOSS: 1.178318977355957
LOSS: 1.1779656410217285
LOSS: 1.1776198148727417
LOSS: 1.1772855520248413
LOSS: 1.1769660711288452
LOSS: 1.1766642332077026
LOSS: 1.1763817071914673
LOSS: 1.1761195659637451
LOSS: 1.175876498222351
LOSS: 1.1756515502929688
LOSS: 1.1754409074783325
LOSS: 1.1752424240112305
LOSS: 1.1750527620315552
LOSS: 1.1748690605163574
LOSS: 1.1746891736984253
LOSS: 1.1745116710662842
LOSS: 1.1743358373641968
LOSS: 1.174161672592163
LOSS: 1.1739904880523682
LOSS: 1.173823595046997
LOSS: 1.1736623048782349
LOSS: 1.1735082864761353
LOSS: 1.173363208770752
LOSS: 1.1732275485992432
LOSS: 1.1731013059616089
LOSS: 1.1729844808578491
LOSS: 1.1728758811950684
LOSS: 1.172774076461792
LOSS: 1.1726778745651245
LOSS: 1.1725860834121704
LOSS: 1.172497272491455
LOSS: 1.1724103689193726
LOSS: 1.1723254919052124
LOSS: 1.1722419261932373
LOSS: 1.1721605062484741
LOSS: 1.172080636024475
LOSS: 1.172003149986267
LOSS: 1.1719285249710083
LOSS: 1.1718568801879883
LOSS: 1.171788215637207
LOSS: 1.1717222929000854
LOSS: 1.171658992767334
LOSS: 1.171598196029663
LOSS: 1.171539306640625
LOSS: 1.1714822053909302
LOSS: 1.171426773071289
LOSS: 1.171372652053833
LOSS: 1.171319842338562
LOSS: 1.1712679862976074
LOSS: 1.171217679977417
LOSS: 1.1711686849594116
LOSS: 1.1711210012435913
LOSS: 1.1710748672485352
LOSS: 1.171030044555664
LOSS: 1.1709868907928467
LOSS: 1.1709450483322144
LOSS: 1.170904517173767
LOSS: 1.1708650588989258
LOSS: 1.1708263158798218
LOSS: 1.293160319328308
LOSS: 1.2828575372695923
LOSS: 1.272998571395874
LOSS: 1.2636468410491943
LOSS: 1.2548645734786987
LOSS: 1.2467093467712402
LOSS: 1.2392282485961914
LOSS: 1.2324525117874146
LOSS: 1.2263916730880737
LOSS: 1.221031665802002
LOSS: 1.216338872909546
LOSS: 1.2122691869735718
LOSS: 1.208776593208313
LOSS: 1.2058173418045044
LOSS: 1.2033478021621704
LOSS: 1.2013189792633057
LOSS: 1.1996737718582153
LOSS: 1.1983451843261719
LOSS: 1.1972581148147583
LOSS: 1.1963353157043457
LOSS: 1.1955041885375977
LOSS: 1.194704294204712
LOSS: 1.1938929557800293
LOSS: 1.1930474042892456
LOSS: 1.1921656131744385
LOSS: 1.1912627220153809
LOSS: 1.1903645992279053
LOSS: 1.1895031929016113
LOSS: 1.188709020614624
LOSS: 1.1880061626434326
LOSS: 1.1874094009399414
LOSS: 1.1869219541549683
LOSS: 1.1865369081497192
LOSS: 1.1862387657165527
LOSS: 1.1860074996948242
LOSS: 1.1858221292495728
LOSS: 1.1856626272201538
LOSS: 1.1855130195617676
LOSS: 1.185361623764038
LOSS: 1.1852012872695923
LOSS: 1.1850296258926392
LOSS: 1.18484628200531
LOSS: 1.1846551895141602
LOSS: 1.1844593286514282
LOSS: 1.1842631101608276
LOSS: 1.1840709447860718
LOSS: 1.1838854551315308
LOSS: 1.1837092638015747
LOSS: 1.183543086051941
LOSS: 1.183387279510498
LOSS: 1.1832404136657715
LOSS: 1.1831018924713135
LOSS: 1.1829701662063599
LOSS: 1.1828436851501465
LOSS: 1.1827211380004883
LOSS: 1.1826024055480957
LOSS: 1.1824873685836792
LOSS: 1.1823761463165283
LOSS: 1.1822704076766968
LOSS: 1.1821705102920532
LOSS: 1.1820783615112305
LOSS: 1.1819945573806763
LOSS: 1.181920051574707
LOSS: 1.1818549633026123
LOSS: 1.1817985773086548
LOSS: 1.1817504167556763
LOSS: 1.1817090511322021
LOSS: 1.181673288345337
LOSS: 1.1816414594650269
LOSS: 1.1816117763519287
LOSS: 1.1815836429595947
LOSS: 1.1815561056137085
LOSS: 1.1815283298492432
LOSS: 1.1815004348754883
LOSS: 1.1814724206924438
LOSS: 1.181444764137268
LOSS: 1.1814175844192505
LOSS: 1.1813915967941284
LOSS: 1.1813669204711914
LOSS: 1.181343913078308
LOSS: 1.181322455406189
LOSS: 1.1813029050827026
LOSS: 1.181284785270691
LOSS: 1.1812680959701538
LOSS: 1.1812528371810913
LOSS: 1.1812385320663452
LOSS: 1.1812254190444946
LOSS: 1.1812126636505127
LOSS: 1.1812012195587158
LOSS: 1.1811904907226562
LOSS: 1.1811808347702026
LOSS: 1.1811721324920654
LOSS: 1.181164264678955
LOSS: 1.1811573505401611
LOSS: 1.1811513900756836
LOSS: 1.1811459064483643
LOSS: 1.1811412572860718
LOSS: 1.1811368465423584
LOSS: 1.1811331510543823
LOSS: 1.1811294555664062
LOSS: 1.2991918325424194
LOSS: 1.2888065576553345
LOSS: 1.2788574695587158
LOSS: 1.2694035768508911
LOSS: 1.2605046033859253
LOSS: 1.2522164583206177
LOSS: 1.2445886135101318
LOSS: 1.2376598119735718
LOSS: 1.2314516305923462
LOSS: 1.2259645462036133
LOSS: 1.221176028251648
LOSS: 1.2170429229736328
LOSS: 1.2135059833526611
LOSS: 1.2104984521865845
LOSS: 1.2079495191574097
LOSS: 1.2057864665985107
LOSS: 1.203933596611023
LOSS: 1.202314853668213
LOSS: 1.2008552551269531
LOSS: 1.1994872093200684
LOSS: 1.1981561183929443
LOSS: 1.1968238353729248
LOSS: 1.1954728364944458
LOSS: 1.1941038370132446
LOSS: 1.192733883857727
LOSS: 1.1913901567459106
LOSS: 1.1901048421859741
LOSS: 1.188909888267517
LOSS: 1.187830924987793
LOSS: 1.1868853569030762
LOSS: 1.186079740524292
LOSS: 1.185409665107727
LOSS: 1.1848615407943726
LOSS: 1.184415340423584
LOSS: 1.1840484142303467
LOSS: 1.1837377548217773
LOSS: 1.1834629774093628
LOSS: 1.1832079887390137
LOSS: 1.182960867881775
LOSS: 1.182714581489563
LOSS: 1.1824665069580078
LOSS: 1.182215929031372
LOSS: 1.1819655895233154
LOSS: 1.1817190647125244
LOSS: 1.1814812421798706
LOSS: 1.18125581741333
LOSS: 1.1810463666915894
LOSS: 1.180856466293335
LOSS: 1.1806869506835938
LOSS: 1.1805381774902344
LOSS: 1.1804090738296509
LOSS: 1.1802979707717896
LOSS: 1.180201768875122
LOSS: 1.1801174879074097
LOSS: 1.1800415515899658
LOSS: 1.1799713373184204
LOSS: 1.1799039840698242
LOSS: 1.1798384189605713
LOSS: 1.1797723770141602
LOSS: 1.1797059774398804
LOSS: 1.1796388626098633
LOSS: 1.1795716285705566
LOSS: 1.1795047521591187
LOSS: 1.179438829421997
LOSS: 1.1793745756149292
LOSS: 1.1793118715286255
LOSS: 1.1792511940002441
LOSS: 1.179192066192627
LOSS: 1.1791346073150635
LOSS: 1.179078221321106
LOSS: 1.1790225505828857
LOSS: 1.1789671182632446
LOSS: 1.178911805152893
LOSS: 1.1788564920425415
LOSS: 1.1788015365600586
LOSS: 1.1787463426589966
LOSS: 1.178692102432251
LOSS: 1.1786388158798218
LOSS: 1.1785871982574463
LOSS: 1.1785368919372559
LOSS: 1.1784889698028564
LOSS: 1.1784429550170898
LOSS: 1.1783992052078247
LOSS: 1.178357720375061
LOSS: 1.1783182621002197
LOSS: 1.178280234336853
LOSS: 1.1782439947128296
LOSS: 1.1782089471817017
LOSS: 1.178174614906311
LOSS: 1.1781413555145264
LOSS: 1.1781082153320312
LOSS: 1.178075909614563
LOSS: 1.1780433654785156
LOSS: 1.1780110597610474
LOSS: 1.1779788732528687
LOSS: 1.177946925163269
LOSS: 1.1779148578643799
LOSS: 1.1778826713562012
LOSS: 1.1778508424758911
LOSS: 1.177818775177002
LOSS: 1.2991918325424194
LOSS: 1.2888065576553345
LOSS: 1.2788574695587158
LOSS: 1.2694035768508911
LOSS: 1.2605046033859253
LOSS: 1.2522164583206177
LOSS: 1.2445886135101318
LOSS: 1.2376598119735718
LOSS: 1.2314516305923462
LOSS: 1.2259645462036133
LOSS: 1.221176028251648
LOSS: 1.2170429229736328
LOSS: 1.2135059833526611
LOSS: 1.2104984521865845
LOSS: 1.2079495191574097
LOSS: 1.2057864665985107
LOSS: 1.203933596611023
LOSS: 1.202314853668213
LOSS: 1.2008552551269531
LOSS: 1.1994872093200684
LOSS: 1.1981561183929443
LOSS: 1.1968238353729248
LOSS: 1.1954728364944458
LOSS: 1.1941038370132446
LOSS: 1.192733883857727
LOSS: 1.1913901567459106
LOSS: 1.1901048421859741
LOSS: 1.188909888267517
LOSS: 1.187830924987793
LOSS: 1.1868853569030762
LOSS: 1.186079740524292
LOSS: 1.185409665107727
LOSS: 1.1848615407943726
LOSS: 1.184415340423584
LOSS: 1.1840484142303467
LOSS: 1.1837377548217773
LOSS: 1.1834629774093628
LOSS: 1.1832079887390137
LOSS: 1.182960867881775
LOSS: 1.182714581489563
LOSS: 1.1824665069580078
LOSS: 1.182215929031372
LOSS: 1.1819655895233154
LOSS: 1.1817190647125244
LOSS: 1.1814812421798706
LOSS: 1.18125581741333
LOSS: 1.1810463666915894
LOSS: 1.180856466293335
LOSS: 1.1528860330581665
LOSS: 1.1526075601577759
LOSS: 1.1523199081420898
LOSS: 1.1520284414291382
LOSS: 1.1517397165298462
LOSS: 1.1514586210250854
LOSS: 1.1511887311935425
LOSS: 1.1509335041046143
LOSS: 1.1506941318511963
LOSS: 1.1504697799682617
LOSS: 1.1502591371536255
LOSS: 1.150059700012207
LOSS: 1.1498682498931885
LOSS: 1.1496814489364624
LOSS: 1.1494966745376587
LOSS: 1.1493103504180908
LOSS: 1.149121642112732
LOSS: 1.1489291191101074
LOSS: 1.1487324237823486
LOSS: 1.1485321521759033
LOSS: 1.1483299732208252
LOSS: 1.148126482963562
LOSS: 1.147923469543457
LOSS: 1.1477223634719849
LOSS: 1.1475245952606201
LOSS: 1.147330641746521
LOSS: 1.1471413373947144
LOSS: 1.1469563245773315
LOSS: 1.1467759609222412
LOSS: 1.1465996503829956
LOSS: 1.1464266777038574
LOSS: 1.1462568044662476
LOSS: 1.1460895538330078
LOSS: 1.1459239721298218
LOSS: 1.145760178565979
LOSS: 1.14559805393219
LOSS: 1.1454373598098755
LOSS: 1.1452783346176147
LOSS: 1.1451213359832764
LOSS: 1.14496648311615
LOSS: 1.144813895225525
LOSS: 1.144663691520691
LOSS: 1.1445165872573853
LOSS: 1.1443718671798706
LOSS: 1.144229769706726
LOSS: 1.1440904140472412
LOSS: 1.1439539194107056
LOSS: 1.1438194513320923
LOSS: 1.14368736743927
LOSS: 1.1435575485229492
LOSS: 1.1434298753738403
LOSS: 1.143304467201233
LOSS: 1.1431810855865479
LOSS: 1.1430600881576538
LOSS: 1.295655369758606
LOSS: 1.2850128412246704
LOSS: 1.2748113870620728
LOSS: 1.2651125192642212
LOSS: 1.2559771537780762
LOSS: 1.2474638223648071
LOSS: 1.2396256923675537
LOSS: 1.2325067520141602
LOSS: 1.2261378765106201
LOSS: 1.220532774925232
LOSS: 1.2156842947006226
LOSS: 1.211565375328064
LOSS: 1.2081282138824463
LOSS: 1.2053091526031494
LOSS: 1.2030316591262817
LOSS: 1.2012083530426025
LOSS: 1.1997443437576294
LOSS: 1.1985405683517456
LOSS: 1.1974999904632568
LOSS: 1.1965348720550537
LOSS: 1.1955746412277222
LOSS: 1.194571614265442
LOSS: 1.19350266456604
LOSS: 1.192367672920227
LOSS: 1.1911848783493042
LOSS: 1.1899844408035278
LOSS: 1.1888011693954468
LOSS: 1.1876691579818726
LOSS: 1.1866170167922974
LOSS: 1.1856648921966553
LOSS: 1.1848236322402954
LOSS: 1.184094786643982
LOSS: 1.1834717988967896
LOSS: 1.1829421520233154
LOSS: 1.1824897527694702
LOSS: 1.1820963621139526
LOSS: 1.18174409866333
LOSS: 1.1814171075820923
LOSS: 1.18110191822052
LOSS: 1.1807889938354492
LOSS: 1.1804721355438232
LOSS: 1.1801488399505615
LOSS: 1.1798200607299805
LOSS: 1.1794884204864502
LOSS: 1.179159164428711
LOSS: 1.1788378953933716
LOSS: 1.1785309314727783
LOSS: 1.1782435178756714
LOSS: 1.177980661392212
LOSS: 1.177744746208191
LOSS: 1.1775373220443726
LOSS: 1.177357792854309
LOSS: 1.1772041320800781
LOSS: 1.1770732402801514
LOSS: 1.1769604682922363
LOSS: 1.1768617630004883
LOSS: 1.1767722368240356
LOSS: 1.1766880750656128
LOSS: 1.1766060590744019
LOSS: 1.1765234470367432
LOSS: 1.1764394044876099
LOSS: 1.1763532161712646
LOSS: 1.176265001296997
LOSS: 1.1761753559112549
LOSS: 1.1760854721069336
LOSS: 1.1759966611862183
LOSS: 1.1759096384048462
LOSS: 1.1758249998092651
LOSS: 1.1757433414459229
LOSS: 1.1756649017333984
LOSS: 1.1755889654159546
LOSS: 1.175516128540039
LOSS: 1.175445318222046
LOSS: 1.175376296043396
LOSS: 1.1753088235855103
LOSS: 1.1752426624298096
LOSS: 1.1751774549484253
LOSS: 1.1751136779785156
LOSS: 1.1750510931015015
LOSS: 1.1749902963638306
LOSS: 1.174931287765503
LOSS: 1.1748743057250977
LOSS: 1.174819827079773
LOSS: 1.1747673749923706
LOSS: 1.174717664718628
LOSS: 1.1746701002120972
LOSS: 1.1746245622634888
LOSS: 1.1745808124542236
LOSS: 1.1745387315750122
LOSS: 1.1744976043701172
LOSS: 1.1744575500488281
LOSS: 1.1744182109832764
LOSS: 1.1743793487548828
LOSS: 1.174340844154358
LOSS: 1.1743024587631226
LOSS: 1.1742644309997559
LOSS: 1.1742265224456787
LOSS: 1.1741890907287598
LOSS: 1.1741520166397095
LOSS: 1.1741153001785278
LOSS: 1.2882088422775269
LOSS: 1.276635766029358
LOSS: 1.2653884887695312
LOSS: 1.2545208930969238
LOSS: 1.2440868616104126
LOSS: 1.2341384887695312
LOSS: 1.2247214317321777
LOSS: 1.2158722877502441
LOSS: 1.207613468170166
LOSS: 1.199950933456421
LOSS: 1.1928775310516357
LOSS: 1.1863796710968018
LOSS: 1.1804463863372803
LOSS: 1.1750752925872803
LOSS: 1.1702710390090942
LOSS: 1.1660397052764893
LOSS: 1.162380576133728
LOSS: 1.1592788696289062
LOSS: 1.1567007303237915
LOSS: 1.1545913219451904
LOSS: 1.152877926826477
LOSS: 1.1514744758605957
LOSS: 1.1502909660339355
LOSS: 1.149242877960205
LOSS: 1.148258090019226
LOSS: 1.1472854614257812
LOSS: 1.1462939977645874
LOSS: 1.1452746391296387
LOSS: 1.1442346572875977
LOSS: 1.1431931257247925
LOSS: 1.1421754360198975
LOSS: 1.1412080526351929
LOSS: 1.1403135061264038
LOSS: 1.139508605003357
LOSS: 1.1388016939163208
LOSS: 1.1381937265396118
LOSS: 1.1376785039901733
LOSS: 1.137243628501892
LOSS: 1.1368736028671265
LOSS: 1.136550784111023
LOSS: 1.136257290840149
LOSS: 1.1359771490097046
LOSS: 1.1356967687606812
LOSS: 1.1354053020477295
LOSS: 1.1350959539413452
LOSS: 1.1347649097442627
LOSS: 1.1344108581542969
LOSS: 1.134035348892212
LOSS: 1.1336406469345093
LOSS: 1.1332306861877441
LOSS: 1.1328086853027344
LOSS: 1.1323765516281128
LOSS: 1.1319365501403809
LOSS: 1.1314880847930908
LOSS: 1.1310302019119263
LOSS: 1.1305595636367798
LOSS: 1.1300733089447021
LOSS: 1.1295688152313232
LOSS: 1.1290442943572998
LOSS: 1.1285003423690796
LOSS: 1.12794029712677
LOSS: 1.1273702383041382
LOSS: 1.1267971992492676
LOSS: 1.126228928565979
LOSS: 1.1256717443466187
LOSS: 1.1251282691955566
LOSS: 1.1245993375778198
LOSS: 1.1240819692611694
LOSS: 1.1235729455947876
LOSS: 1.1230697631835938
LOSS: 1.1225719451904297
LOSS: 1.1220821142196655
LOSS: 1.1216048002243042
LOSS: 1.1211462020874023
LOSS: 1.1207126379013062
LOSS: 1.120309591293335
LOSS: 1.1199394464492798
LOSS: 1.1196019649505615
LOSS: 1.1192948818206787
LOSS: 1.119011640548706
LOSS: 1.1187458038330078
LOSS: 1.11849045753479
LOSS: 1.1182392835617065
LOSS: 1.1179865598678589
LOSS: 1.1177301406860352
LOSS: 1.1174674034118652
LOSS: 1.1171975135803223
LOSS: 1.1169190406799316
LOSS: 1.1166318655014038
LOSS: 1.1163336038589478
LOSS: 1.1160225868225098
LOSS: 1.1156991720199585
LOSS: 1.115363597869873
LOSS: 1.1150215864181519
LOSS: 1.1146819591522217
LOSS: 1.1143567562103271
LOSS: 1.1140588521957397
LOSS: 1.1137980222702026
LOSS: 1.1135776042938232
LOSS: 1.1133986711502075
LOSS: 1.289784550666809
LOSS: 1.2793811559677124
LOSS: 1.2694400548934937
LOSS: 1.260021448135376
LOSS: 1.25118088722229
LOSS: 1.242963194847107
LOSS: 1.2353971004486084
LOSS: 1.2284964323043823
LOSS: 1.222264289855957
LOSS: 1.216693639755249
LOSS: 1.2117657661437988
LOSS: 1.207450032234192
LOSS: 1.2037094831466675
LOSS: 1.2005045413970947
LOSS: 1.1977956295013428
LOSS: 1.1955409049987793
LOSS: 1.1936947107315063
LOSS: 1.1922050714492798
LOSS: 1.191014051437378
LOSS: 1.190058708190918
LOSS: 1.1892729997634888
LOSS: 1.188591480255127
LOSS: 1.187953233718872
LOSS: 1.1873100996017456
LOSS: 1.1866319179534912
LOSS: 1.185907006263733
LOSS: 1.185139775276184
LOSS: 1.1843442916870117
LOSS: 1.183542251586914
LOSS: 1.1827569007873535
LOSS: 1.1820098161697388
LOSS: 1.1813206672668457
LOSS: 1.1807032823562622
LOSS: 1.180167317390442
LOSS: 1.179716944694519
LOSS: 1.1793510913848877
LOSS: 1.179064393043518
LOSS: 1.178848147392273
LOSS: 1.1786904335021973
LOSS: 1.1785781383514404
LOSS: 1.1784974336624146
LOSS: 1.178436279296875
LOSS: 1.1783827543258667
LOSS: 1.1783287525177002
LOSS: 1.1782678365707397
LOSS: 1.1781965494155884
LOSS: 1.1781134605407715
LOSS: 1.1780195236206055
LOSS: 1.177917242050171
LOSS: 1.1778091192245483
LOSS: 1.1776989698410034
LOSS: 1.1775898933410645
LOSS: 1.1774848699569702
LOSS: 1.1773860454559326
LOSS: 1.1772948503494263
LOSS: 1.1772116422653198
LOSS: 1.177135944366455
LOSS: 1.1770663261413574
LOSS: 1.177001714706421
LOSS: 1.1769403219223022
LOSS: 1.1768802404403687
LOSS: 1.176820993423462
LOSS: 1.1767621040344238
LOSS: 1.1767034530639648
LOSS: 1.176646113395691
LOSS: 1.1765908002853394
LOSS: 1.1765389442443848
LOSS: 1.176491618156433
LOSS: 1.1764492988586426
LOSS: 1.176412582397461
LOSS: 1.176381230354309
LOSS: 1.1763548851013184
LOSS: 1.176332712173462
LOSS: 1.1763139963150024
LOSS: 1.1762971878051758
LOSS: 1.176281452178955
LOSS: 1.176265835762024
LOSS: 1.2231673002243042
LOSS: 1.22263765335083
LOSS: 1.2221014499664307
LOSS: 1.2214967012405396
LOSS: 1.2207897901535034
LOSS: 1.2199736833572388
LOSS: 1.2190651893615723
LOSS: 1.2180954217910767
LOSS: 1.2171050310134888
LOSS: 1.2161338329315186
LOSS: 1.2152169942855835
LOSS: 1.214379906654358
LOSS: 1.2136378288269043
LOSS: 1.2129932641983032
LOSS: 1.2124398946762085
LOSS: 1.211962342262268
LOSS: 1.211540699005127
LOSS: 1.2111538648605347
LOSS: 1.2107799053192139
LOSS: 1.210401177406311
LOSS: 1.2100045680999756
LOSS: 1.2095823287963867
LOSS: 1.2091317176818848
LOSS: 1.2086557149887085
LOSS: 1.2081612348556519
LOSS: 1.207656979560852
LOSS: 1.207154393196106
LOSS: 1.2066646814346313
LOSS: 1.2061978578567505
LOSS: 1.2057626247406006
LOSS: 1.2053649425506592
LOSS: 1.2050081491470337
LOSS: 1.204693078994751
LOSS: 1.2044181823730469
LOSS: 1.2041798830032349
LOSS: 1.2039731740951538
LOSS: 1.203792691230774
LOSS: 1.2036330699920654
LOSS: 1.203488826751709
LOSS: 1.2033559083938599
LOSS: 1.2032309770584106
LOSS: 1.2031118869781494
LOSS: 1.2029973268508911
LOSS: 1.202886939048767
LOSS: 1.2027809619903564
LOSS: 1.2026795148849487
LOSS: 1.2025833129882812
LOSS: 1.2024927139282227
LOSS: 1.2024074792861938
LOSS: 1.2023274898529053
LOSS: 1.202252745628357
LOSS: 1.2021819353103638
LOSS: 1.2021149396896362
LOSS: 1.2020504474639893
LOSS: 1.2019883394241333
LOSS: 1.2019277811050415
LOSS: 1.2018687725067139
LOSS: 1.2018109560012817
LOSS: 1.201754093170166
LOSS: 1.201698660850525
LOSS: 1.2016443014144897
LOSS: 1.2015916109085083
LOSS: 1.201540470123291
LOSS: 1.201490879058838
LOSS: 1.2014425992965698
LOSS: 1.2013957500457764
LOSS: 1.2013497352600098
LOSS: 1.2013049125671387
LOSS: 1.2012603282928467
LOSS: 1.2012161016464233
LOSS: 1.2011719942092896
LOSS: 1.2011281251907349
LOSS: 1.2010842561721802
LOSS: 1.2010406255722046
LOSS: 1.2009971141815186
LOSS: 1.2009539604187012
LOSS: 1.2009114027023315
LOSS: 1.2008696794509888
LOSS: 1.2008286714553833
LOSS: 1.2007883787155151
LOSS: 1.2007490396499634
LOSS: 1.2007105350494385
LOSS: 1.200673222541809
LOSS: 1.2874476909637451
LOSS: 1.2763926982879639
LOSS: 1.2657418251037598
LOSS: 1.255556344985962
LOSS: 1.2458981275558472
LOSS: 1.2368260622024536
LOSS: 1.228393793106079
LOSS: 1.2206438779830933
LOSS: 1.2136025428771973
LOSS: 1.2072784900665283
LOSS: 1.2016609907150269
LOSS: 1.1967260837554932
LOSS: 1.19244384765625
LOSS: 1.1887820959091187
LOSS: 1.1857080459594727
LOSS: 1.1831824779510498
LOSS: 1.1811552047729492
LOSS: 1.179560899734497
LOSS: 1.178317904472351
LOSS: 1.1773343086242676
LOSS: 1.176512360572815
LOSS: 1.1757607460021973
LOSS: 1.175003170967102
LOSS: 1.174184799194336
LOSS: 1.173275113105774
LOSS: 1.1722683906555176
LOSS: 1.1711804866790771
LOSS: 1.1700403690338135
LOSS: 1.1688868999481201
LOSS: 1.167759656906128
LOSS: 1.1666953563690186
LOSS: 1.1657238006591797
LOSS: 1.1648646593093872
LOSS: 1.1641274690628052
LOSS: 1.1635106801986694
LOSS: 1.1630043983459473
LOSS: 1.1625908613204956
LOSS: 1.162247657775879
LOSS: 1.161950945854187
LOSS: 1.161677360534668
LOSS: 1.161406397819519
LOSS: 1.1611231565475464
LOSS: 1.1608173847198486
LOSS: 1.1604865789413452
LOSS: 1.160132646560669
LOSS: 1.1597621440887451
LOSS: 1.1593846082687378
LOSS: 1.1590113639831543
LOSS: 1.1586525440216064
LOSS: 1.1583173274993896
LOSS: 1.1580113172531128
LOSS: 1.1577377319335938
LOSS: 1.1574963331222534
LOSS: 1.1572836637496948
LOSS: 1.1570947170257568
LOSS: 1.1569222211837769
LOSS: 1.1567599773406982
LOSS: 1.156602144241333
LOSS: 1.1564441919326782
LOSS: 1.1562831401824951
LOSS: 1.1561187505722046
LOSS: 1.1559520959854126
LOSS: 1.155785322189331
LOSS: 1.1556215286254883
LOSS: 1.1554640531539917
LOSS: 1.1553155183792114
LOSS: 1.1551774740219116
LOSS: 1.1550509929656982
LOSS: 1.1549352407455444
LOSS: 1.1548291444778442
LOSS: 1.1547304391860962
LOSS: 1.1546369791030884
LOSS: 1.1545467376708984
LOSS: 1.1544580459594727
LOSS: 1.1543703079223633
LOSS: 1.1542831659317017
LOSS: 1.1541972160339355
LOSS: 1.1541129350662231
LOSS: 1.15403151512146
LOSS: 1.1539535522460938
LOSS: 1.1538790464401245
LOSS: 1.1538084745407104
LOSS: 1.1537411212921143
LOSS: 1.1536768674850464
LOSS: 1.1536141633987427
LOSS: 1.1535528898239136
LOSS: 1.1534924507141113
LOSS: 1.153432846069336
LOSS: 1.1533737182617188
LOSS: 1.153315544128418
LOSS: 1.1532584428787231
LOSS: 1.153203010559082
LOSS: 1.1531493663787842
LOSS: 1.1530977487564087
LOSS: 1.1530479192733765
LOSS: 1.152999758720398
LOSS: 1.1529533863067627
LOSS: 1.152908444404602
LOSS: 1.1528643369674683
LOSS: 1.1528215408325195
LOSS: 1.2882088422775269
LOSS: 1.276635766029358
LOSS: 1.2653884887695312
LOSS: 1.2545208930969238
LOSS: 1.2440868616104126
LOSS: 1.2341384887695312
LOSS: 1.2247214317321777
LOSS: 1.2158722877502441
LOSS: 1.207613468170166
LOSS: 1.199950933456421
LOSS: 1.1928775310516357
LOSS: 1.1863796710968018
LOSS: 1.1804463863372803
LOSS: 1.1750752925872803
LOSS: 1.1702710390090942
LOSS: 1.1660397052764893
LOSS: 1.162380576133728
LOSS: 1.1592788696289062
LOSS: 1.1567007303237915
LOSS: 1.1545913219451904
LOSS: 1.152877926826477
LOSS: 1.1514744758605957
LOSS: 1.1502909660339355
LOSS: 1.149242877960205
LOSS: 1.148258090019226
LOSS: 1.1472854614257812
LOSS: 1.1462939977645874
LOSS: 1.1452746391296387
LOSS: 1.1442346572875977
LOSS: 1.1431931257247925
LOSS: 1.1421754360198975
LOSS: 1.1412080526351929
LOSS: 1.1403135061264038
LOSS: 1.139508605003357
LOSS: 1.1388016939163208
LOSS: 1.1381937265396118
LOSS: 1.1376785039901733
LOSS: 1.137243628501892
LOSS: 1.1368736028671265
LOSS: 1.136550784111023
LOSS: 1.136257290840149
LOSS: 1.1359771490097046
LOSS: 1.1356967687606812
LOSS: 1.1354053020477295
LOSS: 1.1350959539413452
LOSS: 1.1347649097442627
LOSS: 1.1344108581542969
LOSS: 1.134035348892212
LOSS: 1.1336406469345093
LOSS: 1.1332306861877441
LOSS: 1.1328086853027344
LOSS: 1.1323765516281128
LOSS: 1.1319365501403809
LOSS: 1.1314880847930908
LOSS: 1.1310302019119263
LOSS: 1.1305595636367798
LOSS: 1.1300733089447021
LOSS: 1.1295688152313232
LOSS: 1.1290442943572998
LOSS: 1.1285003423690796
LOSS: 1.12794029712677
LOSS: 1.1273702383041382
LOSS: 1.1267971992492676
LOSS: 1.126228928565979
LOSS: 1.1256717443466187
LOSS: 1.1251282691955566
LOSS: 1.1245993375778198
LOSS: 1.1240819692611694
LOSS: 1.1235729455947876
LOSS: 1.1230697631835938
LOSS: 1.1225719451904297
LOSS: 1.1220821142196655
LOSS: 1.1216048002243042
LOSS: 1.1211462020874023
LOSS: 1.1207126379013062
LOSS: 1.120309591293335
LOSS: 1.1199394464492798
LOSS: 1.1196019649505615
LOSS: 1.1192948818206787
LOSS: 1.119011640548706
LOSS: 1.1187458038330078
LOSS: 1.11849045753479
LOSS: 1.1182392835617065
LOSS: 1.1179865598678589
LOSS: 1.1177301406860352
LOSS: 1.1174674034118652
LOSS: 1.1171975135803223
LOSS: 1.1169190406799316
LOSS: 1.1166318655014038
LOSS: 1.1163336038589478
LOSS: 1.1160225868225098
LOSS: 1.1156991720199585
LOSS: 1.115363597869873
LOSS: 1.1150215864181519
LOSS: 1.1146819591522217
LOSS: 1.1143567562103271
LOSS: 1.1140588521957397
LOSS: 1.1137980222702026
LOSS: 1.1135776042938232
LOSS: 1.1133986711502075
LOSS: 1.2928510904312134
LOSS: 1.2824299335479736
LOSS: 1.2724214792251587
LOSS: 1.2628774642944336
LOSS: 1.2538490295410156
LOSS: 1.2453864812850952
LOSS: 1.2375353574752808
LOSS: 1.2303335666656494
LOSS: 1.2238059043884277
LOSS: 1.2179615497589111
LOSS: 1.2127914428710938
LOSS: 1.2082700729370117
LOSS: 1.2043598890304565
LOSS: 1.2010174989700317
LOSS: 1.1981946229934692
LOSS: 1.1958386898040771
LOSS: 1.1938892602920532
LOSS: 1.1922773122787476
LOSS: 1.190925121307373
LOSS: 1.1897517442703247
LOSS: 1.188678503036499
LOSS: 1.1876389980316162
LOSS: 1.1865841150283813
LOSS: 1.1854850053787231
LOSS: 1.184335708618164
LOSS: 1.1831482648849487
LOSS: 1.1819477081298828
LOSS: 1.1807669401168823
LOSS: 1.179640531539917
LOSS: 1.1785997152328491
LOSS: 1.1776684522628784
LOSS: 1.1768622398376465
LOSS: 1.1761860847473145
LOSS: 1.1756362915039062
LOSS: 1.1752007007598877
LOSS: 1.1748617887496948
LOSS: 1.174597144126892
LOSS: 1.1743847131729126
LOSS: 1.1742030382156372
LOSS: 1.1740331649780273
LOSS: 1.1738612651824951
LOSS: 1.1736781597137451
LOSS: 1.173479676246643
LOSS: 1.173266053199768
LOSS: 1.1730409860610962
LOSS: 1.1728099584579468
LOSS: 1.1725807189941406
LOSS: 1.1776989698410034
LOSS: 1.1775898933410645
LOSS: 1.1774848699569702
LOSS: 1.1773860454559326
LOSS: 1.1772948503494263
LOSS: 1.1772116422653198
LOSS: 1.177135944366455
LOSS: 1.1770663261413574
LOSS: 1.177001714706421
LOSS: 1.1769403219223022
LOSS: 1.1768802404403687
LOSS: 1.176820993423462
LOSS: 1.1767621040344238
LOSS: 1.1767034530639648
LOSS: 1.176646113395691
LOSS: 1.1765908002853394
LOSS: 1.1765389442443848
LOSS: 1.176491618156433
LOSS: 1.1764492988586426
LOSS: 1.176412582397461
LOSS: 1.176381230354309
LOSS: 1.1763548851013184
LOSS: 1.176332712173462
LOSS: 1.1763139963150024
LOSS: 1.1762971878051758
LOSS: 1.176281452178955
LOSS: 1.176265835762024
LOSS: 1.1762495040893555
LOSS: 1.1762322187423706
LOSS: 1.1762133836746216
LOSS: 1.1761937141418457
LOSS: 1.1761727333068848
LOSS: 1.1761512756347656
LOSS: 1.176129937171936
LOSS: 1.176108717918396
LOSS: 1.1760883331298828
LOSS: 1.176068902015686
LOSS: 1.1760504245758057
LOSS: 1.1760329008102417
LOSS: 1.1760166883468628
LOSS: 1.1760010719299316
LOSS: 1.1759861707687378
LOSS: 1.1759718656539917
LOSS: 1.1759579181671143
LOSS: 1.1759445667266846
LOSS: 1.1759313344955444
LOSS: 1.175918459892273
LOSS: 1.1759058237075806
LOSS: 1.1758935451507568
LOSS: 1.1758817434310913
LOSS: 1.3057231903076172
LOSS: 1.2957597970962524
LOSS: 1.286278247833252
LOSS: 1.2773466110229492
LOSS: 1.2690315246582031
LOSS: 1.2613952159881592
LOSS: 1.254490613937378
LOSS: 1.248355746269226
LOSS: 1.2430078983306885
LOSS: 1.2384401559829712
LOSS: 1.234621286392212
LOSS: 1.231500506401062
LOSS: 1.2290124893188477
LOSS: 1.227083683013916
LOSS: 1.2256308794021606
LOSS: 1.2245618104934692
LOSS: 1.2237753868103027
LOSS: 1.2231673002243042
LOSS: 1.22263765335083
LOSS: 1.2221014499664307
LOSS: 1.2214967012405396
LOSS: 1.2207897901535034
LOSS: 1.2199736833572388
LOSS: 1.2190651893615723
LOSS: 1.2180954217910767
LOSS: 1.2171050310134888
LOSS: 1.2161338329315186
LOSS: 1.2152169942855835
LOSS: 1.214379906654358
LOSS: 1.2136378288269043
LOSS: 1.2129932641983032
LOSS: 1.2124398946762085
LOSS: 1.211962342262268
LOSS: 1.211540699005127
LOSS: 1.2111538648605347
LOSS: 1.2107799053192139
LOSS: 1.210401177406311
LOSS: 1.2100045680999756
LOSS: 1.2095823287963867
LOSS: 1.2091317176818848
LOSS: 1.2086557149887085
LOSS: 1.2081612348556519
LOSS: 1.207656979560852
LOSS: 1.207154393196106
LOSS: 1.2066646814346313
LOSS: 1.2061978578567505
LOSS: 1.2057626247406006
LOSS: 1.2053649425506592
LOSS: 1.2050081491470337
LOSS: 1.204693078994751
LOSS: 1.2044181823730469
LOSS: 1.2041798830032349
LOSS: 1.2039731740951538
LOSS: 1.203792691230774
LOSS: 1.2036330699920654
LOSS: 1.203488826751709
LOSS: 1.2033559083938599
LOSS: 1.2032309770584106
LOSS: 1.2031118869781494
LOSS: 1.2029973268508911
LOSS: 1.202886939048767
LOSS: 1.2027809619903564
LOSS: 1.2026795148849487
LOSS: 1.2025833129882812
LOSS: 1.2024927139282227
LOSS: 1.2024074792861938
LOSS: 1.2023274898529053
LOSS: 1.202252745628357
LOSS: 1.2021819353103638
LOSS: 1.2021149396896362
LOSS: 1.2020504474639893
LOSS: 1.2019883394241333
LOSS: 1.2019277811050415
LOSS: 1.2018687725067139
LOSS: 1.2018109560012817
LOSS: 1.201754093170166
LOSS: 1.201698660850525
LOSS: 1.2016443014144897
LOSS: 1.2015916109085083
LOSS: 1.201540470123291
LOSS: 1.201490879058838
LOSS: 1.2014425992965698
LOSS: 1.2013957500457764
LOSS: 1.2013497352600098
LOSS: 1.2013049125671387
LOSS: 1.2012603282928467
LOSS: 1.2012161016464233
LOSS: 1.2011719942092896
LOSS: 1.2011281251907349
LOSS: 1.2010842561721802
LOSS: 1.2010406255722046
LOSS: 1.2009971141815186
LOSS: 1.2009539604187012
LOSS: 1.2009114027023315
LOSS: 1.2008696794509888
LOSS: 1.2008286714553833
LOSS: 1.2007883787155151
LOSS: 1.2007490396499634
LOSS: 1.2007105350494385
LOSS: 1.200673222541809
LOSS: 1.2874476909637451
LOSS: 1.2763926982879639
LOSS: 1.2657418251037598
LOSS: 1.255556344985962
LOSS: 1.2458981275558472
LOSS: 1.2368260622024536
LOSS: 1.228393793106079
LOSS: 1.2206438779830933
LOSS: 1.2136025428771973
LOSS: 1.2072784900665283
LOSS: 1.2016609907150269
LOSS: 1.1967260837554932
LOSS: 1.19244384765625
LOSS: 1.1887820959091187
LOSS: 1.1857080459594727
LOSS: 1.1831824779510498
LOSS: 1.1811552047729492
LOSS: 1.179560899734497
LOSS: 1.178317904472351
LOSS: 1.1773343086242676
LOSS: 1.176512360572815
LOSS: 1.1757607460021973
LOSS: 1.175003170967102
LOSS: 1.174184799194336
LOSS: 1.173275113105774
LOSS: 1.1722683906555176
LOSS: 1.1711804866790771
LOSS: 1.1700403690338135
LOSS: 1.1688868999481201
LOSS: 1.167759656906128
LOSS: 1.1666953563690186
LOSS: 1.1657238006591797
LOSS: 1.1648646593093872
LOSS: 1.1641274690628052
LOSS: 1.1635106801986694
LOSS: 1.1630043983459473
LOSS: 1.1625908613204956
LOSS: 1.162247657775879
LOSS: 1.161950945854187
LOSS: 1.161677360534668
LOSS: 1.161406397819519
LOSS: 1.1611231565475464
LOSS: 1.1608173847198486
LOSS: 1.1604865789413452
LOSS: 1.160132646560669
LOSS: 1.1597621440887451
LOSS: 1.1593846082687378
LOSS: 1.1590113639831543
LOSS: 1.1586525440216064
LOSS: 1.1583173274993896
LOSS: 1.1580113172531128
LOSS: 1.1577377319335938
LOSS: 1.1574963331222534
LOSS: 1.1572836637496948
LOSS: 1.1570947170257568
LOSS: 1.1569222211837769
LOSS: 1.1567599773406982
LOSS: 1.156602144241333
LOSS: 1.1564441919326782
LOSS: 1.1562831401824951
LOSS: 1.1561187505722046
LOSS: 1.1559520959854126
LOSS: 1.155785322189331
LOSS: 1.1556215286254883
LOSS: 1.1554640531539917
LOSS: 1.1553155183792114
LOSS: 1.1551774740219116
LOSS: 1.1550509929656982
LOSS: 1.1549352407455444
LOSS: 1.1548291444778442
LOSS: 1.1547304391860962
LOSS: 1.1546369791030884
LOSS: 1.1545467376708984
LOSS: 1.1544580459594727
LOSS: 1.1543703079223633
LOSS: 1.1542831659317017
LOSS: 1.1541972160339355
LOSS: 1.1541129350662231
LOSS: 1.15403151512146
LOSS: 1.1539535522460938
LOSS: 1.1538790464401245
LOSS: 1.1538084745407104
LOSS: 1.1537411212921143
LOSS: 1.1536768674850464
LOSS: 1.1536141633987427
LOSS: 1.1535528898239136
LOSS: 1.1534924507141113
LOSS: 1.153432846069336
LOSS: 1.1533737182617188
LOSS: 1.153315544128418
LOSS: 1.1532584428787231
LOSS: 1.153203010559082
LOSS: 1.1531493663787842
LOSS: 1.1530977487564087
LOSS: 1.1530479192733765
LOSS: 1.152999758720398
LOSS: 1.1529533863067627
LOSS: 1.152908444404602
LOSS: 1.1528643369674683
LOSS: 1.1528215408325195
LOSS: 1.3104385137557983
LOSS: 1.3003697395324707
LOSS: 1.2907471656799316
LOSS: 1.2816346883773804
LOSS: 1.2730956077575684
LOSS: 1.2651901245117188
LOSS: 1.2579703330993652
LOSS: 1.25147545337677
LOSS: 1.2457243204116821
LOSS: 1.2407125234603882
LOSS: 1.2364130020141602
LOSS: 1.2327805757522583
LOSS: 1.2297604084014893
LOSS: 1.2272922992706299
LOSS: 1.2253098487854004
LOSS: 1.2237380743026733
LOSS: 1.2224924564361572
LOSS: 1.2214808464050293
LOSS: 1.220610499382019
LOSS: 1.2197986841201782
LOSS: 1.2189793586730957
LOSS: 1.2181111574172974
LOSS: 1.2171765565872192
LOSS: 1.2161818742752075
LOSS: 1.2151492834091187
LOSS: 1.214111566543579
LOSS: 1.2131049633026123
LOSS: 1.2121626138687134
LOSS: 1.211310863494873
LOSS: 1.2105664014816284
LOSS: 1.2099356651306152
LOSS: 1.2094154357910156
LOSS: 1.2089935541152954
LOSS: 1.2086522579193115
LOSS: 1.2083710432052612
LOSS: 1.2081279754638672
LOSS: 1.207903265953064
LOSS: 1.2076799869537354
LOSS: 1.207445740699768
LOSS: 1.2071934938430786
LOSS: 1.2069196701049805
LOSS: 1.2066259384155273
LOSS: 1.2063170671463013
LOSS: 1.2059993743896484
LOSS: 1.2056808471679688
LOSS: 1.2053686380386353
LOSS: 1.2050707340240479
LOSS: 1.2047919034957886
LOSS: 1.2045356035232544
LOSS: 1.20430326461792
LOSS: 1.2040940523147583
LOSS: 1.2039052248001099
LOSS: 1.2037335634231567
LOSS: 1.2035753726959229
LOSS: 1.2034251689910889
LOSS: 1.2032805681228638
LOSS: 1.2031387090682983
LOSS: 1.2029975652694702
LOSS: 1.2028573751449585
LOSS: 1.2027181386947632
LOSS: 1.2025810480117798
LOSS: 1.2024476528167725
LOSS: 1.2023193836212158
LOSS: 1.2021981477737427
LOSS: 1.2020834684371948
LOSS: 1.2019767761230469
LOSS: 1.2018773555755615
LOSS: 1.2017847299575806
LOSS: 1.2016974687576294
LOSS: 1.2016148567199707
LOSS: 1.2015353441238403
LOSS: 1.2014588117599487
LOSS: 1.2013840675354004
LOSS: 1.2013109922409058
LOSS: 1.2012393474578857
LOSS: 1.201169729232788
LOSS: 1.2011016607284546
LOSS: 1.2010363340377808
LOSS: 1.200973391532898
LOSS: 1.200913429260254
LOSS: 1.1762495040893555
LOSS: 1.1762322187423706
LOSS: 1.1762133836746216
LOSS: 1.1761937141418457
LOSS: 1.1761727333068848
LOSS: 1.1761512756347656
LOSS: 1.176129937171936
LOSS: 1.176108717918396
LOSS: 1.1760883331298828
LOSS: 1.176068902015686
LOSS: 1.1760504245758057
LOSS: 1.1760329008102417
LOSS: 1.1760166883468628
LOSS: 1.1760010719299316
LOSS: 1.1759861707687378
LOSS: 1.1759718656539917
LOSS: 1.1759579181671143
LOSS: 1.1759445667266846
LOSS: 1.1759313344955444
LOSS: 1.175918459892273
LOSS: 1.1759058237075806
LOSS: 1.1758935451507568
LOSS: 1.1758817434310913
LOSS: 1.291763424873352
LOSS: 1.2810899019241333
LOSS: 1.2708497047424316
LOSS: 1.261102318763733
LOSS: 1.2519062757492065
LOSS: 1.2433170080184937
LOSS: 1.2353830337524414
LOSS: 1.228141188621521
LOSS: 1.2216140031814575
LOSS: 1.215804934501648
LOSS: 1.2107003927230835
LOSS: 1.2062716484069824
LOSS: 1.2024815082550049
LOSS: 1.1992881298065186
LOSS: 1.1966443061828613
LOSS: 1.1944947242736816
LOSS: 1.1927738189697266
LOSS: 1.1914032697677612
LOSS: 1.1902955770492554
LOSS: 1.1893603801727295
LOSS: 1.188511848449707
LOSS: 1.1876780986785889
LOSS: 1.18680739402771
LOSS: 1.185870885848999
LOSS: 1.1848623752593994
LOSS: 1.1837937831878662
LOSS: 1.1826900243759155
LOSS: 1.181583285331726
LOSS: 1.1805064678192139
LOSS: 1.179489016532898
LOSS: 1.178554892539978
LOSS: 1.1777185201644897
LOSS: 1.1769864559173584
LOSS: 1.1763570308685303
LOSS: 1.1758217811584473
LOSS: 1.1753679513931274
LOSS: 1.1749796867370605
LOSS: 1.1746410131454468
LOSS: 1.1743364334106445
LOSS: 1.1740520000457764
LOSS: 1.1737780570983887
LOSS: 1.1735068559646606
LOSS: 1.1732347011566162
LOSS: 1.1729609966278076
LOSS: 1.17268705368042
LOSS: 1.17241632938385
LOSS: 1.1721537113189697
LOSS: 1.171903371810913
LOSS: 1.1716699600219727
LOSS: 1.171457290649414
LOSS: 1.1712675094604492
LOSS: 1.1711012125015259
LOSS: 1.1709578037261963
LOSS: 1.1708353757858276
LOSS: 1.170730471611023
LOSS: 1.1706397533416748
LOSS: 1.1705588102340698
LOSS: 1.1704840660095215
LOSS: 1.1704117059707642
LOSS: 1.170338749885559
LOSS: 1.1702638864517212
LOSS: 1.170186161994934
LOSS: 1.17010498046875
LOSS: 1.170021414756775
LOSS: 1.169935941696167
LOSS: 1.1698498725891113
LOSS: 1.169764518737793
LOSS: 1.1696803569793701
LOSS: 1.1695988178253174
LOSS: 1.1695196628570557
LOSS: 1.169443130493164
LOSS: 1.169369101524353
LOSS: 1.1692970991134644
LOSS: 1.1692264080047607
LOSS: 1.1691571474075317
LOSS: 1.1690881252288818
LOSS: 1.1690199375152588
LOSS: 1.1689516305923462
LOSS: 1.1688835620880127
LOSS: 1.168816089630127
LOSS: 1.168749213218689
LOSS: 1.1686832904815674
LOSS: 1.1686182022094727
LOSS: 1.1685551404953003
LOSS: 1.1684932708740234
LOSS: 1.1684330701828003
LOSS: 1.1683746576309204
LOSS: 1.1683179140090942
LOSS: 1.1682627201080322
LOSS: 1.1682085990905762
LOSS: 1.1681561470031738
LOSS: 1.1681047677993774
LOSS: 1.1680545806884766
LOSS: 1.1680058240890503
LOSS: 1.1679582595825195
LOSS: 1.1679117679595947
LOSS: 1.1678664684295654
LOSS: 1.1678225994110107
LOSS: 1.1677801609039307
LOSS: 1.1677392721176147
LOSS: 1.2841957807540894
LOSS: 1.2728476524353027
LOSS: 1.2619030475616455
LOSS: 1.2514255046844482
LOSS: 1.2414772510528564
LOSS: 1.232118010520935
LOSS: 1.2234009504318237
LOSS: 1.2153674364089966
LOSS: 1.2080427408218384
LOSS: 1.201433539390564
LOSS: 1.1955275535583496
LOSS: 1.1902997493743896
LOSS: 1.1857197284698486
LOSS: 1.1817586421966553
LOSS: 1.178389072418213
LOSS: 1.1755821704864502
LOSS: 1.173302173614502
LOSS: 1.171501874923706
LOSS: 1.1701205968856812
LOSS: 1.169084906578064
LOSS: 1.1683132648468018
LOSS: 1.1677217483520508
LOSS: 1.1672319173812866
LOSS: 1.1667778491973877
LOSS: 1.1663097143173218
LOSS: 1.1657993793487549
LOSS: 1.165236473083496
LOSS: 1.1646286249160767
LOSS: 1.1639952659606934
LOSS: 1.1633621454238892
LOSS: 1.162757158279419
LOSS: 1.1622052192687988
LOSS: 1.1617250442504883
LOSS: 1.1613270044326782
LOSS: 1.1610133647918701
LOSS: 1.1607786417007446
LOSS: 1.1606113910675049
LOSS: 1.160495638847351
LOSS: 1.1604140996932983
LOSS: 1.1603500843048096
LOSS: 1.1602888107299805
LOSS: 1.1602190732955933
LOSS: 1.160132884979248
LOSS: 1.1600265502929688
LOSS: 1.1599000692367554
LOSS: 1.1597555875778198
LOSS: 1.159597396850586
LOSS: 1.1594315767288208
LOSS: 1.1592642068862915
LOSS: 1.159100890159607
LOSS: 1.1589477062225342
LOSS: 1.1588077545166016
LOSS: 1.1586833000183105
LOSS: 1.1585760116577148
LOSS: 1.1584854125976562
LOSS: 1.1584089994430542
LOSS: 1.1583454608917236
LOSS: 1.1582920551300049
LOSS: 1.1582446098327637
LOSS: 1.1582014560699463
LOSS: 1.1581597328186035
LOSS: 1.158117651939392
LOSS: 1.1580742597579956
LOSS: 1.1580290794372559
LOSS: 1.157982587814331
LOSS: 1.1579347848892212
LOSS: 1.1578868627548218
LOSS: 1.1578398942947388
LOSS: 1.15779447555542
LOSS: 1.157751202583313
LOSS: 1.1577110290527344
LOSS: 1.1576734781265259
LOSS: 1.1576390266418457
LOSS: 1.1576071977615356
LOSS: 1.1575778722763062
LOSS: 1.1575499773025513
LOSS: 1.157523512840271
LOSS: 1.1574978828430176
LOSS: 1.1574732065200806
LOSS: 1.1574485301971436
LOSS: 1.1574243307113647
LOSS: 1.157400369644165
LOSS: 1.157376766204834
LOSS: 1.1573536396026611
LOSS: 1.1573309898376465
LOSS: 1.1573095321655273
LOSS: 1.1572885513305664
LOSS: 1.1572681665420532
LOSS: 1.157248854637146
LOSS: 1.157230019569397
LOSS: 1.1572118997573853
LOSS: 1.157193899154663
LOSS: 1.1571762561798096
LOSS: 1.1571588516235352
LOSS: 1.1571416854858398
LOSS: 1.1571241617202759
LOSS: 1.1571071147918701
LOSS: 1.1570900678634644
LOSS: 1.1570727825164795
LOSS: 1.1570558547973633
LOSS: 1.3104385137557983
LOSS: 1.3003697395324707
LOSS: 1.2907471656799316
LOSS: 1.2816346883773804
LOSS: 1.2730956077575684
LOSS: 1.2651901245117188
LOSS: 1.2579703330993652
LOSS: 1.25147545337677
LOSS: 1.2457243204116821
LOSS: 1.2407125234603882
LOSS: 1.2364130020141602
LOSS: 1.2327805757522583
LOSS: 1.2297604084014893
LOSS: 1.2272922992706299
LOSS: 1.2253098487854004
LOSS: 1.2237380743026733
LOSS: 1.2224924564361572
LOSS: 1.2214808464050293
LOSS: 1.220610499382019
LOSS: 1.2197986841201782
LOSS: 1.2189793586730957
LOSS: 1.2181111574172974
LOSS: 1.2171765565872192
LOSS: 1.2161818742752075
LOSS: 1.2151492834091187
LOSS: 1.214111566543579
LOSS: 1.2131049633026123
LOSS: 1.2121626138687134
LOSS: 1.211310863494873
LOSS: 1.2105664014816284
LOSS: 1.2099356651306152
LOSS: 1.2094154357910156
LOSS: 1.2089935541152954
LOSS: 1.2086522579193115
LOSS: 1.2083710432052612
LOSS: 1.2081279754638672
LOSS: 1.207903265953064
LOSS: 1.2076799869537354
LOSS: 1.207445740699768
LOSS: 1.2071934938430786
LOSS: 1.2069196701049805
LOSS: 1.2066259384155273
LOSS: 1.2063170671463013
LOSS: 1.2059993743896484
LOSS: 1.2056808471679688
LOSS: 1.2053686380386353
LOSS: 1.2050707340240479
LOSS: 1.2047919034957886
LOSS: 1.2045356035232544
LOSS: 1.20430326461792
LOSS: 1.2040940523147583
LOSS: 1.2039052248001099
LOSS: 1.2037335634231567
LOSS: 1.2035753726959229
LOSS: 1.2034251689910889
LOSS: 1.2032805681228638
LOSS: 1.2031387090682983
LOSS: 1.2029975652694702
LOSS: 1.2028573751449585
LOSS: 1.2027181386947632
LOSS: 1.2025810480117798
LOSS: 1.2024476528167725
LOSS: 1.2023193836212158
LOSS: 1.2021981477737427
LOSS: 1.2020834684371948
LOSS: 1.2019767761230469
LOSS: 1.2018773555755615
LOSS: 1.2017847299575806
LOSS: 1.2016974687576294
LOSS: 1.2016148567199707
LOSS: 1.2015353441238403
LOSS: 1.2014588117599487
LOSS: 1.2013840675354004
LOSS: 1.2013109922409058
LOSS: 1.2012393474578857
LOSS: 1.201169729232788
LOSS: 1.2011016607284546
LOSS: 1.2010363340377808
LOSS: 1.200973391532898
LOSS: 1.200913429260254
LOSS: 1.2008557319641113
LOSS: 1.2008012533187866
LOSS: 1.200749158859253
LOSS: 1.2006986141204834
LOSS: 1.2006497383117676
LOSS: 1.2006022930145264
LOSS: 1.2005559206008911
LOSS: 1.2005105018615723
LOSS: 1.2004657983779907
LOSS: 1.200421690940857
LOSS: 1.2003787755966187
LOSS: 1.2003368139266968
LOSS: 1.2002958059310913
LOSS: 1.2002562284469604
LOSS: 1.200217843055725
LOSS: 1.2001808881759644
LOSS: 1.2001447677612305
LOSS: 1.2001101970672607
LOSS: 1.2000762224197388
LOSS: 1.2000432014465332
LOSS: 1.2877933979034424
LOSS: 1.2760796546936035
LOSS: 1.2646828889846802
LOSS: 1.2536580562591553
LOSS: 1.2430610656738281
LOSS: 1.2329477071762085
LOSS: 1.2233705520629883
LOSS: 1.1806869506835938
LOSS: 1.1805381774902344
LOSS: 1.1804090738296509
LOSS: 1.1802979707717896
LOSS: 1.180201768875122
LOSS: 1.1801174879074097
LOSS: 1.1800415515899658
LOSS: 1.1799713373184204
LOSS: 1.1799039840698242
LOSS: 1.1798384189605713
LOSS: 1.1797723770141602
LOSS: 1.1797059774398804
LOSS: 1.1796388626098633
LOSS: 1.1795716285705566
LOSS: 1.1795047521591187
LOSS: 1.179438829421997
LOSS: 1.1793745756149292
LOSS: 1.1793118715286255
LOSS: 1.1792511940002441
LOSS: 1.179192066192627
LOSS: 1.1791346073150635
LOSS: 1.179078221321106
LOSS: 1.1790225505828857
LOSS: 1.1789671182632446
LOSS: 1.178911805152893
LOSS: 1.1788564920425415
LOSS: 1.1788015365600586
LOSS: 1.1787463426589966
LOSS: 1.178692102432251
LOSS: 1.1786388158798218
LOSS: 1.1785871982574463
LOSS: 1.1785368919372559
LOSS: 1.1784889698028564
LOSS: 1.1784429550170898
LOSS: 1.1783992052078247
LOSS: 1.178357720375061
LOSS: 1.1783182621002197
LOSS: 1.178280234336853
LOSS: 1.1782439947128296
LOSS: 1.1782089471817017
LOSS: 1.178174614906311
LOSS: 1.1781413555145264
LOSS: 1.1781082153320312
LOSS: 1.178075909614563
LOSS: 1.1780433654785156
LOSS: 1.1780110597610474
LOSS: 1.1779788732528687
LOSS: 1.177946925163269
LOSS: 1.1779148578643799
LOSS: 1.1778826713562012
LOSS: 1.1778508424758911
LOSS: 1.177818775177002
LOSS: 1.2936463356018066
LOSS: 1.2830274105072021
LOSS: 1.2728681564331055
LOSS: 1.2632390260696411
LOSS: 1.254209041595459
LOSS: 1.245842456817627
LOSS: 1.238193154335022
LOSS: 1.2312989234924316
LOSS: 1.2251760959625244
LOSS: 1.219817042350769
LOSS: 1.2151918411254883
LOSS: 1.2112574577331543
LOSS: 1.2079651355743408
LOSS: 1.2052658796310425
LOSS: 1.203109860420227
LOSS: 1.2014410495758057
LOSS: 1.2001934051513672
LOSS: 1.199288249015808
LOSS: 1.19863760471344
LOSS: 1.19814932346344
LOSS: 1.1977360248565674
LOSS: 1.1973240375518799
LOSS: 1.196859359741211
LOSS: 1.196312427520752
LOSS: 1.195676565170288
LOSS: 1.19496488571167
LOSS: 1.1942051649093628
LOSS: 1.1934306621551514
LOSS: 1.1926758289337158
LOSS: 1.191969633102417
LOSS: 1.1913325786590576
LOSS: 1.1907745599746704
LOSS: 1.1902952194213867
LOSS: 1.1898846626281738
LOSS: 1.1895267963409424
LOSS: 1.1892013549804688
LOSS: 1.1888874769210815
LOSS: 1.1885664463043213
LOSS: 1.1882222890853882
LOSS: 1.18784499168396
LOSS: 1.1874288320541382
LOSS: 1.1869735717773438
LOSS: 1.1864838600158691
LOSS: 1.1859679222106934
LOSS: 1.1854360103607178
LOSS: 1.1849006414413452
LOSS: 1.1843740940093994
LOSS: 1.1838676929473877
LOSS: 1.18339204788208
LOSS: 1.1829545497894287
LOSS: 1.182560682296753
LOSS: 1.1822127103805542
LOSS: 1.1819099187850952
LOSS: 1.1816505193710327
LOSS: 1.1814287900924683
LOSS: 1.1812392473220825
LOSS: 1.1810753345489502
LOSS: 1.180930256843567
LOSS: 1.1807981729507446
LOSS: 1.1806739568710327
LOSS: 1.180553913116455
LOSS: 1.1804356575012207
LOSS: 1.1803178787231445
LOSS: 1.1802005767822266
LOSS: 1.180084466934204
LOSS: 1.1799708604812622
LOSS: 1.179861068725586
LOSS: 1.1797562837600708
LOSS: 1.1796579360961914
LOSS: 1.179566740989685
LOSS: 1.1794836521148682
LOSS: 1.1794078350067139
LOSS: 1.1793394088745117
LOSS: 1.179277777671814
LOSS: 1.1792223453521729
LOSS: 1.1791718006134033
LOSS: 1.1791259050369263
LOSS: 1.1790835857391357
LOSS: 1.1790440082550049
LOSS: 1.1790070533752441
LOSS: 1.178971767425537
LOSS: 1.1789377927780151
LOSS: 1.1789050102233887
LOSS: 1.1788725852966309
LOSS: 1.1788409948349
LOSS: 1.1788097620010376
LOSS: 1.1787784099578857
LOSS: 1.1787476539611816
LOSS: 1.178716778755188
LOSS: 1.1786866188049316
LOSS: 1.1786565780639648
LOSS: 1.1786274909973145
LOSS: 1.178599238395691
LOSS: 1.1785719394683838
LOSS: 1.1785457134246826
LOSS: 1.1785204410552979
LOSS: 1.1784965991973877
LOSS: 1.1784733533859253
LOSS: 1.1784512996673584
LOSS: 1.1784298419952393
LOSS: 1.291763424873352
LOSS: 1.2810899019241333
LOSS: 1.2708497047424316
LOSS: 1.261102318763733
LOSS: 1.2519062757492065
LOSS: 1.2433170080184937
LOSS: 1.2353830337524414
LOSS: 1.228141188621521
LOSS: 1.2216140031814575
LOSS: 1.215804934501648
LOSS: 1.2107003927230835
LOSS: 1.2062716484069824
LOSS: 1.2024815082550049
LOSS: 1.1992881298065186
LOSS: 1.1966443061828613
LOSS: 1.1944947242736816
LOSS: 1.1927738189697266
LOSS: 1.1914032697677612
LOSS: 1.1902955770492554
LOSS: 1.1893603801727295
LOSS: 1.188511848449707
LOSS: 1.1876780986785889
LOSS: 1.18680739402771
LOSS: 1.185870885848999
LOSS: 1.1848623752593994
LOSS: 1.1837937831878662
LOSS: 1.1826900243759155
LOSS: 1.181583285331726
LOSS: 1.1805064678192139
LOSS: 1.179489016532898
LOSS: 1.178554892539978
LOSS: 1.1777185201644897
LOSS: 1.1769864559173584
LOSS: 1.1763570308685303
LOSS: 1.1758217811584473
LOSS: 1.1753679513931274
LOSS: 1.1749796867370605
LOSS: 1.1746410131454468
LOSS: 1.1743364334106445
LOSS: 1.1740520000457764
LOSS: 1.1737780570983887
LOSS: 1.1735068559646606
LOSS: 1.1732347011566162
LOSS: 1.1729609966278076
LOSS: 1.17268705368042
LOSS: 1.17241632938385
LOSS: 1.1721537113189697
LOSS: 1.171903371810913
LOSS: 1.1716699600219727
LOSS: 1.171457290649414
LOSS: 1.1712675094604492
LOSS: 1.1711012125015259
LOSS: 1.1709578037261963
LOSS: 1.1708353757858276
LOSS: 1.170730471611023
LOSS: 1.1706397533416748
LOSS: 1.1705588102340698
LOSS: 1.1704840660095215
LOSS: 1.1704117059707642
LOSS: 1.170338749885559
LOSS: 1.1702638864517212
LOSS: 1.170186161994934
LOSS: 1.17010498046875
LOSS: 1.170021414756775
LOSS: 1.169935941696167
LOSS: 1.1698498725891113
LOSS: 1.169764518737793
LOSS: 1.1696803569793701
LOSS: 1.1695988178253174
LOSS: 1.1695196628570557
LOSS: 1.169443130493164
LOSS: 1.169369101524353
LOSS: 1.1692970991134644
LOSS: 1.1692264080047607
LOSS: 1.1691571474075317
LOSS: 1.1690881252288818
LOSS: 1.1690199375152588
LOSS: 1.1689516305923462
LOSS: 1.1688835620880127
LOSS: 1.168816089630127
LOSS: 1.168749213218689
LOSS: 1.1686832904815674
LOSS: 1.1686182022094727
LOSS: 1.1685551404953003
LOSS: 1.1684932708740234
LOSS: 1.1684330701828003
LOSS: 1.1683746576309204
LOSS: 1.1683179140090942
LOSS: 1.1682627201080322
LOSS: 1.1682085990905762
LOSS: 1.1681561470031738
LOSS: 1.1681047677993774
LOSS: 1.1680545806884766
LOSS: 1.1680058240890503
LOSS: 1.1679582595825195
LOSS: 1.1679117679595947
LOSS: 1.1678664684295654
LOSS: 1.1678225994110107
LOSS: 1.1677801609039307
LOSS: 1.1677392721176147
LOSS: 1.294985055923462
LOSS: 1.2847070693969727
LOSS: 1.274919033050537
LOSS: 1.2656850814819336
LOSS: 1.2570676803588867
LOSS: 1.2491247653961182
LOSS: 1.2419047355651855
LOSS: 1.235440731048584
LOSS: 1.2297457456588745
LOSS: 1.2248095273971558
LOSS: 1.2206015586853027
LOSS: 1.2170777320861816
LOSS: 1.2141879796981812
LOSS: 1.211879849433899
LOSS: 1.2100964784622192
LOSS: 1.2087719440460205
LOSS: 1.2078276872634888
LOSS: 1.2071735858917236
LOSS: 1.206712007522583
LOSS: 1.2063474655151367
LOSS: 1.2059956789016724
LOSS: 1.2055926322937012
LOSS: 1.2050994634628296
LOSS: 1.2045032978057861
LOSS: 1.2038145065307617
LOSS: 1.2030599117279053
LOSS: 1.2022771835327148
LOSS: 1.2015063762664795
LOSS: 1.2007837295532227
LOSS: 1.2001391649246216
LOSS: 1.1995913982391357
LOSS: 1.1991479396820068
LOSS: 1.1988062858581543
LOSS: 1.1985546350479126
LOSS: 1.1983751058578491
LOSS: 1.1982457637786865
LOSS: 1.1981444358825684
LOSS: 1.1980503797531128
LOSS: 1.1979460716247559
LOSS: 1.1978187561035156
LOSS: 1.1976615190505981
LOSS: 1.197471022605896
LOSS: 1.1972496509552002
LOSS: 1.1970022916793823
LOSS: 1.1967359781265259
LOSS: 1.1964597702026367
LOSS: 1.1961817741394043
LOSS: 1.1959099769592285
LOSS: 1.1956499814987183
LOSS: 1.1954059600830078
LOSS: 1.1951795816421509
LOSS: 1.1949710845947266
LOSS: 1.194778323173523
LOSS: 1.1945981979370117
LOSS: 1.194427251815796
LOSS: 1.1942616701126099
LOSS: 1.1940982341766357
LOSS: 1.193934440612793
LOSS: 1.1937687397003174
LOSS: 1.1936007738113403
LOSS: 1.1934304237365723
LOSS: 1.1932592391967773
LOSS: 1.193088173866272
LOSS: 1.1929190158843994
LOSS: 1.1927529573440552
LOSS: 1.1925911903381348
LOSS: 1.1924344301223755
LOSS: 1.1922823190689087
LOSS: 1.1921348571777344
LOSS: 1.1919915676116943
LOSS: 1.1918517351150513
LOSS: 1.191713809967041
LOSS: 1.1915780305862427
LOSS: 1.1914430856704712
LOSS: 1.1913093328475952
LOSS: 1.1911765336990356
LOSS: 1.191044807434082
LOSS: 1.190914511680603
LOSS: 1.190786361694336
LOSS: 1.1723594665527344
LOSS: 1.1721521615982056
LOSS: 1.1719635725021362
LOSS: 1.1717946529388428
LOSS: 1.1716465950012207
LOSS: 1.171517014503479
LOSS: 1.1714036464691162
LOSS: 1.171302318572998
LOSS: 1.171209692955017
LOSS: 1.1711220741271973
LOSS: 1.1710370779037476
LOSS: 1.1709527969360352
LOSS: 1.1708688735961914
LOSS: 1.1707847118377686
LOSS: 1.1707013845443726
LOSS: 1.1706191301345825
LOSS: 1.1705387830734253
LOSS: 1.1704604625701904
LOSS: 1.1703840494155884
LOSS: 1.1703096628189087
LOSS: 1.1702369451522827
LOSS: 1.170165777206421
LOSS: 1.170095682144165
LOSS: 1.1700265407562256
LOSS: 1.169958472251892
LOSS: 1.1698917150497437
LOSS: 1.1698265075683594
LOSS: 1.1697627305984497
LOSS: 1.1697001457214355
LOSS: 1.1696394681930542
LOSS: 1.1695797443389893
LOSS: 1.1695210933685303
LOSS: 1.169463038444519
LOSS: 1.1694058179855347
LOSS: 1.1693487167358398
LOSS: 1.1692923307418823
LOSS: 1.169236660003662
LOSS: 1.1691813468933105
LOSS: 1.1691268682479858
LOSS: 1.1690735816955566
LOSS: 1.1690212488174438
LOSS: 1.1689702272415161
LOSS: 1.1689202785491943
LOSS: 1.1688710451126099
LOSS: 1.1688228845596313
LOSS: 1.168775200843811
LOSS: 1.168727993965149
LOSS: 1.1686813831329346
LOSS: 1.1686348915100098
LOSS: 1.1685888767242432
LOSS: 1.1685432195663452
LOSS: 1.1684976816177368
LOSS: 1.1684529781341553
LOSS: 1.2928510904312134
LOSS: 1.2824299335479736
LOSS: 1.2724214792251587
LOSS: 1.2628774642944336
LOSS: 1.2538490295410156
LOSS: 1.2453864812850952
LOSS: 1.2375353574752808
LOSS: 1.2303335666656494
LOSS: 1.2238059043884277
LOSS: 1.2179615497589111
LOSS: 1.2127914428710938
LOSS: 1.2082700729370117
LOSS: 1.2043598890304565
LOSS: 1.2010174989700317
LOSS: 1.1981946229934692
LOSS: 1.1958386898040771
LOSS: 1.1938892602920532
LOSS: 1.1922773122787476
LOSS: 1.190925121307373
LOSS: 1.1897517442703247
LOSS: 1.188678503036499
LOSS: 1.1876389980316162
LOSS: 1.1865841150283813
LOSS: 1.1854850053787231
LOSS: 1.184335708618164
LOSS: 1.1831482648849487
LOSS: 1.1819477081298828
LOSS: 1.1807669401168823
LOSS: 1.179640531539917
LOSS: 1.1785997152328491
LOSS: 1.1776684522628784
LOSS: 1.1768622398376465
LOSS: 1.1761860847473145
LOSS: 1.1756362915039062
LOSS: 1.1752007007598877
LOSS: 1.1748617887496948
LOSS: 1.174597144126892
LOSS: 1.1743847131729126
LOSS: 1.1742030382156372
LOSS: 1.1740331649780273
LOSS: 1.1738612651824951
LOSS: 1.1736781597137451
LOSS: 1.173479676246643
LOSS: 1.173266053199768
LOSS: 1.1730409860610962
LOSS: 1.1728099584579468
LOSS: 1.1725807189941406
LOSS: 1.1723594665527344
LOSS: 1.1721521615982056
LOSS: 1.1719635725021362
LOSS: 1.1717946529388428
LOSS: 1.1716465950012207
LOSS: 1.171517014503479
LOSS: 1.1714036464691162
LOSS: 1.171302318572998
LOSS: 1.171209692955017
LOSS: 1.1711220741271973
LOSS: 1.1710370779037476
LOSS: 1.1709527969360352
LOSS: 1.1708688735961914
LOSS: 1.1707847118377686
LOSS: 1.1707013845443726
LOSS: 1.1706191301345825
LOSS: 1.1705387830734253
LOSS: 1.1704604625701904
LOSS: 1.1703840494155884
LOSS: 1.1703096628189087
LOSS: 1.1702369451522827
LOSS: 1.170165777206421
LOSS: 1.170095682144165
LOSS: 1.1700265407562256
LOSS: 1.169958472251892
LOSS: 1.1698917150497437
LOSS: 1.1698265075683594
LOSS: 1.1697627305984497
LOSS: 1.1697001457214355
LOSS: 1.1696394681930542
LOSS: 1.1695797443389893
LOSS: 1.1695210933685303
LOSS: 1.169463038444519
LOSS: 1.1694058179855347
LOSS: 1.1693487167358398
LOSS: 1.1692923307418823
LOSS: 1.169236660003662
LOSS: 1.1691813468933105
LOSS: 1.1691268682479858
LOSS: 1.1690735816955566
LOSS: 1.1690212488174438
LOSS: 1.1689702272415161
LOSS: 1.1689202785491943
LOSS: 1.1688710451126099
LOSS: 1.1688228845596313
LOSS: 1.168775200843811
LOSS: 1.168727993965149
LOSS: 1.1686813831329346
LOSS: 1.1686348915100098
LOSS: 1.1685888767242432
LOSS: 1.1685432195663452
LOSS: 1.1684976816177368
LOSS: 1.1684529781341553
LOSS: 1.2841957807540894
LOSS: 1.2728476524353027
LOSS: 1.2619030475616455
LOSS: 1.2514255046844482
LOSS: 1.2414772510528564
LOSS: 1.232118010520935
LOSS: 1.2234009504318237
LOSS: 1.2153674364089966
LOSS: 1.2080427408218384
LOSS: 1.201433539390564
LOSS: 1.1955275535583496
LOSS: 1.1902997493743896
LOSS: 1.1857197284698486
LOSS: 1.1817586421966553
LOSS: 1.178389072418213
LOSS: 1.1755821704864502
LOSS: 1.173302173614502
LOSS: 1.171501874923706
LOSS: 1.1701205968856812
LOSS: 1.169084906578064
LOSS: 1.1683132648468018
LOSS: 1.1677217483520508
LOSS: 1.1672319173812866
LOSS: 1.1667778491973877
LOSS: 1.1663097143173218
LOSS: 1.1657993793487549
LOSS: 1.165236473083496
LOSS: 1.1646286249160767
LOSS: 1.1639952659606934
LOSS: 1.1633621454238892
LOSS: 1.162757158279419
LOSS: 1.1622052192687988
LOSS: 1.1617250442504883
LOSS: 1.1613270044326782
LOSS: 1.1610133647918701
LOSS: 1.1607786417007446
LOSS: 1.1606113910675049
LOSS: 1.160495638847351
LOSS: 1.1604140996932983
LOSS: 1.1603500843048096
LOSS: 1.1602888107299805
LOSS: 1.1602190732955933
LOSS: 1.160132884979248
LOSS: 1.1600265502929688
LOSS: 1.1599000692367554
LOSS: 1.1597555875778198
LOSS: 1.159597396850586
LOSS: 1.1594315767288208
LOSS: 1.1592642068862915
LOSS: 1.159100890159607
LOSS: 1.1589477062225342
LOSS: 1.1588077545166016
LOSS: 1.1586833000183105
LOSS: 1.1585760116577148
LOSS: 1.1584854125976562
LOSS: 1.1584089994430542
LOSS: 1.1583454608917236
LOSS: 1.1582920551300049
LOSS: 1.1582446098327637
LOSS: 1.1582014560699463
LOSS: 1.1581597328186035
LOSS: 1.158117651939392
LOSS: 1.1580742597579956
LOSS: 1.1580290794372559
LOSS: 1.157982587814331
LOSS: 1.1579347848892212
LOSS: 1.1578868627548218
LOSS: 1.1578398942947388
LOSS: 1.15779447555542
LOSS: 1.157751202583313
LOSS: 1.1577110290527344
LOSS: 1.1576734781265259
LOSS: 1.1576390266418457
LOSS: 1.1576071977615356
LOSS: 1.1575778722763062
LOSS: 1.1575499773025513
LOSS: 1.157523512840271
LOSS: 1.1574978828430176
LOSS: 1.1574732065200806
LOSS: 1.1574485301971436
LOSS: 1.1574243307113647
LOSS: 1.157400369644165
LOSS: 1.157376766204834
LOSS: 1.1573536396026611
LOSS: 1.1573309898376465
LOSS: 1.1573095321655273
LOSS: 1.1572885513305664
LOSS: 1.1572681665420532
LOSS: 1.157248854637146
LOSS: 1.157230019569397
LOSS: 1.1572118997573853
LOSS: 1.157193899154663
LOSS: 1.1571762561798096
LOSS: 1.1571588516235352
LOSS: 1.1571416854858398
LOSS: 1.1571241617202759
LOSS: 1.1571071147918701
LOSS: 1.1570900678634644
LOSS: 1.1570727825164795
LOSS: 1.1570558547973633
LOSS: 1.300906777381897
LOSS: 1.2908804416656494
LOSS: 1.281315803527832
LOSS: 1.272279977798462
LOSS: 1.2638391256332397
LOSS: 1.256055474281311
LOSS: 1.2489819526672363
LOSS: 1.242656946182251
LOSS: 1.2370983362197876
LOSS: 1.232298493385315
LOSS: 1.2282251119613647
LOSS: 1.224824070930481
LOSS: 1.2220295667648315
LOSS: 1.2197688817977905
LOSS: 1.217965841293335
LOSS: 1.2165398597717285
LOSS: 1.2154054641723633
LOSS: 1.2144749164581299
LOSS: 1.21366286277771
LOSS: 1.2128952741622925
LOSS: 1.2121156454086304
LOSS: 1.2112897634506226
LOSS: 1.2104068994522095
LOSS: 1.2094757556915283
LOSS: 1.2085206508636475
LOSS: 1.2075731754302979
LOSS: 1.2066677808761597
LOSS: 1.2058347463607788
LOSS: 1.205096960067749
LOSS: 1.2044670581817627
LOSS: 1.2039486169815063
LOSS: 1.2035351991653442
LOSS: 1.2032139301300049
LOSS: 1.2029682397842407
LOSS: 1.2027796506881714
LOSS: 1.2026294469833374
LOSS: 1.2025007009506226
LOSS: 1.2023776769638062
LOSS: 1.2022470235824585
LOSS: 1.2020992040634155
LOSS: 1.201928973197937
LOSS: 1.2017360925674438
LOSS: 1.2015252113342285
LOSS: 1.2013052701950073
LOSS: 1.201086163520813
LOSS: 1.2008771896362305
LOSS: 1.2006862163543701
LOSS: 1.2005174160003662
LOSS: 1.2003706693649292
LOSS: 1.2002432346343994
LOSS: 1.200129747390747
LOSS: 1.2000231742858887
LOSS: 1.1999170780181885
LOSS: 1.199805736541748
LOSS: 1.1996861696243286
LOSS: 1.1995564699172974
LOSS: 1.1994186639785767
LOSS: 1.199275016784668
LOSS: 1.1991299390792847
LOSS: 1.198987603187561
LOSS: 1.1988519430160522
LOSS: 1.1987254619598389
LOSS: 1.1986100673675537
LOSS: 1.1985057592391968
LOSS: 1.1984113454818726
LOSS: 1.1983249187469482
LOSS: 1.1982444524765015
LOSS: 1.1981674432754517
LOSS: 1.198091745376587
LOSS: 1.1980160474777222
LOSS: 1.1979392766952515
LOSS: 1.1978615522384644
LOSS: 1.1977823972702026
LOSS: 1.19770348072052
LOSS: 1.1976253986358643
LOSS: 1.197548508644104
LOSS: 1.1974737644195557
LOSS: 1.1906607151031494
LOSS: 1.190537929534912
LOSS: 1.1904184818267822
LOSS: 1.1903029680252075
LOSS: 1.1901910305023193
LOSS: 1.1900826692581177
LOSS: 1.189978003501892
LOSS: 1.1898765563964844
LOSS: 1.1897779703140259
LOSS: 1.1896824836730957
LOSS: 1.1895893812179565
LOSS: 1.1894991397857666
LOSS: 1.189410924911499
LOSS: 1.1893248558044434
LOSS: 1.1892412900924683
LOSS: 1.1891599893569946
LOSS: 1.189081072807312
LOSS: 1.1890039443969727
LOSS: 1.1889293193817139
LOSS: 1.1888563632965088
LOSS: 1.1887855529785156
LOSS: 1.2984665632247925
LOSS: 1.2881735563278198
LOSS: 1.2782930135726929
LOSS: 1.2688796520233154
LOSS: 1.2599873542785645
LOSS: 1.251666784286499
LOSS: 1.2439613342285156
LOSS: 1.2369040250778198
LOSS: 1.2305117845535278
LOSS: 1.2247823476791382
LOSS: 1.2196952104568481
LOSS: 1.2152143716812134
LOSS: 1.211296558380127
LOSS: 1.2078967094421387
LOSS: 1.204967975616455
LOSS: 1.2024613618850708
LOSS: 1.2003220319747925
LOSS: 1.1984869241714478
LOSS: 1.1968865394592285
LOSS: 1.195449709892273
LOSS: 1.1941099166870117
LOSS: 1.1928129196166992
LOSS: 1.191520094871521
LOSS: 1.1902118921279907
LOSS: 1.1888867616653442
LOSS: 1.1875571012496948
LOSS: 1.1862457990646362
LOSS: 1.1849805116653442
LOSS: 1.183788776397705
LOSS: 1.1826953887939453
LOSS: 1.1817187070846558
LOSS: 1.180869698524475
LOSS: 1.1801509857177734
LOSS: 1.1795586347579956
LOSS: 1.1790812015533447
LOSS: 1.1787042617797852
LOSS: 1.178409218788147
LOSS: 1.1781772375106812
LOSS: 1.1779896020889282
LOSS: 1.1778303384780884
LOSS: 1.177685260772705
LOSS: 1.177544355392456
LOSS: 1.177401065826416
LOSS: 1.1772516965866089
LOSS: 1.177095890045166
LOSS: 1.1769353151321411
LOSS: 1.1767730712890625
LOSS: 1.1766135692596436
LOSS: 1.1764603853225708
LOSS: 1.1763174533843994
LOSS: 1.1761878728866577
LOSS: 1.1760722398757935
LOSS: 1.1759711503982544
LOSS: 1.1758835315704346
LOSS: 1.175807237625122
LOSS: 1.1757398843765259
LOSS: 1.1756781339645386
LOSS: 1.1756194829940796
LOSS: 1.1755609512329102
LOSS: 1.1755003929138184
LOSS: 1.1754372119903564
LOSS: 1.175370693206787
LOSS: 1.1753003597259521
LOSS: 1.175227403640747
LOSS: 1.1751528978347778
LOSS: 1.175077199935913
LOSS: 1.175002098083496
LOSS: 1.174928069114685
LOSS: 1.1748557090759277
LOSS: 1.1747853755950928
LOSS: 1.1747167110443115
LOSS: 1.174649715423584
LOSS: 1.1745840311050415
LOSS: 1.1745193004608154
LOSS: 1.1744555234909058
LOSS: 1.1743921041488647
LOSS: 1.174329400062561
LOSS: 1.1742671728134155
LOSS: 1.1742053031921387
LOSS: 1.1741446256637573
LOSS: 1.1740851402282715
LOSS: 1.1740272045135498
LOSS: 1.1739706993103027
LOSS: 1.1739164590835571
LOSS: 1.173863410949707
LOSS: 1.1738123893737793
LOSS: 1.1737626791000366
LOSS: 1.1737146377563477
LOSS: 1.173667550086975
LOSS: 1.1736212968826294
LOSS: 1.173575520515442
LOSS: 1.1735305786132812
LOSS: 1.173485517501831
LOSS: 1.1734411716461182
LOSS: 1.1733970642089844
LOSS: 1.1733533143997192
LOSS: 1.1733100414276123
LOSS: 1.1732673645019531
LOSS: 1.1732250452041626
LOSS: 1.173183560371399
LOSS: 1.300906777381897
LOSS: 1.2908804416656494
LOSS: 1.281315803527832
LOSS: 1.272279977798462
LOSS: 1.2638391256332397
LOSS: 1.256055474281311
LOSS: 1.2489819526672363
LOSS: 1.242656946182251
LOSS: 1.2370983362197876
LOSS: 1.232298493385315
LOSS: 1.2282251119613647
LOSS: 1.224824070930481
LOSS: 1.2220295667648315
LOSS: 1.2197688817977905
LOSS: 1.217965841293335
LOSS: 1.2165398597717285
LOSS: 1.2154054641723633
LOSS: 1.2144749164581299
LOSS: 1.21366286277771
LOSS: 1.2128952741622925
LOSS: 1.2121156454086304
LOSS: 1.2112897634506226
LOSS: 1.2104068994522095
LOSS: 1.2094757556915283
LOSS: 1.2085206508636475
LOSS: 1.2075731754302979
LOSS: 1.2066677808761597
LOSS: 1.2058347463607788
LOSS: 1.205096960067749
LOSS: 1.2044670581817627
LOSS: 1.2039486169815063
LOSS: 1.2035351991653442
LOSS: 1.2032139301300049
LOSS: 1.2029682397842407
LOSS: 1.2027796506881714
LOSS: 1.2026294469833374
LOSS: 1.2025007009506226
LOSS: 1.2023776769638062
LOSS: 1.2022470235824585
LOSS: 1.2020992040634155
LOSS: 1.201928973197937
LOSS: 1.2017360925674438
LOSS: 1.2015252113342285
LOSS: 1.2013052701950073
LOSS: 1.201086163520813
LOSS: 1.2008771896362305
LOSS: 1.2006862163543701
LOSS: 1.2005174160003662
LOSS: 1.2003706693649292
LOSS: 1.2002432346343994
LOSS: 1.200129747390747
LOSS: 1.2000231742858887
LOSS: 1.1999170780181885
LOSS: 1.199805736541748
LOSS: 1.1996861696243286
LOSS: 1.1995564699172974
LOSS: 1.1994186639785767
LOSS: 1.199275016784668
LOSS: 1.1991299390792847
LOSS: 1.198987603187561
LOSS: 1.1988519430160522
LOSS: 1.1987254619598389
LOSS: 1.1986100673675537
LOSS: 1.1985057592391968
LOSS: 1.1984113454818726
LOSS: 1.1983249187469482
LOSS: 1.1982444524765015
LOSS: 1.1981674432754517
LOSS: 1.198091745376587
LOSS: 1.1980160474777222
LOSS: 1.1979392766952515
LOSS: 1.1978615522384644
LOSS: 1.1977823972702026
LOSS: 1.19770348072052
LOSS: 1.1976253986358643
LOSS: 1.197548508644104
LOSS: 1.1974737644195557
LOSS: 1.1974011659622192
LOSS: 1.197331428527832
LOSS: 1.197264313697815
LOSS: 1.1971989870071411
LOSS: 1.1971358060836792
LOSS: 1.1970739364624023
LOSS: 1.1970131397247314
LOSS: 1.1969531774520874
LOSS: 1.1968945264816284
LOSS: 1.1968365907669067
LOSS: 1.1967802047729492
LOSS: 1.1967251300811768
LOSS: 1.196671962738037
LOSS: 1.1966203451156616
LOSS: 1.1965705156326294
LOSS: 1.1965219974517822
LOSS: 1.1964753866195679
LOSS: 1.1964302062988281
LOSS: 1.1963863372802734
LOSS: 1.1963437795639038
LOSS: 1.1963022947311401
LOSS: 1.1962618827819824
LOSS: 1.1962227821350098
LOSS: 1.2984665632247925
LOSS: 1.2881735563278198
LOSS: 1.2782930135726929
LOSS: 1.2688796520233154
LOSS: 1.2599873542785645
LOSS: 1.251666784286499
LOSS: 1.2439613342285156
LOSS: 1.2369040250778198
LOSS: 1.2305117845535278
LOSS: 1.2247823476791382
LOSS: 1.2196952104568481
LOSS: 1.2152143716812134
LOSS: 1.211296558380127
LOSS: 1.2078967094421387
LOSS: 1.204967975616455
LOSS: 1.2024613618850708
LOSS: 1.2003220319747925
LOSS: 1.1984869241714478
LOSS: 1.1968865394592285
LOSS: 1.195449709892273
LOSS: 1.1941099166870117
LOSS: 1.1928129196166992
LOSS: 1.191520094871521
LOSS: 1.1902118921279907
LOSS: 1.1888867616653442
LOSS: 1.1875571012496948
LOSS: 1.1862457990646362
LOSS: 1.1849805116653442
LOSS: 1.183788776397705
LOSS: 1.1826953887939453
LOSS: 1.1817187070846558
LOSS: 1.180869698524475
LOSS: 1.1801509857177734
LOSS: 1.1795586347579956
LOSS: 1.1790812015533447
LOSS: 1.1787042617797852
LOSS: 1.178409218788147
LOSS: 1.1781772375106812
LOSS: 1.1779896020889282
LOSS: 1.1778303384780884
LOSS: 1.177685260772705
LOSS: 1.177544355392456
LOSS: 1.177401065826416
LOSS: 1.1772516965866089
LOSS: 1.177095890045166
LOSS: 1.1769353151321411
LOSS: 1.1767730712890625
LOSS: 1.1766135692596436
LOSS: 1.1764603853225708
LOSS: 1.1763174533843994
LOSS: 1.1761878728866577
LOSS: 1.1760722398757935
LOSS: 1.1759711503982544
LOSS: 1.1758835315704346
LOSS: 1.175807237625122
LOSS: 1.1757398843765259
LOSS: 1.1756781339645386
LOSS: 1.1756194829940796
LOSS: 1.1755609512329102
LOSS: 1.1755003929138184
LOSS: 1.1754372119903564
LOSS: 1.175370693206787
LOSS: 1.1753003597259521
LOSS: 1.175227403640747
LOSS: 1.1751528978347778
LOSS: 1.175077199935913
LOSS: 1.175002098083496
LOSS: 1.174928069114685
LOSS: 1.1748557090759277
LOSS: 1.1747853755950928
LOSS: 1.1747167110443115
LOSS: 1.174649715423584
LOSS: 1.1745840311050415
LOSS: 1.1745193004608154
LOSS: 1.1744555234909058
LOSS: 1.1743921041488647
LOSS: 1.174329400062561
LOSS: 1.1742671728134155
LOSS: 1.1742053031921387
LOSS: 1.1741446256637573
LOSS: 1.1740851402282715
LOSS: 1.1740272045135498
LOSS: 1.1739706993103027
LOSS: 1.1739164590835571
LOSS: 1.173863410949707
LOSS: 1.1738123893737793
LOSS: 1.1737626791000366
LOSS: 1.1737146377563477
LOSS: 1.173667550086975
LOSS: 1.1736212968826294
LOSS: 1.173575520515442
LOSS: 1.1735305786132812
LOSS: 1.173485517501831
LOSS: 1.1734411716461182
LOSS: 1.1733970642089844
LOSS: 1.1733533143997192
LOSS: 1.1733100414276123
LOSS: 1.1732673645019531
LOSS: 1.1732250452041626
LOSS: 1.173183560371399
LOSS: 1.29647696018219
LOSS: 1.28584623336792
LOSS: 1.2756354808807373
LOSS: 1.2659066915512085
LOSS: 1.2567214965820312
LOSS: 1.248139500617981
LOSS: 1.2402147054672241
LOSS: 1.2329915761947632
LOSS: 1.2264996767044067
LOSS: 1.2207509279251099
LOSS: 1.2008557319641113
LOSS: 1.2008012533187866
LOSS: 1.200749158859253
LOSS: 1.2006986141204834
LOSS: 1.2006497383117676
LOSS: 1.2006022930145264
LOSS: 1.2005559206008911
LOSS: 1.2005105018615723
LOSS: 1.2004657983779907
LOSS: 1.200421690940857
LOSS: 1.2003787755966187
LOSS: 1.2003368139266968
LOSS: 1.2002958059310913
LOSS: 1.2002562284469604
LOSS: 1.200217843055725
LOSS: 1.2001808881759644
LOSS: 1.2001447677612305
LOSS: 1.2001101970672607
LOSS: 1.2000762224197388
LOSS: 1.2000432014465332
LOSS: 1.2936463356018066
LOSS: 1.2830274105072021
LOSS: 1.2728681564331055
LOSS: 1.2632390260696411
LOSS: 1.254209041595459
LOSS: 1.245842456817627
LOSS: 1.238193154335022
LOSS: 1.2312989234924316
LOSS: 1.2251760959625244
LOSS: 1.219817042350769
LOSS: 1.2151918411254883
LOSS: 1.2112574577331543
LOSS: 1.2079651355743408
LOSS: 1.2052658796310425
LOSS: 1.203109860420227
LOSS: 1.2014410495758057
LOSS: 1.2001934051513672
LOSS: 1.199288249015808
LOSS: 1.19863760471344
LOSS: 1.19814932346344
LOSS: 1.1977360248565674
LOSS: 1.1973240375518799
LOSS: 1.196859359741211
LOSS: 1.196312427520752
LOSS: 1.195676565170288
LOSS: 1.19496488571167
LOSS: 1.1942051649093628
LOSS: 1.1934306621551514
LOSS: 1.1926758289337158
LOSS: 1.191969633102417
LOSS: 1.1913325786590576
LOSS: 1.1907745599746704
LOSS: 1.1902952194213867
LOSS: 1.1898846626281738
LOSS: 1.1895267963409424
LOSS: 1.1892013549804688
LOSS: 1.1888874769210815
LOSS: 1.1885664463043213
LOSS: 1.1882222890853882
LOSS: 1.18784499168396
LOSS: 1.1874288320541382
LOSS: 1.1869735717773438
LOSS: 1.1864838600158691
LOSS: 1.1859679222106934
LOSS: 1.1854360103607178
LOSS: 1.1849006414413452
LOSS: 1.1843740940093994
LOSS: 1.1838676929473877
LOSS: 1.18339204788208
LOSS: 1.1829545497894287
LOSS: 1.182560682296753
LOSS: 1.1822127103805542
LOSS: 1.1819099187850952
LOSS: 1.1816505193710327
LOSS: 1.1814287900924683
LOSS: 1.1812392473220825
LOSS: 1.1810753345489502
LOSS: 1.180930256843567
LOSS: 1.1807981729507446
LOSS: 1.1806739568710327
LOSS: 1.180553913116455
LOSS: 1.1804356575012207
LOSS: 1.1803178787231445
LOSS: 1.1802005767822266
LOSS: 1.180084466934204
LOSS: 1.1799708604812622
LOSS: 1.179861068725586
LOSS: 1.1797562837600708
LOSS: 1.1796579360961914
LOSS: 1.179566740989685
LOSS: 1.1794836521148682
LOSS: 1.1794078350067139
LOSS: 1.1793394088745117
LOSS: 1.179277777671814
LOSS: 1.1792223453521729
LOSS: 1.1791718006134033
LOSS: 1.1791259050369263
LOSS: 1.1790835857391357
LOSS: 1.1790440082550049
LOSS: 1.1790070533752441
LOSS: 1.178971767425537
LOSS: 1.1789377927780151
LOSS: 1.1789050102233887
LOSS: 1.1788725852966309
LOSS: 1.1788409948349
LOSS: 1.1788097620010376
LOSS: 1.1787784099578857
LOSS: 1.1787476539611816
LOSS: 1.178716778755188
LOSS: 1.1786866188049316
LOSS: 1.1786565780639648
LOSS: 1.1786274909973145
LOSS: 1.178599238395691
LOSS: 1.1785719394683838
LOSS: 1.1785457134246826
LOSS: 1.1785204410552979
LOSS: 1.1784965991973877
LOSS: 1.1784733533859253
LOSS: 1.1784512996673584
LOSS: 1.1784298419952393
LOSS: 1.2985174655914307
LOSS: 1.2880785465240479
LOSS: 1.278102159500122
LOSS: 1.268654465675354
LOSS: 1.259799838066101
LOSS: 1.2515970468521118
LOSS: 1.2440932989120483
LOSS: 1.2373188734054565
LOSS: 1.2312815189361572
LOSS: 1.2259656190872192
LOSS: 1.2213389873504639
LOSS: 1.2173634767532349
LOSS: 1.214002251625061
LOSS: 1.211221694946289
LOSS: 1.208984613418579
LOSS: 1.2072445154190063
LOSS: 1.2059389352798462
LOSS: 1.2049905061721802
LOSS: 1.2043098211288452
LOSS: 1.2038030624389648
LOSS: 1.2033817768096924
LOSS: 1.2029714584350586
LOSS: 1.202519416809082
LOSS: 1.2019968032836914
LOSS: 1.2013983726501465
LOSS: 1.200737476348877
LOSS: 1.2000411748886108
LOSS: 1.199341893196106
LOSS: 1.1986724138259888
LOSS: 1.198060393333435
LOSS: 1.1975252628326416
LOSS: 1.1970763206481934
LOSS: 1.1967133283615112
LOSS: 1.196426272392273
LOSS: 1.196199893951416
LOSS: 1.196014165878296
LOSS: 1.1958481073379517
LOSS: 1.1956827640533447
LOSS: 1.1955019235610962
LOSS: 1.1952937841415405
LOSS: 1.1950522661209106
LOSS: 1.1947762966156006
LOSS: 1.1944694519042969
LOSS: 1.1941379308700562
LOSS: 1.193791389465332
LOSS: 1.19343900680542
LOSS: 1.1930911540985107
LOSS: 1.1927564144134521
LOSS: 1.1924413442611694
LOSS: 1.1921510696411133
LOSS: 1.1918882131576538
LOSS: 1.1916522979736328
LOSS: 1.1914417743682861
LOSS: 1.1912537813186646
LOSS: 1.191084384918213
LOSS: 1.1909294128417969
LOSS: 1.1907849311828613
LOSS: 1.1906476020812988
LOSS: 1.190515398979187
LOSS: 1.1903867721557617
LOSS: 1.190260648727417
LOSS: 1.1901381015777588
LOSS: 1.1900190114974976
LOSS: 1.1899045705795288
LOSS: 1.1897951364517212
LOSS: 1.1896916627883911
LOSS: 1.1895942687988281
LOSS: 1.1895027160644531
LOSS: 1.189416766166687
LOSS: 1.1893357038497925
LOSS: 1.1892589330673218
LOSS: 1.1891857385635376
LOSS: 1.189115047454834
LOSS: 1.189046859741211
LOSS: 1.1889804601669312
LOSS: 1.1889158487319946
LOSS: 1.1888529062271118
LOSS: 1.1887919902801514
LOSS: 1.1887333393096924
LOSS: 1.1886769533157349
LOSS: 1.188623070716858
LOSS: 1.188571810722351
LOSS: 1.1885230541229248
LOSS: 1.1884771585464478
LOSS: 1.188433289527893
LOSS: 1.1883913278579712
LOSS: 1.1883513927459717
LOSS: 1.188312292098999
LOSS: 1.188274621963501
LOSS: 1.1882374286651611
LOSS: 1.1882011890411377
LOSS: 1.188165545463562
LOSS: 1.1881307363510132
LOSS: 1.188096523284912
LOSS: 1.1880629062652588
LOSS: 1.1880301237106323
LOSS: 1.1879981756210327
LOSS: 1.1879669427871704
LOSS: 1.187936544418335
LOSS: 1.1879067420959473
LOSS: 1.2877933979034424
LOSS: 1.2760796546936035
LOSS: 1.2646828889846802
LOSS: 1.2536580562591553
LOSS: 1.2430610656738281
LOSS: 1.2329477071762085
LOSS: 1.2233705520629883
LOSS: 1.2143778800964355
LOSS: 1.2060093879699707
LOSS: 1.1982940435409546
LOSS: 1.1912474632263184
LOSS: 1.1848715543746948
LOSS: 1.179155707359314
LOSS: 1.1740800142288208
LOSS: 1.1696183681488037
LOSS: 1.1657390594482422
LOSS: 1.1624040603637695
LOSS: 1.1595659255981445
LOSS: 1.157166838645935
LOSS: 1.1551374197006226
LOSS: 1.153400182723999
LOSS: 1.1518751382827759
LOSS: 1.1504864692687988
LOSS: 1.1491692066192627
LOSS: 1.1478753089904785
LOSS: 1.1465754508972168
LOSS: 1.1452598571777344
LOSS: 1.1439346075057983
LOSS: 1.142618179321289
LOSS: 1.1413358449935913
LOSS: 1.1401153802871704
LOSS: 1.1389822959899902
LOSS: 1.1379574537277222
LOSS: 1.1370548009872437
LOSS: 1.1362810134887695
LOSS: 1.1356345415115356
LOSS: 1.1351077556610107
LOSS: 1.1346876621246338
LOSS: 1.1343575716018677
LOSS: 1.1340991258621216
LOSS: 1.1338940858840942
LOSS: 1.1337250471115112
LOSS: 1.1335771083831787
LOSS: 1.1334383487701416
LOSS: 1.1332999467849731
LOSS: 1.133156657218933
LOSS: 1.1330060958862305
LOSS: 1.1328486204147339
LOSS: 1.1326860189437866
LOSS: 1.1325215101242065
LOSS: 1.1323591470718384
LOSS: 1.132202386856079
LOSS: 1.1320544481277466
LOSS: 1.1319178342819214
LOSS: 1.1317936182022095
LOSS: 1.1316814422607422
LOSS: 1.1315799951553345
LOSS: 1.1314880847930908
LOSS: 1.1314029693603516
LOSS: 1.131321668624878
LOSS: 1.1312419176101685
LOSS: 1.1311613321304321
LOSS: 1.1310786008834839
LOSS: 1.130993366241455
LOSS: 1.1309044361114502
LOSS: 1.1308135986328125
LOSS: 1.1307213306427002
LOSS: 1.1306287050247192
LOSS: 1.1305378675460815
LOSS: 1.1304495334625244
LOSS: 1.130365252494812
LOSS: 1.130285620689392
LOSS: 1.1302109956741333
LOSS: 1.1301413774490356
LOSS: 1.1300761699676514
LOSS: 1.1300153732299805
LOSS: 1.129957675933838
LOSS: 1.1299030780792236
LOSS: 1.1298503875732422
LOSS: 1.1297988891601562
LOSS: 1.1297489404678345
LOSS: 1.129699945449829
LOSS: 1.129651427268982
LOSS: 1.1296038627624512
LOSS: 1.1295573711395264
LOSS: 1.129512071609497
LOSS: 1.1294677257537842
LOSS: 1.129424810409546
LOSS: 1.1293833255767822
LOSS: 1.1293432712554932
LOSS: 1.1293041706085205
LOSS: 1.129266381263733
LOSS: 1.1292293071746826
LOSS: 1.1291931867599487
LOSS: 1.129157304763794
LOSS: 1.129122018814087
LOSS: 1.129087209701538
LOSS: 1.1290526390075684
LOSS: 1.1290184259414673
LOSS: 1.1289843320846558
LOSS: 1.293118953704834
LOSS: 1.2824419736862183
LOSS: 1.2721832990646362
LOSS: 1.2624050378799438
LOSS: 1.2531694173812866
LOSS: 1.244537115097046
LOSS: 1.2365630865097046
LOSS: 1.229293942451477
LOSS: 1.2227628231048584
LOSS: 1.2169855833053589
LOSS: 1.2119585275650024
LOSS: 1.2143778800964355
LOSS: 1.2060093879699707
LOSS: 1.1982940435409546
LOSS: 1.1912474632263184
LOSS: 1.1848715543746948
LOSS: 1.179155707359314
LOSS: 1.1740800142288208
LOSS: 1.1696183681488037
LOSS: 1.1657390594482422
LOSS: 1.1624040603637695
LOSS: 1.1595659255981445
LOSS: 1.157166838645935
LOSS: 1.1551374197006226
LOSS: 1.153400182723999
LOSS: 1.1518751382827759
LOSS: 1.1504864692687988
LOSS: 1.1491692066192627
LOSS: 1.1478753089904785
LOSS: 1.1465754508972168
LOSS: 1.1452598571777344
LOSS: 1.1439346075057983
LOSS: 1.142618179321289
LOSS: 1.1413358449935913
LOSS: 1.1401153802871704
LOSS: 1.1389822959899902
LOSS: 1.1379574537277222
LOSS: 1.1370548009872437
LOSS: 1.1362810134887695
LOSS: 1.1356345415115356
LOSS: 1.1351077556610107
LOSS: 1.1346876621246338
LOSS: 1.1343575716018677
LOSS: 1.1340991258621216
LOSS: 1.1338940858840942
LOSS: 1.1337250471115112
LOSS: 1.1335771083831787
LOSS: 1.1334383487701416
LOSS: 1.1332999467849731
LOSS: 1.133156657218933
LOSS: 1.1330060958862305
LOSS: 1.1328486204147339
LOSS: 1.1326860189437866
LOSS: 1.1325215101242065
LOSS: 1.1323591470718384
LOSS: 1.132202386856079
LOSS: 1.1320544481277466
LOSS: 1.1319178342819214
LOSS: 1.1317936182022095
LOSS: 1.1316814422607422
LOSS: 1.1315799951553345
LOSS: 1.1314880847930908
LOSS: 1.1314029693603516
LOSS: 1.131321668624878
LOSS: 1.1312419176101685
LOSS: 1.1311613321304321
LOSS: 1.1310786008834839
LOSS: 1.130993366241455
LOSS: 1.1309044361114502
LOSS: 1.1308135986328125
LOSS: 1.1307213306427002
LOSS: 1.1306287050247192
LOSS: 1.1305378675460815
LOSS: 1.1304495334625244
LOSS: 1.130365252494812
LOSS: 1.130285620689392
LOSS: 1.1302109956741333
LOSS: 1.1301413774490356
LOSS: 1.1300761699676514
LOSS: 1.1300153732299805
LOSS: 1.129957675933838
LOSS: 1.1299030780792236
LOSS: 1.1298503875732422
LOSS: 1.1297988891601562
LOSS: 1.1297489404678345
LOSS: 1.129699945449829
LOSS: 1.129651427268982
LOSS: 1.1296038627624512
LOSS: 1.1295573711395264
LOSS: 1.129512071609497
LOSS: 1.1294677257537842
LOSS: 1.129424810409546
LOSS: 1.1293833255767822
LOSS: 1.1293432712554932
LOSS: 1.1293041706085205
LOSS: 1.129266381263733
LOSS: 1.1292293071746826
LOSS: 1.1291931867599487
LOSS: 1.129157304763794
LOSS: 1.129122018814087
LOSS: 1.129087209701538
LOSS: 1.1290526390075684
LOSS: 1.1290184259414673
LOSS: 1.1289843320846558
LOSS: 1.294985055923462
LOSS: 1.2847070693969727
LOSS: 1.274919033050537
LOSS: 1.2656850814819336
LOSS: 1.2570676803588867
LOSS: 1.2491247653961182
LOSS: 1.2419047355651855
LOSS: 1.235440731048584
LOSS: 1.2297457456588745
LOSS: 1.2248095273971558
LOSS: 1.2206015586853027
LOSS: 1.2170777320861816
LOSS: 1.2141879796981812
LOSS: 1.211879849433899
LOSS: 1.2100964784622192
LOSS: 1.2087719440460205
LOSS: 1.2078276872634888
LOSS: 1.2071735858917236
LOSS: 1.206712007522583
LOSS: 1.2063474655151367
LOSS: 1.2059956789016724
LOSS: 1.2055926322937012
LOSS: 1.2050994634628296
LOSS: 1.2045032978057861
LOSS: 1.2038145065307617
LOSS: 1.2030599117279053
LOSS: 1.2022771835327148
LOSS: 1.2015063762664795
LOSS: 1.2007837295532227
LOSS: 1.2001391649246216
LOSS: 1.1995913982391357
LOSS: 1.1991479396820068
LOSS: 1.1988062858581543
LOSS: 1.1985546350479126
LOSS: 1.1983751058578491
LOSS: 1.1982457637786865
LOSS: 1.1981444358825684
LOSS: 1.1980503797531128
LOSS: 1.1979460716247559
LOSS: 1.1978187561035156
LOSS: 1.1976615190505981
LOSS: 1.197471022605896
LOSS: 1.1972496509552002
LOSS: 1.1970022916793823
LOSS: 1.1967359781265259
LOSS: 1.1964597702026367
LOSS: 1.1961817741394043
LOSS: 1.1959099769592285
LOSS: 1.1956499814987183
LOSS: 1.1954059600830078
LOSS: 1.1951795816421509
LOSS: 1.1949710845947266
LOSS: 1.194778323173523
LOSS: 1.1945981979370117
LOSS: 1.194427251815796
LOSS: 1.1942616701126099
LOSS: 1.1940982341766357
LOSS: 1.193934440612793
LOSS: 1.1937687397003174
LOSS: 1.1936007738113403
LOSS: 1.1934304237365723
LOSS: 1.1932592391967773
LOSS: 1.193088173866272
LOSS: 1.1929190158843994
LOSS: 1.1927529573440552
LOSS: 1.1925911903381348
LOSS: 1.1924344301223755
LOSS: 1.1922823190689087
LOSS: 1.1921348571777344
LOSS: 1.1919915676116943
LOSS: 1.1918517351150513
LOSS: 1.191713809967041
LOSS: 1.1915780305862427
LOSS: 1.1914430856704712
LOSS: 1.1913093328475952
LOSS: 1.1911765336990356
LOSS: 1.191044807434082
LOSS: 1.190914511680603
LOSS: 1.190786361694336
LOSS: 1.1906607151031494
LOSS: 1.190537929534912
LOSS: 1.1904184818267822
LOSS: 1.1903029680252075
LOSS: 1.1901910305023193
LOSS: 1.1900826692581177
LOSS: 1.189978003501892
LOSS: 1.1898765563964844
LOSS: 1.1897779703140259
LOSS: 1.1896824836730957
LOSS: 1.1895893812179565
LOSS: 1.1894991397857666
LOSS: 1.189410924911499
LOSS: 1.1893248558044434
LOSS: 1.1892412900924683
LOSS: 1.1891599893569946
LOSS: 1.189081072807312
LOSS: 1.1890039443969727
LOSS: 1.1889293193817139
LOSS: 1.1888563632965088
LOSS: 1.1887855529785156
LOSS: 1.285916805267334
LOSS: 1.274188756942749
LOSS: 1.2628053426742554
LOSS: 1.2518184185028076
LOSS: 1.24127995967865
LOSS: 1.231239914894104
LOSS: 1.2217442989349365
LOSS: 1.2128314971923828
LOSS: 1.2045294046401978
LOSS: 1.1968519687652588
LOSS: 1.1897993087768555
LOSS: 1.1833600997924805
LOSS: 1.1775174140930176
LOSS: 1.1722544431686401
LOSS: 1.1675575971603394
LOSS: 1.1634153127670288
LOSS: 1.1598135232925415
LOSS: 1.1567296981811523
LOSS: 1.1541286706924438
LOSS: 1.1519609689712524
LOSS: 1.1501628160476685
LOSS: 1.14866042137146
LOSS: 1.147376537322998
LOSS: 1.1462364196777344
LOSS: 1.145175576210022
LOSS: 1.1441445350646973
LOSS: 1.1431115865707397
LOSS: 1.1420636177062988
LOSS: 1.1410030126571655
LOSS: 1.139944314956665
LOSS: 1.1389102935791016
LOSS: 1.137925624847412
LOSS: 1.1370148658752441
LOSS: 1.1361982822418213
LOSS: 1.135489821434021
LOSS: 1.1348960399627686
LOSS: 1.1344165802001953
LOSS: 1.1340446472167969
LOSS: 1.133768081665039
LOSS: 1.1335713863372803
LOSS: 1.1334365606307983
LOSS: 1.133346438407898
LOSS: 1.1332842111587524
LOSS: 1.1332354545593262
LOSS: 1.1331889629364014
LOSS: 1.1331367492675781
LOSS: 1.1330740451812744
LOSS: 1.132999062538147
LOSS: 1.1329132318496704
LOSS: 1.1328182220458984
LOSS: 1.1327189207077026
LOSS: 1.1326191425323486
LOSS: 1.1325238943099976
LOSS: 1.1324362754821777
LOSS: 1.1323597431182861
LOSS: 1.1322951316833496
LOSS: 1.1322431564331055
LOSS: 1.1322026252746582
LOSS: 1.132171630859375
LOSS: 1.132148265838623
LOSS: 1.1321289539337158
LOSS: 1.132111668586731
LOSS: 1.1320936679840088
LOSS: 1.1320734024047852
LOSS: 1.1320494413375854
LOSS: 1.132021427154541
LOSS: 1.1319897174835205
LOSS: 1.1319546699523926
LOSS: 1.131917953491211
LOSS: 1.1318800449371338
LOSS: 1.1318427324295044
LOSS: 1.131806492805481
LOSS: 1.1317723989486694
LOSS: 1.131740689277649
LOSS: 1.1317113637924194
LOSS: 1.13168466091156
LOSS: 1.131659984588623
LOSS: 1.1316365003585815
LOSS: 1.1316144466400146
LOSS: 1.131592869758606
LOSS: 1.1315715312957764
LOSS: 1.1315501928329468
LOSS: 1.1315289735794067
LOSS: 1.1315077543258667
LOSS: 1.1314866542816162
LOSS: 1.1314661502838135
LOSS: 1.1314464807510376
LOSS: 1.131427526473999
LOSS: 1.1314095258712769
LOSS: 1.1313928365707397
LOSS: 1.1313774585723877
LOSS: 1.131363034248352
LOSS: 1.131349802017212
LOSS: 1.131337285041809
LOSS: 1.1313254833221436
LOSS: 1.1313142776489258
LOSS: 1.1313036680221558
LOSS: 1.1312930583953857
LOSS: 1.131283164024353
LOSS: 1.1312732696533203
LOSS: 1.2976713180541992
LOSS: 1.287284255027771
LOSS: 1.2773559093475342
LOSS: 1.2679535150527954
LOSS: 1.2591423988342285
LOSS: 1.2509818077087402
LOSS: 1.2435191869735718
LOSS: 1.23678457736969
LOSS: 1.2307847738265991
LOSS: 1.2255041599273682
LOSS: 1.2209091186523438
LOSS: 1.2169594764709473
LOSS: 1.2136154174804688
LOSS: 1.2108395099639893
LOSS: 1.2085920572280884
LOSS: 1.2068251371383667
LOSS: 1.205477237701416
LOSS: 1.2044733762741089
LOSS: 1.2037280797958374
LOSS: 1.2031525373458862
LOSS: 1.2026633024215698
LOSS: 1.2021913528442383
LOSS: 1.201688289642334
LOSS: 1.2011287212371826
LOSS: 1.200510025024414
LOSS: 1.1998472213745117
LOSS: 1.1991667747497559
LOSS: 1.1985009908676147
LOSS: 1.197880506515503
LOSS: 1.1973304748535156
LOSS: 1.1968672275543213
LOSS: 1.1964974403381348
LOSS: 1.1962177753448486
LOSS: 1.196018099784851
LOSS: 1.1958825588226318
LOSS: 1.1957924365997314
LOSS: 1.1957287788391113
LOSS: 1.2157365083694458
LOSS: 1.2114287614822388
LOSS: 1.2077867984771729
LOSS: 1.2047597169876099
LOSS: 1.2022902965545654
LOSS: 1.2003135681152344
LOSS: 1.1987552642822266
LOSS: 1.1975314617156982
LOSS: 1.1965510845184326
LOSS: 1.1957224607467651
LOSS: 1.1949621438980103
LOSS: 1.1942018270492554
LOSS: 1.1933940649032593
LOSS: 1.1925160884857178
LOSS: 1.1915656328201294
LOSS: 1.1905585527420044
LOSS: 1.1895222663879395
LOSS: 1.1884883642196655
LOSS: 1.1874884366989136
LOSS: 1.1865488290786743
LOSS: 1.1856889724731445
LOSS: 1.184918999671936
LOSS: 1.184240460395813
LOSS: 1.1836470365524292
LOSS: 1.1831276416778564
LOSS: 1.1826666593551636
LOSS: 1.1822479963302612
LOSS: 1.1818550825119019
LOSS: 1.1814738512039185
LOSS: 1.1810932159423828
LOSS: 1.1807055473327637
LOSS: 1.1803065538406372
LOSS: 1.1798958778381348
LOSS: 1.179476022720337
LOSS: 1.1790517568588257
LOSS: 1.1786296367645264
LOSS: 1.1782162189483643
LOSS: 1.177818775177002
LOSS: 1.177443265914917
LOSS: 1.1770943403244019
LOSS: 1.1767754554748535
LOSS: 1.1764874458312988
LOSS: 1.1762299537658691
LOSS: 1.1760005950927734
LOSS: 1.1757956743240356
LOSS: 1.1756113767623901
LOSS: 1.1754428148269653
LOSS: 1.1752851009368896
LOSS: 1.1751353740692139
LOSS: 1.1749893426895142
LOSS: 1.1748454570770264
LOSS: 1.1747030019760132
LOSS: 1.1745609045028687
LOSS: 1.1744199991226196
LOSS: 1.1742817163467407
LOSS: 1.1741467714309692
LOSS: 1.1740165948867798
LOSS: 1.1738919019699097
LOSS: 1.1737734079360962
LOSS: 1.1736618280410767
LOSS: 1.1735570430755615
LOSS: 1.1734588146209717
LOSS: 1.1733661890029907
LOSS: 1.1732789278030396
LOSS: 1.1731961965560913
LOSS: 1.1731171607971191
LOSS: 1.1730412244796753
LOSS: 1.1729683876037598
LOSS: 1.1728978157043457
LOSS: 1.1728301048278809
LOSS: 1.1727643013000488
LOSS: 1.1727007627487183
LOSS: 1.1726397275924683
LOSS: 1.1725813150405884
LOSS: 1.1725249290466309
LOSS: 1.1724706888198853
LOSS: 1.1724189519882202
LOSS: 1.1723688840866089
LOSS: 1.1723203659057617
LOSS: 1.1722735166549683
LOSS: 1.1722283363342285
LOSS: 1.1721841096878052
LOSS: 1.1721410751342773
LOSS: 1.1720993518829346
LOSS: 1.172058343887329
LOSS: 1.1720186471939087
LOSS: 1.1719800233840942
LOSS: 1.1719425916671753
LOSS: 1.171906590461731
LOSS: 1.171871542930603
LOSS: 1.3028688430786133
LOSS: 1.2930055856704712
LOSS: 1.2836313247680664
LOSS: 1.2748128175735474
LOSS: 1.266614556312561
LOSS: 1.2590951919555664
LOSS: 1.252301573753357
LOSS: 1.246262788772583
LOSS: 1.2409846782684326
LOSS: 1.2364462614059448
LOSS: 1.2326040267944336
LOSS: 1.2293992042541504
LOSS: 1.2267662286758423
LOSS: 1.224636435508728
LOSS: 1.2229366302490234
LOSS: 1.2215863466262817
LOSS: 1.2204983234405518
LOSS: 1.2195820808410645
LOSS: 1.2187503576278687
LOSS: 1.2179296016693115
LOSS: 1.2170664072036743
LOSS: 1.2161310911178589
LOSS: 1.2151200771331787
LOSS: 1.2140501737594604
LOSS: 1.2129539251327515
LOSS: 1.2118725776672363
LOSS: 1.2108476161956787
LOSS: 1.2099164724349976
LOSS: 1.209106683731079
LOSS: 1.2084338665008545
LOSS: 1.2079007625579834
LOSS: 1.2074971199035645
LOSS: 1.2072023153305054
LOSS: 1.2069883346557617
LOSS: 1.2068231105804443
LOSS: 1.2066760063171387
LOSS: 1.2065197229385376
LOSS: 1.2063348293304443
LOSS: 1.2061103582382202
LOSS: 1.2058441638946533
LOSS: 1.2055407762527466
LOSS: 1.2052099704742432
LOSS: 1.204864263534546
LOSS: 1.204515814781189
LOSS: 1.2041749954223633
LOSS: 1.203848958015442
LOSS: 1.2035411596298218
LOSS: 1.2032510042190552
LOSS: 1.2029750347137451
LOSS: 1.2027075290679932
LOSS: 1.2024418115615845
LOSS: 1.2021716833114624
LOSS: 1.2018924951553345
LOSS: 1.201601266860962
LOSS: 1.2012969255447388
LOSS: 1.2009812593460083
LOSS: 1.200656771659851
LOSS: 1.2003278732299805
LOSS: 1.1999987363815308
LOSS: 1.1996731758117676
LOSS: 1.199354648590088
LOSS: 1.199045181274414
LOSS: 1.198745846748352
LOSS: 1.198456883430481
LOSS: 1.1981768608093262
LOSS: 1.1979047060012817
LOSS: 1.1976386308670044
LOSS: 1.1973776817321777
LOSS: 1.1971209049224854
LOSS: 1.1968685388565063
LOSS: 1.196620225906372
LOSS: 1.1963770389556885
LOSS: 1.1961400508880615
LOSS: 1.195910096168518
LOSS: 1.1956877708435059
LOSS: 1.195473313331604
LOSS: 1.195266604423523
LOSS: 1.1950674057006836
LOSS: 1.1948745250701904
LOSS: 1.1946865320205688
LOSS: 1.1945034265518188
LOSS: 1.1943243741989136
LOSS: 1.1941487789154053
LOSS: 1.1939764022827148
LOSS: 1.1938080787658691
LOSS: 1.19364333152771
LOSS: 1.193482518196106
LOSS: 1.1933258771896362
LOSS: 1.1931730508804321
LOSS: 1.1930238008499146
LOSS: 1.1928775310516357
LOSS: 1.1927340030670166
LOSS: 1.1925932168960571
LOSS: 1.19245445728302
LOSS: 1.192318320274353
LOSS: 1.192184567451477
LOSS: 1.192053198814392
LOSS: 1.1919245719909668
LOSS: 1.1917980909347534
LOSS: 1.1916738748550415
LOSS: 1.29647696018219
LOSS: 1.28584623336792
LOSS: 1.2756354808807373
LOSS: 1.2659066915512085
LOSS: 1.2567214965820312
LOSS: 1.248139500617981
LOSS: 1.2402147054672241
LOSS: 1.2329915761947632
LOSS: 1.2264996767044067
LOSS: 1.2207509279251099
LOSS: 1.2157365083694458
LOSS: 1.2114287614822388
LOSS: 1.2077867984771729
LOSS: 1.2047597169876099
LOSS: 1.2022902965545654
LOSS: 1.2003135681152344
LOSS: 1.1987552642822266
LOSS: 1.1975314617156982
LOSS: 1.1965510845184326
LOSS: 1.1957224607467651
LOSS: 1.1949621438980103
LOSS: 1.1942018270492554
LOSS: 1.1933940649032593
LOSS: 1.1925160884857178
LOSS: 1.1915656328201294
LOSS: 1.1905585527420044
LOSS: 1.1895222663879395
LOSS: 1.1884883642196655
LOSS: 1.1874884366989136
LOSS: 1.1865488290786743
LOSS: 1.1856889724731445
LOSS: 1.184918999671936
LOSS: 1.184240460395813
LOSS: 1.1836470365524292
LOSS: 1.1831276416778564
LOSS: 1.1826666593551636
LOSS: 1.1822479963302612
LOSS: 1.1818550825119019
LOSS: 1.1814738512039185
LOSS: 1.1810932159423828
LOSS: 1.1807055473327637
LOSS: 1.1803065538406372
LOSS: 1.1798958778381348
LOSS: 1.179476022720337
LOSS: 1.1790517568588257
LOSS: 1.1786296367645264
LOSS: 1.1782162189483643
LOSS: 1.177818775177002
LOSS: 1.177443265914917
LOSS: 1.1770943403244019
LOSS: 1.1767754554748535
LOSS: 1.1764874458312988
LOSS: 1.1762299537658691
LOSS: 1.1760005950927734
LOSS: 1.1757956743240356
LOSS: 1.1756113767623901
LOSS: 1.1754428148269653
LOSS: 1.1752851009368896
LOSS: 1.1751353740692139
LOSS: 1.1749893426895142
LOSS: 1.1748454570770264
LOSS: 1.1747030019760132
LOSS: 1.1745609045028687
LOSS: 1.1744199991226196
LOSS: 1.1742817163467407
LOSS: 1.1741467714309692
LOSS: 1.1740165948867798
LOSS: 1.1738919019699097
LOSS: 1.1737734079360962
LOSS: 1.1736618280410767
LOSS: 1.1735570430755615
LOSS: 1.1734588146209717
LOSS: 1.1733661890029907
LOSS: 1.1732789278030396
LOSS: 1.1731961965560913
LOSS: 1.1731171607971191
LOSS: 1.1730412244796753
LOSS: 1.1729683876037598
LOSS: 1.1728978157043457
LOSS: 1.1728301048278809
LOSS: 1.1727643013000488
LOSS: 1.1727007627487183
LOSS: 1.1726397275924683
LOSS: 1.1725813150405884
LOSS: 1.1725249290466309
LOSS: 1.1724706888198853
LOSS: 1.1724189519882202
LOSS: 1.1723688840866089
LOSS: 1.1723203659057617
LOSS: 1.1722735166549683
LOSS: 1.1722283363342285
LOSS: 1.1721841096878052
LOSS: 1.1721410751342773
LOSS: 1.1720993518829346
LOSS: 1.172058343887329
LOSS: 1.1720186471939087
LOSS: 1.1719800233840942
LOSS: 1.1719425916671753
LOSS: 1.171906590461731
LOSS: 1.171871542930603
LOSS: 1.3028688430786133
LOSS: 1.2930055856704712
LOSS: 1.2836313247680664
LOSS: 1.2748128175735474
LOSS: 1.266614556312561
LOSS: 1.2590951919555664
LOSS: 1.252301573753357
LOSS: 1.246262788772583
LOSS: 1.2409846782684326
LOSS: 1.2364462614059448
LOSS: 1.2326040267944336
LOSS: 1.2293992042541504
LOSS: 1.2267662286758423
LOSS: 1.224636435508728
LOSS: 1.2229366302490234
LOSS: 1.2215863466262817
LOSS: 1.2204983234405518
LOSS: 1.2195820808410645
LOSS: 1.2187503576278687
LOSS: 1.2179296016693115
LOSS: 1.2170664072036743
LOSS: 1.2161310911178589
LOSS: 1.2151200771331787
LOSS: 1.2140501737594604
LOSS: 1.2129539251327515
LOSS: 1.2118725776672363
LOSS: 1.2108476161956787
LOSS: 1.2099164724349976
LOSS: 1.209106683731079
LOSS: 1.2084338665008545
LOSS: 1.2079007625579834
LOSS: 1.2074971199035645
LOSS: 1.2072023153305054
LOSS: 1.2069883346557617
LOSS: 1.2068231105804443
LOSS: 1.2066760063171387
LOSS: 1.2065197229385376
LOSS: 1.2063348293304443
LOSS: 1.2061103582382202
LOSS: 1.1974011659622192
LOSS: 1.197331428527832
LOSS: 1.197264313697815
LOSS: 1.1971989870071411
LOSS: 1.1971358060836792
LOSS: 1.1970739364624023
LOSS: 1.1970131397247314
LOSS: 1.1969531774520874
LOSS: 1.1968945264816284
LOSS: 1.1968365907669067
LOSS: 1.1967802047729492
LOSS: 1.1967251300811768
LOSS: 1.196671962738037
LOSS: 1.1966203451156616
LOSS: 1.1965705156326294
LOSS: 1.1965219974517822
LOSS: 1.1964753866195679
LOSS: 1.1964302062988281
LOSS: 1.1963863372802734
LOSS: 1.1963437795639038
LOSS: 1.1963022947311401
LOSS: 1.1962618827819824
LOSS: 1.1962227821350098
LOSS: 1.2985174655914307
LOSS: 1.2880785465240479
LOSS: 1.278102159500122
LOSS: 1.268654465675354
LOSS: 1.259799838066101
LOSS: 1.2515970468521118
LOSS: 1.2440932989120483
LOSS: 1.2373188734054565
LOSS: 1.2312815189361572
LOSS: 1.2259656190872192
LOSS: 1.2213389873504639
LOSS: 1.2173634767532349
LOSS: 1.214002251625061
LOSS: 1.211221694946289
LOSS: 1.208984613418579
LOSS: 1.2072445154190063
LOSS: 1.2059389352798462
LOSS: 1.2049905061721802
LOSS: 1.2043098211288452
LOSS: 1.2038030624389648
LOSS: 1.2033817768096924
LOSS: 1.2029714584350586
LOSS: 1.202519416809082
LOSS: 1.2019968032836914
LOSS: 1.2013983726501465
LOSS: 1.200737476348877
LOSS: 1.2000411748886108
LOSS: 1.199341893196106
LOSS: 1.1986724138259888
LOSS: 1.198060393333435
LOSS: 1.1975252628326416
LOSS: 1.1970763206481934
LOSS: 1.1967133283615112
LOSS: 1.196426272392273
LOSS: 1.196199893951416
LOSS: 1.196014165878296
LOSS: 1.1958481073379517
LOSS: 1.1956827640533447
LOSS: 1.1955019235610962
LOSS: 1.1952937841415405
LOSS: 1.1950522661209106
LOSS: 1.1947762966156006
LOSS: 1.1944694519042969
LOSS: 1.1941379308700562
LOSS: 1.193791389465332
LOSS: 1.19343900680542
LOSS: 1.1930911540985107
LOSS: 1.1927564144134521
LOSS: 1.1924413442611694
LOSS: 1.1921510696411133
LOSS: 1.1918882131576538
LOSS: 1.1916522979736328
LOSS: 1.1914417743682861
LOSS: 1.1912537813186646
LOSS: 1.191084384918213
LOSS: 1.1909294128417969
LOSS: 1.1907849311828613
LOSS: 1.1906476020812988
LOSS: 1.190515398979187
LOSS: 1.1903867721557617
LOSS: 1.190260648727417
LOSS: 1.1901381015777588
LOSS: 1.1900190114974976
LOSS: 1.1899045705795288
LOSS: 1.1897951364517212
LOSS: 1.1896916627883911
LOSS: 1.1895942687988281
LOSS: 1.1895027160644531
LOSS: 1.189416766166687
LOSS: 1.1893357038497925
LOSS: 1.1892589330673218
LOSS: 1.1891857385635376
LOSS: 1.189115047454834
LOSS: 1.189046859741211
LOSS: 1.1889804601669312
LOSS: 1.1889158487319946
LOSS: 1.1888529062271118
LOSS: 1.1887919902801514
LOSS: 1.1887333393096924
LOSS: 1.1886769533157349
LOSS: 1.188623070716858
LOSS: 1.188571810722351
LOSS: 1.1885230541229248
LOSS: 1.1884771585464478
LOSS: 1.188433289527893
LOSS: 1.1883913278579712
LOSS: 1.1883513927459717
LOSS: 1.188312292098999
LOSS: 1.188274621963501
LOSS: 1.1882374286651611
LOSS: 1.1882011890411377
LOSS: 1.188165545463562
LOSS: 1.1881307363510132
LOSS: 1.188096523284912
LOSS: 1.1880629062652588
LOSS: 1.1880301237106323
LOSS: 1.1879981756210327
LOSS: 1.1879669427871704
LOSS: 1.187936544418335
LOSS: 1.1879067420959473
LOSS: 1.2976713180541992
LOSS: 1.287284255027771
LOSS: 1.2773559093475342
LOSS: 1.2679535150527954
LOSS: 1.2591423988342285
LOSS: 1.2509818077087402
LOSS: 1.2435191869735718
LOSS: 1.23678457736969
LOSS: 1.2307847738265991
LOSS: 1.2255041599273682
LOSS: 1.2209091186523438
LOSS: 1.2169594764709473
LOSS: 1.2136154174804688
LOSS: 1.2108395099639893
LOSS: 1.2085920572280884
LOSS: 1.2068251371383667
LOSS: 1.205477237701416
LOSS: 1.2044733762741089
LOSS: 1.2037280797958374
LOSS: 1.2031525373458862
LOSS: 1.2026633024215698
LOSS: 1.2021913528442383
LOSS: 1.201688289642334
LOSS: 1.2011287212371826
LOSS: 1.200510025024414
LOSS: 1.1998472213745117
LOSS: 1.1991667747497559
LOSS: 1.1985009908676147
LOSS: 1.197880506515503
LOSS: 1.1973304748535156
LOSS: 1.1968672275543213
LOSS: 1.1964974403381348
LOSS: 1.1962177753448486
LOSS: 1.196018099784851
LOSS: 1.1958825588226318
LOSS: 1.1957924365997314
LOSS: 1.1957287788391113
LOSS: 1.1956748962402344
LOSS: 1.1956167221069336
LOSS: 1.1955442428588867
LOSS: 1.1954524517059326
LOSS: 1.195339322090149
LOSS: 1.1952073574066162
LOSS: 1.1950603723526
LOSS: 1.1949046850204468
LOSS: 1.1947466135025024
LOSS: 1.1945921182632446
LOSS: 1.1944465637207031
LOSS: 1.1943128108978271
LOSS: 1.194192886352539
LOSS: 1.194087028503418
LOSS: 1.1939929723739624
LOSS: 1.193908929824829
LOSS: 1.193831205368042
LOSS: 1.1937570571899414
LOSS: 1.1936838626861572
LOSS: 1.1936089992523193
LOSS: 1.1935317516326904
LOSS: 1.1934518814086914
LOSS: 1.1933698654174805
LOSS: 1.1932871341705322
LOSS: 1.1932048797607422
LOSS: 1.193124532699585
LOSS: 1.1930471658706665
LOSS: 1.1929737329483032
LOSS: 1.192903995513916
LOSS: 1.1928378343582153
LOSS: 1.1927745342254639
LOSS: 1.1927130222320557
LOSS: 1.1926524639129639
LOSS: 1.192591667175293
LOSS: 1.1925303936004639
LOSS: 1.1924680471420288
LOSS: 1.1924042701721191
LOSS: 1.1923398971557617
LOSS: 1.1922746896743774
LOSS: 1.1922096014022827
LOSS: 1.1921449899673462
LOSS: 1.192081093788147
LOSS: 1.1920183897018433
LOSS: 1.191956877708435
LOSS: 1.1918964385986328
LOSS: 1.191836953163147
LOSS: 1.1917780637741089
LOSS: 1.1917190551757812
LOSS: 1.1916605234146118
LOSS: 1.1916011571884155
LOSS: 1.1915411949157715
LOSS: 1.1914807558059692
LOSS: 1.1914193630218506
LOSS: 1.1913576126098633
LOSS: 1.1912953853607178
LOSS: 1.1912330389022827
LOSS: 1.1911704540252686
LOSS: 1.1911078691482544
LOSS: 1.1910454034805298
LOSS: 1.1909830570220947
LOSS: 1.1909207105636597
LOSS: 1.1908587217330933
LOSS: 1.1907966136932373
LOSS: 1.293118953704834
LOSS: 1.2824419736862183
LOSS: 1.2721832990646362
LOSS: 1.2624050378799438
LOSS: 1.2531694173812866
LOSS: 1.244537115097046
LOSS: 1.2365630865097046
LOSS: 1.229293942451477
LOSS: 1.2227628231048584
LOSS: 1.2169855833053589
LOSS: 1.2119585275650024
LOSS: 1.207658290863037
LOSS: 1.204045057296753
LOSS: 1.2010654211044312
LOSS: 1.1986563205718994
LOSS: 1.1967437267303467
LOSS: 1.1952427625656128
LOSS: 1.194059133529663
LOSS: 1.1930928230285645
LOSS: 1.1922448873519897
LOSS: 1.19142746925354
LOSS: 1.1905721426010132
LOSS: 1.189634919166565
LOSS: 1.188597321510315
LOSS: 1.1874650716781616
LOSS: 1.1862620115280151
LOSS: 1.1850231885910034
LOSS: 1.183788776397705
LOSS: 1.1825973987579346
LOSS: 1.1814826726913452
LOSS: 1.1804698705673218
LOSS: 1.1795743703842163
LOSS: 1.17880117893219
LOSS: 1.178147554397583
LOSS: 1.1776025295257568
LOSS: 1.177151083946228
LOSS: 1.1767741441726685
LOSS: 1.17645263671875
LOSS: 1.1761690378189087
LOSS: 1.175907015800476
LOSS: 1.1756547689437866
LOSS: 1.1754039525985718
LOSS: 1.175149917602539
LOSS: 1.1748918294906616
LOSS: 1.1746314764022827
LOSS: 1.1743733882904053
LOSS: 1.1741220951080322
LOSS: 1.1738834381103516
LOSS: 1.1736619472503662
LOSS: 1.1734620332717896
LOSS: 1.1732860803604126
LOSS: 1.173134684562683
LOSS: 1.173006534576416
LOSS: 1.1728994846343994
LOSS: 1.1728097200393677
LOSS: 1.1727325916290283
LOSS: 1.1726644039154053
LOSS: 1.172600269317627
LOSS: 1.1725373268127441
LOSS: 1.1724733114242554
LOSS: 1.172406554222107
LOSS: 1.1723368167877197
LOSS: 1.1722652912139893
LOSS: 1.1721928119659424
LOSS: 1.1721210479736328
LOSS: 1.1720516681671143
LOSS: 1.1719859838485718
LOSS: 1.1719248294830322
LOSS: 1.171868920326233
LOSS: 1.1718186140060425
LOSS: 1.1717727184295654
LOSS: 1.1717313528060913
LOSS: 1.1716927289962769
LOSS: 1.171656608581543
LOSS: 1.171621561050415
LOSS: 1.171587586402893
LOSS: 1.1715534925460815
LOSS: 1.1715197563171387
LOSS: 1.1714859008789062
LOSS: 1.1714524030685425
LOSS: 1.1714191436767578
LOSS: 1.1713868379592896
LOSS: 1.1713558435440063
LOSS: 1.17132568359375
LOSS: 1.1712969541549683
LOSS: 1.1712698936462402
LOSS: 1.1712440252304077
LOSS: 1.1712192296981812
LOSS: 1.171195149421692
LOSS: 1.1711719036102295
LOSS: 1.1711491346359253
LOSS: 1.1711266040802002
LOSS: 1.1711045503616333
LOSS: 1.171082615852356
LOSS: 1.1710610389709473
LOSS: 1.1710399389266968
LOSS: 1.1710189580917358
LOSS: 1.1709990501403809
LOSS: 1.1709792613983154
LOSS: 1.1709601879119873
LOSS: 1.2892119884490967
LOSS: 1.278752088546753
LOSS: 1.2687238454818726
LOSS: 1.2591850757598877
LOSS: 1.2501920461654663
LOSS: 1.2417963743209839
LOSS: 1.2340410947799683
LOSS: 1.2269566059112549
LOSS: 1.207658290863037
LOSS: 1.204045057296753
LOSS: 1.2010654211044312
LOSS: 1.1986563205718994
LOSS: 1.1967437267303467
LOSS: 1.1952427625656128
LOSS: 1.194059133529663
LOSS: 1.1930928230285645
LOSS: 1.1922448873519897
LOSS: 1.19142746925354
LOSS: 1.1905721426010132
LOSS: 1.189634919166565
LOSS: 1.188597321510315
LOSS: 1.1874650716781616
LOSS: 1.1862620115280151
LOSS: 1.1850231885910034
LOSS: 1.183788776397705
LOSS: 1.1825973987579346
LOSS: 1.1814826726913452
LOSS: 1.1804698705673218
LOSS: 1.1795743703842163
LOSS: 1.17880117893219
LOSS: 1.178147554397583
LOSS: 1.1776025295257568
LOSS: 1.177151083946228
LOSS: 1.1767741441726685
LOSS: 1.17645263671875
LOSS: 1.1761690378189087
LOSS: 1.175907015800476
LOSS: 1.1756547689437866
LOSS: 1.1754039525985718
LOSS: 1.175149917602539
LOSS: 1.1748918294906616
LOSS: 1.1746314764022827
LOSS: 1.1743733882904053
LOSS: 1.1741220951080322
LOSS: 1.1738834381103516
LOSS: 1.1736619472503662
LOSS: 1.1734620332717896
LOSS: 1.1732860803604126
LOSS: 1.173134684562683
LOSS: 1.173006534576416
LOSS: 1.1728994846343994
LOSS: 1.1728097200393677
LOSS: 1.1727325916290283
LOSS: 1.1726644039154053
LOSS: 1.172600269317627
LOSS: 1.1725373268127441
LOSS: 1.1724733114242554
LOSS: 1.172406554222107
LOSS: 1.1723368167877197
LOSS: 1.1722652912139893
LOSS: 1.1721928119659424
LOSS: 1.1721210479736328
LOSS: 1.1720516681671143
LOSS: 1.1719859838485718
LOSS: 1.1719248294830322
LOSS: 1.171868920326233
LOSS: 1.1718186140060425
LOSS: 1.1717727184295654
LOSS: 1.1717313528060913
LOSS: 1.1716927289962769
LOSS: 1.171656608581543
LOSS: 1.171621561050415
LOSS: 1.171587586402893
LOSS: 1.1715534925460815
LOSS: 1.1715197563171387
LOSS: 1.1714859008789062
LOSS: 1.1714524030685425
LOSS: 1.1714191436767578
LOSS: 1.1713868379592896
LOSS: 1.1713558435440063
LOSS: 1.17132568359375
LOSS: 1.1712969541549683
LOSS: 1.1712698936462402
LOSS: 1.1712440252304077
LOSS: 1.1712192296981812
LOSS: 1.171195149421692
LOSS: 1.1711719036102295
LOSS: 1.1711491346359253
LOSS: 1.1711266040802002
LOSS: 1.1711045503616333
LOSS: 1.171082615852356
LOSS: 1.1710610389709473
LOSS: 1.1710399389266968
LOSS: 1.1710189580917358
LOSS: 1.1709990501403809
LOSS: 1.1709792613983154
LOSS: 1.1709601879119873
LOSS: 1.285916805267334
LOSS: 1.274188756942749
LOSS: 1.2628053426742554
LOSS: 1.2518184185028076
LOSS: 1.24127995967865
LOSS: 1.231239914894104
LOSS: 1.2217442989349365
LOSS: 1.2128314971923828
LOSS: 1.2045294046401978
LOSS: 1.1968519687652588
LOSS: 1.1897993087768555
LOSS: 1.1833600997924805
LOSS: 1.1775174140930176
LOSS: 1.1722544431686401
LOSS: 1.1675575971603394
LOSS: 1.1634153127670288
LOSS: 1.1598135232925415
LOSS: 1.1567296981811523
LOSS: 1.1541286706924438
LOSS: 1.1519609689712524
LOSS: 1.1501628160476685
LOSS: 1.14866042137146
LOSS: 1.147376537322998
LOSS: 1.1462364196777344
LOSS: 1.145175576210022
LOSS: 1.1441445350646973
LOSS: 1.1431115865707397
LOSS: 1.1420636177062988
LOSS: 1.1410030126571655
LOSS: 1.139944314956665
LOSS: 1.1389102935791016
LOSS: 1.137925624847412
LOSS: 1.1370148658752441
LOSS: 1.1361982822418213
LOSS: 1.135489821434021
LOSS: 1.1348960399627686
LOSS: 1.1344165802001953
LOSS: 1.1340446472167969
LOSS: 1.133768081665039
LOSS: 1.1335713863372803
LOSS: 1.1334365606307983
LOSS: 1.133346438407898
LOSS: 1.1332842111587524
LOSS: 1.1332354545593262
LOSS: 1.1331889629364014
LOSS: 1.1331367492675781
LOSS: 1.1330740451812744
LOSS: 1.132999062538147
LOSS: 1.1329132318496704
LOSS: 1.1328182220458984
LOSS: 1.1327189207077026
LOSS: 1.1326191425323486
LOSS: 1.1325238943099976
LOSS: 1.1324362754821777
LOSS: 1.1323597431182861
LOSS: 1.1322951316833496
LOSS: 1.1322431564331055
LOSS: 1.1322026252746582
LOSS: 1.132171630859375
LOSS: 1.132148265838623
LOSS: 1.1321289539337158
LOSS: 1.132111668586731
LOSS: 1.1320936679840088
LOSS: 1.1320734024047852
LOSS: 1.1320494413375854
LOSS: 1.132021427154541
LOSS: 1.1319897174835205
LOSS: 1.1319546699523926
LOSS: 1.131917953491211
LOSS: 1.1318800449371338
LOSS: 1.1318427324295044
LOSS: 1.131806492805481
LOSS: 1.1317723989486694
LOSS: 1.131740689277649
LOSS: 1.1317113637924194
LOSS: 1.13168466091156
LOSS: 1.131659984588623
LOSS: 1.1316365003585815
LOSS: 1.1316144466400146
LOSS: 1.131592869758606
LOSS: 1.1315715312957764
LOSS: 1.1315501928329468
LOSS: 1.1315289735794067
LOSS: 1.1315077543258667
LOSS: 1.1314866542816162
LOSS: 1.1314661502838135
LOSS: 1.1314464807510376
LOSS: 1.131427526473999
LOSS: 1.1314095258712769
LOSS: 1.1313928365707397
LOSS: 1.1313774585723877
LOSS: 1.131363034248352
LOSS: 1.131349802017212
LOSS: 1.131337285041809
LOSS: 1.1313254833221436
LOSS: 1.1313142776489258
LOSS: 1.1313036680221558
LOSS: 1.1312930583953857
LOSS: 1.131283164024353
LOSS: 1.1312732696533203
LOSS: 1.3108967542648315
LOSS: 1.3006268739700317
LOSS: 1.2907721996307373
LOSS: 1.2813957929611206
LOSS: 1.272560715675354
LOSS: 1.2643275260925293
LOSS: 1.2567497491836548
LOSS: 1.2498699426651
LOSS: 1.2437142133712769
LOSS: 1.2382878065109253
LOSS: 1.2335748672485352
LOSS: 1.2295414209365845
LOSS: 1.2261409759521484
LOSS: 1.2233202457427979
LOSS: 1.2210180759429932
LOSS: 1.2191646099090576
LOSS: 1.2176792621612549
LOSS: 1.2164711952209473
LOSS: 1.2154451608657837
LOSS: 1.2145109176635742
LOSS: 1.2135919332504272
LOSS: 1.2126340866088867
LOSS: 1.2116080522537231
LOSS: 1.2105093002319336
LOSS: 1.2093530893325806
LOSS: 1.208167552947998
LOSS: 1.2069883346557617
LOSS: 1.2058502435684204
LOSS: 1.2047847509384155
LOSS: 1.2038146257400513
LOSS: 1.2029536962509155
LOSS: 1.2022061347961426
LOSS: 1.2015677690505981
LOSS: 1.2010265588760376
LOSS: 1.2005661725997925
LOSS: 1.2001678943634033
LOSS: 1.1998121738433838
LOSS: 1.1994807720184326
LOSS: 1.1991586685180664
LOSS: 1.1988343000411987
LOSS: 1.1985002756118774
LOSS: 1.1981537342071533
LOSS: 1.1977949142456055
LOSS: 1.197427749633789
LOSS: 1.1970576047897339
LOSS: 1.1966915130615234
LOSS: 1.1963368654251099
LOSS: 1.1959999799728394
LOSS: 1.1956863403320312
LOSS: 1.195399522781372
LOSS: 1.1951415538787842
LOSS: 1.1949118375778198
LOSS: 1.1947087049484253
LOSS: 1.1945289373397827
LOSS: 1.1943684816360474
LOSS: 1.194223165512085
LOSS: 1.1940884590148926
LOSS: 1.193961262702942
LOSS: 1.1938390731811523
LOSS: 1.1937199831008911
LOSS: 1.193603515625
LOSS: 1.1934900283813477
LOSS: 1.1933801174163818
LOSS: 1.1932746171951294
LOSS: 1.193175196647644
LOSS: 1.1930819749832153
LOSS: 1.1929956674575806
LOSS: 1.1929163932800293
LOSS: 1.1928434371948242
LOSS: 1.1927766799926758
LOSS: 1.1927146911621094
LOSS: 1.192656397819519
LOSS: 1.1926014423370361
LOSS: 1.1925485134124756
LOSS: 1.1924967765808105
LOSS: 1.1924465894699097
LOSS: 1.1923974752426147
LOSS: 1.1923493146896362
LOSS: 1.1923025846481323
LOSS: 1.192257285118103
LOSS: 1.1922136545181274
LOSS: 1.1921720504760742
LOSS: 1.1921322345733643
LOSS: 1.1920945644378662
LOSS: 1.1920584440231323
LOSS: 1.1920239925384521
LOSS: 1.191990852355957
LOSS: 1.1919585466384888
LOSS: 1.191927433013916
LOSS: 1.1918965578079224
LOSS: 1.191866397857666
LOSS: 1.1918365955352783
LOSS: 1.191807508468628
LOSS: 1.1917787790298462
LOSS: 1.1917506456375122
LOSS: 1.191723346710205
LOSS: 1.1916966438293457
LOSS: 1.1916708946228027
LOSS: 1.1916457414627075
LOSS: 1.19162118434906
LOSS: 1.2892119884490967
LOSS: 1.278752088546753
LOSS: 1.2687238454818726
LOSS: 1.2591850757598877
LOSS: 1.2501920461654663
LOSS: 1.2417963743209839
LOSS: 1.2340410947799683
LOSS: 1.2269566059112549
LOSS: 1.2205554246902466
LOSS: 1.2148308753967285
LOSS: 1.2097597122192383
LOSS: 1.2053101062774658
LOSS: 1.2014485597610474
LOSS: 1.1981450319290161
LOSS: 1.195369839668274
LOSS: 1.193089246749878
LOSS: 1.1912577152252197
LOSS: 1.1898167133331299
LOSS: 1.1886928081512451
LOSS: 1.1878035068511963
LOSS: 1.187064528465271
LOSS: 1.1863975524902344
LOSS: 1.1857391595840454
LOSS: 1.1850457191467285
LOSS: 1.184296727180481
LOSS: 1.1834913492202759
LOSS: 1.1826459169387817
LOSS: 1.1817878484725952
LOSS: 1.1809495687484741
LOSS: 1.1801626682281494
LOSS: 1.1794542074203491
LOSS: 1.1788430213928223
LOSS: 1.1783392429351807
LOSS: 1.1779437065124512
LOSS: 1.177648901939392
LOSS: 1.177440881729126
LOSS: 1.1773014068603516
LOSS: 1.1772106885910034
LOSS: 1.177148699760437
LOSS: 1.1770977973937988
LOSS: 1.1770437955856323
LOSS: 1.1769763231277466
LOSS: 1.2058441638946533
LOSS: 1.2055407762527466
LOSS: 1.2052099704742432
LOSS: 1.204864263534546
LOSS: 1.204515814781189
LOSS: 1.2041749954223633
LOSS: 1.203848958015442
LOSS: 1.2035411596298218
LOSS: 1.2032510042190552
LOSS: 1.2029750347137451
LOSS: 1.2027075290679932
LOSS: 1.2024418115615845
LOSS: 1.2021716833114624
LOSS: 1.2018924951553345
LOSS: 1.201601266860962
LOSS: 1.2012969255447388
LOSS: 1.2009812593460083
LOSS: 1.200656771659851
LOSS: 1.2003278732299805
LOSS: 1.1999987363815308
LOSS: 1.1996731758117676
LOSS: 1.199354648590088
LOSS: 1.199045181274414
LOSS: 1.198745846748352
LOSS: 1.198456883430481
LOSS: 1.1981768608093262
LOSS: 1.1979047060012817
LOSS: 1.1976386308670044
LOSS: 1.1973776817321777
LOSS: 1.1971209049224854
LOSS: 1.1968685388565063
LOSS: 1.196620225906372
LOSS: 1.1963770389556885
LOSS: 1.1961400508880615
LOSS: 1.195910096168518
LOSS: 1.1956877708435059
LOSS: 1.195473313331604
LOSS: 1.195266604423523
LOSS: 1.1950674057006836
LOSS: 1.1948745250701904
LOSS: 1.1946865320205688
LOSS: 1.1945034265518188
LOSS: 1.1943243741989136
LOSS: 1.1941487789154053
LOSS: 1.1939764022827148
LOSS: 1.1938080787658691
LOSS: 1.19364333152771
LOSS: 1.193482518196106
LOSS: 1.1933258771896362
LOSS: 1.1931730508804321
LOSS: 1.1930238008499146
LOSS: 1.1928775310516357
LOSS: 1.1927340030670166
LOSS: 1.1925932168960571
LOSS: 1.19245445728302
LOSS: 1.192318320274353
LOSS: 1.192184567451477
LOSS: 1.192053198814392
LOSS: 1.1919245719909668
LOSS: 1.1917980909347534
LOSS: 1.1916738748550415
LOSS: 1.2938685417175293
LOSS: 1.2830662727355957
LOSS: 1.2726784944534302
LOSS: 1.2627686262130737
LOSS: 1.2533999681472778
LOSS: 1.2446328401565552
LOSS: 1.2365206480026245
LOSS: 1.2291051149368286
LOSS: 1.2224109172821045
LOSS: 1.2164429426193237
LOSS: 1.2111866474151611
LOSS: 1.2066117525100708
LOSS: 1.2026787996292114
LOSS: 1.1993451118469238
LOSS: 1.196564793586731
LOSS: 1.1942867040634155
LOSS: 1.1924492120742798
LOSS: 1.1909795999526978
LOSS: 1.189793586730957
LOSS: 1.1888017654418945
LOSS: 1.1879171133041382
LOSS: 1.187063455581665
LOSS: 1.1861823797225952
LOSS: 1.1852387189865112
LOSS: 1.1842204332351685
LOSS: 1.1831361055374146
LOSS: 1.182008981704712
LOSS: 1.1808726787567139
LOSS: 1.179762601852417
LOSS: 1.178712010383606
LOSS: 1.177748441696167
LOSS: 1.1768901348114014
LOSS: 1.176146149635315
LOSS: 1.17551589012146
LOSS: 1.1749916076660156
LOSS: 1.1745587587356567
LOSS: 1.1741995811462402
LOSS: 1.1738948822021484
LOSS: 1.1736260652542114
LOSS: 1.173377275466919
LOSS: 1.1731350421905518
LOSS: 1.1728904247283936
LOSS: 1.1726393699645996
LOSS: 1.1723800897598267
LOSS: 1.1721148490905762
LOSS: 1.171848177909851
LOSS: 1.1715859174728394
LOSS: 1.1713337898254395
LOSS: 1.1710975170135498
LOSS: 1.1708812713623047
LOSS: 1.1706875562667847
LOSS: 1.1705174446105957
LOSS: 1.1703691482543945
LOSS: 1.1702405214309692
LOSS: 1.1701267957687378
LOSS: 1.1700242757797241
LOSS: 1.1699274778366089
LOSS: 1.1698329448699951
LOSS: 1.1697371006011963
LOSS: 1.1696382761001587
LOSS: 1.1695349216461182
LOSS: 1.1694278717041016
LOSS: 1.1693181991577148
LOSS: 1.1692081689834595
LOSS: 1.16909921169281
LOSS: 1.1689941883087158
LOSS: 1.1688940525054932
LOSS: 1.1688005924224854
LOSS: 1.1687136888504028
LOSS: 1.1686344146728516
LOSS: 1.168561339378357
LOSS: 1.1684937477111816
LOSS: 1.1684306859970093
LOSS: 1.1683708429336548
LOSS: 1.1683132648468018
LOSS: 1.1682571172714233
LOSS: 1.1682014465332031
LOSS: 1.1681462526321411
LOSS: 1.1680911779403687
LOSS: 1.1680364608764648
LOSS: 1.1679824590682983
LOSS: 1.1679290533065796
LOSS: 1.1678766012191772
LOSS: 1.1678252220153809
LOSS: 1.167775273323059
LOSS: 1.1677265167236328
LOSS: 1.167678952217102
LOSS: 1.1676322221755981
LOSS: 1.1675865650177002
LOSS: 1.1675416231155396
LOSS: 1.1674973964691162
LOSS: 1.167453408241272
LOSS: 1.1674106121063232
LOSS: 1.1673678159713745
LOSS: 1.1673263311386108
LOSS: 1.1672852039337158
LOSS: 1.1672451496124268
LOSS: 1.1672061681747437
LOSS: 1.1671680212020874
LOSS: 1.167130947113037
LOSS: 1.2941234111785889
LOSS: 1.2834584712982178
LOSS: 1.2732837200164795
LOSS: 1.2636691331863403
LOSS: 1.2546833753585815
LOSS: 1.246391773223877
LOSS: 1.2388511896133423
LOSS: 1.2321054935455322
LOSS: 1.2261792421340942
LOSS: 1.2210731506347656
LOSS: 1.2167630195617676
LOSS: 1.213201642036438
LOSS: 1.2103251218795776
LOSS: 1.2080602645874023
LOSS: 1.2063285112380981
LOSS: 1.205046534538269
LOSS: 1.2041269540786743
LOSS: 1.2034778594970703
LOSS: 1.2030061483383179
LOSS: 1.202624797821045
LOSS: 1.2022591829299927
LOSS: 1.2018535137176514
LOSS: 1.201374888420105
LOSS: 1.200812578201294
LOSS: 1.200175404548645
LOSS: 1.1994856595993042
LOSS: 1.1987725496292114
LOSS: 1.198066234588623
LOSS: 1.1973931789398193
LOSS: 1.1967720985412598
LOSS: 1.196213722229004
LOSS: 1.195719838142395
LOSS: 1.195285439491272
LOSS: 1.1949005126953125
LOSS: 1.1945515871047974
LOSS: 1.19422447681427
LOSS: 1.1939057111740112
LOSS: 1.1935842037200928
LOSS: 1.193251609802246
LOSS: 1.1929036378860474
LOSS: 1.1925381422042847
LOSS: 1.1921583414077759
LOSS: 1.1917685270309448
LOSS: 1.1913753747940063
LOSS: 1.1909865140914917
LOSS: 1.1906101703643799
LOSS: 1.190253496170044
LOSS: 1.1899217367172241
LOSS: 1.1896190643310547
LOSS: 1.1893470287322998
LOSS: 1.1891053915023804
LOSS: 1.1888916492462158
LOSS: 1.1887024641036987
LOSS: 1.188532829284668
LOSS: 1.1883782148361206
LOSS: 1.188233733177185
LOSS: 1.1880954504013062
LOSS: 1.1879602670669556
LOSS: 1.1878260374069214
LOSS: 1.1876922845840454
LOSS: 1.1875585317611694
LOSS: 1.187425971031189
LOSS: 1.1872962713241577
LOSS: 1.1871706247329712
LOSS: 1.187050461769104
LOSS: 1.1869372129440308
LOSS: 1.1868314743041992
LOSS: 1.1867334842681885
LOSS: 1.1866430044174194
LOSS: 1.1865599155426025
LOSS: 1.1864829063415527
LOSS: 1.1864114999771118
LOSS: 1.1863441467285156
LOSS: 1.186280608177185
LOSS: 1.1862202882766724
LOSS: 1.1861624717712402
LOSS: 1.1861069202423096
LOSS: 1.1860538721084595
LOSS: 1.18600332736969
LOSS: 1.1859554052352905
LOSS: 1.1859101057052612
LOSS: 1.185867428779602
LOSS: 1.1858274936676025
LOSS: 1.1857901811599731
LOSS: 1.1857551336288452
LOSS: 1.1857218742370605
LOSS: 1.1856905221939087
LOSS: 1.1856606006622314
LOSS: 1.1856321096420288
LOSS: 1.1856043338775635
LOSS: 1.1855775117874146
LOSS: 1.1855515241622925
LOSS: 1.1855264902114868
LOSS: 1.185502529144287
LOSS: 1.1854792833328247
LOSS: 1.1854573488235474
LOSS: 1.1854362487792969
LOSS: 1.185416340827942
LOSS: 1.1853973865509033
LOSS: 1.1853793859481812
LOSS: 1.2938685417175293
LOSS: 1.2830662727355957
LOSS: 1.2726784944534302
LOSS: 1.2627686262130737
LOSS: 1.2533999681472778
LOSS: 1.2446328401565552
LOSS: 1.2365206480026245
LOSS: 1.2291051149368286
LOSS: 1.2224109172821045
LOSS: 1.2164429426193237
LOSS: 1.2111866474151611
LOSS: 1.2066117525100708
LOSS: 1.2026787996292114
LOSS: 1.1993451118469238
LOSS: 1.196564793586731
LOSS: 1.1942867040634155
LOSS: 1.1924492120742798
LOSS: 1.1909795999526978
LOSS: 1.189793586730957
LOSS: 1.1888017654418945
LOSS: 1.1879171133041382
LOSS: 1.187063455581665
LOSS: 1.1861823797225952
LOSS: 1.1852387189865112
LOSS: 1.1842204332351685
LOSS: 1.1831361055374146
LOSS: 1.182008981704712
LOSS: 1.1808726787567139
LOSS: 1.179762601852417
LOSS: 1.178712010383606
LOSS: 1.177748441696167
LOSS: 1.1768901348114014
LOSS: 1.176146149635315
LOSS: 1.17551589012146
LOSS: 1.1749916076660156
LOSS: 1.1745587587356567
LOSS: 1.1741995811462402
LOSS: 1.1738948822021484
LOSS: 1.1736260652542114
LOSS: 1.173377275466919
LOSS: 1.1731350421905518
LOSS: 1.1728904247283936
LOSS: 1.1726393699645996
LOSS: 1.1723800897598267
LOSS: 1.1721148490905762
LOSS: 1.171848177909851
LOSS: 1.1715859174728394
LOSS: 1.1713337898254395
LOSS: 1.1710975170135498
LOSS: 1.1708812713623047
LOSS: 1.1706875562667847
LOSS: 1.1705174446105957
LOSS: 1.1703691482543945
LOSS: 1.1702405214309692
LOSS: 1.1701267957687378
LOSS: 1.1700242757797241
LOSS: 1.1699274778366089
LOSS: 1.1698329448699951
LOSS: 1.1697371006011963
LOSS: 1.1696382761001587
LOSS: 1.1695349216461182
LOSS: 1.1694278717041016
LOSS: 1.1693181991577148
LOSS: 1.1692081689834595
LOSS: 1.16909921169281
LOSS: 1.1689941883087158
LOSS: 1.1688940525054932
LOSS: 1.1688005924224854
LOSS: 1.1687136888504028
LOSS: 1.1956748962402344
LOSS: 1.1956167221069336
LOSS: 1.1955442428588867
LOSS: 1.1954524517059326
LOSS: 1.195339322090149
LOSS: 1.1952073574066162
LOSS: 1.1950603723526
LOSS: 1.1949046850204468
LOSS: 1.1947466135025024
LOSS: 1.1945921182632446
LOSS: 1.1944465637207031
LOSS: 1.1943128108978271
LOSS: 1.194192886352539
LOSS: 1.194087028503418
LOSS: 1.1939929723739624
LOSS: 1.193908929824829
LOSS: 1.193831205368042
LOSS: 1.1937570571899414
LOSS: 1.1936838626861572
LOSS: 1.1936089992523193
LOSS: 1.1935317516326904
LOSS: 1.1934518814086914
LOSS: 1.1933698654174805
LOSS: 1.1932871341705322
LOSS: 1.1932048797607422
LOSS: 1.193124532699585
LOSS: 1.1930471658706665
LOSS: 1.1929737329483032
LOSS: 1.192903995513916
LOSS: 1.1928378343582153
LOSS: 1.1927745342254639
LOSS: 1.1927130222320557
LOSS: 1.1926524639129639
LOSS: 1.192591667175293
LOSS: 1.1925303936004639
LOSS: 1.1924680471420288
LOSS: 1.1924042701721191
LOSS: 1.1923398971557617
LOSS: 1.1922746896743774
LOSS: 1.1922096014022827
LOSS: 1.1921449899673462
LOSS: 1.192081093788147
LOSS: 1.1920183897018433
LOSS: 1.191956877708435
LOSS: 1.1918964385986328
LOSS: 1.191836953163147
LOSS: 1.1917780637741089
LOSS: 1.1917190551757812
LOSS: 1.1916605234146118
LOSS: 1.1916011571884155
LOSS: 1.1915411949157715
LOSS: 1.1914807558059692
LOSS: 1.1914193630218506
LOSS: 1.1913576126098633
LOSS: 1.1912953853607178
LOSS: 1.1912330389022827
LOSS: 1.1911704540252686
LOSS: 1.1911078691482544
LOSS: 1.1910454034805298
LOSS: 1.1909830570220947
LOSS: 1.1909207105636597
LOSS: 1.1908587217330933
LOSS: 1.1907966136932373
LOSS: 1.2915199995040894
LOSS: 1.28067946434021
LOSS: 1.2702480554580688
LOSS: 1.2602819204330444
LOSS: 1.250836730003357
LOSS: 1.2419646978378296
LOSS: 1.233711838722229
LOSS: 1.2261137962341309
LOSS: 1.2191920280456543
LOSS: 1.2129501104354858
LOSS: 1.2073746919631958
LOSS: 1.202438235282898
LOSS: 1.1981064081192017
LOSS: 1.1943426132202148
LOSS: 1.191110372543335
LOSS: 1.1883699893951416
LOSS: 1.1860746145248413
LOSS: 1.1841665506362915
LOSS: 1.1825767755508423
LOSS: 1.1812275648117065
LOSS: 1.180038571357727
LOSS: 1.1789336204528809
LOSS: 1.1778490543365479
LOSS: 1.1767394542694092
LOSS: 1.175579309463501
LOSS: 1.1743627786636353
LOSS: 1.1731011867523193
LOSS: 1.171817660331726
LOSS: 1.170541524887085
LOSS: 1.1693044900894165
LOSS: 1.168134331703186
LOSS: 1.1670535802841187
LOSS: 1.1660776138305664
LOSS: 1.165212631225586
LOSS: 1.1644576787948608
LOSS: 1.1638048887252808
LOSS: 1.1632415056228638
LOSS: 1.162751317024231
LOSS: 1.1623173952102661
LOSS: 1.1619224548339844
LOSS: 1.1615519523620605
LOSS: 1.161193609237671
LOSS: 1.1608381271362305
LOSS: 1.1604806184768677
LOSS: 1.1601189374923706
LOSS: 1.1597530841827393
LOSS: 1.1593866348266602
LOSS: 1.1590231657028198
LOSS: 1.1586681604385376
LOSS: 1.158326268196106
LOSS: 1.1580017805099487
LOSS: 1.1576985120773315
LOSS: 1.1574172973632812
LOSS: 1.1571593284606934
LOSS: 1.1569240093231201
LOSS: 1.1567085981369019
LOSS: 1.1565104722976685
LOSS: 1.1563268899917603
LOSS: 1.1561541557312012
LOSS: 1.155989170074463
LOSS: 1.1558297872543335
LOSS: 1.155674695968628
LOSS: 1.1555225849151611
LOSS: 1.1553733348846436
LOSS: 1.1552274227142334
LOSS: 1.1550854444503784
LOSS: 1.1549482345581055
LOSS: 1.1548168659210205
LOSS: 1.1546915769577026
LOSS: 1.1545718908309937
LOSS: 1.1544592380523682
LOSS: 1.154351830482483
LOSS: 1.1542503833770752
LOSS: 1.1541529893875122
LOSS: 1.1540592908859253
LOSS: 1.1539692878723145
LOSS: 1.153881311416626
LOSS: 1.1537954807281494
LOSS: 1.1537115573883057
LOSS: 1.1536293029785156
LOSS: 1.1535495519638062
LOSS: 1.153471827507019
LOSS: 1.1533962488174438
LOSS: 1.1533234119415283
LOSS: 1.1532529592514038
LOSS: 1.1531853675842285
LOSS: 1.1531201601028442
LOSS: 1.1530576944351196
LOSS: 1.1529968976974487
LOSS: 1.1529383659362793
LOSS: 1.1528816223144531
LOSS: 1.1528258323669434
LOSS: 1.1527715921401978
LOSS: 1.152718424797058
LOSS: 1.1526663303375244
LOSS: 1.1526153087615967
LOSS: 1.1525650024414062
LOSS: 1.1525157690048218
LOSS: 1.1524678468704224
LOSS: 1.1524211168289185
LOSS: 1.3108967542648315
LOSS: 1.3006268739700317
LOSS: 1.2907721996307373
LOSS: 1.2813957929611206
LOSS: 1.272560715675354
LOSS: 1.2643275260925293
LOSS: 1.2567497491836548
LOSS: 1.2498699426651
LOSS: 1.2437142133712769
LOSS: 1.2382878065109253
LOSS: 1.2335748672485352
LOSS: 1.2295414209365845
LOSS: 1.2261409759521484
LOSS: 1.2233202457427979
LOSS: 1.2210180759429932
LOSS: 1.2191646099090576
LOSS: 1.2176792621612549
LOSS: 1.2164711952209473
LOSS: 1.2154451608657837
LOSS: 1.2145109176635742
LOSS: 1.2135919332504272
LOSS: 1.2126340866088867
LOSS: 1.2116080522537231
LOSS: 1.2105093002319336
LOSS: 1.2093530893325806
LOSS: 1.208167552947998
LOSS: 1.2069883346557617
LOSS: 1.2058502435684204
LOSS: 1.2047847509384155
LOSS: 1.2038146257400513
LOSS: 1.2029536962509155
LOSS: 1.2022061347961426
LOSS: 1.2015677690505981
LOSS: 1.2010265588760376
LOSS: 1.2005661725997925
LOSS: 1.2001678943634033
LOSS: 1.1998121738433838
LOSS: 1.1994807720184326
LOSS: 1.1991586685180664
LOSS: 1.1988343000411987
LOSS: 1.1985002756118774
LOSS: 1.1981537342071533
LOSS: 1.1977949142456055
LOSS: 1.197427749633789
LOSS: 1.1970576047897339
LOSS: 1.1966915130615234
LOSS: 1.1963368654251099
LOSS: 1.1959999799728394
LOSS: 1.1956863403320312
LOSS: 1.195399522781372
LOSS: 1.1951415538787842
LOSS: 1.1949118375778198
LOSS: 1.1947087049484253
LOSS: 1.1945289373397827
LOSS: 1.1943684816360474
LOSS: 1.194223165512085
LOSS: 1.1940884590148926
LOSS: 1.193961262702942
LOSS: 1.1938390731811523
LOSS: 1.1937199831008911
LOSS: 1.193603515625
LOSS: 1.1934900283813477
LOSS: 1.1933801174163818
LOSS: 1.1932746171951294
LOSS: 1.193175196647644
LOSS: 1.1930819749832153
LOSS: 1.1929956674575806
LOSS: 1.1929163932800293
LOSS: 1.1928434371948242
LOSS: 1.1927766799926758
LOSS: 1.1927146911621094
LOSS: 1.192656397819519
LOSS: 1.1926014423370361
LOSS: 1.1925485134124756
LOSS: 1.1924967765808105
LOSS: 1.1924465894699097
LOSS: 1.1923974752426147
LOSS: 1.1923493146896362
LOSS: 1.1923025846481323
LOSS: 1.192257285118103
LOSS: 1.1922136545181274
LOSS: 1.1921720504760742
LOSS: 1.1921322345733643
LOSS: 1.1920945644378662
LOSS: 1.1920584440231323
LOSS: 1.1920239925384521
LOSS: 1.191990852355957
LOSS: 1.1919585466384888
LOSS: 1.191927433013916
LOSS: 1.1918965578079224
LOSS: 1.191866397857666
LOSS: 1.1918365955352783
LOSS: 1.191807508468628
LOSS: 1.1917787790298462
LOSS: 1.1917506456375122
LOSS: 1.191723346710205
LOSS: 1.1916966438293457
LOSS: 1.1916708946228027
LOSS: 1.1916457414627075
LOSS: 1.19162118434906
LOSS: 1.3603109121322632
LOSS: 1.355654001235962
LOSS: 1.3514022827148438
LOSS: 1.3475708961486816
LOSS: 1.3441778421401978
LOSS: 1.3412129878997803
LOSS: 1.3386181592941284
LOSS: 1.3362799882888794
LOSS: 1.3340520858764648
LOSS: 1.3318027257919312
LOSS: 1.3294563293457031
LOSS: 1.327001690864563
LOSS: 1.32447350025177
LOSS: 1.32192862033844
LOSS: 1.3194241523742676
LOSS: 1.3170030117034912
LOSS: 1.314687728881836
LOSS: 1.3124793767929077
LOSS: 1.3103618621826172
LOSS: 1.308314323425293
LOSS: 1.306318759918213
LOSS: 1.304369330406189
LOSS: 1.3024728298187256
LOSS: 1.3006473779678345
LOSS: 1.2989176511764526
LOSS: 1.2973097562789917
LOSS: 1.2958452701568604
LOSS: 1.2945351600646973
LOSS: 1.2933796644210815
LOSS: 1.2923665046691895
LOSS: 1.2914749383926392
LOSS: 1.2906794548034668
LOSS: 1.2899583578109741
LOSS: 1.2892953157424927
LOSS: 1.2886821031570435
LOSS: 1.2881157398223877
LOSS: 1.2875957489013672
LOSS: 1.2871211767196655
LOSS: 1.286688208580017
LOSS: 1.286291241645813
LOSS: 1.2859221696853638
LOSS: 1.28557288646698
LOSS: 1.2852381467819214
LOSS: 1.2849138975143433
LOSS: 1.2845999002456665
LOSS: 1.2842963933944702
LOSS: 1.2840081453323364
LOSS: 1.2837374210357666
LOSS: 1.2834858894348145
LOSS: 1.2832533121109009
LOSS: 1.2830361127853394
LOSS: 1.2828309535980225
LOSS: 1.2826323509216309
LOSS: 1.2824369668960571
LOSS: 1.2822413444519043
LOSS: 1.2820440530776978
LOSS: 1.2818424701690674
LOSS: 1.2816381454467773
LOSS: 1.2814306020736694
LOSS: 1.2812222242355347
LOSS: 1.281016230583191
LOSS: 1.280816912651062
LOSS: 1.2806293964385986
LOSS: 1.2804588079452515
LOSS: 1.2803086042404175
LOSS: 1.2801791429519653
LOSS: 1.2800694704055786
LOSS: 1.2205554246902466
LOSS: 1.2148308753967285
LOSS: 1.2097597122192383
LOSS: 1.2053101062774658
LOSS: 1.2014485597610474
LOSS: 1.1981450319290161
LOSS: 1.195369839668274
LOSS: 1.193089246749878
LOSS: 1.1912577152252197
LOSS: 1.1898167133331299
LOSS: 1.1886928081512451
LOSS: 1.1878035068511963
LOSS: 1.187064528465271
LOSS: 1.1863975524902344
LOSS: 1.1857391595840454
LOSS: 1.1850457191467285
LOSS: 1.184296727180481
LOSS: 1.1834913492202759
LOSS: 1.1826459169387817
LOSS: 1.1817878484725952
LOSS: 1.1809495687484741
LOSS: 1.1801626682281494
LOSS: 1.1794542074203491
LOSS: 1.1788430213928223
LOSS: 1.1783392429351807
LOSS: 1.1779437065124512
LOSS: 1.177648901939392
LOSS: 1.177440881729126
LOSS: 1.1773014068603516
LOSS: 1.1772106885910034
LOSS: 1.177148699760437
LOSS: 1.1770977973937988
LOSS: 1.1770437955856323
LOSS: 1.1769763231277466
LOSS: 1.1768896579742432
LOSS: 1.1767812967300415
LOSS: 1.176653265953064
LOSS: 1.1765100955963135
LOSS: 1.176357388496399
LOSS: 1.1762018203735352
LOSS: 1.1760497093200684
LOSS: 1.1759072542190552
LOSS: 1.1757782697677612
LOSS: 1.1756654977798462
LOSS: 1.1755694150924683
LOSS: 1.175488829612732
LOSS: 1.1754217147827148
LOSS: 1.1753652095794678
LOSS: 1.1753157377243042
LOSS: 1.1752698421478271
LOSS: 1.175224781036377
LOSS: 1.1751781702041626
LOSS: 1.1751291751861572
LOSS: 1.1750773191452026
LOSS: 1.175022840499878
LOSS: 1.1749672889709473
LOSS: 1.1749112606048584
LOSS: 1.1748566627502441
LOSS: 1.1748039722442627
LOSS: 1.1747543811798096
LOSS: 1.1747077703475952
LOSS: 1.1746644973754883
LOSS: 1.1746244430541992
LOSS: 1.174586296081543
LOSS: 1.1745498180389404
LOSS: 1.1745144128799438
LOSS: 1.1744788885116577
LOSS: 1.174443006515503
LOSS: 1.1744067668914795
LOSS: 1.17436945438385
LOSS: 1.1743320226669312
LOSS: 1.174293875694275
LOSS: 1.174255609512329
LOSS: 1.174217700958252
LOSS: 1.1741801500320435
LOSS: 1.1741434335708618
LOSS: 1.1741070747375488
LOSS: 1.1740715503692627
LOSS: 1.1740365028381348
LOSS: 1.1740018129348755
LOSS: 1.1739674806594849
LOSS: 1.1739329099655151
LOSS: 1.1738982200622559
LOSS: 1.1738632917404175
LOSS: 1.1738277673721313
LOSS: 1.173791527748108
LOSS: 1.1737552881240845
LOSS: 1.1737186908721924
LOSS: 1.1736812591552734
LOSS: 1.1736434698104858
LOSS: 1.1736054420471191
LOSS: 1.1735670566558838
LOSS: 1.2915199995040894
LOSS: 1.28067946434021
LOSS: 1.2702480554580688
LOSS: 1.2602819204330444
LOSS: 1.250836730003357
LOSS: 1.2419646978378296
LOSS: 1.233711838722229
LOSS: 1.2261137962341309
LOSS: 1.2191920280456543
LOSS: 1.2129501104354858
LOSS: 1.2073746919631958
LOSS: 1.202438235282898
LOSS: 1.1981064081192017
LOSS: 1.1943426132202148
LOSS: 1.191110372543335
LOSS: 1.1883699893951416
LOSS: 1.1860746145248413
LOSS: 1.1841665506362915
LOSS: 1.1825767755508423
LOSS: 1.1812275648117065
LOSS: 1.180038571357727
LOSS: 1.1789336204528809
LOSS: 1.1778490543365479
LOSS: 1.1767394542694092
LOSS: 1.175579309463501
LOSS: 1.1743627786636353
LOSS: 1.1731011867523193
LOSS: 1.171817660331726
LOSS: 1.170541524887085
LOSS: 1.1693044900894165
LOSS: 1.168134331703186
LOSS: 1.1670535802841187
LOSS: 1.1660776138305664
LOSS: 1.165212631225586
LOSS: 1.1644576787948608
LOSS: 1.1638048887252808
LOSS: 1.1632415056228638
LOSS: 1.162751317024231
LOSS: 1.1623173952102661
LOSS: 1.1619224548339844
LOSS: 1.1615519523620605
LOSS: 1.161193609237671
LOSS: 1.1608381271362305
LOSS: 1.1604806184768677
LOSS: 1.1601189374923706
LOSS: 1.1597530841827393
LOSS: 1.1593866348266602
LOSS: 1.1590231657028198
LOSS: 1.1586681604385376
LOSS: 1.158326268196106
LOSS: 1.1580017805099487
LOSS: 1.1576985120773315
LOSS: 1.1574172973632812
LOSS: 1.1571593284606934
LOSS: 1.1569240093231201
LOSS: 1.1567085981369019
LOSS: 1.1565104722976685
LOSS: 1.1563268899917603
LOSS: 1.1561541557312012
LOSS: 1.155989170074463
LOSS: 1.1558297872543335
LOSS: 1.155674695968628
LOSS: 1.1555225849151611
LOSS: 1.1553733348846436
LOSS: 1.1552274227142334
LOSS: 1.1550854444503784
LOSS: 1.1549482345581055
LOSS: 1.1548168659210205
LOSS: 1.1546915769577026
LOSS: 1.1545718908309937
LOSS: 1.1544592380523682
LOSS: 1.154351830482483
LOSS: 1.1542503833770752
LOSS: 1.1541529893875122
LOSS: 1.1540592908859253
LOSS: 1.1539692878723145
LOSS: 1.153881311416626
LOSS: 1.1537954807281494
LOSS: 1.1537115573883057
LOSS: 1.1536293029785156
LOSS: 1.1535495519638062
LOSS: 1.153471827507019
LOSS: 1.1533962488174438
LOSS: 1.1533234119415283
LOSS: 1.1532529592514038
LOSS: 1.1531853675842285
LOSS: 1.1531201601028442
LOSS: 1.1530576944351196
LOSS: 1.1529968976974487
LOSS: 1.1529383659362793
LOSS: 1.1528816223144531
LOSS: 1.1528258323669434
LOSS: 1.1527715921401978
LOSS: 1.152718424797058
LOSS: 1.1526663303375244
LOSS: 1.1526153087615967
LOSS: 1.1525650024414062
LOSS: 1.1525157690048218
LOSS: 1.1524678468704224
LOSS: 1.1524211168289185
LOSS: 1.344598412513733
LOSS: 1.3391921520233154
LOSS: 1.3341917991638184
LOSS: 1.3296390771865845
LOSS: 1.325561285018921
LOSS: 1.3219642639160156
LOSS: 1.3188272714614868
LOSS: 1.316090703010559
LOSS: 1.313654899597168
LOSS: 1.3113923072814941
LOSS: 1.309181809425354
LOSS: 1.3069394826889038
LOSS: 1.3046331405639648
LOSS: 1.3022758960723877
LOSS: 1.2999070882797241
LOSS: 1.297571063041687
LOSS: 1.2952990531921387
LOSS: 1.293103814125061
LOSS: 1.290985345840454
LOSS: 1.2889355421066284
LOSS: 1.286939263343811
LOSS: 1.2849783897399902
LOSS: 1.2830374240875244
LOSS: 1.2811084985733032
LOSS: 1.2791931629180908
LOSS: 1.2773023843765259
LOSS: 1.2754541635513306
LOSS: 1.2736700773239136
LOSS: 1.2719699144363403
LOSS: 1.2703698873519897
LOSS: 1.2688775062561035
LOSS: 1.267491340637207
LOSS: 1.2662010192871094
LOSS: 1.2649900913238525
LOSS: 1.263838768005371
LOSS: 1.2627304792404175
LOSS: 1.2616528272628784
LOSS: 1.2605998516082764
LOSS: 1.2595723867416382
LOSS: 1.25857412815094
LOSS: 1.257610559463501
LOSS: 1.2566865682601929
LOSS: 1.2558038234710693
LOSS: 1.2549620866775513
LOSS: 1.2541587352752686
LOSS: 1.2533904314041138
LOSS: 1.2526549100875854
LOSS: 1.2519502639770508
LOSS: 1.2512762546539307
LOSS: 1.250636339187622
LOSS: 1.2500296831130981
LOSS: 1.249457597732544
LOSS: 1.2489209175109863
LOSS: 1.2484186887741089
LOSS: 1.2479482889175415
LOSS: 1.2475069761276245
LOSS: 1.2470921277999878
LOSS: 1.2467013597488403
LOSS: 1.2463332414627075
LOSS: 1.2459864616394043
LOSS: 1.2456624507904053
LOSS: 1.245361328125
LOSS: 1.2450824975967407
LOSS: 1.244823694229126
LOSS: 1.2445850372314453
LOSS: 1.2443628311157227
LOSS: 1.2441542148590088
LOSS: 1.2439589500427246
LOSS: 1.2437738180160522
LOSS: 1.2436007261276245
LOSS: 1.243436574935913
LOSS: 1.243283987045288
LOSS: 1.2431397438049316
LOSS: 1.2430055141448975
LOSS: 1.2428783178329468
LOSS: 1.2427573204040527
LOSS: 1.2426413297653198
LOSS: 1.2425282001495361
LOSS: 1.2424179315567017
LOSS: 1.2423104047775269
LOSS: 1.2422057390213013
LOSS: 1.2421058416366577
LOSS: 1.2420085668563843
LOSS: 1.2419167757034302
LOSS: 1.2418283224105835
LOSS: 1.2417446374893188
LOSS: 1.2416636943817139
LOSS: 1.2415850162506104
LOSS: 1.2415086030960083
LOSS: 1.2414335012435913
LOSS: 1.2413595914840698
LOSS: 1.241287350654602
LOSS: 1.241216778755188
LOSS: 1.2411476373672485
LOSS: 1.241079568862915
LOSS: 1.2410136461257935
LOSS: 1.240949034690857
LOSS: 1.2408854961395264
LOSS: 1.2408225536346436
LOSS: 1.2407606840133667
LOSS: 1.3603109121322632
LOSS: 1.355654001235962
LOSS: 1.3514022827148438
LOSS: 1.3475708961486816
LOSS: 1.3441778421401978
LOSS: 1.3412129878997803
LOSS: 1.3386181592941284
LOSS: 1.3362799882888794
LOSS: 1.3340520858764648
LOSS: 1.3318027257919312
LOSS: 1.3294563293457031
LOSS: 1.327001690864563
LOSS: 1.32447350025177
LOSS: 1.32192862033844
LOSS: 1.3194241523742676
LOSS: 1.3170030117034912
LOSS: 1.314687728881836
LOSS: 1.3124793767929077
LOSS: 1.3103618621826172
LOSS: 1.308314323425293
LOSS: 1.306318759918213
LOSS: 1.304369330406189
LOSS: 1.3024728298187256
LOSS: 1.3006473779678345
LOSS: 1.2989176511764526
LOSS: 1.2973097562789917
LOSS: 1.2958452701568604
LOSS: 1.2945351600646973
LOSS: 1.2933796644210815
LOSS: 1.2923665046691895
LOSS: 1.2914749383926392
LOSS: 1.2906794548034668
LOSS: 1.2899583578109741
LOSS: 1.2892953157424927
LOSS: 1.2886821031570435
LOSS: 1.2881157398223877
LOSS: 1.2875957489013672
LOSS: 1.2871211767196655
LOSS: 1.1768896579742432
LOSS: 1.1767812967300415
LOSS: 1.176653265953064
LOSS: 1.1765100955963135
LOSS: 1.176357388496399
LOSS: 1.1762018203735352
LOSS: 1.1760497093200684
LOSS: 1.1759072542190552
LOSS: 1.1757782697677612
LOSS: 1.1756654977798462
LOSS: 1.1755694150924683
LOSS: 1.175488829612732
LOSS: 1.1754217147827148
LOSS: 1.1753652095794678
LOSS: 1.1753157377243042
LOSS: 1.1752698421478271
LOSS: 1.175224781036377
LOSS: 1.1751781702041626
LOSS: 1.1751291751861572
LOSS: 1.1750773191452026
LOSS: 1.175022840499878
LOSS: 1.1749672889709473
LOSS: 1.1749112606048584
LOSS: 1.1748566627502441
LOSS: 1.1748039722442627
LOSS: 1.1747543811798096
LOSS: 1.1747077703475952
LOSS: 1.1746644973754883
LOSS: 1.1746244430541992
LOSS: 1.174586296081543
LOSS: 1.1745498180389404
LOSS: 1.1745144128799438
LOSS: 1.1744788885116577
LOSS: 1.174443006515503
LOSS: 1.1744067668914795
LOSS: 1.17436945438385
LOSS: 1.1743320226669312
LOSS: 1.174293875694275
LOSS: 1.174255609512329
LOSS: 1.174217700958252
LOSS: 1.1741801500320435
LOSS: 1.1741434335708618
LOSS: 1.1741070747375488
LOSS: 1.1740715503692627
LOSS: 1.1740365028381348
LOSS: 1.1740018129348755
LOSS: 1.1739674806594849
LOSS: 1.1739329099655151
LOSS: 1.1738982200622559
LOSS: 1.1738632917404175
LOSS: 1.1738277673721313
LOSS: 1.173791527748108
LOSS: 1.1737552881240845
LOSS: 1.1737186908721924
LOSS: 1.1736812591552734
LOSS: 1.1736434698104858
LOSS: 1.1736054420471191
LOSS: 1.1735670566558838
LOSS: 1.3527852296829224
LOSS: 1.3473974466323853
LOSS: 1.3423806428909302
LOSS: 1.3377764225006104
LOSS: 1.333614706993103
LOSS: 1.329901099205017
LOSS: 1.3266046047210693
LOSS: 1.3236485719680786
LOSS: 1.3209127187728882
LOSS: 1.3182638883590698
LOSS: 1.3155959844589233
LOSS: 1.3128563165664673
LOSS: 1.3100461959838867
LOSS: 1.3072017431259155
LOSS: 1.3043742179870605
LOSS: 1.3016146421432495
LOSS: 1.2989611625671387
LOSS: 1.296434760093689
LOSS: 1.2940359115600586
LOSS: 1.291749358177185
LOSS: 1.2895501852035522
LOSS: 1.2874131202697754
LOSS: 1.285319209098816
LOSS: 1.2832603454589844
LOSS: 1.2812403440475464
LOSS: 1.2792723178863525
LOSS: 1.277374267578125
LOSS: 1.275565505027771
LOSS: 1.2738615274429321
LOSS: 1.2722716331481934
LOSS: 1.270795226097107
LOSS: 1.2694239616394043
LOSS: 1.268142819404602
LOSS: 1.2669357061386108
LOSS: 1.2657889127731323
LOSS: 1.264694094657898
LOSS: 1.2636501789093018
LOSS: 1.2626606225967407
LOSS: 1.2617313861846924
LOSS: 1.2608678340911865
LOSS: 1.2600716352462769
LOSS: 1.2593406438827515
LOSS: 1.2586697340011597
LOSS: 1.2580499649047852
LOSS: 1.2574738264083862
LOSS: 1.2569341659545898
LOSS: 1.2564269304275513
LOSS: 1.255950689315796
LOSS: 1.2555058002471924
LOSS: 1.2550928592681885
LOSS: 1.254712462425232
LOSS: 1.2543624639511108
LOSS: 1.254040002822876
LOSS: 1.2537407875061035
LOSS: 1.2534599304199219
LOSS: 1.253193974494934
LOSS: 1.2529399394989014
LOSS: 1.2526971101760864
LOSS: 1.2524651288986206
LOSS: 1.2522441148757935
LOSS: 1.252034306526184
LOSS: 1.251835584640503
LOSS: 1.2516471147537231
LOSS: 1.2514675855636597
LOSS: 1.2512937784194946
LOSS: 1.2511277198791504
LOSS: 1.2509676218032837
LOSS: 1.2508143186569214
LOSS: 1.2506685256958008
LOSS: 1.2505302429199219
LOSS: 1.2504006624221802
LOSS: 1.250279188156128
LOSS: 1.2501654624938965
LOSS: 1.2500585317611694
LOSS: 1.2499586343765259
LOSS: 1.249864101409912
LOSS: 1.2497742176055908
LOSS: 1.249688982963562
LOSS: 1.2496082782745361
LOSS: 1.2495301961898804
LOSS: 1.2494553327560425
LOSS: 1.2493826150894165
LOSS: 1.2493112087249756
LOSS: 1.2492412328720093
LOSS: 1.2491730451583862
LOSS: 1.2491064071655273
LOSS: 1.2490415573120117
LOSS: 1.248978853225708
LOSS: 1.2489187717437744
LOSS: 1.248861312866211
LOSS: 1.2488051652908325
LOSS: 1.2487521171569824
LOSS: 1.2487013339996338
LOSS: 1.2486523389816284
LOSS: 1.2486045360565186
LOSS: 1.2485582828521729
LOSS: 1.2485133409500122
LOSS: 1.2484700679779053
LOSS: 1.2484272718429565
LOSS: 1.248386025428772
LOSS: 1.3501087427139282
LOSS: 1.345090627670288
LOSS: 1.3404526710510254
LOSS: 1.3362332582473755
LOSS: 1.3324596881866455
LOSS: 1.329137921333313
LOSS: 1.3262429237365723
LOSS: 1.323707938194275
LOSS: 1.3214263916015625
LOSS: 1.3192685842514038
LOSS: 1.3171154260635376
LOSS: 1.314893364906311
LOSS: 1.3125824928283691
LOSS: 1.3102095127105713
LOSS: 1.3078243732452393
LOSS: 1.3054810762405396
LOSS: 1.3032208681106567
LOSS: 1.3010690212249756
LOSS: 1.2990351915359497
LOSS: 1.2971168756484985
LOSS: 1.2953004837036133
LOSS: 1.2935667037963867
LOSS: 1.2918952703475952
LOSS: 1.2902709245681763
LOSS: 1.2886871099472046
LOSS: 1.2871448993682861
LOSS: 1.2856520414352417
LOSS: 1.2842206954956055
LOSS: 1.2828620672225952
LOSS: 1.281585931777954
LOSS: 1.2803943157196045
LOSS: 1.279283881187439
LOSS: 1.2782427072525024
LOSS: 1.2772554159164429
LOSS: 1.276304841041565
LOSS: 1.2753764390945435
LOSS: 1.2744578123092651
LOSS: 1.2735451459884644
LOSS: 1.2726384401321411
LOSS: 1.2717419862747192
LOSS: 1.2708617448806763
LOSS: 1.2700026035308838
LOSS: 1.2691682577133179
LOSS: 1.2683600187301636
LOSS: 1.2675758600234985
LOSS: 1.2668148279190063
LOSS: 1.2660751342773438
LOSS: 1.2653560638427734
LOSS: 1.264658808708191
LOSS: 1.2639858722686768
LOSS: 1.2633405923843384
LOSS: 1.2627249956130981
LOSS: 1.262141227722168
LOSS: 1.261590600013733
LOSS: 1.261071801185608
LOSS: 1.2605825662612915
LOSS: 1.2601208686828613
LOSS: 1.259684443473816
LOSS: 1.2592705488204956
LOSS: 1.2588800191879272
LOSS: 1.2585103511810303
LOSS: 1.258162260055542
LOSS: 1.2578362226486206
LOSS: 1.257529854774475
LOSS: 1.25724196434021
LOSS: 1.256971836090088
LOSS: 1.2567170858383179
LOSS: 1.2564765214920044
LOSS: 1.2562494277954102
LOSS: 1.2560336589813232
LOSS: 1.2558304071426392
LOSS: 1.2556383609771729
LOSS: 1.2554574012756348
LOSS: 1.2552863359451294
LOSS: 1.2551249265670776
LOSS: 1.254971981048584
LOSS: 1.2548266649246216
LOSS: 1.254688024520874
LOSS: 1.254555106163025
LOSS: 1.2544281482696533
LOSS: 1.2543069124221802
LOSS: 1.2541906833648682
LOSS: 1.2540793418884277
LOSS: 1.2539728879928589
LOSS: 1.2538701295852661
LOSS: 1.253771424293518
LOSS: 1.2536765336990356
LOSS: 1.2535841464996338
LOSS: 1.253495216369629
LOSS: 1.253409504890442
LOSS: 1.2533270120620728
LOSS: 1.253246545791626
LOSS: 1.2531696557998657
LOSS: 1.253095030784607
LOSS: 1.25302255153656
LOSS: 1.2529529333114624
LOSS: 1.2528852224349976
LOSS: 1.2528198957443237
LOSS: 1.2527567148208618
LOSS: 1.2526956796646118
LOSS: 1.3527852296829224
LOSS: 1.3473974466323853
LOSS: 1.3423806428909302
LOSS: 1.3377764225006104
LOSS: 1.333614706993103
LOSS: 1.329901099205017
LOSS: 1.3266046047210693
LOSS: 1.3236485719680786
LOSS: 1.3209127187728882
LOSS: 1.3182638883590698
LOSS: 1.3155959844589233
LOSS: 1.3128563165664673
LOSS: 1.3100461959838867
LOSS: 1.3072017431259155
LOSS: 1.3043742179870605
LOSS: 1.3016146421432495
LOSS: 1.2989611625671387
LOSS: 1.296434760093689
LOSS: 1.2940359115600586
LOSS: 1.291749358177185
LOSS: 1.2895501852035522
LOSS: 1.2874131202697754
LOSS: 1.285319209098816
LOSS: 1.2832603454589844
LOSS: 1.2812403440475464
LOSS: 1.2792723178863525
LOSS: 1.277374267578125
LOSS: 1.275565505027771
LOSS: 1.2738615274429321
LOSS: 1.2722716331481934
LOSS: 1.270795226097107
LOSS: 1.2694239616394043
LOSS: 1.268142819404602
LOSS: 1.2669357061386108
LOSS: 1.2657889127731323
LOSS: 1.264694094657898
LOSS: 1.2636501789093018
LOSS: 1.2626606225967407
LOSS: 1.2617313861846924
LOSS: 1.2608678340911865
LOSS: 1.2600716352462769
LOSS: 1.2593406438827515
LOSS: 1.2586697340011597
LOSS: 1.2580499649047852
LOSS: 1.2574738264083862
LOSS: 1.2569341659545898
LOSS: 1.2564269304275513
LOSS: 1.255950689315796
LOSS: 1.2555058002471924
LOSS: 1.2550928592681885
LOSS: 1.254712462425232
LOSS: 1.2543624639511108
LOSS: 1.254040002822876
LOSS: 1.2537407875061035
LOSS: 1.2534599304199219
LOSS: 1.253193974494934
LOSS: 1.2529399394989014
LOSS: 1.2526971101760864
LOSS: 1.2524651288986206
LOSS: 1.2522441148757935
LOSS: 1.252034306526184
LOSS: 1.251835584640503
LOSS: 1.2516471147537231
LOSS: 1.2514675855636597
LOSS: 1.2512937784194946
LOSS: 1.2511277198791504
LOSS: 1.2509676218032837
LOSS: 1.2508143186569214
LOSS: 1.2506685256958008
LOSS: 1.2505302429199219
LOSS: 1.2504006624221802
LOSS: 1.250279188156128
LOSS: 1.1686344146728516
LOSS: 1.168561339378357
LOSS: 1.1684937477111816
LOSS: 1.1684306859970093
LOSS: 1.1683708429336548
LOSS: 1.1683132648468018
LOSS: 1.1682571172714233
LOSS: 1.1682014465332031
LOSS: 1.1681462526321411
LOSS: 1.1680911779403687
LOSS: 1.1680364608764648
LOSS: 1.1679824590682983
LOSS: 1.1679290533065796
LOSS: 1.1678766012191772
LOSS: 1.1678252220153809
LOSS: 1.167775273323059
LOSS: 1.1677265167236328
LOSS: 1.167678952217102
LOSS: 1.1676322221755981
LOSS: 1.1675865650177002
LOSS: 1.1675416231155396
LOSS: 1.1674973964691162
LOSS: 1.167453408241272
LOSS: 1.1674106121063232
LOSS: 1.1673678159713745
LOSS: 1.1673263311386108
LOSS: 1.1672852039337158
LOSS: 1.1672451496124268
LOSS: 1.1672061681747437
LOSS: 1.1671680212020874
LOSS: 1.167130947113037
LOSS: 1.2941234111785889
LOSS: 1.2834584712982178
LOSS: 1.2732837200164795
LOSS: 1.2636691331863403
LOSS: 1.2546833753585815
LOSS: 1.246391773223877
LOSS: 1.2388511896133423
LOSS: 1.2321054935455322
LOSS: 1.2261792421340942
LOSS: 1.2210731506347656
LOSS: 1.2167630195617676
LOSS: 1.213201642036438
LOSS: 1.2103251218795776
LOSS: 1.2080602645874023
LOSS: 1.2063285112380981
LOSS: 1.205046534538269
LOSS: 1.2041269540786743
LOSS: 1.2034778594970703
LOSS: 1.2030061483383179
LOSS: 1.202624797821045
LOSS: 1.2022591829299927
LOSS: 1.2018535137176514
LOSS: 1.201374888420105
LOSS: 1.200812578201294
LOSS: 1.200175404548645
LOSS: 1.1994856595993042
LOSS: 1.1987725496292114
LOSS: 1.198066234588623
LOSS: 1.1973931789398193
LOSS: 1.1967720985412598
LOSS: 1.196213722229004
LOSS: 1.195719838142395
LOSS: 1.195285439491272
LOSS: 1.1949005126953125
LOSS: 1.1945515871047974
LOSS: 1.19422447681427
LOSS: 1.1939057111740112
LOSS: 1.1935842037200928
LOSS: 1.193251609802246
LOSS: 1.1929036378860474
LOSS: 1.1925381422042847
LOSS: 1.1921583414077759
LOSS: 1.1917685270309448
LOSS: 1.1913753747940063
LOSS: 1.1909865140914917
LOSS: 1.1906101703643799
LOSS: 1.190253496170044
LOSS: 1.1899217367172241
LOSS: 1.1896190643310547
LOSS: 1.1893470287322998
LOSS: 1.1891053915023804
LOSS: 1.1888916492462158
LOSS: 1.1887024641036987
LOSS: 1.188532829284668
LOSS: 1.1883782148361206
LOSS: 1.188233733177185
LOSS: 1.1880954504013062
LOSS: 1.1879602670669556
LOSS: 1.1878260374069214
LOSS: 1.1876922845840454
LOSS: 1.1875585317611694
LOSS: 1.187425971031189
LOSS: 1.1872962713241577
LOSS: 1.1871706247329712
LOSS: 1.187050461769104
LOSS: 1.1869372129440308
LOSS: 1.1868314743041992
LOSS: 1.1867334842681885
LOSS: 1.1866430044174194
LOSS: 1.1865599155426025
LOSS: 1.1864829063415527
LOSS: 1.1864114999771118
LOSS: 1.1863441467285156
LOSS: 1.186280608177185
LOSS: 1.1862202882766724
LOSS: 1.1861624717712402
LOSS: 1.1861069202423096
LOSS: 1.1860538721084595
LOSS: 1.18600332736969
LOSS: 1.1859554052352905
LOSS: 1.1859101057052612
LOSS: 1.185867428779602
LOSS: 1.1858274936676025
LOSS: 1.1857901811599731
LOSS: 1.1857551336288452
LOSS: 1.1857218742370605
LOSS: 1.1856905221939087
LOSS: 1.1856606006622314
LOSS: 1.1856321096420288
LOSS: 1.1856043338775635
LOSS: 1.1855775117874146
LOSS: 1.1855515241622925
LOSS: 1.1855264902114868
LOSS: 1.185502529144287
LOSS: 1.1854792833328247
LOSS: 1.1854573488235474
LOSS: 1.1854362487792969
LOSS: 1.185416340827942
LOSS: 1.1853973865509033
LOSS: 1.1853793859481812
LOSS: 1.3530642986297607
LOSS: 1.3472950458526611
LOSS: 1.3418731689453125
LOSS: 1.3368263244628906
LOSS: 1.332173466682434
LOSS: 1.327931523323059
LOSS: 1.3240951299667358
LOSS: 1.3206232786178589
LOSS: 1.317433476448059
LOSS: 1.3144117593765259
LOSS: 1.3114426136016846
LOSS: 1.3084440231323242
LOSS: 1.305387258529663
LOSS: 1.3022940158843994
LOSS: 1.299216389656067
LOSS: 1.296218752861023
LOSS: 1.2933578491210938
LOSS: 1.290674090385437
LOSS: 1.2881855964660645
LOSS: 1.2858905792236328
LOSS: 1.2837730646133423
LOSS: 1.281809687614441
LOSS: 1.2799807786941528
LOSS: 1.278272271156311
LOSS: 1.2766807079315186
LOSS: 1.2752108573913574
LOSS: 1.2738720178604126
LOSS: 1.2726733684539795
LOSS: 1.2716196775436401
LOSS: 1.270707368850708
LOSS: 1.2699230909347534
LOSS: 1.26924467086792
LOSS: 1.2686433792114258
LOSS: 1.268090844154358
LOSS: 1.267561674118042
LOSS: 1.2670388221740723
LOSS: 1.2665146589279175
LOSS: 1.2659912109375
LOSS: 1.2654752731323242
LOSS: 1.2649757862091064
LOSS: 1.2645026445388794
LOSS: 1.2640600204467773
LOSS: 1.263649344444275
LOSS: 1.2632684707641602
LOSS: 1.2629122734069824
LOSS: 1.2625755071640015
LOSS: 1.2622551918029785
LOSS: 1.2619507312774658
LOSS: 1.2616616487503052
LOSS: 1.2613903284072876
LOSS: 1.2611383199691772
LOSS: 1.2609059810638428
LOSS: 1.2606916427612305
LOSS: 1.2604926824569702
LOSS: 1.2603046894073486
LOSS: 1.2601238489151
LOSS: 1.2599432468414307
LOSS: 1.2597614526748657
LOSS: 1.25957453250885
LOSS: 1.2593797445297241
LOSS: 1.2591735124588013
LOSS: 1.2589521408081055
LOSS: 1.2587107419967651
LOSS: 1.25844407081604
LOSS: 1.2581475973129272
LOSS: 1.2578173875808716
LOSS: 1.2574526071548462
LOSS: 1.2570526599884033
LOSS: 1.2566217184066772
LOSS: 1.2561638355255127
LOSS: 1.2556862831115723
LOSS: 1.25519597530365
LOSS: 1.2547019720077515
LOSS: 1.254210114479065
LOSS: 1.2537294626235962
LOSS: 1.2532624006271362
LOSS: 1.2528164386749268
LOSS: 1.2523936033248901
LOSS: 1.2519956827163696
LOSS: 1.2516250610351562
LOSS: 1.2512818574905396
LOSS: 1.2509641647338867
LOSS: 1.250673532485962
LOSS: 1.2504054307937622
LOSS: 1.2501604557037354
LOSS: 1.249936819076538
LOSS: 1.2497318983078003
LOSS: 1.249544620513916
LOSS: 1.2493733167648315
LOSS: 1.2492176294326782
LOSS: 1.2490739822387695
LOSS: 1.24894380569458
LOSS: 1.2488240003585815
LOSS: 1.2487139701843262
LOSS: 1.2486125230789185
LOSS: 1.2485185861587524
LOSS: 1.2484309673309326
LOSS: 1.2483490705490112
LOSS: 1.2482733726501465
LOSS: 1.2482010126113892
LOSS: 1.344598412513733
LOSS: 1.3391921520233154
LOSS: 1.3341917991638184
LOSS: 1.3296390771865845
LOSS: 1.325561285018921
LOSS: 1.3219642639160156
LOSS: 1.3188272714614868
LOSS: 1.316090703010559
LOSS: 1.313654899597168
LOSS: 1.3113923072814941
LOSS: 1.309181809425354
LOSS: 1.3069394826889038
LOSS: 1.3046331405639648
LOSS: 1.3022758960723877
LOSS: 1.2999070882797241
LOSS: 1.297571063041687
LOSS: 1.2952990531921387
LOSS: 1.293103814125061
LOSS: 1.290985345840454
LOSS: 1.2889355421066284
LOSS: 1.286939263343811
LOSS: 1.2849783897399902
LOSS: 1.2830374240875244
LOSS: 1.2811084985733032
LOSS: 1.2791931629180908
LOSS: 1.2773023843765259
LOSS: 1.2754541635513306
LOSS: 1.2736700773239136
LOSS: 1.2719699144363403
LOSS: 1.2703698873519897
LOSS: 1.2688775062561035
LOSS: 1.267491340637207
LOSS: 1.2662010192871094
LOSS: 1.2649900913238525
LOSS: 1.263838768005371
LOSS: 1.2627304792404175
LOSS: 1.2616528272628784
LOSS: 1.2605998516082764
LOSS: 1.2595723867416382
LOSS: 1.25857412815094
LOSS: 1.257610559463501
LOSS: 1.2566865682601929
LOSS: 1.2558038234710693
LOSS: 1.2549620866775513
LOSS: 1.2541587352752686
LOSS: 1.2533904314041138
LOSS: 1.2526549100875854
LOSS: 1.2519502639770508
LOSS: 1.2512762546539307
LOSS: 1.250636339187622
LOSS: 1.2500296831130981
LOSS: 1.249457597732544
LOSS: 1.2489209175109863
LOSS: 1.2484186887741089
LOSS: 1.2479482889175415
LOSS: 1.2475069761276245
LOSS: 1.2470921277999878
LOSS: 1.2467013597488403
LOSS: 1.2463332414627075
LOSS: 1.2459864616394043
LOSS: 1.2456624507904053
LOSS: 1.245361328125
LOSS: 1.2450824975967407
LOSS: 1.244823694229126
LOSS: 1.2445850372314453
LOSS: 1.2443628311157227
LOSS: 1.2441542148590088
LOSS: 1.2439589500427246
LOSS: 1.2437738180160522
LOSS: 1.2436007261276245
LOSS: 1.243436574935913
LOSS: 1.243283987045288
LOSS: 1.2431397438049316
LOSS: 1.2430055141448975
LOSS: 1.2428783178329468
LOSS: 1.2427573204040527
LOSS: 1.2426413297653198
LOSS: 1.2425282001495361
LOSS: 1.2424179315567017
LOSS: 1.2423104047775269
LOSS: 1.2422057390213013
LOSS: 1.2421058416366577
LOSS: 1.2420085668563843
LOSS: 1.2419167757034302
LOSS: 1.2418283224105835
LOSS: 1.2417446374893188
LOSS: 1.2416636943817139
LOSS: 1.2415850162506104
LOSS: 1.2415086030960083
LOSS: 1.2414335012435913
LOSS: 1.2413595914840698
LOSS: 1.241287350654602
LOSS: 1.241216778755188
LOSS: 1.2411476373672485
LOSS: 1.241079568862915
LOSS: 1.2410136461257935
LOSS: 1.240949034690857
LOSS: 1.2408854961395264
LOSS: 1.2408225536346436
LOSS: 1.2407606840133667
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
LOSS: 1.2501654624938965
LOSS: 1.2500585317611694
LOSS: 1.2499586343765259
LOSS: 1.249864101409912
LOSS: 1.2497742176055908
LOSS: 1.249688982963562
LOSS: 1.2496082782745361
LOSS: 1.2495301961898804
LOSS: 1.2494553327560425
LOSS: 1.2493826150894165
LOSS: 1.2493112087249756
LOSS: 1.2492412328720093
LOSS: 1.2491730451583862
LOSS: 1.2491064071655273
LOSS: 1.2490415573120117
LOSS: 1.248978853225708
LOSS: 1.2489187717437744
LOSS: 1.248861312866211
LOSS: 1.2488051652908325
LOSS: 1.2487521171569824
LOSS: 1.2487013339996338
LOSS: 1.2486523389816284
LOSS: 1.2486045360565186
LOSS: 1.2485582828521729
LOSS: 1.2485133409500122
LOSS: 1.2484700679779053
LOSS: 1.2484272718429565
LOSS: 1.248386025428772
LOSS: 1.3501087427139282
LOSS: 1.345090627670288
LOSS: 1.3404526710510254
LOSS: 1.3362332582473755
LOSS: 1.3324596881866455
LOSS: 1.329137921333313
LOSS: 1.3262429237365723
LOSS: 1.323707938194275
LOSS: 1.3214263916015625
LOSS: 1.3192685842514038
LOSS: 1.3171154260635376
LOSS: 1.314893364906311
LOSS: 1.3125824928283691
LOSS: 1.3102095127105713
LOSS: 1.3078243732452393
LOSS: 1.3054810762405396
LOSS: 1.3032208681106567
LOSS: 1.3010690212249756
LOSS: 1.2990351915359497
LOSS: 1.2971168756484985
LOSS: 1.2953004837036133
LOSS: 1.2935667037963867
LOSS: 1.2918952703475952
LOSS: 1.2902709245681763
LOSS: 1.2886871099472046
LOSS: 1.2871448993682861
LOSS: 1.2856520414352417
LOSS: 1.2842206954956055
LOSS: 1.2828620672225952
LOSS: 1.281585931777954
LOSS: 1.2803943157196045
LOSS: 1.279283881187439
LOSS: 1.2782427072525024
LOSS: 1.2772554159164429
LOSS: 1.276304841041565
LOSS: 1.2753764390945435
LOSS: 1.2744578123092651
LOSS: 1.2735451459884644
LOSS: 1.2726384401321411
LOSS: 1.2717419862747192
LOSS: 1.2708617448806763
LOSS: 1.2700026035308838
LOSS: 1.2691682577133179
LOSS: 1.2683600187301636
LOSS: 1.2675758600234985
LOSS: 1.2668148279190063
LOSS: 1.2660751342773438
LOSS: 1.2653560638427734
LOSS: 1.264658808708191
LOSS: 1.2639858722686768
LOSS: 1.2633405923843384
LOSS: 1.2627249956130981
LOSS: 1.262141227722168
LOSS: 1.261590600013733
LOSS: 1.261071801185608
LOSS: 1.2605825662612915
LOSS: 1.2601208686828613
LOSS: 1.259684443473816
LOSS: 1.2592705488204956
LOSS: 1.2588800191879272
LOSS: 1.2585103511810303
LOSS: 1.258162260055542
LOSS: 1.2578362226486206
LOSS: 1.257529854774475
LOSS: 1.25724196434021
LOSS: 1.256971836090088
LOSS: 1.2567170858383179
LOSS: 1.2564765214920044
LOSS: 1.2562494277954102
LOSS: 1.2560336589813232
LOSS: 1.2558304071426392
LOSS: 1.2556383609771729
LOSS: 1.2554574012756348
LOSS: 1.2552863359451294
LOSS: 1.2551249265670776
LOSS: 1.254971981048584
LOSS: 1.2548266649246216
LOSS: 1.254688024520874
LOSS: 1.254555106163025
LOSS: 1.2544281482696533
LOSS: 1.2543069124221802
LOSS: 1.2541906833648682
LOSS: 1.2540793418884277
LOSS: 1.2539728879928589
LOSS: 1.2538701295852661
LOSS: 1.253771424293518
LOSS: 1.2536765336990356
LOSS: 1.2535841464996338
LOSS: 1.253495216369629
LOSS: 1.253409504890442
LOSS: 1.2533270120620728
LOSS: 1.253246545791626
LOSS: 1.2531696557998657
LOSS: 1.253095030784607
LOSS: 1.25302255153656
LOSS: 1.2529529333114624
LOSS: 1.2528852224349976
LOSS: 1.2528198957443237
LOSS: 1.2527567148208618
LOSS: 1.2526956796646118
LOSS: 1.3554649353027344
LOSS: 1.3501559495925903
LOSS: 1.3451969623565674
LOSS: 1.340628981590271
LOSS: 1.3364830017089844
LOSS: 1.332769751548767
LOSS: 1.3294649124145508
LOSS: 1.3264992237091064
LOSS: 1.3237591981887817
LOSS: 1.3211097717285156
LOSS: 1.3184340000152588
LOSS: 1.3156671524047852
LOSS: 1.3128010034561157
LOSS: 1.309870958328247
LOSS: 1.3069324493408203
LOSS: 1.3040419816970825
LOSS: 1.3012425899505615
LOSS: 1.2985590696334839
LOSS: 1.295994758605957
LOSS: 1.2935354709625244
LOSS: 1.291154384613037
LOSS: 1.288821816444397
LOSS: 1.286513090133667
LOSS: 1.2842124700546265
LOSS: 1.2819174528121948
LOSS: 1.2796366214752197
LOSS: 1.2773860692977905
LOSS: 1.27518630027771
LOSS: 1.2730542421340942
LOSS: 1.271003007888794
LOSS: 1.2690380811691284
LOSS: 1.2671557664871216
LOSS: 1.2653484344482422
LOSS: 1.2636066675186157
LOSS: 1.2619233131408691
LOSS: 1.260298252105713
LOSS: 1.2587358951568604
LOSS: 1.2572447061538696
LOSS: 1.2558344602584839
LOSS: 1.2545127868652344
LOSS: 1.2532837390899658
LOSS: 1.2521445751190186
LOSS: 1.2510895729064941
LOSS: 1.250109314918518
LOSS: 1.2491949796676636
LOSS: 1.2483378648757935
LOSS: 1.2475322484970093
LOSS: 1.2467762231826782
LOSS: 1.2460674047470093
LOSS: 1.245405912399292
LOSS: 1.2447896003723145
LOSS: 1.2442173957824707
LOSS: 1.2436853647232056
LOSS: 1.2431892156600952
LOSS: 1.2427241802215576
LOSS: 1.2422876358032227
LOSS: 1.241876482963562
LOSS: 1.241490364074707
LOSS: 1.2411280870437622
LOSS: 1.240789771080017
LOSS: 1.2404751777648926
LOSS: 1.2401812076568604
LOSS: 1.2399063110351562
LOSS: 1.2396475076675415
LOSS: 1.2394027709960938
LOSS: 1.2391700744628906
LOSS: 1.2389483451843262
LOSS: 1.238736867904663
LOSS: 1.2385363578796387
LOSS: 1.2383451461791992
LOSS: 1.2381638288497925
LOSS: 1.2379921674728394
LOSS: 1.2378276586532593
LOSS: 1.2376708984375
LOSS: 1.237520456314087
LOSS: 1.23737633228302
LOSS: 1.2372374534606934
LOSS: 1.2371047735214233
LOSS: 1.2369776964187622
LOSS: 1.2368557453155518
LOSS: 1.236737847328186
LOSS: 1.2366249561309814
LOSS: 1.236515760421753
LOSS: 1.2364094257354736
LOSS: 1.2363070249557495
LOSS: 1.2362076044082642
LOSS: 1.2361117601394653
LOSS: 1.2360183000564575
LOSS: 1.2359285354614258
LOSS: 1.2358413934707642
LOSS: 1.2357569932937622
LOSS: 1.2356752157211304
LOSS: 1.235595464706421
LOSS: 1.2355176210403442
LOSS: 1.23544180393219
LOSS: 1.2353684902191162
LOSS: 1.235296607017517
LOSS: 1.2352266311645508
LOSS: 1.2351583242416382
LOSS: 1.235092043876648
LOSS: 1.3556853532791138
LOSS: 1.3507788181304932
LOSS: 1.3462269306182861
LOSS: 1.3420703411102295
LOSS: 1.3383376598358154
LOSS: 1.3350350856781006
LOSS: 1.332130789756775
LOSS: 1.3295459747314453
LOSS: 1.3271605968475342
LOSS: 1.3248461484909058
LOSS: 1.3225064277648926
LOSS: 1.320101022720337
LOSS: 1.3176411390304565
LOSS: 1.3151713609695435
LOSS: 1.3127477169036865
LOSS: 1.3104221820831299
LOSS: 1.3082301616668701
LOSS: 1.3061866760253906
LOSS: 1.3042875528335571
LOSS: 1.3025139570236206
LOSS: 1.3008414506912231
LOSS: 1.2992511987686157
LOSS: 1.2977337837219238
LOSS: 1.2962929010391235
LOSS: 1.2949416637420654
LOSS: 1.2936997413635254
LOSS: 1.2925872802734375
LOSS: 1.2916204929351807
LOSS: 1.2908077239990234
LOSS: 1.2901458740234375
LOSS: 1.2896206378936768
LOSS: 1.289209008216858
LOSS: 1.2888829708099365
LOSS: 1.2886165380477905
LOSS: 1.2883868217468262
LOSS: 1.2881797552108765
LOSS: 1.2879880666732788
LOSS: 1.2878097295761108
LOSS: 1.2876449823379517
LOSS: 1.2874943017959595
LOSS: 1.2873555421829224
LOSS: 1.287225365638733
LOSS: 1.2870975732803345
LOSS: 1.2869688272476196
LOSS: 1.28683602809906
LOSS: 1.2866978645324707
LOSS: 1.2865567207336426
LOSS: 1.286415934562683
LOSS: 1.2862811088562012
LOSS: 1.2861555814743042
LOSS: 1.2860416173934937
LOSS: 1.2859388589859009
LOSS: 1.2858456373214722
LOSS: 1.2857586145401
LOSS: 1.285675287246704
LOSS: 1.2855926752090454
LOSS: 1.285509467124939
LOSS: 1.2854259014129639
LOSS: 1.2853420972824097
LOSS: 1.2852590084075928
LOSS: 1.2851753234863281
LOSS: 1.2850903272628784
LOSS: 1.2850019931793213
LOSS: 1.284908413887024
LOSS: 1.284806728363037
LOSS: 1.2846956253051758
LOSS: 1.2845745086669922
LOSS: 1.2844421863555908
LOSS: 1.2842999696731567
LOSS: 1.284147024154663
LOSS: 1.2839856147766113
LOSS: 1.2838163375854492
LOSS: 1.2836403846740723
LOSS: 1.283460021018982
LOSS: 1.283275842666626
LOSS: 1.2830888032913208
LOSS: 1.282900094985962
LOSS: 1.2827080488204956
LOSS: 1.2825125455856323
LOSS: 1.2823104858398438
LOSS: 1.282098412513733
LOSS: 1.2818719148635864
LOSS: 1.2816270589828491
LOSS: 1.2813584804534912
LOSS: 1.2810600996017456
LOSS: 1.2807259559631348
LOSS: 1.280351161956787
LOSS: 1.2799299955368042
LOSS: 1.2794572114944458
LOSS: 1.2789298295974731
LOSS: 1.2783490419387817
LOSS: 1.2777166366577148
LOSS: 1.2770390510559082
LOSS: 1.2763251066207886
LOSS: 1.2755861282348633
LOSS: 1.2748353481292725
LOSS: 1.2740850448608398
LOSS: 1.2733495235443115
LOSS: 1.272637128829956
LOSS: 1.2719581127166748
LOSS: 1.352192997932434
LOSS: 1.3468166589736938
LOSS: 1.3418159484863281
LOSS: 1.2799770832061768
LOSS: 1.2798957824707031
LOSS: 1.2798218727111816
LOSS: 1.279751181602478
LOSS: 1.2796801328659058
LOSS: 1.279607892036438
LOSS: 1.2795339822769165
LOSS: 1.2794585227966309
LOSS: 1.2793818712234497
LOSS: 1.2793043851852417
LOSS: 1.2792274951934814
LOSS: 1.2791510820388794
LOSS: 1.2790772914886475
LOSS: 1.2790048122406006
LOSS: 1.2789373397827148
LOSS: 1.278873085975647
LOSS: 1.2788128852844238
LOSS: 1.2787585258483887
LOSS: 1.2787082195281982
LOSS: 1.2786606550216675
LOSS: 1.278616189956665
LOSS: 1.2785733938217163
LOSS: 1.2785323858261108
LOSS: 1.2784913778305054
LOSS: 1.2784513235092163
LOSS: 1.2784106731414795
LOSS: 1.2783710956573486
LOSS: 1.27833092212677
LOSS: 1.2782925367355347
LOSS: 1.2782540321350098
LOSS: 1.2782176733016968
LOSS: 1.2781811952590942
LOSS: 1.278146505355835
LOSS: 1.3530642986297607
LOSS: 1.3472950458526611
LOSS: 1.3418731689453125
LOSS: 1.3368263244628906
LOSS: 1.332173466682434
LOSS: 1.327931523323059
LOSS: 1.3240951299667358
LOSS: 1.3206232786178589
LOSS: 1.317433476448059
LOSS: 1.3144117593765259
LOSS: 1.3114426136016846
LOSS: 1.3084440231323242
LOSS: 1.305387258529663
LOSS: 1.3022940158843994
LOSS: 1.299216389656067
LOSS: 1.296218752861023
LOSS: 1.2933578491210938
LOSS: 1.290674090385437
LOSS: 1.2881855964660645
LOSS: 1.2858905792236328
LOSS: 1.2837730646133423
LOSS: 1.281809687614441
LOSS: 1.2799807786941528
LOSS: 1.278272271156311
LOSS: 1.2766807079315186
LOSS: 1.2752108573913574
LOSS: 1.2738720178604126
LOSS: 1.2726733684539795
LOSS: 1.2716196775436401
LOSS: 1.270707368850708
LOSS: 1.2699230909347534
LOSS: 1.26924467086792
LOSS: 1.2686433792114258
LOSS: 1.268090844154358
LOSS: 1.267561674118042
LOSS: 1.2670388221740723
LOSS: 1.2665146589279175
LOSS: 1.2659912109375
LOSS: 1.2654752731323242
LOSS: 1.2649757862091064
LOSS: 1.2645026445388794
LOSS: 1.2640600204467773
LOSS: 1.263649344444275
LOSS: 1.2632684707641602
LOSS: 1.2629122734069824
LOSS: 1.2625755071640015
LOSS: 1.2622551918029785
LOSS: 1.2619507312774658
LOSS: 1.2616616487503052
LOSS: 1.2613903284072876
LOSS: 1.2611383199691772
LOSS: 1.2609059810638428
LOSS: 1.2606916427612305
LOSS: 1.2604926824569702
LOSS: 1.2603046894073486
LOSS: 1.2601238489151
LOSS: 1.2599432468414307
LOSS: 1.2597614526748657
LOSS: 1.25957453250885
LOSS: 1.2593797445297241
LOSS: 1.2591735124588013
LOSS: 1.2589521408081055
LOSS: 1.2587107419967651
LOSS: 1.25844407081604
LOSS: 1.2581475973129272
LOSS: 1.2578173875808716
LOSS: 1.2574526071548462
LOSS: 1.2570526599884033
LOSS: 1.2566217184066772
LOSS: 1.2561638355255127
LOSS: 1.2556862831115723
LOSS: 1.25519597530365
LOSS: 1.2547019720077515
LOSS: 1.254210114479065
LOSS: 1.2537294626235962
LOSS: 1.2532624006271362
LOSS: 1.2528164386749268
LOSS: 1.2523936033248901
LOSS: 1.2519956827163696
LOSS: 1.2516250610351562
LOSS: 1.2512818574905396
LOSS: 1.2509641647338867
LOSS: 1.250673532485962
LOSS: 1.2504054307937622
LOSS: 1.2501604557037354
LOSS: 1.249936819076538
LOSS: 1.2497318983078003
LOSS: 1.249544620513916
LOSS: 1.2493733167648315
LOSS: 1.2492176294326782
LOSS: 1.2490739822387695
LOSS: 1.24894380569458
LOSS: 1.2488240003585815
LOSS: 1.2487139701843262
LOSS: 1.2486125230789185
LOSS: 1.2485185861587524
LOSS: 1.2484309673309326
LOSS: 1.2483490705490112
LOSS: 1.2482733726501465
LOSS: 1.2482010126113892
LOSS: 1.344248652458191
LOSS: 1.338470697402954
LOSS: 1.333083152770996
LOSS: 1.3281331062316895
LOSS: 1.3236584663391113
LOSS: 1.3196818828582764
LOSS: 1.3162004947662354
LOSS: 1.3131740093231201
LOSS: 1.3105195760726929
LOSS: 1.3081135749816895
LOSS: 1.3058123588562012
LOSS: 1.3034842014312744
LOSS: 1.3010414838790894
LOSS: 1.2984542846679688
LOSS: 1.2957435846328735
LOSS: 1.2929638624191284
LOSS: 1.2901822328567505
LOSS: 1.2874609231948853
LOSS: 1.2848467826843262
LOSS: 1.282364010810852
LOSS: 1.2800158262252808
LOSS: 1.2777886390686035
LOSS: 1.2756619453430176
LOSS: 1.273614525794983
LOSS: 1.2716326713562012
LOSS: 1.2697116136550903
LOSS: 1.2678569555282593
LOSS: 1.2660820484161377
LOSS: 1.264404535293579
LOSS: 1.2628419399261475
LOSS: 1.2614085674285889
LOSS: 1.2601096630096436
LOSS: 1.2589442729949951
LOSS: 1.257898211479187
LOSS: 1.2569520473480225
LOSS: 1.2560791969299316
LOSS: 1.255255937576294
LOSS: 1.2544608116149902
LOSS: 1.2536805868148804
LOSS: 1.2529091835021973
LOSS: 1.2521476745605469
LOSS: 1.2514046430587769
LOSS: 1.250685214996338
LOSS: 1.2499988079071045
LOSS: 1.2493503093719482
LOSS: 1.2487413883209229
LOSS: 1.2481732368469238
LOSS: 1.2476410865783691
LOSS: 1.247144103050232
LOSS: 1.2466803789138794
LOSS: 1.2462488412857056
LOSS: 1.2458475828170776
LOSS: 1.2454807758331299
LOSS: 1.2451467514038086
LOSS: 1.2448461055755615
LOSS: 1.2445753812789917
LOSS: 1.2443331480026245
LOSS: 1.2441171407699585
LOSS: 1.2439202070236206
LOSS: 1.2437399625778198
LOSS: 1.2435736656188965
LOSS: 1.2434183359146118
LOSS: 1.2432739734649658
LOSS: 1.2431403398513794
LOSS: 1.243016004562378
LOSS: 1.2429015636444092
LOSS: 1.2427963018417358
LOSS: 1.2426995038986206
LOSS: 1.2426087856292725
LOSS: 1.2425243854522705
LOSS: 1.2424426078796387
LOSS: 1.242363691329956
LOSS: 1.242287039756775
LOSS: 1.2422147989273071
LOSS: 1.2421438694000244
LOSS: 1.2420780658721924
LOSS: 1.2420132160186768
LOSS: 1.2419508695602417
LOSS: 1.241891622543335
LOSS: 1.241832971572876
LOSS: 1.2417758703231812
LOSS: 1.2417175769805908
LOSS: 1.241662859916687
LOSS: 1.2416090965270996
LOSS: 1.2415533065795898
LOSS: 1.2415013313293457
LOSS: 1.2414480447769165
LOSS: 1.2413966655731201
LOSS: 1.2413439750671387
LOSS: 1.2412930727005005
LOSS: 1.2412432432174683
LOSS: 1.2411915063858032
LOSS: 1.2411400079727173
LOSS: 1.2410893440246582
LOSS: 1.2410393953323364
LOSS: 1.2409878969192505
LOSS: 1.24093759059906
LOSS: 1.2408866882324219
LOSS: 1.240835428237915
LOSS: 1.2407842874526978
LOSS: 1.3510816097259521
LOSS: 1.3464486598968506
LOSS: 1.342218279838562
LOSS: 1.3384292125701904
LOSS: 1.335108757019043
LOSS: 1.332261562347412
LOSS: 1.3298542499542236
LOSS: 1.327804446220398
LOSS: 1.3259835243225098
LOSS: 1.3242484331130981
LOSS: 1.3224812746047974
LOSS: 1.3206182718276978
LOSS: 1.3186532258987427
LOSS: 1.3166172504425049
LOSS: 1.3145525455474854
LOSS: 1.3124994039535522
LOSS: 1.3104910850524902
LOSS: 1.308548927307129
LOSS: 1.306679368019104
LOSS: 1.3048756122589111
LOSS: 1.3031216859817505
LOSS: 1.301400899887085
LOSS: 1.299702525138855
LOSS: 1.2980260848999023
LOSS: 1.2963817119598389
LOSS: 1.2947884798049927
LOSS: 1.2932701110839844
LOSS: 1.2918498516082764
LOSS: 1.2905449867248535
LOSS: 1.2893644571304321
LOSS: 1.2883062362670898
LOSS: 1.2873578071594238
LOSS: 1.2864999771118164
LOSS: 1.2857122421264648
LOSS: 1.2849751710891724
LOSS: 1.2842777967453003
LOSS: 1.283614993095398
LOSS: 1.2829877138137817
LOSS: 1.2824002504348755
LOSS: 1.281856894493103
LOSS: 1.281359314918518
LOSS: 1.2809046506881714
LOSS: 1.2804882526397705
LOSS: 1.280103087425232
LOSS: 1.2797415256500244
LOSS: 1.2793973684310913
LOSS: 1.279069185256958
LOSS: 1.2787545919418335
LOSS: 1.2784560918807983
LOSS: 1.2781765460968018
LOSS: 1.2779160737991333
LOSS: 1.2776747941970825
LOSS: 1.277451992034912
LOSS: 1.2772427797317505
LOSS: 1.2770441770553589
LOSS: 1.2768535614013672
LOSS: 1.2766684293746948
LOSS: 1.2764886617660522
LOSS: 1.276315450668335
LOSS: 1.276149034500122
LOSS: 1.2759915590286255
LOSS: 1.275843620300293
LOSS: 1.2757043838500977
LOSS: 1.2755728960037231
LOSS: 1.275448203086853
LOSS: 1.275329351425171
LOSS: 1.2752158641815186
LOSS: 1.275106430053711
LOSS: 1.2750022411346436
LOSS: 1.274902105331421
LOSS: 1.2748066186904907
LOSS: 1.2747139930725098
LOSS: 1.2746249437332153
LOSS: 1.2745367288589478
LOSS: 1.2744512557983398
LOSS: 1.2743664979934692
LOSS: 1.2742841243743896
LOSS: 1.2742033004760742
LOSS: 1.2741256952285767
LOSS: 1.2740504741668701
LOSS: 1.2739784717559814
LOSS: 1.2739089727401733
LOSS: 1.2738417387008667
LOSS: 1.2737770080566406
LOSS: 1.2737133502960205
LOSS: 1.2736515998840332
LOSS: 1.2735902070999146
LOSS: 1.273531198501587
LOSS: 1.2734726667404175
LOSS: 1.2734160423278809
LOSS: 1.2733601331710815
LOSS: 1.2733060121536255
LOSS: 1.2732532024383545
LOSS: 1.2732017040252686
LOSS: 1.2731510400772095
LOSS: 1.2731010913848877
LOSS: 1.2730530500411987
LOSS: 1.273005485534668LOSS: 1.3510816097259521
LOSS: 1.3464486598968506
LOSS: 1.342218279838562
LOSS: 1.3384292125701904
LOSS: 1.335108757019043
LOSS: 1.332261562347412
LOSS: 1.3298542499542236
LOSS: 1.327804446220398
LOSS: 1.3259835243225098
LOSS: 1.3242484331130981
LOSS: 1.3224812746047974
LOSS: 1.3206182718276978
LOSS: 1.3186532258987427
LOSS: 1.3166172504425049
LOSS: 1.3145525455474854
LOSS: 1.3124994039535522
LOSS: 1.3104910850524902
LOSS: 1.308548927307129
LOSS: 1.306679368019104
LOSS: 1.3048756122589111
LOSS: 1.3031216859817505
LOSS: 1.301400899887085
LOSS: 1.299702525138855
LOSS: 1.2980260848999023
LOSS: 1.2963817119598389
LOSS: 1.2947884798049927
LOSS: 1.2932701110839844
LOSS: 1.2918498516082764
LOSS: 1.2905449867248535
LOSS: 1.2893644571304321
LOSS: 1.2883062362670898
LOSS: 1.2873578071594238
LOSS: 1.2864999771118164
LOSS: 1.2857122421264648
LOSS: 1.2849751710891724
LOSS: 1.2842777967453003
LOSS: 1.283614993095398
LOSS: 1.2829877138137817
LOSS: 1.2824002504348755
LOSS: 1.281856894493103
LOSS: 1.281359314918518
LOSS: 1.2809046506881714
LOSS: 1.2804882526397705
LOSS: 1.280103087425232
LOSS: 1.2797415256500244
LOSS: 1.2793973684310913
LOSS: 1.279069185256958
LOSS: 1.2787545919418335
LOSS: 1.2784560918807983
LOSS: 1.2781765460968018
LOSS: 1.2779160737991333
LOSS: 1.2776747941970825
LOSS: 1.277451992034912
LOSS: 1.2772427797317505
LOSS: 1.2770441770553589
LOSS: 1.2768535614013672
LOSS: 1.2766684293746948
LOSS: 1.2764886617660522
LOSS: 1.276315450668335
LOSS: 1.276149034500122
LOSS: 1.2759915590286255
LOSS: 1.275843620300293
LOSS: 1.2757043838500977
LOSS: 1.2755728960037231
LOSS: 1.275448203086853
LOSS: 1.275329351425171
LOSS: 1.2752158641815186
LOSS: 1.275106430053711
LOSS: 1.2750022411346436
LOSS: 1.274902105331421
LOSS: 1.2748066186904907
LOSS: 1.2747139930725098
LOSS: 1.2746249437332153
LOSS: 1.2745367288589478
LOSS: 1.2744512557983398
LOSS: 1.2743664979934692
LOSS: 1.2742841243743896
LOSS: 1.2742033004760742
LOSS: 1.2741256952285767
LOSS: 1.2740504741668701
LOSS: 1.2739784717559814
LOSS: 1.2739089727401733
LOSS: 1.2738417387008667
LOSS: 1.2737770080566406
LOSS: 1.2737133502960205
LOSS: 1.2736515998840332
LOSS: 1.2735902070999146
LOSS: 1.273531198501587
LOSS: 1.2734726667404175
LOSS: 1.2734160423278809
LOSS: 1.2733601331710815
LOSS: 1.2733060121536255
LOSS: 1.2732532024383545
LOSS: 1.2732017040252686
LOSS: 1.2731510400772095
LOSS: 1.2731010913848877
LOSS: 1.2730530500411987
LOSS: 1.273005485534668
LOSS: 1.2729599475860596
LOSS: 1.2729147672653198
LOSS: 1.3554649353027344
LOSS: 1.3501559495925903
LOSS: 1.3451969623565674
LOSS: 1.340628981590271
LOSS: 1.3364830017089844
LOSS: 1.332769751548767
LOSS: 1.3294649124145508
LOSS: 1.3264992237091064
LOSS: 1.3237591981887817
LOSS: 1.3211097717285156
LOSS: 1.3184340000152588
LOSS: 1.3156671524047852
LOSS: 1.3128010034561157
LOSS: 1.309870958328247
LOSS: 1.3069324493408203
LOSS: 1.3040419816970825
LOSS: 1.3012425899505615
LOSS: 1.2985590696334839
LOSS: 1.295994758605957
LOSS: 1.2935354709625244
LOSS: 1.291154384613037
LOSS: 1.288821816444397
LOSS: 1.286513090133667
LOSS: 1.2842124700546265
LOSS: 1.2819174528121948
LOSS: 1.2796366214752197
LOSS: 1.2773860692977905
LOSS: 1.27518630027771
LOSS: 1.2730542421340942
LOSS: 1.271003007888794
LOSS: 1.2690380811691284
LOSS: 1.2671557664871216
LOSS: 1.2653484344482422
LOSS: 1.2636066675186157
LOSS: 1.2619233131408691
LOSS: 1.260298252105713
LOSS: 1.2587358951568604
LOSS: 1.2572447061538696
LOSS: 1.2558344602584839
LOSS: 1.2545127868652344
LOSS: 1.2532837390899658
LOSS: 1.2521445751190186
LOSS: 1.2510895729064941
LOSS: 1.250109314918518
LOSS: 1.2491949796676636
LOSS: 1.2483378648757935
LOSS: 1.2475322484970093
LOSS: 1.2467762231826782
LOSS: 1.2460674047470093
LOSS: 1.245405912399292
LOSS: 1.2447896003723145
LOSS: 1.2442173957824707
LOSS: 1.2436853647232056
LOSS: 1.2431892156600952
LOSS: 1.2427241802215576
LOSS: 1.2422876358032227
LOSS: 1.241876482963562
LOSS: 1.241490364074707
LOSS: 1.2411280870437622
LOSS: 1.240789771080017
LOSS: 1.2404751777648926
LOSS: 1.2401812076568604
LOSS: 1.2399063110351562
LOSS: 1.2396475076675415
LOSS: 1.2394027709960938
LOSS: 1.2391700744628906
LOSS: 1.2389483451843262
LOSS: 1.238736867904663
LOSS: 1.2385363578796387
LOSS: 1.2383451461791992
LOSS: 1.2381638288497925
LOSS: 1.2379921674728394
LOSS: 1.2378276586532593
LOSS: 1.2376708984375
LOSS: 1.237520456314087
LOSS: 1.23737633228302
LOSS: 1.2372374534606934
LOSS: 1.2371047735214233
LOSS: 1.2369776964187622
LOSS: 1.2368557453155518
LOSS: 1.236737847328186
LOSS: 1.2366249561309814
LOSS: 1.236515760421753
LOSS: 1.2364094257354736
LOSS: 1.2363070249557495
LOSS: 1.2362076044082642
LOSS: 1.2361117601394653
LOSS: 1.2360183000564575
LOSS: 1.2359285354614258
LOSS: 1.2358413934707642
LOSS: 1.2357569932937622
LOSS: 1.2356752157211304
LOSS: 1.235595464706421
LOSS: 1.2355176210403442
LOSS: 1.23544180393219
LOSS: 1.2353684902191162
LOSS: 1.235296607017517
LOSS: 1.2352266311645508
LOSS: 1.2351583242416382
LOSS: 1.235092043876648
LOSS: 1.3556853532791138
LOSS: 1.3507788181304932
LOSS: 1.3462269306182861
LOSS: 1.3420703411102295
LOSS: 1.3383376598358154
LOSS: 1.3350350856781006
LOSS: 1.332130789756775
LOSS: 1.3295459747314453
LOSS: 1.3271605968475342
LOSS: 1.3248461484909058
LOSS: 1.3225064277648926
LOSS: 1.320101022720337
LOSS: 1.3176411390304565
LOSS: 1.3151713609695435
LOSS: 1.3127477169036865
LOSS: 1.3104221820831299
LOSS: 1.3082301616668701
LOSS: 1.3061866760253906
LOSS: 1.3042875528335571
LOSS: 1.3025139570236206
LOSS: 1.3008414506912231
LOSS: 1.2992511987686157
LOSS: 1.2977337837219238
LOSS: 1.2962929010391235
LOSS: 1.2949416637420654
LOSS: 1.2936997413635254
LOSS: 1.2925872802734375
LOSS: 1.2916204929351807
LOSS: 1.2908077239990234
LOSS: 1.2901458740234375
LOSS: 1.2896206378936768
LOSS: 1.289209008216858
LOSS: 1.2888829708099365
LOSS: 1.2886165380477905
LOSS: 1.2883868217468262
LOSS: 1.2881797552108765
LOSS: 1.2879880666732788
LOSS: 1.2878097295761108
LOSS: 1.2876449823379517
LOSS: 1.2874943017959595
LOSS: 1.2873555421829224
LOSS: 1.287225365638733
LOSS: 1.2870975732803345
LOSS: 1.2869688272476196
LOSS: 1.28683602809906
LOSS: 1.2866978645324707
LOSS: 1.2865567207336426
LOSS: 1.286415934562683
LOSS: 1.2862811088562012
LOSS: 1.2861555814743042
LOSS: 1.2860416173934937
LOSS: 1.2859388589859009
LOSS: 1.2858456373214722
LOSS: 1.2857586145401
LOSS: 1.285675287246704
LOSS: 1.2855926752090454
LOSS: 1.285509467124939
LOSS: 1.2854259014129639
LOSS: 1.2853420972824097
LOSS: 1.2852590084075928
LOSS: 1.2851753234863281
LOSS: 1.2850903272628784
LOSS: 1.2850019931793213
LOSS: 1.284908413887024
LOSS: 1.284806728363037
LOSS: 1.2846956253051758
LOSS: 1.2845745086669922
LOSS: 1.2844421863555908
LOSS: 1.2842999696731567
LOSS: 1.284147024154663
LOSS: 1.2839856147766113
LOSS: 1.2838163375854492
LOSS: 1.2836403846740723
LOSS: 1.283460021018982
LOSS: 1.283275842666626
LOSS: 1.2830888032913208
LOSS: 1.282900094985962
LOSS: 1.2827080488204956
LOSS: 1.2825125455856323
LOSS: 1.2823104858398438
LOSS: 1.282098412513733
LOSS: 1.2818719148635864
LOSS: 1.2816270589828491
LOSS: 1.2813584804534912
LOSS: 1.2810600996017456
LOSS: 1.2807259559631348
LOSS: 1.280351161956787
LOSS: 1.2799299955368042
LOSS: 1.2794572114944458
LOSS: 1.2789298295974731
LOSS: 1.2783490419387817
LOSS: 1.2777166366577148
LOSS: 1.2770390510559082
LOSS: 1.2763251066207886
LOSS: 1.2755861282348633
LOSS: 1.2748353481292725
LOSS: 1.2740850448608398
LOSS: 1.2733495235443115
LOSS: 1.272637128829956
LOSS: 1.2719581127166748
LOSS: 1.3457741737365723
LOSS: 1.3398966789245605
LOSS: 1.3343027830123901
LOSS: 1.3290380239486694
LOSS: 1.3241432905197144
LOSS: 1.319649577140808
LOSS: 1.315568208694458
LOSS: 1.3118815422058105
LOSS: 1.3085368871688843
LOSS: 1.3054473400115967
LOSS: 1.302510142326355
LOSS: 1.2996323108673096
LOSS: 1.2967578172683716
LOSS: 1.2938778400421143
LOSS: 1.2910239696502686
LOSS: 1.2882506847381592
LOSS: 1.2856173515319824
LOSS: 1.28317391872406
LOSS: 1.280953288078308
LOSS: 1.2789682149887085
LOSS: 1.277212142944336
LOSS: 1.2756636142730713
LOSS: 1.2742915153503418
LOSS: 1.2730611562728882
LOSS: 1.2719395160675049
LOSS: 1.2708992958068848
LOSS: 1.2699214220046997
LOSS: 1.2689948081970215
LOSS: 1.268115520477295
LOSS: 1.2672839164733887
LOSS: 1.2665027379989624LOSS: 1.286688208580017
LOSS: 1.286291241645813
LOSS: 1.2859221696853638
LOSS: 1.28557288646698
LOSS: 1.2852381467819214
LOSS: 1.2849138975143433
LOSS: 1.2845999002456665
LOSS: 1.2842963933944702
LOSS: 1.2840081453323364
LOSS: 1.2837374210357666
LOSS: 1.2834858894348145
LOSS: 1.2832533121109009
LOSS: 1.2830361127853394
LOSS: 1.2828309535980225
LOSS: 1.2826323509216309
LOSS: 1.2824369668960571
LOSS: 1.2822413444519043
LOSS: 1.2820440530776978
LOSS: 1.2818424701690674
LOSS: 1.2816381454467773
LOSS: 1.2814306020736694
LOSS: 1.2812222242355347
LOSS: 1.281016230583191
LOSS: 1.280816912651062
LOSS: 1.2806293964385986
LOSS: 1.2804588079452515
LOSS: 1.2803086042404175
LOSS: 1.2801791429519653
LOSS: 1.2800694704055786
LOSS: 1.2799770832061768
LOSS: 1.2798957824707031
LOSS: 1.2798218727111816
LOSS: 1.279751181602478
LOSS: 1.2796801328659058
LOSS: 1.279607892036438
LOSS: 1.2795339822769165
LOSS: 1.2794585227966309
LOSS: 1.2793818712234497
LOSS: 1.2793043851852417
LOSS: 1.2792274951934814
LOSS: 1.2791510820388794
LOSS: 1.2790772914886475
LOSS: 1.2790048122406006
LOSS: 1.2789373397827148
LOSS: 1.278873085975647
LOSS: 1.2788128852844238
LOSS: 1.2787585258483887
LOSS: 1.2787082195281982
LOSS: 1.2786606550216675
LOSS: 1.278616189956665
LOSS: 1.2785733938217163
LOSS: 1.2785323858261108
LOSS: 1.2784913778305054
LOSS: 1.2784513235092163
LOSS: 1.2784106731414795
LOSS: 1.2783710956573486
LOSS: 1.27833092212677
LOSS: 1.2782925367355347
LOSS: 1.2782540321350098
LOSS: 1.2782176733016968
LOSS: 1.2781811952590942
LOSS: 1.278146505355835
LOSS: 1.352192997932434
LOSS: 1.3468166589736938
LOSS: 1.3418159484863281
LOSS: 1.3372418880462646
LOSS: 1.3331365585327148
LOSS: 1.3295197486877441
LOSS: 1.3263778686523438
LOSS: 1.323648452758789
LOSS: 1.3212215900421143
LOSS: 1.3189589977264404
LOSS: 1.3167314529418945
LOSS: 1.3144563436508179
LOSS: 1.3121111392974854
LOSS: 1.3097233772277832
LOSS: 1.3073484897613525
LOSS: 1.3050488233566284
LOSS: 1.302877426147461
LOSS: 1.3008695840835571
LOSS: 1.2990386486053467
LOSS: 1.2973772287368774
LOSS: 1.295862078666687
LOSS: 1.2944612503051758
LOSS: 1.2931421995162964
LOSS: 1.2918792963027954
LOSS: 1.2906574010849
LOSS: 1.2894731760025024
LOSS: 1.2883329391479492
LOSS: 1.2872499227523804
LOSS: 1.2862372398376465
LOSS: 1.2853057384490967
LOSS: 1.2844581604003906
LOSS: 1.283691167831421
LOSS: 1.2829924821853638
LOSS: 1.2823485136032104
LOSS: 1.2817445993423462
LOSS: 1.2811715602874756
LOSS: 1.2806243896484375
LOSS: 1.280105471611023
LOSS: 1.2796189785003662
LOSS: 1.2791705131530762
LOSS: 1.2787632942199707
LOSS: 1.2783970832824707
LOSS: 1.2780689001083374
LOSS: 1.2777737379074097
LOSS: 1.2775039672851562
LOSS: 1.2772526741027832
LOSS: 1.277013897895813
LOSS: 1.276782751083374
LOSS: 1.2765579223632812
LOSS: 1.2763395309448242
LOSS: 1.2761276960372925
LOSS: 1.2759239673614502
LOSS: 1.2757298946380615
LOSS: 1.2755444049835205
LOSS: 1.2753664255142212
LOSS: 1.2751926183700562
LOSS: 1.275022268295288
LOSS: 1.2748510837554932
LOSS: 1.2746764421463013
LOSS: 1.274497628211975
LOSS: 1.2743138074874878
LOSS: 1.2741234302520752
LOSS: 1.2739243507385254
LOSS: 1.2737168073654175
LOSS: 1.2734957933425903
LOSS: 1.2732614278793335
LOSS: 1.2730112075805664
LOSS: 1.2727446556091309
LOSS: 1.2724621295928955
LOSS: 1.2721660137176514
LOSS: 1.271859049797058
LOSS: 1.2715442180633545
LOSS: 1.2712260484695435
LOSS: 1.2709081172943115
LOSS: 1.2705937623977661
LOSS: 1.270286202430725
LOSS: 1.269989252090454
LOSS: 1.2697038650512695
LOSS: 1.2694330215454102
LOSS: 1.2691779136657715
LOSS: 1.2689393758773804
LOSS: 1.268716812133789
LOSS: 1.2685108184814453
LOSS: 1.2683188915252686
LOSS: 1.268140435218811
LOSS: 1.267974615097046
LOSS: 1.267817735671997
LOSS: 1.2676701545715332
LOSS: 1.2675310373306274
LOSS: 1.2673978805541992
LOSS: 1.2672710418701172
LOSS: 1.2671513557434082
LOSS: 1.2670356035232544
LOSS: 1.2669256925582886
LOSS: 1.2668201923370361
LOSS: 1.2667189836502075
LOSS: 1.2666220664978027
LOSS: 1.2665290832519531
LOSS: 1.2664391994476318
LOSS: 1.2663516998291016
LOSS: 1.344248652458191
LOSS: 1.338470697402954
LOSS: 1.333083152770996
LOSS: 1.3281331062316895
LOSS: 1.3236584663391113
LOSS: 1.3196818828582764
LOSS: 1.3162004947662354
LOSS: 1.3131740093231201
LOSS: 1.3105195760726929
LOSS: 1.3081135749816895
LOSS: 1.3058123588562012
LOSS: 1.3034842014312744
LOSS: 1.3010414838790894
LOSS: 1.2984542846679688
LOSS: 1.2957435846328735
LOSS: 1.2929638624191284
LOSS: 1.2901822328567505
LOSS: 1.2874609231948853
LOSS: 1.2848467826843262
LOSS: 1.282364010810852
LOSS: 1.2800158262252808
LOSS: 1.2777886390686035
LOSS: 1.2756619453430176
LOSS: 1.273614525794983
LOSS: 1.2716326713562012
LOSS: 1.2697116136550903
LOSS: 1.2678569555282593
LOSS: 1.2660820484161377
LOSS: 1.264404535293579
LOSS: 1.2628419399261475
LOSS: 1.2614085674285889
LOSS: 1.2601096630096436
LOSS: 1.2589442729949951
LOSS: 1.257898211479187
LOSS: 1.2569520473480225
LOSS: 1.2560791969299316
LOSS: 1.255255937576294
LOSS: 1.2544608116149902
LOSS: 1.2536805868148804
LOSS: 1.2529091835021973
LOSS: 1.2521476745605469
LOSS: 1.2514046430587769
LOSS: 1.250685214996338
LOSS: 1.2499988079071045
LOSS: 1.2493503093719482
LOSS: 1.2487413883209229
LOSS: 1.2481732368469238
LOSS: 1.2476410865783691
LOSS: 1.247144103050232
LOSS: 1.2466803789138794
LOSS: 1.2462488412857056
LOSS: 1.2458475828170776
LOSS: 1.2454807758331299
LOSS: 1.2451467514038086
LOSS: 1.2448461055755615
LOSS: 1.2445753812789917
LOSS: 1.2443331480026245
LOSS: 1.2441171407699585
LOSS: 1.2439202070236206
LOSS: 1.2437399625778198
LOSS: 1.2435736656188965
LOSS: 1.2434183359146118
LOSS: 1.2432739734649658
LOSS: 1.2431403398513794
LOSS: 1.243016004562378
LOSS: 1.2429015636444092
LOSS: 1.2427963018417358
LOSS: 1.2426995038986206
LOSS: 1.2426087856292725
LOSS: 1.2425243854522705
LOSS: 1.2424426078796387
LOSS: 1.242363691329956
LOSS: 1.242287039756775
LOSS: 1.2422147989273071
LOSS: 1.2421438694000244
LOSS: 1.2420780658721924
LOSS: 1.2420132160186768
LOSS: 1.2419508695602417
LOSS: 1.241891622543335
LOSS: 1.241832971572876
LOSS: 1.2417758703231812
LOSS: 1.2417175769805908
LOSS: 1.241662859916687
LOSS: 1.2416090965270996
LOSS: 1.2415533065795898
LOSS: 1.2415013313293457
LOSS: 1.2414480447769165
LOSS: 1.2413966655731201
LOSS: 1.2413439750671387
LOSS: 1.2412930727005005
LOSS: 1.2412432432174683
LOSS: 1.2411915063858032
LOSS: 1.2411400079727173
LOSS: 1.2410893440246582
LOSS: 1.2410393953323364
LOSS: 1.2409878969192505
LOSS: 1.24093759059906
LOSS: 1.2408866882324219
LOSS: 1.240835428237915
LOSS: 1.2407842874526978
LOSS: 1.3700227737426758
LOSS: 1.3656330108642578
LOSS: 1.36160147190094
LOSS: 1.3579576015472412
LOSS: 1.3547016382217407
LOSS: 1.3517811298370361
LOSS: 1.3490805625915527
LOSS: 1.34645676612854
LOSS: 1.3438081741333008
LOSS: 1.341110110282898
LOSS: 1.3383959531784058
LOSS: 1.3357223272323608
LOSS: 1.3331414461135864
LOSS: 1.3306877613067627
LOSS: 1.3283699750900269
LOSS: 1.326171636581421
LOSS: 1.324060320854187
LOSS: 1.322000503540039
LOSS: 1.3199659585952759
LOSS: 1.3179471492767334
LOSS: 1.3159514665603638
LOSS: 1.314000129699707
LOSS: 1.312121033668518
LOSS: 1.3103452920913696
LOSS: 1.3086988925933838
LOSS: 1.3072025775909424
LOSS: 1.3058662414550781
LOSS: 1.3046916723251343
LOSS: 1.3036729097366333
LOSS: 1.3028000593185425
LOSS: 1.302062749862671
LOSS: 1.3014506101608276
LOSS: 1.3009525537490845
LOSS: 1.300557017326355
LOSS: 1.3002463579177856
LOSS: 1.300002932548523
LOSS: 1.2998056411743164
LOSS: 1.299634337425232
LOSS: 1.2994729280471802
LOSS: 1.2993083000183105
LOSS: 1.2991348505020142
LOSS: 1.2989516258239746
LOSS: 1.2987604141235352
LOSS: 1.298563838005066
LOSS: 1.2983660697937012
LOSS: 1.2981666326522827
LOSS: 1.2979661226272583
LOSS: 1.2977635860443115
LOSS: 1.2975592613220215
LOSS: 1.2973531484603882
LOSS: 1.2971478700637817
LOSS: 1.296945333480835
LOSS: 1.2967442274093628
LOSS: 1.2965458631515503
LOSS: 1.2963459491729736
LOSS: 1.2961416244506836
LOSS: 1.29592764377594
LOSS: 1.2957017421722412
LOSS: 1.2954622507095337
LOSS: 1.295211672782898
LOSS: 1.2949541807174683
LOSS: 1.2946953773498535
LOSS: 1.2944425344467163
LOSS: 1.29420006275177
LOSS: 1.293971300125122
LOSS: 1.2937573194503784
LOSS: 1.2935553789138794
LOSS: 1.2933624982833862

LOSS: 1.2657742500305176
LOSS: 1.2650980949401855
LOSS: 1.264471411705017
LOSS: 1.2638881206512451
LOSS: 1.2633413076400757
LOSS: 1.262823462486267
LOSS: 1.2623289823532104
LOSS: 1.261855959892273
LOSS: 1.2614039182662964
LOSS: 1.2609761953353882
LOSS: 1.2605761289596558
LOSS: 1.2602072954177856
LOSS: 1.259871244430542
LOSS: 1.259567379951477
LOSS: 1.2592922449111938
LOSS: 1.2590405941009521
LOSS: 1.2588064670562744
LOSS: 1.258583903312683
LOSS: 1.2583682537078857
LOSS: 1.2581562995910645
LOSS: 1.2579463720321655
LOSS: 1.2577385902404785
LOSS: 1.2575345039367676
LOSS: 1.2573350667953491
LOSS: 1.2571429014205933
LOSS: 1.256958246231079
LOSS: 1.2567825317382812
LOSS: 1.2566155195236206
LOSS: 1.2564575672149658
LOSS: 1.2563077211380005
LOSS: 1.256165862083435
LOSS: 1.256030797958374
LOSS: 1.255902647972107
LOSS: 1.2557799816131592
LOSS: 1.2556623220443726
LOSS: 1.2555487155914307
LOSS: 1.2554377317428589
LOSS: 1.2553298473358154
LOSS: 1.2552238702774048
LOSS: 1.255120038986206
LOSS: 1.2550185918807983
LOSS: 1.254919171333313
LOSS: 1.2548233270645142
LOSS: 1.2547310590744019
LOSS: 1.254642367362976
LOSS: 1.2545572519302368
LOSS: 1.2544755935668945
LOSS: 1.2543970346450806
LOSS: 1.2543209791183472
LOSS: 1.2542469501495361
LOSS: 1.2541745901107788
LOSS: 1.2541040182113647
LOSS: 1.254035234451294
LOSS: 1.2539677619934082
LOSS: 1.2539020776748657
LOSS: 1.2538381814956665
LOSS: 1.2537758350372314
LOSS: 1.25371515750885
LOSS: 1.2536566257476807
LOSS: 1.2535995244979858
LOSS: 1.2535439729690552
LOSS: 1.2534898519515991
LOSS: 1.2534373998641968
LOSS: 1.2533859014511108
LOSS: 1.2533358335494995
LOSS: 1.253287434577942
LOSS: 1.253239393234253
LOSS: 1.2531923055648804
LOSS: 1.2531465291976929
LOSS: 1.3496202230453491
LOSS: 1.3443987369537354
LOSS: 1.3395591974258423
LOSS: 1.3351356983184814
LOSS: 1.331161618232727
LOSS: 1.3276517391204834
LOSS: 1.3245913982391357
LOSS: 1.3219270706176758
LOSS: 1.3195618391036987
LOSS: 1.3173637390136719
LOSS: 1.3151962757110596
LOSS: 1.3129534721374512
LOSS: 1.3105851411819458
LOSS: 1.3080964088439941
LOSS: 1.3055323362350464
LOSS: 1.3029563426971436
LOSS: 1.3004307746887207
LOSS: 1.2980051040649414
LOSS: 1.29570734500885
LOSS: 1.2935456037521362
LOSS: 1.2915093898773193
LOSS: 1.2895797491073608
LOSS: 1.287735939025879
LOSS: 1.2859636545181274
LOSS: 1.2842581272125244
LOSS: 1.2826259136199951
LOSS: 1.281080722808838
LOSS: 1.2796411514282227
LOSS: 1.2783260345458984
LOSS: 1.2771490812301636
LOSS: 1.2761162519454956
LOSS: 1.275223731994629
LOSS: 1.27445650100708
LOSS: 1.2737925052642822
LOSS: 1.2732049226760864
LOSS: 1.2726671695709229
LOSS: 1.2721586227416992
LOSS: 1.27166748046875
LOSS: 1.2711886167526245
LOSS: 1.270722508430481
LOSS: 1.270271897315979
LOSS: 1.2698400020599365
LOSS: 1.269426941871643
LOSS: 1.269030213356018
LOSS: 1.2686446905136108
LOSS: 1.2682640552520752
LOSS: 1.2678813934326172
LOSS: 1.2674930095672607
LOSS: 1.2670985460281372
LOSS: 1.2666966915130615
LOSS: 1.266294240951538
LOSS: 1.2658978700637817
LOSS: 1.265514850616455
LOSS: 1.265152931213379
LOSS: 1.2648138999938965
LOSS: 1.264499545097351
LOSS: 1.2642077207565308
LOSS: 1.2639344930648804
LOSS: 1.2636761665344238
LOSS: 1.2634313106536865
LOSS: 1.2631971836090088
LOSS: 1.2629725933074951
LOSS: 1.2627592086791992
LOSS: 1.2625541687011719
LOSS: 1.2623579502105713
LOSS: 1.2621686458587646
LOSS: 1.261984944343567
LOSS: 1.261804223060608
LOSS: 1.2616246938705444
LOSS: 1.2614471912384033
LOSS: 1.261269450187683
LOSS: 1.2610926628112793
LOSS: 1.2609156370162964
LOSS: 1.2607413530349731
LOSS: 1.2605668306350708
LOSS: 1.2603925466537476
LOSS: 1.2602190971374512
LOSS: 1.260042667388916
LOSS: 1.2598645687103271
LOSS: 1.2596851587295532
LOSS: 1.2595027685165405
LOSS: 1.2593189477920532
LOSS: 1.2591347694396973
LOSS: 1.2589473724365234
LOSS: 1.2587610483169556
LOSS: 1.2585753202438354
LOSS: 1.2583897113800049
LOSS: 1.2582063674926758
LOSS: 1.2580267190933228
LOSS: 1.2578511238098145
LOSS: 1.2576804161071777
LOSS: 1.2575161457061768
LOSS: 1.2573548555374146
LOSS: 1.2571998834609985
LOSS: 1.2570489645004272
LOSS: 1.2569011449813843
LOSS: 1.2567576169967651
LOSS: 1.256618618965149
LOSS: 1.2564857006072998
LOSS: 1.256358027458191
LOSS: 1.3457741737365723
LOSS: 1.3398966789245605
LOSS: 1.3343027830123901
LOSS: 1.3290380239486694
LOSS: 1.3241432905197144
LOSS: 1.319649577140808
LOSS: 1.315568208694458
LOSS: 1.3118815422058105
LOSS: 1.3085368871688843
LOSS: 1.3054473400115967
LOSS: 1.302510142326355
LOSS: 1.2996323108673096
LOSS: 1.2967578172683716
LOSS: 1.2938778400421143
LOSS: 1.2910239696502686
LOSS: 1.2882506847381592
LOSS: 1.2856173515319824
LOSS: 1.28317391872406
LOSS: 1.280953288078308
LOSS: 1.2789682149887085
LOSS: 1.277212142944336
LOSS: 1.2756636142730713
LOSS: 1.2742915153503418
LOSS: 1.2730611562728882
LOSS: 1.2719395160675049
LOSS: 1.2708992958068848
LOSS: 1.2699214220046997
LOSS: 1.2689948081970215
LOSS: 1.268115520477295
LOSS: 1.2672839164733887
LOSS: 1.2665027379989624
LOSS: 1.2657742500305176
LOSS: 1.2650980949401855
LOSS: 1.264471411705017
LOSS: 1.2638881206512451
LOSS: 1.2633413076400757
LOSS: 1.262823462486267
LOSS: 1.2623289823532104
LOSS: 1.261855959892273
LOSS: 1.2614039182662964
LOSS: 1.2609761953353882
LOSS: 1.2605761289596558
LOSS: 1.2602072954177856
LOSS: 1.259871244430542
LOSS: 1.259567379951477
LOSS: 1.2592922449111938
LOSS: 1.2590405941009521
LOSS: 1.2588064670562744
LOSS: 1.258583903312683
LOSS: 1.2583682537078857
LOSS: 1.2581562995910645
LOSS: 1.2579463720321655
LOSS: 1.2577385902404785
LOSS: 1.2575345039367676
LOSS: 1.2573350667953491
LOSS: 1.2571429014205933
LOSS: 1.256958246231079
LOSS: 1.2567825317382812
LOSS: 1.2566155195236206
LOSS: 1.2564575672149658
LOSS: 1.2563077211380005
LOSS: 1.256165862083435
LOSS: 1.256030797958374
LOSS: 1.255902647972107
LOSS: 1.2557799816131592
LOSS: 1.2556623220443726
LOSS: 1.2555487155914307
LOSS: 1.2554377317428589
LOSS: 1.2553298473358154
LOSS: 1.2552238702774048
LOSS: 1.255120038986206
LOSS: 1.2550185918807983
LOSS: 1.254919171333313
LOSS: 1.2548233270645142
LOSS: 1.2547310590744019
LOSS: 1.254642367362976
LOSS: 1.2545572519302368
LOSS: 1.2544755935668945
LOSS: 1.2543970346450806
LOSS: 1.2543209791183472
LOSS: 1.2542469501495361
LOSS: 1.2541745901107788
LOSS: 1.2541040182113647
LOSS: 1.254035234451294
LOSS: 1.2539677619934082
LOSS: 1.2539020776748657
LOSS: 1.2538381814956665
LOSS: 1.2537758350372314
LOSS: 1.25371515750885
LOSS: 1.2536566257476807
LOSS: 1.2535995244979858
LOSS: 1.2535439729690552
LOSS: 1.2534898519515991
LOSS: 1.2534373998641968
LOSS: 1.2533859014511108
LOSS: 1.2533358335494995
LOSS: 1.253287434577942
LOSS: 1.253239393234253
LOSS: 1.2531923055648804
LOSS: 1.2531465291976929
LOSS: 1.3496202230453491
LOSS: 1.3443987369537354
LOSS: 1.3395591974258423
LOSS: 1.3351356983184814
LOSS: 1.331161618232727
LOSS: 1.3276517391204834
LOSS: 1.3245913982391357
LOSS: 1.3219270706176758
LOSS: 1.3195618391036987
LOSS: 1.3173637390136719
LOSS: 1.3151962757110596
LOSS: 1.3129534721374512
LOSS: 1.3105851411819458
LOSS: 1.3080964088439941
LOSS: 1.3055323362350464
LOSS: 1.3029563426971436
LOSS: 1.3004307746887207
LOSS: 1.2980051040649414
LOSS: 1.29570734500885
LOSS: 1.2935456037521362
LOSS: 1.2915093898773193
LOSS: 1.2895797491073608
LOSS: 1.287735939025879
LOSS: 1.2859636545181274
LOSS: 1.2842581272125244
LOSS: 1.2826259136199951
LOSS: 1.281080722808838
LOSS: 1.2796411514282227
LOSS: 1.2783260345458984
LOSS: 1.2771490812301636
LOSS: 1.2761162519454956
LOSS: 1.275223731994629
LOSS: 1.27445650100708
LOSS: 1.2737925052642822
LOSS: 1.2732049226760864
LOSS: 1.2726671695709229
LOSS: 1.2721586227416992
LOSS: 1.27166748046875
LOSS: 1.2711886167526245
LOSS: 1.270722508430481
LOSS: 1.270271897315979
LOSS: 1.2698400020599365
LOSS: 1.269426941871643
LOSS: 1.269030213356018
LOSS: 1.2686446905136108
LOSS: 1.2682640552520752
LOSS: 1.2678813934326172
LOSS: 1.2674930095672607
LOSS: 1.2670985460281372
LOSS: 1.2666966915130615
LOSS: 1.266294240951538
LOSS: 1.2658978700637817
LOSS: 1.265514850616455
LOSS: 1.265152931213379
LOSS: 1.2648138999938965
LOSS: 1.264499545097351
LOSS: 1.2642077207565308
LOSS: 1.2639344930648804
LOSS: 1.2636761665344238
LOSS: 1.2634313106536865
LOSS: 1.2631971836090088
LOSS: 1.2629725933074951
LOSS: 1.3372418880462646
LOSS: 1.3331365585327148
LOSS: 1.3295197486877441
LOSS: 1.3263778686523438
LOSS: 1.323648452758789
LOSS: 1.3212215900421143
LOSS: 1.3189589977264404
LOSS: 1.3167314529418945
LOSS: 1.3144563436508179
LOSS: 1.3121111392974854
LOSS: 1.3097233772277832
LOSS: 1.3073484897613525
LOSS: 1.3050488233566284
LOSS: 1.302877426147461
LOSS: 1.3008695840835571
LOSS: 1.2990386486053467
LOSS: 1.2973772287368774
LOSS: 1.295862078666687
LOSS: 1.2944612503051758
LOSS: 1.2931421995162964
LOSS: 1.2918792963027954
LOSS: 1.2906574010849
LOSS: 1.2894731760025024
LOSS: 1.2883329391479492
LOSS: 1.2872499227523804
LOSS: 1.2862372398376465
LOSS: 1.2853057384490967
LOSS: 1.2844581604003906
LOSS: 1.283691167831421
LOSS: 1.2829924821853638
LOSS: 1.2823485136032104
LOSS: 1.2817445993423462
LOSS: 1.2811715602874756
LOSS: 1.2806243896484375
LOSS: 1.280105471611023
LOSS: 1.2796189785003662
LOSS: 1.2791705131530762
LOSS: 1.2787632942199707
LOSS: 1.2783970832824707
LOSS: 1.2780689001083374
LOSS: 1.2777737379074097
LOSS: 1.2775039672851562
LOSS: 1.2772526741027832
LOSS: 1.277013897895813
LOSS: 1.276782751083374
LOSS: 1.2765579223632812
LOSS: 1.2763395309448242
LOSS: 1.2761276960372925
LOSS: 1.2759239673614502
LOSS: 1.2757298946380615
LOSS: 1.2755444049835205
LOSS: 1.2753664255142212
LOSS: 1.2751926183700562
LOSS: 1.275022268295288
LOSS: 1.2748510837554932
LOSS: 1.2746764421463013
LOSS: 1.274497628211975
LOSS: 1.2743138074874878
LOSS: 1.2741234302520752
LOSS: 1.2739243507385254
LOSS: 1.2737168073654175
LOSS: 1.2734957933425903
LOSS: 1.2732614278793335
LOSS: 1.2730112075805664
LOSS: 1.2727446556091309
LOSS: 1.2724621295928955
LOSS: 1.2721660137176514
LOSS: 1.271859049797058
LOSS: 1.2715442180633545
LOSS: 1.2712260484695435
LOSS: 1.2709081172943115
LOSS: 1.2705937623977661
LOSS: 1.270286202430725
LOSS: 1.269989252090454
LOSS: 1.2697038650512695
LOSS: 1.2694330215454102
LOSS: 1.2691779136657715
LOSS: 1.2689393758773804
LOSS: 1.268716812133789
LOSS: 1.2685108184814453
LOSS: 1.2683188915252686
LOSS: 1.268140435218811
LOSS: 1.267974615097046
LOSS: 1.267817735671997
LOSS: 1.2676701545715332
LOSS: 1.2675310373306274
LOSS: 1.2673978805541992
LOSS: 1.2672710418701172
LOSS: 1.2671513557434082
LOSS: 1.2670356035232544
LOSS: 1.2669256925582886
LOSS: 1.2668201923370361
LOSS: 1.2667189836502075
LOSS: 1.2666220664978027
LOSS: 1.2665290832519531
LOSS: 1.2664391994476318
LOSS: 1.2663516998291016
LOSS: 1.3482834100723267
LOSS: 1.3428949117660522
LOSS: 1.337864875793457
LOSS: 1.3332322835922241
LOSS: 1.3290282487869263
LOSS: 1.325266718864441
LOSS: 1.3219335079193115
LOSS: 1.3189740180969238
LOSS: 1.316288948059082
LOSS: 1.313746690750122
LOSS: 1.3112144470214844
LOSS: 1.3085952997207642
LOSS: 1.3058481216430664
LOSS: 1.3029838800430298
LOSS: 1.3000496625900269
LOSS: 1.2971060276031494
LOSS: 1.2942121028900146
LOSS: 1.2914130687713623
LOSS: 1.2887341976165771
LOSS: 1.2861815690994263
LOSS: 1.2837451696395874
LOSS: 1.2814066410064697
LOSS: 1.2791469097137451
LOSS: 1.2769526243209839
LOSS: 1.2748206853866577
LOSS: 1.2727570533752441
LOSS: 1.2707762718200684
LOSS: 1.2688966989517212
LOSS: 1.2671359777450562
LOSS: 1.265507698059082
LOSS: 1.2640166282653809
LOSS: 1.2626599073410034
LOSS: 1.26142418384552
LOSS: 1.260291337966919
LOSS: 1.2592405080795288
LOSS: 1.2582534551620483
LOSS: 1.257318139076233
LOSS: 1.2564289569854736
LOSS: 1.2555856704711914
LOSS: 1.2547935247421265
LOSS: 1.2540570497512817
LOSS: 1.2533788681030273
LOSS: 1.252759337425232
LOSS: 1.2521940469741821
LOSS: 1.2516770362854004
LOSS: 1.2511998414993286
LOSS: 1.2507562637329102
LOSS: 1.2503407001495361
LOSS: 1.2499516010284424
LOSS: 1.249587893486023
LOSS: 1.2492496967315674
LOSS: 1.2489393949508667
LOSS: 1.2486572265625
LOSS: 1.2484009265899658
LOSS: 1.248169183731079
LOSS: 1.247957468032837
LOSS: 1.2477625608444214
LOSS: 1.247580885887146
LOSS: 1.2474101781845093
LOSS: 1.2472493648529053
LOSS: 1.2470996379852295
LOSS: 1.246958613395691
LOSS: 1.2468276023864746
LOSS: 1.2467067241668701
LOSS: 1.2465941905975342
LOSS: 1.2464877367019653
LOSS: 1.2463880777359009
LOSS: 1.2462915182113647
LOSS: 1.2461986541748047
LOSS: 1.246109127998352
LOSS: 1.246023416519165
LOSS: 1.2459404468536377
LOSS: 1.2458624839782715
LOSS: 1.2457876205444336
LOSS: 1.2457162141799927
LOSS: 1.2456482648849487
LOSS: 1.2455823421478271
LOSS: 1.2455185651779175
LOSS: 1.2454569339752197
LOSS: 1.2453972101211548
LOSS: 1.2453396320343018
LOSS: 1.2452841997146606
LOSS: 1.2452309131622314
LOSS: 1.2451794147491455
LOSS: 1.2451295852661133
LOSS: 1.2450817823410034
LOSS: 1.24503493309021
LOSS: 1.244989037513733
LOSS: 1.2449442148208618
LOSS: 1.2449008226394653
LOSS: 1.244857668876648
LOSS: 1.244816780090332
LOSS: 1.2447764873504639
LOSS: 1.244737148284912
LOSS: 1.2446982860565186
LOSS: 1.2446603775024414
LOSS: 1.2446233034133911
LOSS: 1.2445861101150513
LOSS: 1.2445502281188965
LOSS: 1.2445149421691895
LOSS: 1.3700227737426758
LOSS: 1.3656330108642578
LOSS: 1.36160147190094
LOSS: 1.3579576015472412
LOSS: 1.3547016382217407
LOSS: 1.3517811298370361
LOSS: 1.3490805625915527
LOSS: 1.34645676612854
LOSS: 1.3438081741333008
LOSS: 1.341110110282898
LOSS: 1.3383959531784058
LOSS: 1.3357223272323608
LOSS: 1.3331414461135864
LOSS: 1.3306877613067627
LOSS: 1.3283699750900269
LOSS: 1.326171636581421
LOSS: 1.324060320854187
LOSS: 1.322000503540039
LOSS: 1.3199659585952759
LOSS: 1.3179471492767334
LOSS: 1.3159514665603638
LOSS: 1.314000129699707
LOSS: 1.312121033668518
LOSS: 1.3103452920913696
LOSS: 1.3086988925933838
LOSS: 1.3072025775909424
LOSS: 1.3058662414550781
LOSS: 1.3046916723251343
LOSS: 1.3036729097366333
LOSS: 1.3028000593185425
LOSS: 1.302062749862671
LOSS: 1.3014506101608276
LOSS: 1.3009525537490845
LOSS: 1.300557017326355
LOSS: 1.3002463579177856
LOSS: 1.300002932548523
LOSS: 1.2998056411743164
LOSS: 1.299634337425232
LOSS: 1.2994729280471802
LOSS: 1.2993083000183105
LOSS: 1.2991348505020142
LOSS: 1.2989516258239746
LOSS: 1.2987604141235352
LOSS: 1.298563838005066
LOSS: 1.2983660697937012
LOSS: 1.2981666326522827
LOSS: 1.2979661226272583
LOSS: 1.2977635860443115
LOSS: 1.2975592613220215
LOSS: 1.2973531484603882
LOSS: 1.2971478700637817
LOSS: 1.296945333480835
LOSS: 1.2967442274093628
LOSS: 1.2965458631515503
LOSS: 1.2963459491729736
LOSS: 1.2961416244506836
LOSS: 1.29592764377594
LOSS: 1.2957017421722412
LOSS: 1.2954622507095337
LOSS: 1.295211672782898
LOSS: 1.2949541807174683
LOSS: 1.2946953773498535
LOSS: 1.2944425344467163
LOSS: 1.29420006275177
LOSS: 1.293971300125122
LOSS: 1.2937573194503784
LOSS: 1.2935553789138794
LOSS: 1.2933624982833862
LOSS: 1.2931756973266602
LOSS: 1.2929918766021729
LOSS: 1.292808175086975
LOSS: 1.2926223278045654
LOSS: 1.292432427406311
LOSS: 1.2922357320785522
LOSS: 1.2920308113098145
LOSS: 1.2918157577514648
LOSS: 1.2915884256362915
LOSS: 1.291344404220581
LOSS: 1.2910821437835693
LOSS: 1.2907960414886475
LOSS: 1.2904820442199707
LOSS: 1.2901362180709839
LOSS: 1.2897555828094482
LOSS: 1.2893377542495728
LOSS: 1.288882851600647
LOSS: 1.2883936166763306
LOSS: 1.2878718376159668
LOSS: 1.2873239517211914
LOSS: 1.2867565155029297
LOSS: 1.2861753702163696
LOSS: 1.285590410232544
LOSS: 1.2850098609924316
LOSS: 1.2844429016113281
LOSS: 1.2838959693908691
LOSS: 1.2833752632141113
LOSS: 1.2828857898712158
LOSS: 1.2824301719665527
LOSS: 1.2820091247558594
LOSS: 1.2816225290298462
LOSS: 1.281268835067749
LOSS: 1.354232668876648
LOSS: 1.3490283489227295
LOSS: 1.3442245721817017
LOSS: 1.3398573398590088
LOSS: 1.335943341255188
LOSS: 1.3324683904647827
LOSS: 1.3293832540512085
LOSS: 1.3265944719314575
LOSS: 1.3239753246307373
LOSS: 1.3214036226272583
LOSS: 1.3188024759292603
LOSS: 1.3161526918411255
LOSS: 1.313480019569397
LOSS: 1.3108329772949219
LOSS: 1.3082612752914429
LOSS: 1.3058017492294312
LOSS: 1.3034690618515015
LOSS: 1.3012545108795166
LOSS: 1.2991344928741455
LOSS: 1.297080397605896
LOSS: 1.2950676679611206
LOSS: 1.2930829524993896
LOSS: 1.291126012802124
LOSS: 1.2892088890075684
LOSS: 1.287353277206421
LOSS: 1.2855843305587769
LOSS: 1.2839267253875732
LOSS: 1.2823981046676636
LOSS: 1.2810059785842896
LOSS: 1.2797470092773438
LOSS: 1.278605341911316
LOSS: 1.2775605916976929
LOSS: 1.2765882015228271

LOSS: 1.2729599475860596
LOSS: 1.2729147672653198
LOSS: 1.358837366104126
LOSS: 1.3534045219421387
LOSS: 1.348307728767395
LOSS: 1.3435863256454468
LOSS: 1.3392674922943115
LOSS: 1.3353548049926758
LOSS: 1.3318126201629639
LOSS: 1.3285564184188843
LOSS: 1.3254629373550415
LOSS: 1.322401523590088
LOSS: 1.319280982017517
LOSS: 1.3160676956176758
LOSS: 1.3127832412719727
LOSS: 1.3094807863235474
LOSS: 1.3062235116958618
LOSS: 1.3030672073364258
LOSS: 1.300049066543579
LOSS: 1.297184705734253
LOSS: 1.294468641281128
LOSS: 1.2918838262557983
LOSS: 1.2894104719161987
LOSS: 1.287034034729004
LOSS: 1.284751296043396
LOSS: 1.2825684547424316
LOSS: 1.2804999351501465
LOSS: 1.2785612344741821
LOSS: 1.2767658233642578
LOSS: 1.2751187086105347
LOSS: 1.273614525794983
LOSS: 1.2722373008728027
LOSS: 1.2709614038467407
LOSS: 1.2697595357894897
LOSS: 1.2686041593551636
LOSS: 1.2674778699874878
LOSS: 1.2663702964782715
LOSS: 1.2652807235717773
LOSS: 1.2642123699188232
LOSS: 1.2631702423095703
LOSS: 1.2621575593948364
LOSS: 1.2611727714538574
LOSS: 1.260211706161499
LOSS: 1.259268045425415
LOSS: 1.258337140083313
LOSS: 1.2574152946472168
LOSS: 1.2565038204193115
LOSS: 1.2556076049804688
LOSS: 1.2547321319580078
LOSS: 1.2538845539093018
LOSS: 1.253070592880249
LOSS: 1.2522938251495361
LOSS: 1.2515549659729004
LOSS: 1.2508524656295776
LOSS: 1.2501837015151978
LOSS: 1.249547004699707
LOSS: 1.2489418983459473
LOSS: 1.2483664751052856
LOSS: 1.2478222846984863
LOSS: 1.2473084926605225
LOSS: 1.2468243837356567
LOSS: 1.2463676929473877
LOSS: 1.2459375858306885
LOSS: 1.2455295324325562
LOSS: 1.245143175125122
LOSS: 1.2447773218154907
LOSS: 1.2444310188293457
LOSS: 1.2441037893295288
LOSS: 1.2437958717346191
LOSS: 1.2435060739517212
LOSS: 1.243234395980835
LOSS: 1.2429784536361694
LOSS: 1.2427376508712769
LOSS: 1.2425103187561035
LOSS: 1.242295742034912
LOSS: 1.2420930862426758
LOSS: 1.2419027090072632
LOSS: 1.2417237758636475
LOSS: 1.241554856300354
LOSS: 1.2413959503173828
LOSS: 1.2412450313568115
LOSS: 1.2411015033721924
LOSS: 1.2409639358520508
LOSS: 1.240831732749939
LOSS: 1.2407044172286987
LOSS: 1.240581750869751
LOSS: 1.2404643297195435
LOSS: 1.2403512001037598
LOSS: 1.2402430772781372
LOSS: 1.2401392459869385
LOSS: 1.2400394678115845
LOSS: 1.2399438619613647
LOSS: 1.2398518323898315
LOSS: 1.239763855934143
LOSS: 1.239679217338562
LOSS: 1.2395977973937988
LOSS: 1.2395191192626953
LOSS: 1.2394428253173828
LOSS: 1.2393691539764404
LOSS: 1.2392969131469727
LOSS: 1.2392268180847168
LOSS: 1.2391586303710938
LOSS: 1.3482834100723267
LOSS: 1.3428949117660522
LOSS: 1.337864875793457
LOSS: 1.3332322835922241
LOSS: 1.3290282487869263
LOSS: 1.325266718864441
LOSS: 1.3219335079193115
LOSS: 1.3189740180969238
LOSS: 1.316288948059082
LOSS: 1.313746690750122
LOSS: 1.3112144470214844
LOSS: 1.3085952997207642
LOSS: 1.3058481216430664
LOSS: 1.3029838800430298
LOSS: 1.3000496625900269
LOSS: 1.2971060276031494
LOSS: 1.2942121028900146
LOSS: 1.2914130687713623
LOSS: 1.2887341976165771
LOSS: 1.2861815690994263
LOSS: 1.2837451696395874
LOSS: 1.2814066410064697
LOSS: 1.2791469097137451
LOSS: 1.2769526243209839
LOSS: 1.2748206853866577
LOSS: 1.2727570533752441
LOSS: 1.2707762718200684
LOSS: 1.2688966989517212
LOSS: 1.2671359777450562
LOSS: 1.265507698059082
LOSS: 1.2640166282653809
LOSS: 1.2626599073410034
LOSS: 1.26142418384552
LOSS: 1.260291337966919
LOSS: 1.2592405080795288
LOSS: 1.2582534551620483
LOSS: 1.257318139076233
LOSS: 1.2564289569854736
LOSS: 1.2555856704711914
LOSS: 1.2547935247421265
LOSS: 1.2540570497512817
LOSS: 1.2533788681030273
LOSS: 1.252759337425232
LOSS: 1.2521940469741821
LOSS: 1.2516770362854004
LOSS: 1.2511998414993286
LOSS: 1.2507562637329102
LOSS: 1.2503407001495361
LOSS: 1.2499516010284424
LOSS: 1.249587893486023
LOSS: 1.2492496967315674
LOSS: 1.2489393949508667
LOSS: 1.2486572265625
LOSS: 1.2484009265899658
LOSS: 1.248169183731079
LOSS: 1.247957468032837
LOSS: 1.2477625608444214
LOSS: 1.247580885887146
LOSS: 1.2474101781845093
LOSS: 1.2472493648529053
LOSS: 1.2470996379852295
LOSS: 1.246958613395691
LOSS: 1.2468276023864746
LOSS: 1.2467067241668701
LOSS: 1.2465941905975342
LOSS: 1.2464877367019653
LOSS: 1.2463880777359009
LOSS: 1.2462915182113647
LOSS: 1.2461986541748047
LOSS: 1.246109127998352
LOSS: 1.246023416519165
LOSS: 1.2459404468536377
LOSS: 1.2458624839782715
LOSS: 1.2457876205444336
LOSS: 1.2457162141799927
LOSS: 1.2456482648849487
LOSS: 1.2455823421478271
LOSS: 1.2455185651779175
LOSS: 1.2454569339752197
LOSS: 1.2453972101211548
LOSS: 1.2453396320343018
LOSS: 1.2452841997146606
LOSS: 1.2452309131622314
LOSS: 1.2451794147491455
LOSS: 1.2451295852661133
LOSS: 1.2450817823410034
LOSS: 1.24503493309021
LOSS: 1.244989037513733
LOSS: 1.2449442148208618
LOSS: 1.2449008226394653
LOSS: 1.244857668876648
LOSS: 1.244816780090332
LOSS: 1.2447764873504639
LOSS: 1.244737148284912
LOSS: 1.2446982860565186
LOSS: 1.2446603775024414
LOSS: 1.2446233034133911
LOSS: 1.2445861101150513
LOSS: 1.2445502281188965
LOSS: 1.2445149421691895
LOSS: 1.3709746599197388
LOSS: 1.366603136062622
LOSS: 1.362654685974121
LOSS: 1.3591190576553345
LOSS: 1.3559625148773193
LOSS: 1.3530768156051636
LOSS: 1.3502862453460693
LOSS: 1.347432017326355
LOSS: 1.3444494009017944
LOSS: 1.341362714767456
LOSS: 1.3382394313812256
LOSS: 1.3351523876190186
LOSS: 1.332157850265503
LOSS: 1.329284429550171
LOSS: 1.3265328407287598
LOSS: 1.3238837718963623
LOSS: 1.3213119506835938
LOSS: 1.318800687789917
LOSS: 1.3163477182388306
LOSS: 1.3139663934707642
LOSS: 1.3116786479949951
LOSS: 1.3095078468322754
LOSS: 1.3074713945388794
LOSS: 1.3055752515792847
LOSS: 1.3038121461868286
LOSS: 1.3021655082702637
LOSS: 1.300618052482605
LOSS: 1.2991583347320557
LOSS: 1.297784686088562
LOSS: 1.2965033054351807
LOSS: 1.2953224182128906
LOSS: 1.294247031211853
LOSS: 1.2932751178741455
LOSS: 1.2923959493637085
LOSS: 1.2915945053100586
LOSS: 1.2908539772033691
LOSS: 1.2901611328125
LOSS: 1.2895082235336304
LOSS: 1.288892388343811
LOSS: 1.288314938545227
LOSS: 1.2877769470214844
LOSS: 1.2872782945632935
LOSS: 1.2868157625198364
LOSS: 1.2863856554031372
LOSS: 1.2859822511672974
LOSS: 1.2856026887893677
LOSS: 1.2852470874786377
LOSS: 1.284915566444397
LOSS: 1.2846088409423828
LOSS: 1.2843257188796997
LOSS: 1.2840622663497925
LOSS: 1.2838149070739746
LOSS: 1.283578872680664
LOSS: 1.2833518981933594
LOSS: 1.283132791519165
LOSS: 1.282922625541687
LOSS: 1.2827231884002686
LOSS: 1.2825353145599365
LOSS: 1.2823593616485596
LOSS: 1.282193899154663
LOSS: 1.2820371389389038
LOSS: 1.2818881273269653
LOSS: 1.2817456722259521
LOSS: 1.2816088199615479
LOSS: 1.281477928161621
LOSS: 1.2813526391983032
LOSS: 1.2812317609786987
LOSS: 1.2811155319213867
LOSS: 1.281003713607788
LOSS: 1.2808947563171387
LOSS: 1.2807904481887817
LOSS: 1.280690312385559
LOSS: 1.2805941104888916
LOSS: 1.2805019617080688
LOSS: 1.2804133892059326
LOSS: 1.2803269624710083
LOSS: 1.2802433967590332
LOSS: 1.2801614999771118
LOSS: 1.2800822257995605
LOSS: 1.2800053358078003
LOSS: 1.2799303531646729
LOSS: 1.2798577547073364
LOSS: 1.2797874212265015
LOSS: 1.2797198295593262
LOSS: 1.2796530723571777
LOSS: 1.279587745666504
LOSS: 1.279524564743042
LOSS: 1.279463768005371
LOSS: 1.2794034481048584
LOSS: 1.2793445587158203
LOSS: 1.2792868614196777
LOSS: 1.2792309522628784
LOSS: 1.2791759967803955
LOSS: 1.2791221141815186
LOSS: 1.2790696620941162
LOSS: 1.2790186405181885
LOSS: 1.2789682149887085
LOSS: 1.2789183855056763
LOSS: 1.2788704633712769
LOSS: 1.2788236141204834
LOSS: 1.3505852222442627
LOSS: 1.3450922966003418
LOSS: 1.3399248123168945
LOSS: 1.3351247310638428
LOSS: 1.3307266235351562
LOSS: 1.3267513513565063
LOSS: 1.3231955766677856
LOSS: 1.3200228214263916
LOSS: 1.317156434059143
LOSS: 1.3144866228103638
LOSS: 1.3118929862976074
LOSS: 1.3092772960662842
LOSS: 1.3065879344940186
LOSS: 1.3038252592086792
LOSS: 1.3010282516479492
LOSS: 1.2982559204101562
LOSS: 1.2955671548843384
LOSS: 1.2930110692977905
LOSS: 1.2906172275543213
LOSS: 1.2883968353271484
LOSS: 1.2863446474075317
LOSS: 1.28444504737854
LOSS: 1.2826789617538452
LOSS: 1.281030297279358
LOSS: 1.2794893980026245
LOSS: 1.2780532836914062
LOSS: 1.2767257690429688
LOSS: 1.2755132913589478
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
LOSS: 1.2931756973266602
LOSS: 1.2929918766021729
LOSS: 1.292808175086975
LOSS: 1.2926223278045654
LOSS: 1.292432427406311
LOSS: 1.2922357320785522
LOSS: 1.2920308113098145
LOSS: 1.2918157577514648
LOSS: 1.2915884256362915
LOSS: 1.291344404220581
LOSS: 1.2910821437835693
LOSS: 1.2907960414886475
LOSS: 1.2904820442199707
LOSS: 1.2901362180709839
LOSS: 1.2897555828094482
LOSS: 1.2893377542495728
LOSS: 1.288882851600647
LOSS: 1.2883936166763306
LOSS: 1.2878718376159668
LOSS: 1.2873239517211914
LOSS: 1.2867565155029297
LOSS: 1.2861753702163696
LOSS: 1.285590410232544
LOSS: 1.2850098609924316
LOSS: 1.2844429016113281
LOSS: 1.2838959693908691
LOSS: 1.2833752632141113
LOSS: 1.2828857898712158
LOSS: 1.2824301719665527
LOSS: 1.2820091247558594
LOSS: 1.2816225290298462
LOSS: 1.281268835067749
LOSS: 1.358837366104126
LOSS: 1.3534045219421387
LOSS: 1.348307728767395
LOSS: 1.3435863256454468
LOSS: 1.3392674922943115
LOSS: 1.3353548049926758
LOSS: 1.3318126201629639
LOSS: 1.3285564184188843
LOSS: 1.3254629373550415
LOSS: 1.322401523590088
LOSS: 1.319280982017517
LOSS: 1.3160676956176758
LOSS: 1.3127832412719727
LOSS: 1.3094807863235474
LOSS: 1.3062235116958618
LOSS: 1.3030672073364258
LOSS: 1.300049066543579
LOSS: 1.297184705734253
LOSS: 1.294468641281128
LOSS: 1.2918838262557983
LOSS: 1.2894104719161987
LOSS: 1.287034034729004
LOSS: 1.284751296043396
LOSS: 1.2825684547424316
LOSS: 1.2804999351501465
LOSS: 1.2785612344741821
LOSS: 1.2767658233642578
LOSS: 1.2751187086105347
LOSS: 1.273614525794983
LOSS: 1.2722373008728027
LOSS: 1.2709614038467407
LOSS: 1.2697595357894897
LOSS: 1.2686041593551636
LOSS: 1.2674778699874878
LOSS: 1.2663702964782715
LOSS: 1.2652807235717773
LOSS: 1.2642123699188232
LOSS: 1.2631702423095703
LOSS: 1.2621575593948364
LOSS: 1.2611727714538574
LOSS: 1.260211706161499
LOSS: 1.259268045425415
LOSS: 1.258337140083313
LOSS: 1.2574152946472168
LOSS: 1.2565038204193115
LOSS: 1.2556076049804688
LOSS: 1.2547321319580078
LOSS: 1.2538845539093018
LOSS: 1.253070592880249
LOSS: 1.2522938251495361
LOSS: 1.2515549659729004
LOSS: 1.2508524656295776
LOSS: 1.2501837015151978
LOSS: 1.249547004699707
LOSS: 1.2489418983459473
LOSS: 1.2483664751052856
LOSS: 1.2478222846984863
LOSS: 1.2473084926605225
LOSS: 1.2468243837356567
LOSS: 1.2463676929473877
LOSS: 1.2459375858306885
LOSS: 1.2455295324325562
LOSS: 1.245143175125122
LOSS: 1.2447773218154907
LOSS: 1.2444310188293457
LOSS: 1.2441037893295288
LOSS: 1.2437958717346191
LOSS: 1.2435060739517212
LOSS: 1.243234395980835
LOSS: 1.2429784536361694
LOSS: 1.2427376508712769
LOSS: 1.2425103187561035
LOSS: 1.242295742034912
LOSS: 1.2420930862426758
LOSS: 1.2419027090072632
LOSS: 1.2417237758636475
LOSS: 1.241554856300354
LOSS: 1.2413959503173828
LOSS: 1.2412450313568115
LOSS: 1.2411015033721924
LOSS: 1.2409639358520508
LOSS: 1.240831732749939
LOSS: 1.2407044172286987
LOSS: 1.240581750869751
LOSS: 1.2404643297195435
LOSS: 1.2403512001037598
LOSS: 1.2402430772781372
LOSS: 1.2401392459869385
LOSS: 1.2400394678115845
LOSS: 1.2399438619613647
LOSS: 1.2398518323898315
LOSS: 1.239763855934143
LOSS: 1.239679217338562
LOSS: 1.2395977973937988
LOSS: 1.2395191192626953
LOSS: 1.2394428253173828
LOSS: 1.2393691539764404
LOSS: 1.2392969131469727
LOSS: 1.2392268180847168
LOSS: 1.2391586303710938
LOSS: 1.3351126909255981
LOSS: 1.328938364982605
LOSS: 1.3230929374694824
LOSS: 1.317617654800415
LOSS: 1.3125474452972412
LOSS: 1.3079055547714233
LOSS: 1.3036935329437256
LOSS: 1.2998836040496826
LOSS: 1.2964133024215698
LOSS: 1.293192982673645
LOSS: 1.2901225090026855
LOSS: 1.2871111631393433
LOSS: 1.2840914726257324
LOSS: 1.2810274362564087
LOSS: 1.2779139280319214
LOSS: 1.2747747898101807
LOSS: 1.2716541290283203
LOSS: 1.2686067819595337
LOSS: 1.2656875848770142
LOSS: 1.2629441022872925
LOSS: 1.2604092359542847
LOSS: 1.2580987215042114
LOSS: 1.2560112476348877
LOSS: 1.2541319131851196
LOSS: 1.252435326576233
LOSS: 1.250892996788025
LOSS: 1.2494771480560303
LOSS: 1.248162865638733
LOSS: 1.2469327449798584
LOSS: 1.245775580406189
LOSS: 1.2446850538253784
LOSS: 1.243659257888794
LOSS: 1.242698311805725
LOSS: 1.2418017387390137
LOSS: 1.2409687042236328
LOSS: 1.2401942014694214
LOSS: 1.2394726276397705
LOSS: 1.2387961149215698
LOSS: 1.2381565570831299
LOSS: 1.2375454902648926
LOSS: 1.236958384513855
LOSS: 1.2363917827606201
LOSS: 1.235844612121582
LOSS: 1.2353194952011108
LOSS: 1.2348172664642334
LOSS: 1.2343425750732422
LOSS: 1.233896017074585
LOSS: 1.2334797382354736
LOSS: 1.2330938577651978
LOSS: 1.2327367067337036
LOSS: 1.2324068546295166
LOSS: 1.2321035861968994
LOSS: 1.231824278831482
LOSS: 1.2315700054168701
LOSS: 1.2313381433486938
LOSS: 1.2311294078826904
LOSS: 1.2309426069259644
LOSS: 1.2307767868041992
LOSS: 1.230629324913025
LOSS: 1.2304965257644653
LOSS: 1.2303770780563354
LOSS: 1.2302664518356323
LOSS: 1.2301623821258545
LOSS: 1.230062484741211
LOSS: 1.2299656867980957
LOSS: 1.2298715114593506
LOSS: 1.2297791242599487
LOSS: 1.2296903133392334
LOSS: 1.2296041250228882
LOSS: 1.2295221090316772
LOSS: 1.229443907737732
LOSS: 1.2293685674667358
LOSS: 1.2292972803115845
LOSS: 1.2292282581329346
LOSS: 1.229162335395813
LOSS: 1.2290990352630615
LOSS: 1.2290384769439697
LOSS: 1.2289801836013794
LOSS: 1.2289246320724487
LOSS: 1.2288711071014404
LOSS: 1.2288187742233276
LOSS: 1.2287683486938477
LOSS: 1.228719711303711
LOSS: 1.2286715507507324
LOSS: 1.228624939918518
LOSS: 1.2285798788070679
LOSS: 1.2285351753234863
LOSS: 1.2284919023513794
LOSS: 1.2284502983093262
LOSS: 1.2284096479415894
LOSS: 1.2283700704574585
LOSS: 1.2283316850662231
LOSS: 1.228293776512146
LOSS: 1.228257417678833
LOSS: 1.22822105884552
LOSS: 1.2281855344772339
LOSS: 1.2281506061553955
LOSS: 1.228116750717163
LOSS: 1.2280831336975098
LOSS: 1.2280502319335938
LOSS: 1.3709746599197388
LOSS: 1.366603136062622
LOSS: 1.362654685974121
LOSS: 1.3591190576553345
LOSS: 1.3559625148773193
LOSS: 1.3530768156051636
LOSS: 1.3502862453460693
LOSS: 1.347432017326355
LOSS: 1.3444494009017944
LOSS: 1.341362714767456
LOSS: 1.3382394313812256
LOSS: 1.3351523876190186
LOSS: 1.332157850265503
LOSS: 1.329284429550171
LOSS: 1.3265328407287598
LOSS: 1.3238837718963623
LOSS: 1.3213119506835938
LOSS: 1.318800687789917
LOSS: 1.3163477182388306
LOSS: 1.3139663934707642
LOSS: 1.3116786479949951
LOSS: 1.3095078468322754
LOSS: 1.3074713945388794
LOSS: 1.3055752515792847
LOSS: 1.3038121461868286
LOSS: 1.3021655082702637
LOSS: 1.300618052482605
LOSS: 1.2991583347320557
LOSS: 1.297784686088562
LOSS: 1.2965033054351807
LOSS: 1.2953224182128906
LOSS: 1.294247031211853
LOSS: 1.2932751178741455
LOSS: 1.2923959493637085
LOSS: 1.2915945053100586
LOSS: 1.2908539772033691
LOSS: 1.2901611328125
LOSS: 1.2895082235336304
LOSS: 1.288892388343811
LOSS: 1.288314938545227
LOSS: 1.2877769470214844
LOSS: 1.2872782945632935
LOSS: 1.2868157625198364
LOSS: 1.2863856554031372
LOSS: 1.2859822511672974
LOSS: 1.2856026887893677
LOSS: 1.2852470874786377
LOSS: 1.284915566444397
LOSS: 1.2846088409423828
LOSS: 1.2843257188796997
LOSS: 1.2840622663497925
LOSS: 1.2838149070739746
LOSS: 1.283578872680664
LOSS: 1.2833518981933594
LOSS: 1.283132791519165
LOSS: 1.282922625541687
LOSS: 1.2827231884002686
LOSS: 1.2825353145599365
LOSS: 1.2823593616485596
LOSS: 1.282193899154663
LOSS: 1.2820371389389038
LOSS: 1.2818881273269653
LOSS: 1.2817456722259521
LOSS: 1.2816088199615479
LOSS: 1.281477928161621
LOSS: 1.2813526391983032
LOSS: 1.2812317609786987
LOSS: 1.2811155319213867
LOSS: 1.281003713607788
LOSS: 1.2808947563171387
LOSS: 1.2807904481887817
LOSS: 1.280690312385559
LOSS: 1.2805941104888916
LOSS: 1.2805019617080688
LOSS: 1.2804133892059326
LOSS: 1.2803269624710083
LOSS: 1.2802433967590332
LOSS: 1.2801614999771118
LOSS: 1.2800822257995605
LOSS: 1.2800053358078003
LOSS: 1.2799303531646729
LOSS: 1.2798577547073364
LOSS: 1.2797874212265015
LOSS: 1.2797198295593262
LOSS: 1.2796530723571777
LOSS: 1.279587745666504
LOSS: 1.279524564743042
LOSS: 1.279463768005371
LOSS: 1.2794034481048584
LOSS: 1.2793445587158203
LOSS: 1.2792868614196777
LOSS: 1.2792309522628784
LOSS: 1.2791759967803955
LOSS: 1.2791221141815186
LOSS: 1.2790696620941162
LOSS: 1.2790186405181885
LOSS: 1.2789682149887085
LOSS: 1.2789183855056763
LOSS: 1.2627592086791992
LOSS: 1.2625541687011719
LOSS: 1.2623579502105713
LOSS: 1.2621686458587646
LOSS: 1.261984944343567
LOSS: 1.261804223060608
LOSS: 1.2616246938705444
LOSS: 1.2614471912384033
LOSS: 1.261269450187683
LOSS: 1.2610926628112793
LOSS: 1.2609156370162964
LOSS: 1.2607413530349731
LOSS: 1.2605668306350708
LOSS: 1.2603925466537476
LOSS: 1.2602190971374512
LOSS: 1.260042667388916
LOSS: 1.2598645687103271
LOSS: 1.2596851587295532
LOSS: 1.2595027685165405
LOSS: 1.2593189477920532
LOSS: 1.2591347694396973
LOSS: 1.2589473724365234
LOSS: 1.2587610483169556
LOSS: 1.2585753202438354
LOSS: 1.2583897113800049
LOSS: 1.2582063674926758
LOSS: 1.2580267190933228
LOSS: 1.2578511238098145
LOSS: 1.2576804161071777
LOSS: 1.2575161457061768
LOSS: 1.2573548555374146
LOSS: 1.2571998834609985
LOSS: 1.2570489645004272
LOSS: 1.2569011449813843
LOSS: 1.2567576169967651
LOSS: 1.256618618965149
LOSS: 1.2564857006072998
LOSS: 1.256358027458191
LOSS: 1.3563858270645142
LOSS: 1.3513699769973755
LOSS: 1.34676992893219
LOSS: 1.3426315784454346
LOSS: 1.3389860391616821
LOSS: 1.3358337879180908
LOSS: 1.3331270217895508
LOSS: 1.3307586908340454
LOSS: 1.328580379486084
LOSS: 1.326451063156128
LOSS: 1.3242827653884888
LOSS: 1.3220529556274414
LOSS: 1.3197864294052124
LOSS: 1.3175302743911743
LOSS: 1.3153338432312012
LOSS: 1.313236951828003
LOSS: 1.3112598657608032
LOSS: 1.3094037771224976
LOSS: 1.3076523542404175
LOSS: 1.3059775829315186
LOSS: 1.3043485879898071
LOSS: 1.3027393817901611
LOSS: 1.3011364936828613
LOSS: 1.2995390892028809
LOSS: 1.2979590892791748
LOSS: 1.2964164018630981
LOSS: 1.294934868812561
LOSS: 1.2935367822647095
LOSS: 1.2922389507293701
LOSS: 1.2910494804382324
LOSS: 1.2899657487869263
LOSS: 1.2889759540557861
LOSS: 1.2880631685256958
LOSS: 1.2872074842453003
LOSS: 1.2863932847976685
LOSS: 1.2856101989746094
LOSS: 1.2848551273345947
LOSS: 1.2841296195983887
LOSS: 1.2834371328353882
LOSS: 1.2827821969985962
LOSS: 1.2821654081344604
LOSS: 1.2815852165222168
LOSS: 1.2810382843017578
LOSS: 1.2805184125900269
LOSS: 1.2800211906433105
LOSS: 1.2795424461364746
LOSS: 1.2790824174880981
LOSS: 1.2786401510238647
LOSS: 1.2782213687896729
LOSS: 1.2778255939483643
LOSS: 1.2774566411972046
LOSS: 1.2771127223968506
LOSS: 1.2767949104309082
LOSS: 1.2764983177185059
LOSS: 1.2762198448181152
LOSS: 1.2759584188461304
LOSS: 1.2757103443145752
LOSS: 1.275476098060608
LOSS: 1.2752548456192017
LOSS: 1.2750470638275146
LOSS: 1.2748523950576782
LOSS: 1.2746682167053223
LOSS: 1.2744942903518677
LOSS: 1.2743282318115234
LOSS: 1.274168610572815
LOSS: 1.2740141153335571
LOSS: 1.273863434791565
LOSS: 1.2737177610397339
LOSS: 1.2735766172409058
LOSS: 1.2734407186508179
LOSS: 1.2733097076416016
LOSS: 1.2731841802597046
LOSS: 1.2730637788772583
LOSS: 1.2729483842849731
LOSS: 1.272837519645691
LOSS: 1.272731065750122
LOSS: 1.2726300954818726
LOSS: 1.2725337743759155
LOSS: 1.27244234085083
LOSS: 1.2723559141159058
LOSS: 1.2722736597061157
LOSS: 1.2721962928771973
LOSS: 1.2721219062805176
LOSS: 1.2720502614974976
LOSS: 1.271980881690979
LOSS: 1.271913766860962
LOSS: 1.2718483209609985
LOSS: 1.2717845439910889
LOSS: 1.2717221975326538
LOSS: 1.2716610431671143
LOSS: 1.2716014385223389
LOSS: 1.2715427875518799
LOSS: 1.2714855670928955
LOSS: 1.2714293003082275
LOSS: 1.2713747024536133
LOSS: 1.2713216543197632
LOSS: 1.2712701559066772
LOSS: 1.2712194919586182
LOSS: 1.271170735359192
LOSS: 1.2711230516433716
LOSS: 1.3351126909255981
LOSS: 1.328938364982605
LOSS: 1.3230929374694824
LOSS: 1.317617654800415
LOSS: 1.3125474452972412
LOSS: 1.3079055547714233
LOSS: 1.3036935329437256
LOSS: 1.2998836040496826
LOSS: 1.2964133024215698
LOSS: 1.293192982673645
LOSS: 1.2901225090026855
LOSS: 1.2871111631393433
LOSS: 1.2840914726257324
LOSS: 1.2810274362564087
LOSS: 1.2779139280319214
LOSS: 1.2747747898101807
LOSS: 1.2716541290283203
LOSS: 1.2686067819595337
LOSS: 1.2656875848770142
LOSS: 1.2629441022872925
LOSS: 1.2604092359542847
LOSS: 1.2580987215042114
LOSS: 1.2560112476348877
LOSS: 1.2541319131851196
LOSS: 1.252435326576233
LOSS: 1.250892996788025
LOSS: 1.2494771480560303
LOSS: 1.248162865638733
LOSS: 1.2469327449798584
LOSS: 1.245775580406189
LOSS: 1.2446850538253784
LOSS: 1.243659257888794
LOSS: 1.242698311805725
LOSS: 1.2418017387390137
LOSS: 1.2409687042236328
LOSS: 1.2401942014694214
LOSS: 1.2394726276397705
LOSS: 1.2387961149215698
LOSS: 1.2381565570831299
LOSS: 1.2375454902648926
LOSS: 1.236958384513855
LOSS: 1.2363917827606201
LOSS: 1.235844612121582
LOSS: 1.2353194952011108
LOSS: 1.2348172664642334
LOSS: 1.2343425750732422
LOSS: 1.233896017074585
LOSS: 1.2334797382354736
LOSS: 1.2330938577651978
LOSS: 1.2327367067337036
LOSS: 1.2324068546295166
LOSS: 1.2321035861968994
LOSS: 1.231824278831482
LOSS: 1.2315700054168701
LOSS: 1.2313381433486938
LOSS: 1.2311294078826904
LOSS: 1.2309426069259644
LOSS: 1.2307767868041992
LOSS: 1.230629324913025
LOSS: 1.2304965257644653
LOSS: 1.2303770780563354
LOSS: 1.2302664518356323
LOSS: 1.2301623821258545
LOSS: 1.230062484741211
LOSS: 1.2299656867980957
LOSS: 1.2298715114593506
LOSS: 1.2297791242599487
LOSS: 1.2296903133392334
LOSS: 1.2296041250228882
LOSS: 1.2295221090316772
LOSS: 1.229443907737732
LOSS: 1.2293685674667358
LOSS: 1.2292972803115845
LOSS: 1.2292282581329346
LOSS: 1.229162335395813
LOSS: 1.2290990352630615
LOSS: 1.2290384769439697
LOSS: 1.2289801836013794
LOSS: 1.2289246320724487
LOSS: 1.2288711071014404
LOSS: 1.2288187742233276
LOSS: 1.2287683486938477
LOSS: 1.228719711303711
LOSS: 1.2286715507507324
LOSS: 1.228624939918518
LOSS: 1.2285798788070679
LOSS: 1.2285351753234863
LOSS: 1.2284919023513794
LOSS: 1.2284502983093262
LOSS: 1.2284096479415894
LOSS: 1.2283700704574585
LOSS: 1.2283316850662231
LOSS: 1.228293776512146
LOSS: 1.228257417678833
LOSS: 1.22822105884552
LOSS: 1.2281855344772339
LOSS: 1.2281506061553955
LOSS: 1.228116750717163
LOSS: 1.2280831336975098
LOSS: 1.2280502319335938
LOSS: 1.3554375171661377
LOSS: 1.3502599000930786
LOSS: 1.3454068899154663
LOSS: 1.340919852256775
LOSS: 1.3368297815322876
LOSS: 1.3331481218338013
LOSS: 1.3298530578613281
LOSS: 1.3268787860870361
LOSS: 1.3241186141967773
LOSS: 1.3214510679244995
LOSS: 1.3187782764434814
LOSS: 1.316052794456482
LOSS: 1.3132798671722412
LOSS: 1.3104979991912842
LOSS: 1.3077614307403564
LOSS: 1.3051210641860962
LOSS: 1.3026143312454224
LOSS: 1.300258994102478
LOSS: 1.2980525493621826
LOSS: 1.2959779500961304
LOSS: 1.2940109968185425
LOSS: 1.2921303510665894
LOSS: 1.290324330329895
LOSS: 1.2885938882827759
LOSS: 1.286952257156372
LOSS: 1.2854201793670654
LOSS: 1.2840209007263184
LOSS: 1.2827739715576172
LOSS: 1.2816914319992065
LOSS: 1.2807719707489014
LOSS: 1.2800017595291138
LOSS: 1.2793564796447754
LOSS: 1.278803825378418
LOSS: 1.2783128023147583
LOSS: 1.2778582572937012
LOSS: 1.2774226665496826
LOSS: 1.2769989967346191
LOSS: 1.2765870094299316
LOSS: 1.2761896848678589
LOSS: 1.2758103609085083
LOSS: 1.2754508256912231
LOSS: 1.2751091718673706
LOSS: 1.2747817039489746
LOSS: 1.2744635343551636
LOSS: 1.2741501331329346
LOSS: 1.2738410234451294
LOSS: 1.2735366821289062
LOSS: 1.2732402086257935
LOSS: 1.2729562520980835
LOSS: 1.272689700126648
LOSS: 1.2724418640136719
LOSS: 1.2722121477127075
LOSS: 1.2719988822937012
LOSS: 1.2717971801757812
LOSS: 1.2716014385223389
LOSS: 1.2714089155197144
LOSS: 1.2712165117263794
LOSS: 1.2710236310958862
LOSS: 1.2708299160003662
LOSS: 1.2706345319747925
LOSS: 1.2704356908798218
LOSS: 1.2702301740646362
LOSS: 1.2700128555297852
LOSS: 1.2697796821594238
LOSS: 1.2695231437683105
LOSS: 1.2692387104034424
LOSS: 1.2689223289489746
LOSS: 1.2685728073120117
LOSS: 1.268193006515503
LOSS: 1.267786979675293
LOSS: 1.2673649787902832
LOSS: 1.2669373750686646
LOSS: 1.2665148973464966
LOSS: 1.2661033868789673
LOSS: 1.2657098770141602
LOSS: 1.2653353214263916
LOSS: 1.2649786472320557
LOSS: 1.2646360397338867
LOSS: 1.2643040418624878
LOSS: 1.2639793157577515
LOSS: 1.2636568546295166
LOSS: 1.2633336782455444
LOSS: 1.2630046606063843
LOSS: 1.2626700401306152
LOSS: 1.2623244524002075
LOSS: 1.2619681358337402
LOSS: 1.2615976333618164
LOSS: 1.2612136602401733
LOSS: 1.2608147859573364
LOSS: 1.260404348373413
LOSS: 1.2599838972091675
LOSS: 1.2788704633712769
LOSS: 1.2788236141204834
LOSS: 1.3505852222442627
LOSS: 1.3450922966003418
LOSS: 1.3399248123168945
LOSS: 1.3351247310638428
LOSS: 1.3307266235351562
LOSS: 1.3267513513565063
LOSS: 1.3231955766677856
LOSS: 1.3200228214263916
LOSS: 1.317156434059143
LOSS: 1.3144866228103638
LOSS: 1.3118929862976074
LOSS: 1.3092772960662842
LOSS: 1.3065879344940186
LOSS: 1.3038252592086792
LOSS: 1.3010282516479492
LOSS: 1.2982559204101562
LOSS: 1.2955671548843384
LOSS: 1.2930110692977905
LOSS: 1.2906172275543213
LOSS: 1.2883968353271484
LOSS: 1.2863446474075317
LOSS: 1.28444504737854
LOSS: 1.2826789617538452
LOSS: 1.281030297279358
LOSS: 1.2794893980026245
LOSS: 1.2780532836914062
LOSS: 1.2767257690429688
LOSS: 1.2755132913589478
LOSS: 1.2744218111038208
LOSS: 1.2734509706497192
LOSS: 1.2725954055786133
LOSS: 1.271841049194336
LOSS: 1.2711691856384277
LOSS: 1.2705559730529785
LOSS: 1.2699781656265259
LOSS: 1.2694159746170044
LOSS: 1.2688568830490112
LOSS: 1.2682948112487793
LOSS: 1.2677313089370728
LOSS: 1.2671715021133423
LOSS: 1.2666220664978027
LOSS: 1.266086459159851
LOSS: 1.2655667066574097
LOSS: 1.2650611400604248
LOSS: 1.2645649909973145
LOSS: 1.264070987701416
LOSS: 1.263571858406067
LOSS: 1.2630608081817627
LOSS: 1.262532353401184
LOSS: 1.26198410987854
LOSS: 1.2614142894744873
LOSS: 1.260827898979187
LOSS: 1.2602360248565674
LOSS: 1.2596495151519775
LOSS: 1.2590848207473755
LOSS: 1.2585560083389282
LOSS: 1.2580742835998535
LOSS: 1.257642388343811
LOSS: 1.257259488105774
LOSS: 1.2569198608398438
LOSS: 1.2566155195236206
LOSS: 1.2563400268554688
LOSS: 1.2560847997665405
LOSS: 1.2558444738388062
LOSS: 1.2556157112121582
LOSS: 1.255393147468567
LOSS: 1.255174160003662
LOSS: 1.2549554109573364
LOSS: 1.254736065864563
LOSS: 1.254512906074524
LOSS: 1.2542847394943237
LOSS: 1.254050374031067
LOSS: 1.2538074254989624
LOSS: 1.2535536289215088
LOSS: 1.2532871961593628
LOSS: 1.2530030012130737
LOSS: 1.2526987791061401
LOSS: 1.252372145652771
LOSS: 1.2520196437835693
LOSS: 1.2516403198242188
LOSS: 1.251233458518982
LOSS: 1.25080144405365
LOSS: 1.2503465414047241
LOSS: 1.2498719692230225
LOSS: 1.2493822574615479
LOSS: 1.248884916305542
LOSS: 1.2483844757080078
LOSS: 1.2478877305984497
LOSS: 1.2474021911621094
LOSS: 1.246932029724121
LOSS: 1.2464817762374878
LOSS: 1.2460561990737915
LOSS: 1.2456556558609009
LOSS: 1.2452833652496338
LOSS: 1.2449393272399902
LOSS: 1.2446205615997314
LOSS: 1.2443283796310425
LOSS: 1.2440605163574219
LOSS: 1.2438150644302368
LOSS: 1.2435908317565918
LOSS: 1.3407806158065796
LOSS: 1.3343842029571533
LOSS: 1.3282488584518433
LOSS: 1.322424292564392
LOSS: 1.3169589042663574
LOSS: 1.311893105506897
LOSS: 1.3072543144226074
LOSS: 1.3030492067337036
LOSS: 1.2992573976516724
LOSS: 1.2958285808563232
LOSS: 1.2926887273788452
LOSS: 1.2897545099258423
LOSS: 1.2869521379470825
LOSS: 1.284232258796692
LOSS: 1.2815769910812378
LOSS: 1.278995394706726
LOSS: 1.2765166759490967
LOSS: 1.2741771936416626
LOSS: 1.2720110416412354
LOSS: 1.2700427770614624
LOSS: 1.268283724784851
LOSS: 1.2667299509048462
LOSS: 1.2653648853302002
LOSS: 1.2641621828079224
LOSS: 1.263090968132019
LOSS: 1.2621201276779175
LOSS: 1.2612224817276
LOSS: 1.2603766918182373
LOSS: 1.2595692873001099
LOSS: 1.2587931156158447
LOSS: 1.2580468654632568
LOSS: 1.2573317289352417
LOSS: 1.2566503286361694
LOSS: 1.2560042142868042
LOSS: 1.2553942203521729
LOSS: 1.2548176050186157
LOSS: 1.2542706727981567
LOSS: 1.2537497282028198
LOSS: 1.2532492876052856
LOSS: 1.252767562866211
LOSS: 1.2523034811019897
LOSS: 1.2518590688705444
LOSS: 1.2514371871948242
LOSS: 1.2510405778884888
LOSS: 1.2506732940673828
LOSS: 1.2503368854522705
LOSS: 1.250031590461731
LOSS: 1.2497559785842896
LOSS: 1.2495064735412598
LOSS: 1.2492800951004028
LOSS: 1.2490719556808472
LOSS: 1.2488778829574585
LOSS: 1.2486951351165771
LOSS: 1.2485219240188599
LOSS: 1.248356580734253
LOSS: 1.2481986284255981
LOSS: 1.2480475902557373
LOSS: 1.2479031085968018
LOSS: 1.2477651834487915
LOSS: 1.247633934020996
LOSS: 1.247507095336914
LOSS: 1.2473849058151245
LOSS: 1.247266173362732
LOSS: 1.2471507787704468
LOSS: 1.2470386028289795
LOSS: 1.2469297647476196
LOSS: 1.2468247413635254
LOSS: 1.2467234134674072
LOSS: 1.2466264963150024
LOSS: 1.2465338706970215
LOSS: 1.2464455366134644
LOSS: 1.246360421180725
LOSS: 1.246278166770935
LOSS: 1.2461988925933838
LOSS: 1.2461217641830444
LOSS: 1.2460469007492065
LOSS: 1.2459731101989746
LOSS: 1.2459018230438232
LOSS: 1.2458319664001465
LOSS: 1.2457637786865234
LOSS: 1.2456974983215332
LOSS: 1.2456333637237549
LOSS: 1.2455693483352661
LOSS: 1.2455072402954102
LOSS: 1.2454463243484497
LOSS: 1.2453862428665161
LOSS: 1.2453277111053467
LOSS: 1.245270013809204
LOSS: 1.2452139854431152
LOSS: 1.2451590299606323
LOSS: 1.2451047897338867
LOSS: 1.2450525760650635
LOSS: 1.2450008392333984
LOSS: 1.244950532913208
LOSS: 1.244901418685913
LOSS: 1.244852900505066
LOSS: 1.2448058128356934
LOSS: 1.2447601556777954
LOSS: 1.2447148561477661
LOSS: 1.2446709871292114
LOSS: 1.357269525527954
LOSS: 1.3522008657455444
LOSS: 1.3475569486618042
LOSS: 1.3433825969696045
LOSS: 1.3397085666656494
LOSS: 1.3365368843078613
LOSS: 1.3338239192962646
LOSS: 1.3314690589904785
LOSS: 1.3293269872665405
LOSS: 1.327249526977539
LOSS: 1.3251343965530396
LOSS: 1.3229440450668335
LOSS: 1.3206959962844849
LOSS: 1.3184388875961304
LOSS: 1.3162298202514648
LOSS: 1.3141191005706787
LOSS: 1.3121404647827148
LOSS: 1.3103055953979492
LOSS: 1.3086066246032715
LOSS: 1.307020664215088
LOSS: 1.3055200576782227
LOSS: 1.3040800094604492
LOSS: 1.3026868104934692
LOSS: 1.3013381958007812
LOSS: 1.3000423908233643
LOSS: 1.2988125085830688
LOSS: 1.297663927078247
LOSS: 1.2966086864471436
LOSS: 1.2956531047821045
LOSS: 1.2947945594787598
LOSS: 1.2940222024917603
LOSS: 1.2933191061019897
LOSS: 1.2926644086837769
LOSS: 1.2920401096343994
LOSS: 1.291433334350586
LOSS: 1.2908393144607544
LOSS: 1.2902618646621704
LOSS: 1.2897090911865234
LOSS: 1.2891902923583984
LOSS: 1.2887121438980103
LOSS: 1.2882778644561768
LOSS: 1.2878855466842651
LOSS: 1.2875295877456665
LOSS: 1.2872034311294556
LOSS: 1.2868993282318115
LOSS: 1.2866102457046509
LOSS: 1.2863315343856812
LOSS: 1.2860586643218994
LOSS: 1.2857879400253296
LOSS: 1.285513997077942
LOSS: 1.2852296829223633
LOSS: 1.2849277257919312
LOSS: 1.284596562385559
LOSS: 1.2842276096343994
LOSS: 1.2838127613067627
LOSS: 1.2833466529846191
LOSS: 1.2828294038772583
LOSS: 1.2822635173797607
LOSS: 1.2816544771194458
LOSS: 1.2810111045837402
LOSS: 1.2803422212600708
LOSS: 1.2796571254730225
LOSS: 1.2789649963378906
LOSS: 1.278275489807129
LOSS: 1.2775956392288208
LOSS: 1.2769336700439453
LOSS: 1.2762949466705322
LOSS: 1.2756835222244263
LOSS: 1.2751034498214722
LOSS: 1.2745558023452759
LOSS: 1.274040937423706
LOSS: 1.2735581398010254
LOSS: 1.2731064558029175
LOSS: 1.272684931755066
LOSS: 1.272292137145996
LOSS: 1.2719275951385498
LOSS: 1.2715893983840942
LOSS: 1.2712773084640503
LOSS: 1.2709903717041016
LOSS: 1.2707267999649048
LOSS: 1.2704837322235107
LOSS: 1.2702604532241821
LOSS: 1.2700554132461548
LOSS: 1.2698651552200317
LOSS: 1.2696905136108398
LOSS: 1.2695286273956299
LOSS: 1.2693791389465332
LOSS: 1.269240379333496
LOSS: 1.2691115140914917
LOSS: 1.2689913511276245
LOSS: 1.2688791751861572
LOSS: 1.268774390220642
LOSS: 1.2686767578125
LOSS: 1.2685850858688354
LOSS: 1.2684993743896484
LOSS: 1.2684195041656494
LOSS: 1.2683448791503906
LOSS: 1.2682747840881348
LOSS: 1.268208622932434
LOSS: 1.2681467533111572
LOSS: 1.3554375171661377
LOSS: 1.3502599000930786
LOSS: 1.3454068899154663
LOSS: 1.340919852256775
LOSS: 1.3368297815322876
LOSS: 1.3331481218338013
LOSS: 1.3298530578613281
LOSS: 1.3268787860870361
LOSS: 1.3241186141967773
LOSS: 1.3214510679244995
LOSS: 1.3187782764434814
LOSS: 1.316052794456482
LOSS: 1.3132798671722412
LOSS: 1.3104979991912842
LOSS: 1.3077614307403564
LOSS: 1.3051210641860962
LOSS: 1.3026143312454224
LOSS: 1.300258994102478
LOSS: 1.2980525493621826
LOSS: 1.2959779500961304
LOSS: 1.2940109968185425
LOSS: 1.2921303510665894
LOSS: 1.290324330329895
LOSS: 1.2885938882827759
LOSS: 1.286952257156372
LOSS: 1.2854201793670654
LOSS: 1.2840209007263184
LOSS: 1.2827739715576172
LOSS: 1.2744218111038208
LOSS: 1.2734509706497192
LOSS: 1.2725954055786133
LOSS: 1.271841049194336
LOSS: 1.2711691856384277
LOSS: 1.2705559730529785
LOSS: 1.2699781656265259
LOSS: 1.2694159746170044
LOSS: 1.2688568830490112
LOSS: 1.2682948112487793
LOSS: 1.2677313089370728
LOSS: 1.2671715021133423
LOSS: 1.2666220664978027
LOSS: 1.266086459159851
LOSS: 1.2655667066574097
LOSS: 1.2650611400604248
LOSS: 1.2645649909973145
LOSS: 1.264070987701416
LOSS: 1.263571858406067
LOSS: 1.2630608081817627
LOSS: 1.262532353401184
LOSS: 1.26198410987854
LOSS: 1.2614142894744873
LOSS: 1.260827898979187
LOSS: 1.2602360248565674
LOSS: 1.2596495151519775
LOSS: 1.2590848207473755
LOSS: 1.2585560083389282
LOSS: 1.2580742835998535
LOSS: 1.257642388343811
LOSS: 1.257259488105774
LOSS: 1.2569198608398438
LOSS: 1.2566155195236206
LOSS: 1.2563400268554688
LOSS: 1.2560847997665405
LOSS: 1.2558444738388062
LOSS: 1.2556157112121582
LOSS: 1.255393147468567
LOSS: 1.255174160003662
LOSS: 1.2549554109573364
LOSS: 1.254736065864563
LOSS: 1.254512906074524
LOSS: 1.2542847394943237
LOSS: 1.254050374031067
LOSS: 1.2538074254989624
LOSS: 1.2535536289215088
LOSS: 1.2532871961593628
LOSS: 1.2530030012130737
LOSS: 1.2526987791061401
LOSS: 1.252372145652771
LOSS: 1.2520196437835693
LOSS: 1.2516403198242188
LOSS: 1.251233458518982
LOSS: 1.25080144405365
LOSS: 1.2503465414047241
LOSS: 1.2498719692230225
LOSS: 1.2493822574615479
LOSS: 1.248884916305542
LOSS: 1.2483844757080078
LOSS: 1.2478877305984497
LOSS: 1.2474021911621094
LOSS: 1.246932029724121
LOSS: 1.2464817762374878
LOSS: 1.2460561990737915
LOSS: 1.2456556558609009
LOSS: 1.2452833652496338
LOSS: 1.2449393272399902
LOSS: 1.2446205615997314
LOSS: 1.2443283796310425
LOSS: 1.2440605163574219
LOSS: 1.2438150644302368
LOSS: 1.2435908317565918
LOSS: 1.3563858270645142
LOSS: 1.3513699769973755
LOSS: 1.34676992893219
LOSS: 1.3426315784454346
LOSS: 1.3389860391616821
LOSS: 1.3358337879180908
LOSS: 1.3331270217895508
LOSS: 1.3307586908340454
LOSS: 1.328580379486084
LOSS: 1.326451063156128
LOSS: 1.3242827653884888
LOSS: 1.3220529556274414
LOSS: 1.3197864294052124
LOSS: 1.3175302743911743
LOSS: 1.3153338432312012
LOSS: 1.313236951828003
LOSS: 1.3112598657608032
LOSS: 1.3094037771224976
LOSS: 1.3076523542404175
LOSS: 1.3059775829315186
LOSS: 1.3043485879898071
LOSS: 1.3027393817901611
LOSS: 1.3011364936828613
LOSS: 1.2995390892028809
LOSS: 1.2979590892791748
LOSS: 1.2964164018630981
LOSS: 1.294934868812561
LOSS: 1.2935367822647095
LOSS: 1.2922389507293701
LOSS: 1.2910494804382324
LOSS: 1.2899657487869263
LOSS: 1.2889759540557861
LOSS: 1.2880631685256958
LOSS: 1.2872074842453003
LOSS: 1.2863932847976685
LOSS: 1.2856101989746094
LOSS: 1.2848551273345947
LOSS: 1.2841296195983887
LOSS: 1.2834371328353882
LOSS: 1.2827821969985962
LOSS: 1.2821654081344604
LOSS: 1.2815852165222168
LOSS: 1.2810382843017578
LOSS: 1.2805184125900269
LOSS: 1.2800211906433105
LOSS: 1.2795424461364746
LOSS: 1.2790824174880981
LOSS: 1.2786401510238647
LOSS: 1.2782213687896729
LOSS: 1.2778255939483643
LOSS: 1.2774566411972046
LOSS: 1.2771127223968506
LOSS: 1.2767949104309082
LOSS: 1.2764983177185059
LOSS: 1.2762198448181152
LOSS: 1.2759584188461304
LOSS: 1.2757103443145752
LOSS: 1.275476098060608
LOSS: 1.2752548456192017
LOSS: 1.2750470638275146
LOSS: 1.2748523950576782
LOSS: 1.2746682167053223
LOSS: 1.2744942903518677
LOSS: 1.2743282318115234
LOSS: 1.274168610572815
LOSS: 1.2740141153335571
LOSS: 1.273863434791565
LOSS: 1.2737177610397339
LOSS: 1.2735766172409058
LOSS: 1.2734407186508179
LOSS: 1.2733097076416016
LOSS: 1.2731841802597046
LOSS: 1.2730637788772583
LOSS: 1.2729483842849731
LOSS: 1.272837519645691
LOSS: 1.272731065750122
LOSS: 1.2726300954818726
LOSS: 1.2725337743759155
LOSS: 1.27244234085083
LOSS: 1.2723559141159058
LOSS: 1.2722736597061157
LOSS: 1.2721962928771973
LOSS: 1.2721219062805176
LOSS: 1.2720502614974976
LOSS: 1.271980881690979
LOSS: 1.271913766860962
LOSS: 1.2718483209609985
LOSS: 1.2717845439910889
LOSS: 1.2717221975326538
LOSS: 1.2716610431671143
LOSS: 1.2716014385223389
LOSS: 1.2715427875518799
LOSS: 1.2714855670928955
LOSS: 1.2714293003082275
LOSS: 1.2713747024536133
LOSS: 1.2713216543197632
LOSS: 1.2712701559066772
LOSS: 1.2712194919586182
LOSS: 1.271170735359192
LOSS: 1.2711230516433716
LOSS: 1.345860481262207
LOSS: 1.3403980731964111
LOSS: 1.3352552652359009
LOSS: 1.3304736614227295
LOSS: 1.32608962059021
LOSS: 1.3221261501312256
LOSS: 1.3185850381851196
LOSS: 1.3154351711273193
LOSS: 1.3126064538955688
LOSS: 1.3099936246871948
LOSS: 1.3074769973754883
LOSS: 1.3049553632736206
LOSS: 1.3023735284805298
LOSS: 1.2997287511825562
LOSS: 1.2970595359802246
LOSS: 1.2944254875183105
LOSS: 1.2918884754180908
LOSS: 1.2894983291625977
LOSS: 1.2872861623764038
LOSS: 1.2852624654769897
LOSS: 1.2834194898605347
LOSS: 1.2817375659942627
LOSS: 1.2801920175552368
LOSS: 1.2787591218948364
LOSS: 1.277420997619629
LOSS: 1.2761684656143188
LOSS: 1.2749992609024048
LOSS: 1.273916482925415
LOSS: 1.2729254961013794
LOSS: 1.2720296382904053
LOSS: 1.271228551864624
LOSS: 1.270516037940979
LOSS: 1.2698791027069092
LOSS: 1.2693018913269043
LOSS: 1.2687664031982422
LOSS: 1.2682569026947021
LOSS: 1.2677624225616455
LOSS: 1.2672772407531738
LOSS: 1.2668017148971558
LOSS: 1.2663389444351196
LOSS: 1.265892744064331
LOSS: 1.2654670476913452
LOSS: 1.2650641202926636
LOSS: 1.2646816968917847
LOSS: 1.2643170356750488
LOSS: 1.2639669179916382
LOSS: 1.2636278867721558
LOSS: 1.2632981538772583
LOSS: 1.2629786729812622
LOSS: 1.2626721858978271
LOSS: 1.2623817920684814
LOSS: 1.2621104717254639
LOSS: 1.2618613243103027
LOSS: 1.2616348266601562
LOSS: 1.2614285945892334
LOSS: 1.2612411975860596
LOSS: 1.261067509651184
LOSS: 1.2609050273895264
LOSS: 1.260751724243164
LOSS: 1.260604739189148
LOSS: 1.260464072227478
LOSS: 1.2603297233581543
LOSS: 1.2602012157440186
LOSS: 1.2600795030593872
LOSS: 1.2599629163742065
LOSS: 1.2598509788513184
LOSS: 1.2597424983978271
LOSS: 1.2596368789672852
LOSS: 1.2595338821411133
LOSS: 1.2594329118728638
LOSS: 1.2593355178833008
LOSS: 1.259242057800293
LOSS: 1.259151816368103
LOSS: 1.2590656280517578
LOSS: 1.2589837312698364
LOSS: 1.2589048147201538
LOSS: 1.2588286399841309
LOSS: 1.2587558031082153
LOSS: 1.2586852312088013
LOSS: 1.2586175203323364
LOSS: 1.2585521936416626
LOSS: 1.2584885358810425
LOSS: 1.2584278583526611
LOSS: 1.2583694458007812
LOSS: 1.2583129405975342
LOSS: 1.258258581161499
LOSS: 1.2582052946090698
LOSS: 1.2581537961959839
LOSS: 1.258103847503662
LOSS: 1.258054494857788
LOSS: 1.2580071687698364
LOSS: 1.2579609155654907
LOSS: 1.2579164505004883
LOSS: 1.2578727006912231
LOSS: 1.2578303813934326
LOSS: 1.257788896560669
LOSS: 1.2577488422393799
LOSS: 1.2577095031738281
LOSS: 1.2576709985733032
LOSS: 1.2576336860656738
LOSS: 1.3407806158065796
LOSS: 1.3343842029571533
LOSS: 1.3282488584518433
LOSS: 1.322424292564392
LOSS: 1.3169589042663574
LOSS: 1.311893105506897
LOSS: 1.3072543144226074
LOSS: 1.3030492067337036
LOSS: 1.2992573976516724
LOSS: 1.2958285808563232
LOSS: 1.2926887273788452
LOSS: 1.2897545099258423
LOSS: 1.2869521379470825
LOSS: 1.284232258796692
LOSS: 1.2815769910812378
LOSS: 1.278995394706726
LOSS: 1.2765166759490967
LOSS: 1.2741771936416626
LOSS: 1.2720110416412354
LOSS: 1.2700427770614624
LOSS: 1.268283724784851
LOSS: 1.2667299509048462
LOSS: 1.2653648853302002
LOSS: 1.2641621828079224
LOSS: 1.263090968132019
LOSS: 1.2621201276779175
LOSS: 1.2612224817276
LOSS: 1.2603766918182373
LOSS: 1.2595692873001099
LOSS: 1.2587931156158447
LOSS: 1.2580468654632568
LOSS: 1.2573317289352417
LOSS: 1.2566503286361694
LOSS: 1.2560042142868042
LOSS: 1.2553942203521729
LOSS: 1.2548176050186157
LOSS: 1.2542706727981567
LOSS: 1.2537497282028198
LOSS: 1.2532492876052856
LOSS: 1.252767562866211
LOSS: 1.2523034811019897
LOSS: 1.2518590688705444
LOSS: 1.2514371871948242
LOSS: 1.2510405778884888
LOSS: 1.2506732940673828
LOSS: 1.2503368854522705
LOSS: 1.250031590461731
LOSS: 1.2497559785842896
LOSS: 1.2495064735412598
LOSS: 1.2492800951004028
LOSS: 1.2490719556808472
LOSS: 1.2488778829574585
LOSS: 1.2486951351165771
LOSS: 1.2485219240188599
LOSS: 1.248356580734253
LOSS: 1.2481986284255981
LOSS: 1.2480475902557373
LOSS: 1.2479031085968018
LOSS: 1.2756677865982056
LOSS: 1.2747844457626343
LOSS: 1.2739295959472656
LOSS: 1.2730998992919922
LOSS: 1.2722936868667603
LOSS: 1.271508812904358
LOSS: 1.2707412242889404
LOSS: 1.2699857950210571
LOSS: 1.269236445426941
LOSS: 1.2684863805770874
LOSS: 1.2677334547042847
LOSS: 1.2669771909713745
LOSS: 1.2662235498428345
LOSS: 1.2654777765274048
LOSS: 1.2647490501403809
LOSS: 1.264044165611267
LOSS: 1.2633675336837769
LOSS: 1.262721300125122
LOSS: 1.2621079683303833
LOSS: 1.2615245580673218
LOSS: 1.2609702348709106
LOSS: 1.2604429721832275
LOSS: 1.259945273399353
LOSS: 1.259476900100708
LOSS: 1.2590373754501343
LOSS: 1.2586278915405273
LOSS: 1.2582476139068604
LOSS: 1.2578939199447632
LOSS: 1.2575643062591553
LOSS: 1.257257103919983
LOSS: 1.2569667100906372
LOSS: 1.2566947937011719
LOSS: 1.256439208984375
LOSS: 1.256197214126587
LOSS: 1.2559691667556763
LOSS: 1.2557551860809326
LOSS: 1.2555516958236694
LOSS: 1.255358099937439
LOSS: 1.255172610282898
LOSS: 1.2549935579299927
LOSS: 1.2548211812973022
LOSS: 1.2546565532684326
LOSS: 1.2544951438903809
LOSS: 1.2543400526046753
LOSS: 1.2541905641555786
LOSS: 1.2540444135665894
LOSS: 1.253902792930603
LOSS: 1.2537645101547241
LOSS: 1.2536293268203735
LOSS: 1.2534968852996826
LOSS: 1.2533676624298096
LOSS: 1.2532422542572021
LOSS: 1.2531189918518066
LOSS: 1.2529988288879395
LOSS: 1.2528820037841797
LOSS: 1.2527676820755005
LOSS: 1.2526569366455078
LOSS: 1.2525492906570435
LOSS: 1.2524452209472656
LOSS: 1.2523458003997803
LOSS: 1.2522509098052979
LOSS: 1.2521620988845825
LOSS: 1.2520780563354492
LOSS: 1.251999020576477
LOSS: 1.2519218921661377
LOSS: 1.2518446445465088
LOSS: 1.2517659664154053
LOSS: 1.354232668876648
LOSS: 1.3490283489227295
LOSS: 1.3442245721817017
LOSS: 1.3398573398590088
LOSS: 1.335943341255188
LOSS: 1.3324683904647827
LOSS: 1.3293832540512085
LOSS: 1.3265944719314575
LOSS: 1.3239753246307373
LOSS: 1.3214036226272583
LOSS: 1.3188024759292603
LOSS: 1.3161526918411255
LOSS: 1.313480019569397
LOSS: 1.3108329772949219
LOSS: 1.3082612752914429
LOSS: 1.3058017492294312
LOSS: 1.3034690618515015
LOSS: 1.3012545108795166
LOSS: 1.2991344928741455
LOSS: 1.297080397605896
LOSS: 1.2950676679611206
LOSS: 1.2930829524993896
LOSS: 1.291126012802124
LOSS: 1.2892088890075684
LOSS: 1.287353277206421
LOSS: 1.2855843305587769
LOSS: 1.2839267253875732
LOSS: 1.2823981046676636
LOSS: 1.2810059785842896
LOSS: 1.2797470092773438
LOSS: 1.278605341911316
LOSS: 1.2775605916976929
LOSS: 1.2765882015228271
LOSS: 1.2756677865982056
LOSS: 1.2747844457626343
LOSS: 1.2739295959472656
LOSS: 1.2730998992919922
LOSS: 1.2722936868667603
LOSS: 1.271508812904358
LOSS: 1.2707412242889404
LOSS: 1.2699857950210571
LOSS: 1.269236445426941
LOSS: 1.2684863805770874
LOSS: 1.2677334547042847
LOSS: 1.2669771909713745
LOSS: 1.2662235498428345
LOSS: 1.2654777765274048
LOSS: 1.2647490501403809
LOSS: 1.264044165611267
LOSS: 1.2633675336837769
LOSS: 1.262721300125122
LOSS: 1.2621079683303833
LOSS: 1.2615245580673218
LOSS: 1.2609702348709106
LOSS: 1.2604429721832275
LOSS: 1.259945273399353
LOSS: 1.259476900100708
LOSS: 1.2590373754501343
LOSS: 1.2586278915405273
LOSS: 1.2582476139068604
LOSS: 1.2578939199447632
LOSS: 1.2575643062591553
LOSS: 1.257257103919983
LOSS: 1.2569667100906372
LOSS: 1.2566947937011719
LOSS: 1.256439208984375
LOSS: 1.256197214126587
LOSS: 1.2559691667556763
LOSS: 1.2557551860809326
LOSS: 1.2555516958236694
LOSS: 1.255358099937439
LOSS: 1.255172610282898
LOSS: 1.2549935579299927
LOSS: 1.2548211812973022
LOSS: 1.2546565532684326
LOSS: 1.2544951438903809
LOSS: 1.2543400526046753
LOSS: 1.2541905641555786
LOSS: 1.2540444135665894
LOSS: 1.253902792930603
LOSS: 1.2537645101547241
LOSS: 1.2536293268203735
LOSS: 1.2534968852996826
LOSS: 1.2533676624298096
LOSS: 1.2532422542572021
LOSS: 1.2531189918518066
LOSS: 1.2529988288879395
LOSS: 1.2528820037841797
LOSS: 1.2527676820755005
LOSS: 1.2526569366455078
LOSS: 1.2525492906570435
LOSS: 1.2524452209472656
LOSS: 1.2523458003997803
LOSS: 1.2522509098052979
LOSS: 1.2521620988845825
LOSS: 1.2520780563354492
LOSS: 1.251999020576477
LOSS: 1.2519218921661377
LOSS: 1.2518446445465088
LOSS: 1.2517659664154053
LOSS: 1.3633705377578735
LOSS: 1.3589069843292236
LOSS: 1.3548481464385986
LOSS: 1.3512225151062012
LOSS: 1.3480333089828491
LOSS: 1.3452332019805908
LOSS: 1.342707872390747
LOSS: 1.3402972221374512
LOSS: 1.3378610610961914
LOSS: 1.3353350162506104
LOSS: 1.3327265977859497
LOSS: 1.330083966255188
LOSS: 1.327463984489441
LOSS: 1.3249138593673706
LOSS: 1.3224596977233887
LOSS: 1.3201030492782593
LOSS: 1.317823886871338
LOSS: 1.3155890703201294
LOSS: 1.313364028930664
LOSS: 1.3111227750778198
LOSS: 1.3088549375534058
LOSS: 1.3065659999847412
LOSS: 1.3042739629745483
LOSS: 1.3020060062408447
LOSS: 1.299790859222412
LOSS: 1.297655701637268
LOSS: 1.2956211566925049
LOSS: 1.2936972379684448
LOSS: 1.2918843030929565
LOSS: 1.2901753187179565
LOSS: 1.2885593175888062
LOSS: 1.287025809288025
LOSS: 1.285567283630371
LOSS: 1.2841823101043701
LOSS: 1.28286874294281
LOSS: 1.2816262245178223
LOSS: 1.280451774597168
LOSS: 1.2793389558792114
LOSS: 1.2782788276672363
LOSS: 1.2772612571716309
LOSS: 1.2762783765792847
LOSS: 1.2753247022628784
LOSS: 1.274398684501648
LOSS: 1.2735031843185425
LOSS: 1.2726426124572754
LOSS: 1.271822214126587
LOSS: 1.2710450887680054
LOSS: 1.2703124284744263
LOSS: 1.2696232795715332
LOSS: 1.2689756155014038
LOSS: 1.268368124961853
LOSS: 1.2677992582321167
LOSS: 1.2672683000564575
LOSS: 1.26677668094635
LOSS: 1.2663242816925049
LOSS: 1.2659090757369995
LOSS: 1.2655292749404907
LOSS: 1.265181064605713
LOSS: 1.26486074924469
LOSS: 1.2645642757415771
LOSS: 1.264289140701294
LOSS: 1.264032244682312
LOSS: 1.2637929916381836
LOSS: 1.2635694742202759
LOSS: 1.263360619544983
LOSS: 1.2631639242172241
LOSS: 1.2629787921905518
LOSS: 1.2628029584884644
LOSS: 1.262634515762329
LOSS: 1.262473702430725
LOSS: 1.2623203992843628
LOSS: 1.2621744871139526
LOSS: 1.2620357275009155
LOSS: 1.2619034051895142
LOSS: 1.2617777585983276
LOSS: 1.2616572380065918
LOSS: 1.2615424394607544
LOSS: 1.2614331245422363
LOSS: 1.2613286972045898
LOSS: 1.261228322982788
LOSS: 1.2611333131790161
LOSS: 1.2610421180725098
LOSS: 1.2609556913375854
LOSS: 1.2608721256256104
LOSS: 1.2607918977737427
LOSS: 1.2607144117355347
LOSS: 1.2606397867202759
LOSS: 1.2605680227279663
LOSS: 1.2604979276657104
LOSS: 1.2604304552078247
LOSS: 1.2603652477264404
LOSS: 1.260301113128662
LOSS: 1.2602392435073853
LOSS: 1.2601778507232666
LOSS: 1.260118842124939
LOSS: 1.2600609064102173
LOSS: 1.2600045204162598
LOSS: 1.25994873046875
LOSS: 1.2598938941955566
LOSS: 1.259840726852417
LOSS: 1.345860481262207
LOSS: 1.3403980731964111
LOSS: 1.3352552652359009
LOSS: 1.3304736614227295
LOSS: 1.32608962059021
LOSS: 1.3221261501312256
LOSS: 1.3185850381851196
LOSS: 1.3154351711273193
LOSS: 1.3126064538955688
LOSS: 1.3099936246871948
LOSS: 1.3074769973754883
LOSS: 1.3049553632736206
LOSS: 1.3023735284805298
LOSS: 1.2997287511825562
LOSS: 1.2970595359802246
LOSS: 1.2944254875183105
LOSS: 1.2918884754180908
LOSS: 1.2894983291625977
LOSS: 1.2872861623764038
LOSS: 1.2852624654769897
LOSS: 1.2834194898605347
LOSS: 1.2817375659942627
LOSS: 1.2801920175552368
LOSS: 1.2787591218948364
LOSS: 1.277420997619629
LOSS: 1.2761684656143188
LOSS: 1.2749992609024048
LOSS: 1.273916482925415
LOSS: 1.2729254961013794
LOSS: 1.2720296382904053
LOSS: 1.271228551864624
LOSS: 1.270516037940979
LOSS: 1.2698791027069092
LOSS: 1.2693018913269043
LOSS: 1.2687664031982422
LOSS: 1.2682569026947021
LOSS: 1.2677624225616455
LOSS: 1.2672772407531738
LOSS: 1.2668017148971558
LOSS: 1.2663389444351196
LOSS: 1.265892744064331
LOSS: 1.2654670476913452
LOSS: 1.2650641202926636
LOSS: 1.2646816968917847
LOSS: 1.2643170356750488
LOSS: 1.2639669179916382
LOSS: 1.2636278867721558
LOSS: 1.2632981538772583
LOSS: 1.2629786729812622
LOSS: 1.2626721858978271
LOSS: 1.2623817920684814
LOSS: 1.2621104717254639
LOSS: 1.2618613243103027
LOSS: 1.2616348266601562
LOSS: 1.2614285945892334
LOSS: 1.2612411975860596
LOSS: 1.261067509651184
LOSS: 1.2609050273895264
LOSS: 1.260751724243164
LOSS: 1.260604739189148
LOSS: 1.260464072227478
LOSS: 1.2603297233581543
LOSS: 1.2602012157440186
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
LOSS: 1.2477651834487915
LOSS: 1.247633934020996
LOSS: 1.247507095336914
LOSS: 1.2473849058151245
LOSS: 1.247266173362732
LOSS: 1.2471507787704468
LOSS: 1.2470386028289795
LOSS: 1.2469297647476196
LOSS: 1.2468247413635254
LOSS: 1.2467234134674072
LOSS: 1.2466264963150024
LOSS: 1.2465338706970215
LOSS: 1.2464455366134644
LOSS: 1.246360421180725
LOSS: 1.246278166770935
LOSS: 1.2461988925933838
LOSS: 1.2461217641830444
LOSS: 1.2460469007492065
LOSS: 1.2459731101989746
LOSS: 1.2459018230438232
LOSS: 1.2458319664001465
LOSS: 1.2457637786865234
LOSS: 1.2456974983215332
LOSS: 1.2456333637237549
LOSS: 1.2455693483352661
LOSS: 1.2455072402954102
LOSS: 1.2454463243484497
LOSS: 1.2453862428665161
LOSS: 1.2453277111053467
LOSS: 1.245270013809204
LOSS: 1.2452139854431152
LOSS: 1.2451590299606323
LOSS: 1.2451047897338867
LOSS: 1.2450525760650635
LOSS: 1.2450008392333984
LOSS: 1.244950532913208
LOSS: 1.244901418685913
LOSS: 1.244852900505066
LOSS: 1.2448058128356934
LOSS: 1.2447601556777954
LOSS: 1.2447148561477661
LOSS: 1.2446709871292114
LOSS: 1.357269525527954
LOSS: 1.3522008657455444
LOSS: 1.3475569486618042
LOSS: 1.3433825969696045
LOSS: 1.3397085666656494
LOSS: 1.3365368843078613
LOSS: 1.3338239192962646
LOSS: 1.3314690589904785
LOSS: 1.3293269872665405
LOSS: 1.327249526977539
LOSS: 1.3251343965530396
LOSS: 1.3229440450668335
LOSS: 1.3206959962844849
LOSS: 1.3184388875961304
LOSS: 1.3162298202514648
LOSS: 1.3141191005706787
LOSS: 1.3121404647827148
LOSS: 1.3103055953979492
LOSS: 1.3086066246032715
LOSS: 1.307020664215088
LOSS: 1.3055200576782227
LOSS: 1.3040800094604492
LOSS: 1.3026868104934692
LOSS: 1.3013381958007812
LOSS: 1.3000423908233643
LOSS: 1.2988125085830688
LOSS: 1.297663927078247
LOSS: 1.2966086864471436
LOSS: 1.2956531047821045
LOSS: 1.2947945594787598
LOSS: 1.2940222024917603
LOSS: 1.2933191061019897
LOSS: 1.2926644086837769
LOSS: 1.2920401096343994
LOSS: 1.291433334350586
LOSS: 1.2908393144607544
LOSS: 1.2902618646621704
LOSS: 1.2897090911865234
LOSS: 1.2891902923583984
LOSS: 1.2887121438980103
LOSS: 1.2882778644561768
LOSS: 1.2878855466842651
LOSS: 1.2875295877456665
LOSS: 1.2872034311294556
LOSS: 1.2868993282318115
LOSS: 1.2866102457046509
LOSS: 1.2863315343856812
LOSS: 1.2860586643218994
LOSS: 1.2857879400253296
LOSS: 1.285513997077942
LOSS: 1.2852296829223633
LOSS: 1.2849277257919312
LOSS: 1.284596562385559
LOSS: 1.2842276096343994
LOSS: 1.2838127613067627
LOSS: 1.2833466529846191
LOSS: 1.2828294038772583
LOSS: 1.2822635173797607
LOSS: 1.2816544771194458
LOSS: 1.2810111045837402
LOSS: 1.2803422212600708
LOSS: 1.2796571254730225
LOSS: 1.2789649963378906
LOSS: 1.278275489807129
LOSS: 1.2775956392288208
LOSS: 1.2769336700439453
LOSS: 1.2762949466705322
LOSS: 1.2756835222244263
LOSS: 1.2751034498214722
LOSS: 1.2745558023452759
LOSS: 1.274040937423706
LOSS: 1.2735581398010254
LOSS: 1.2731064558029175
LOSS: 1.272684931755066
LOSS: 1.272292137145996
LOSS: 1.2719275951385498
LOSS: 1.2715893983840942
LOSS: 1.2712773084640503
LOSS: 1.2709903717041016
LOSS: 1.2707267999649048
LOSS: 1.2704837322235107
LOSS: 1.2702604532241821
LOSS: 1.2700554132461548
LOSS: 1.2698651552200317
LOSS: 1.2696905136108398
LOSS: 1.2695286273956299
LOSS: 1.2693791389465332
LOSS: 1.269240379333496
LOSS: 1.2691115140914917
LOSS: 1.2689913511276245
LOSS: 1.2688791751861572
LOSS: 1.268774390220642
LOSS: 1.2686767578125
LOSS: 1.2685850858688354
LOSS: 1.2684993743896484
LOSS: 1.2684195041656494
LOSS: 1.2683448791503906
LOSS: 1.2682747840881348
LOSS: 1.268208622932434
LOSS: 1.2681467533111572
LOSS: 1.3719314336776733
LOSS: 1.3668580055236816
LOSS: 1.3621615171432495
LOSS: 1.3578829765319824
LOSS: 1.3540399074554443
LOSS: 1.350602626800537
LOSS: 1.3474760055541992
LOSS: 1.344512939453125
LOSS: 1.3415721654891968
LOSS: 1.338577389717102
LOSS: 1.3355261087417603
LOSS: 1.3324638605117798
LOSS: 1.3294507265090942
LOSS: 1.3265408277511597
LOSS: 1.3237669467926025
LOSS: 1.3211358785629272
LOSS: 1.3186310529708862
LOSS: 1.3162219524383545
LOSS: 1.3138771057128906
LOSS: 1.3115755319595337
LOSS: 1.3093125820159912
LOSS: 1.3070985078811646
LOSS: 1.3049535751342773
LOSS: 1.3029018640518188
LOSS: 1.3009625673294067
LOSS: 1.299147129058838
LOSS: 1.2974549531936646
LOSS: 1.2958725690841675
LOSS: 1.294379472732544
LOSS: 1.292953372001648
LOSS: 1.2915757894515991
LOSS: 1.2902369499206543
LOSS: 1.2889342308044434
LOSS: 1.2876695394515991
LOSS: 1.2864456176757812
LOSS: 1.285261869430542
LOSS: 1.2841148376464844
LOSS: 1.282997488975525
LOSS: 1.2819011211395264
LOSS: 1.280819296836853
LOSS: 1.2797507047653198
LOSS: 1.2786967754364014
LOSS: 1.2776644229888916
LOSS: 1.2766610383987427
LOSS: 1.2756935358047485
LOSS: 1.2747671604156494
LOSS: 1.2738829851150513
LOSS: 1.2730400562286377
LOSS: 1.2722362279891968
LOSS: 1.271470546722412
LOSS: 1.2707419395446777
LOSS: 1.2700519561767578
LOSS: 1.2694019079208374
LOSS: 1.268791913986206
LOSS: 1.2682205438613892
LOSS: 1.2676855325698853
LOSS: 1.2671828269958496
LOSS: 1.2667107582092285
LOSS: 1.2662652730941772
LOSS: 1.2658448219299316
LOSS: 1.2654482126235962
LOSS: 1.2650762796401978
LOSS: 1.2647268772125244
LOSS: 1.264398217201233
LOSS: 1.2640892267227173
LOSS: 1.2637971639633179
LOSS: 1.2635201215744019
LOSS: 1.2632580995559692
LOSS: 1.2630095481872559
LOSS: 1.2627737522125244
LOSS: 1.2625505924224854
LOSS: 1.262339472770691
LOSS: 1.262139081954956
LOSS: 1.2619482278823853
LOSS: 1.261766791343689
LOSS: 1.2615934610366821
LOSS: 1.261428952217102
LOSS: 1.2612712383270264
LOSS: 1.261121392250061
LOSS: 1.2609800100326538
LOSS: 1.2608453035354614
LOSS: 1.2607166767120361
LOSS: 1.260594367980957
LOSS: 1.2604762315750122
LOSS: 1.2603638172149658
LOSS: 1.2602553367614746
LOSS: 1.2601500749588013
LOSS: 1.260048508644104
LOSS: 1.2599496841430664
LOSS: 1.259853720664978
LOSS: 1.259759783744812
LOSS: 1.2596688270568848
LOSS: 1.2595804929733276
LOSS: 1.259494423866272
LOSS: 1.2594112157821655
LOSS: 1.2593311071395874
LOSS: 1.2592527866363525
LOSS: 1.2591772079467773
LOSS: 1.2591031789779663
LOSS: 1.259031891822815
LOSS: 1.3531471490859985
LOSS: 1.3475916385650635
LOSS: 1.3424092531204224
LOSS: 1.3376470804214478
LOSS: 1.3333436250686646
LOSS: 1.3295153379440308
LOSS: 1.3261443376541138
LOSS: 1.32316255569458
LOSS: 1.3204553127288818
LOSS: 1.317884087562561
LOSS: 1.3153302669525146
LOSS: 1.3127261400222778
LOSS: 1.3100600242614746
LOSS: 1.3073606491088867
LOSS: 1.3046733140945435
LOSS: 1.3020457029342651
LOSS: 1.299513816833496
LOSS: 1.2970963716506958
LOSS: 1.294792890548706
LOSS: 1.2925869226455688
LOSS: 1.290453314781189
LOSS: 1.2883672714233398
LOSS: 1.2863129377365112
LOSS: 1.284287929534912
LOSS: 1.2823030948638916
LOSS: 1.2803802490234375
LOSS: 1.2785465717315674
LOSS: 1.2768280506134033
LOSS: 1.2752431631088257
LOSS: 1.2738004922866821
LOSS: 1.2724943161010742
LOSS: 1.2713069915771484
LOSS: 1.2702122926712036
LOSS: 1.269181251525879
LOSS: 1.2681883573532104
LOSS: 1.2672182321548462
LOSS: 1.266265630722046
LOSS: 1.2653340101242065
LOSS: 1.2644325494766235
LOSS: 1.2635715007781982
LOSS: 1.2627596855163574
LOSS: 1.2620010375976562
LOSS: 1.2612955570220947
LOSS: 1.260640025138855
LOSS: 1.2600288391113281
LOSS: 1.2594577074050903
LOSS: 1.2589219808578491
LOSS: 1.2584220170974731
LOSS: 1.2579572200775146
LOSS: 1.2575297355651855
LOSS: 1.257138967514038
LOSS: 1.25678288936615
LOSS: 1.2564582824707031
LOSS: 1.2561612129211426
LOSS: 1.2558856010437012
LOSS: 1.255625605583191
LOSS: 1.2553791999816895
LOSS: 1.2551441192626953
LOSS: 1.2549197673797607
LOSS: 1.2547065019607544
LOSS: 1.254503846168518
LOSS: 1.2543134689331055
LOSS: 1.2541321516036987
LOSS: 1.2539596557617188
LOSS: 1.2537949085235596
LOSS: 1.253636121749878
LOSS: 1.2534823417663574
LOSS: 1.2533332109451294
LOSS: 1.2531890869140625
LOSS: 1.2530510425567627
LOSS: 1.2529189586639404
LOSS: 1.2527917623519897
LOSS: 1.2526700496673584
LOSS: 1.252553105354309
LOSS: 1.252439260482788
LOSS: 1.2523280382156372
LOSS: 1.2522201538085938
LOSS: 1.2521138191223145
LOSS: 1.2520108222961426
LOSS: 1.2519100904464722
LOSS: 1.2518123388290405
LOSS: 1.2517167329788208
LOSS: 1.2516220808029175
LOSS: 1.2515298128128052
LOSS: 1.2514382600784302
LOSS: 1.2513487339019775
LOSS: 1.2512590885162354
LOSS: 1.2511714696884155
LOSS: 1.2816914319992065
LOSS: 1.2807719707489014
LOSS: 1.2800017595291138
LOSS: 1.2793564796447754
LOSS: 1.278803825378418
LOSS: 1.2783128023147583
LOSS: 1.2778582572937012
LOSS: 1.2774226665496826
LOSS: 1.2769989967346191
LOSS: 1.2765870094299316
LOSS: 1.2761896848678589
LOSS: 1.2758103609085083
LOSS: 1.2754508256912231
LOSS: 1.2751091718673706
LOSS: 1.2747817039489746
LOSS: 1.2744635343551636
LOSS: 1.2741501331329346
LOSS: 1.2738410234451294
LOSS: 1.2735366821289062
LOSS: 1.2732402086257935
LOSS: 1.2729562520980835
LOSS: 1.272689700126648
LOSS: 1.2724418640136719
LOSS: 1.2722121477127075
LOSS: 1.2719988822937012
LOSS: 1.2717971801757812
LOSS: 1.2716014385223389
LOSS: 1.2714089155197144
LOSS: 1.2712165117263794
LOSS: 1.2710236310958862
LOSS: 1.2708299160003662
LOSS: 1.2706345319747925
LOSS: 1.2704356908798218
LOSS: 1.2702301740646362
LOSS: 1.2700128555297852
LOSS: 1.2697796821594238
LOSS: 1.2695231437683105
LOSS: 1.2692387104034424
LOSS: 1.2689223289489746
LOSS: 1.2685728073120117
LOSS: 1.268193006515503
LOSS: 1.267786979675293
LOSS: 1.2673649787902832
LOSS: 1.2669373750686646
LOSS: 1.2665148973464966
LOSS: 1.2661033868789673
LOSS: 1.2657098770141602
LOSS: 1.2653353214263916
LOSS: 1.2649786472320557
LOSS: 1.2646360397338867
LOSS: 1.2643040418624878
LOSS: 1.2639793157577515
LOSS: 1.2636568546295166
LOSS: 1.2633336782455444
LOSS: 1.2630046606063843
LOSS: 1.2626700401306152
LOSS: 1.2623244524002075
LOSS: 1.2619681358337402
LOSS: 1.2615976333618164
LOSS: 1.2612136602401733
LOSS: 1.2608147859573364
LOSS: 1.260404348373413
LOSS: 1.2599838972091675
LOSS: 1.2595555782318115
LOSS: 1.2591246366500854
LOSS: 1.2586935758590698
LOSS: 1.2582688331604004
LOSS: 1.257854700088501
LOSS: 1.2574528455734253
LOSS: 1.2570676803588867
LOSS: 1.256702184677124
LOSS: 1.2563577890396118
LOSS: 1.3450164794921875
LOSS: 1.340138554573059
LOSS: 1.3356823921203613
LOSS: 1.3316746950149536
LOSS: 1.3281232118606567
LOSS: 1.3250203132629395
LOSS: 1.3223282098770142
LOSS: 1.3199657201766968
LOSS: 1.3178095817565918
LOSS: 1.3157219886779785
LOSS: 1.3135889768600464
LOSS: 1.311352014541626
LOSS: 1.309009075164795
LOSS: 1.30659818649292
LOSS: 1.304176688194275
LOSS: 1.3018019199371338
LOSS: 1.2995175123214722
LOSS: 1.29734468460083
LOSS: 1.2952826023101807
LOSS: 1.2933157682418823
LOSS: 1.2914222478866577
LOSS: 1.28957998752594
LOSS: 1.2877718210220337
LOSS: 1.2859896421432495
LOSS: 1.2842336893081665
LOSS: 1.2825137376785278
LOSS: 1.2808442115783691
LOSS: 1.2792432308197021
LOSS: 1.277726173400879
LOSS: 1.2763041257858276
LOSS: 1.27498197555542
LOSS: 1.2737559080123901
LOSS: 1.2726163864135742
LOSS: 1.271550178527832
LOSS: 1.270544171333313
LOSS: 1.2695887088775635
LOSS: 1.2686783075332642
LOSS: 1.2678133249282837
LOSS: 1.2669957876205444
LOSS: 1.2662279605865479
LOSS: 1.2655116319656372
LOSS: 1.264844298362732
LOSS: 1.2642199993133545
LOSS: 1.2636293172836304
LOSS: 1.2630608081817627
LOSS: 1.2625041007995605
LOSS: 1.2619504928588867
LOSS: 1.2613928318023682
LOSS: 1.2608267068862915
LOSS: 1.2602506875991821
LOSS: 1.2596646547317505
LOSS: 1.2590676546096802
LOSS: 1.2584604024887085
LOSS: 1.2578436136245728
LOSS: 1.2572177648544312
LOSS: 1.2565854787826538
LOSS: 1.2559504508972168
LOSS: 1.2553175687789917
LOSS: 1.254692792892456
LOSS: 1.2540819644927979
LOSS: 1.2534929513931274
LOSS: 1.252929449081421
LOSS: 1.2523950338363647
LOSS: 1.251890778541565
LOSS: 1.2514194250106812
LOSS: 1.2509781122207642
LOSS: 1.250566005706787
LOSS: 1.2501832246780396
LOSS: 1.2498276233673096
LOSS: 1.2494995594024658
LOSS: 1.249194860458374
LOSS: 1.2489137649536133
LOSS: 1.2486565113067627
LOSS: 1.2484177350997925
LOSS: 1.248198390007019
LOSS: 1.247994065284729
LOSS: 1.2478039264678955
LOSS: 1.2476272583007812
LOSS: 1.2474627494812012
LOSS: 1.2473089694976807
LOSS: 1.2471634149551392
LOSS: 1.247027039527893
LOSS: 1.246897578239441
LOSS: 1.2467749118804932
LOSS: 1.2466589212417603
LOSS: 1.2465485334396362
LOSS: 1.2464417219161987
LOSS: 1.2463411092758179
LOSS: 1.2462458610534668
LOSS: 1.2461555004119873
LOSS: 1.2460708618164062
LOSS: 1.2459907531738281
LOSS: 1.245916724205017
LOSS: 1.2458455562591553
LOSS: 1.2457777261734009
LOSS: 1.2457122802734375
LOSS: 1.2456481456756592
LOSS: 1.2455841302871704
LOSS: 1.2455196380615234
LOSS: 1.2454545497894287
LOSS: 1.3719314336776733
LOSS: 1.3668580055236816
LOSS: 1.3621615171432495
LOSS: 1.3578829765319824
LOSS: 1.3540399074554443
LOSS: 1.350602626800537
LOSS: 1.3474760055541992
LOSS: 1.344512939453125
LOSS: 1.3415721654891968
LOSS: 1.338577389717102
LOSS: 1.3355261087417603
LOSS: 1.3324638605117798
LOSS: 1.3294507265090942
LOSS: 1.3265408277511597
LOSS: 1.3237669467926025
LOSS: 1.3211358785629272
LOSS: 1.3186310529708862
LOSS: 1.3162219524383545
LOSS: 1.3138771057128906
LOSS: 1.3115755319595337
LOSS: 1.3093125820159912
LOSS: 1.3070985078811646
LOSS: 1.3049535751342773
LOSS: 1.3029018640518188
LOSS: 1.3009625673294067
LOSS: 1.299147129058838
LOSS: 1.2974549531936646
LOSS: 1.2958725690841675
LOSS: 1.294379472732544
LOSS: 1.292953372001648
LOSS: 1.2915757894515991
LOSS: 1.2902369499206543
LOSS: 1.2889342308044434
LOSS: 1.2876695394515991
LOSS: 1.2864456176757812
LOSS: 1.285261869430542
LOSS: 1.2841148376464844
LOSS: 1.282997488975525
LOSS: 1.2819011211395264
LOSS: 1.280819296836853
LOSS: 1.2797507047653198
LOSS: 1.2786967754364014
LOSS: 1.2776644229888916
LOSS: 1.2766610383987427
LOSS: 1.2756935358047485
LOSS: 1.2747671604156494
LOSS: 1.2738829851150513
LOSS: 1.2730400562286377
LOSS: 1.2722362279891968
LOSS: 1.271470546722412
LOSS: 1.2707419395446777
LOSS: 1.2700519561767578
LOSS: 1.2694019079208374
LOSS: 1.268791913986206
LOSS: 1.2682205438613892
LOSS: 1.2676855325698853
LOSS: 1.2671828269958496
LOSS: 1.2667107582092285
LOSS: 1.2662652730941772
LOSS: 1.2658448219299316
LOSS: 1.2654482126235962
LOSS: 1.2650762796401978
LOSS: 1.2647268772125244
LOSS: 1.264398217201233
LOSS: 1.2640892267227173
LOSS: 1.2637971639633179
LOSS: 1.2635201215744019
LOSS: 1.2632580995559692
LOSS: 1.2630095481872559
LOSS: 1.2627737522125244
LOSS: 1.2625505924224854
LOSS: 1.262339472770691
LOSS: 1.262139081954956
LOSS: 1.2619482278823853
LOSS: 1.261766791343689
LOSS: 1.2615934610366821
LOSS: 1.261428952217102
LOSS: 1.2612712383270264
LOSS: 1.261121392250061
LOSS: 1.2609800100326538
LOSS: 1.2608453035354614
LOSS: 1.2607166767120361
LOSS: 1.260594367980957
LOSS: 1.2604762315750122
LOSS: 1.2603638172149658
LOSS: 1.2602553367614746
LOSS: 1.2601500749588013
LOSS: 1.260048508644104
LOSS: 1.2599496841430664
LOSS: 1.259853720664978
LOSS: 1.259759783744812
LOSS: 1.2596688270568848
LOSS: 1.2595804929733276
LOSS: 1.259494423866272
LOSS: 1.2594112157821655
LOSS: 1.2593311071395874
LOSS: 1.2592527866363525
LOSS: 1.2591772079467773
LOSS: 1.2591031789779663
LOSS: 1.259031891822815
LOSS: 1.3531471490859985
LOSS: 1.3475916385650635
LOSS: 1.3424092531204224
LOSS: 1.3376470804214478
LOSS: 1.3333436250686646
LOSS: 1.3295153379440308
LOSS: 1.3261443376541138
LOSS: 1.32316255569458
LOSS: 1.3204553127288818
LOSS: 1.317884087562561
LOSS: 1.3153302669525146
LOSS: 1.3127261400222778
LOSS: 1.3100600242614746
LOSS: 1.3073606491088867
LOSS: 1.3046733140945435
LOSS: 1.3020457029342651
LOSS: 1.299513816833496
LOSS: 1.2970963716506958
LOSS: 1.294792890548706
LOSS: 1.2925869226455688
LOSS: 1.290453314781189
LOSS: 1.2883672714233398
LOSS: 1.2863129377365112
LOSS: 1.284287929534912
LOSS: 1.2823030948638916
LOSS: 1.2803802490234375
LOSS: 1.2785465717315674
LOSS: 1.2768280506134033
LOSS: 1.2752431631088257
LOSS: 1.2738004922866821
LOSS: 1.2724943161010742
LOSS: 1.2713069915771484
LOSS: 1.2702122926712036
LOSS: 1.269181251525879
LOSS: 1.2681883573532104
LOSS: 1.2672182321548462
LOSS: 1.266265630722046
LOSS: 1.2653340101242065
LOSS: 1.2644325494766235
LOSS: 1.2635715007781982
LOSS: 1.2627596855163574
LOSS: 1.2620010375976562
LOSS: 1.2612955570220947
LOSS: 1.260640025138855
LOSS: 1.2600288391113281
LOSS: 1.2594577074050903
LOSS: 1.2589219808578491
LOSS: 1.2584220170974731
LOSS: 1.2579572200775146
LOSS: 1.2575297355651855
LOSS: 1.257138967514038
LOSS: 1.25678288936615
LOSS: 1.2564582824707031
LOSS: 1.2561612129211426
LOSS: 1.2558856010437012
LOSS: 1.255625605583191
LOSS: 1.2553791999816895
LOSS: 1.2551441192626953
LOSS: 1.2595555782318115
LOSS: 1.2591246366500854
LOSS: 1.2586935758590698
LOSS: 1.2582688331604004
LOSS: 1.257854700088501
LOSS: 1.2574528455734253
LOSS: 1.2570676803588867
LOSS: 1.256702184677124
LOSS: 1.2563577890396118
LOSS: 1.3633705377578735
LOSS: 1.3589069843292236
LOSS: 1.3548481464385986
LOSS: 1.3512225151062012
LOSS: 1.3480333089828491
LOSS: 1.3452332019805908
LOSS: 1.342707872390747
LOSS: 1.3402972221374512
LOSS: 1.3378610610961914
LOSS: 1.3353350162506104
LOSS: 1.3327265977859497
LOSS: 1.330083966255188
LOSS: 1.327463984489441
LOSS: 1.3249138593673706
LOSS: 1.3224596977233887
LOSS: 1.3201030492782593
LOSS: 1.317823886871338
LOSS: 1.3155890703201294
LOSS: 1.313364028930664
LOSS: 1.3111227750778198
LOSS: 1.3088549375534058
LOSS: 1.3065659999847412
LOSS: 1.3042739629745483
LOSS: 1.3020060062408447
LOSS: 1.299790859222412
LOSS: 1.297655701637268
LOSS: 1.2956211566925049
LOSS: 1.2936972379684448
LOSS: 1.2918843030929565
LOSS: 1.2901753187179565
LOSS: 1.2885593175888062
LOSS: 1.287025809288025
LOSS: 1.285567283630371
LOSS: 1.2841823101043701
LOSS: 1.28286874294281
LOSS: 1.2816262245178223
LOSS: 1.280451774597168
LOSS: 1.2793389558792114
LOSS: 1.2782788276672363
LOSS: 1.2772612571716309
LOSS: 1.2762783765792847
LOSS: 1.2753247022628784
LOSS: 1.274398684501648
LOSS: 1.2735031843185425
LOSS: 1.2726426124572754
LOSS: 1.271822214126587
LOSS: 1.2710450887680054
LOSS: 1.2703124284744263
LOSS: 1.2696232795715332
LOSS: 1.2689756155014038
LOSS: 1.268368124961853
LOSS: 1.2677992582321167
LOSS: 1.2672683000564575
LOSS: 1.26677668094635
LOSS: 1.2663242816925049
LOSS: 1.2659090757369995
LOSS: 1.2655292749404907
LOSS: 1.265181064605713
LOSS: 1.26486074924469
LOSS: 1.2645642757415771
LOSS: 1.264289140701294
LOSS: 1.264032244682312
LOSS: 1.2637929916381836
LOSS: 1.2635694742202759
LOSS: 1.263360619544983
LOSS: 1.2631639242172241
LOSS: 1.2629787921905518
LOSS: 1.2628029584884644
LOSS: 1.262634515762329
LOSS: 1.262473702430725
LOSS: 1.2623203992843628
LOSS: 1.2621744871139526
LOSS: 1.2620357275009155
LOSS: 1.2619034051895142
LOSS: 1.2617777585983276
LOSS: 1.2616572380065918
LOSS: 1.2615424394607544
LOSS: 1.2614331245422363
LOSS: 1.2613286972045898
LOSS: 1.261228322982788
LOSS: 1.2611333131790161
LOSS: 1.2610421180725098
LOSS: 1.2609556913375854
LOSS: 1.2608721256256104
LOSS: 1.2607918977737427
LOSS: 1.2607144117355347
LOSS: 1.2606397867202759
LOSS: 1.2605680227279663
LOSS: 1.2604979276657104
LOSS: 1.2604304552078247
LOSS: 1.2603652477264404
LOSS: 1.260301113128662
LOSS: 1.2602392435073853
LOSS: 1.2601778507232666
LOSS: 1.260118842124939
LOSS: 1.2600609064102173
LOSS: 1.2600045204162598
LOSS: 1.25994873046875
LOSS: 1.2598938941955566
LOSS: 1.259840726852417
LOSS: 1.3419101238250732
LOSS: 1.3358820676803589
LOSS: 1.3301349878311157
LOSS: 1.3247041702270508
LOSS: 1.3196240663528442
LOSS: 1.3149197101593018
LOSS: 1.3105992078781128
LOSS: 1.3066446781158447
LOSS: 1.3030062913894653
LOSS: 1.2996059656143188
LOSS: 1.2963502407073975
LOSS: 1.2931526899337769
LOSS: 1.2899528741836548
LOSS: 1.286724328994751
LOSS: 1.2834749221801758
LOSS: 1.2802399396896362
LOSS: 1.2770702838897705
LOSS: 1.2740223407745361
LOSS: 1.2711466550827026
LOSS: 1.2684799432754517
LOSS: 1.2660423517227173
LOSS: 1.2638367414474487
LOSS: 1.2618517875671387
LOSS: 1.2600657939910889
LOSS: 1.2584524154663086
LOSS: 1.2569838762283325
LOSS: 1.2556346654891968
LOSS: 1.2543832063674927
LOSS: 1.2532140016555786
LOSS: 1.2521172761917114
LOSS: 1.25108802318573
LOSS: 1.2501235008239746
LOSS: 1.2492244243621826
LOSS: 1.2483901977539062
LOSS: 1.2476202249526978
LOSS: 1.246911644935608
LOSS: 1.246260404586792
LOSS: 1.2456611394882202
LOSS: 1.2451092004776
LOSS: 1.2445993423461914
LOSS: 1.2441282272338867
LOSS: 1.243693470954895
LOSS: 1.2432942390441895
LOSS: 1.2429286241531372
LOSS: 1.2425956726074219
LOSS: 1.2422937154769897
LOSS: 1.2420194149017334
LOSS: 1.2417693138122559
LOSS: 1.241540551185608
LOSS: 1.2413283586502075
LOSS: 1.2411316633224487
LOSS: 1.2409483194351196
LOSS: 1.2407764196395874
LOSS: 1.2406163215637207
LOSS: 1.2404677867889404
LOSS: 1.240329384803772
LOSS: 1.2402005195617676
LOSS: 1.2400788068771362
LOSS: 1.2399635314941406
LOSS: 1.2398523092269897
LOSS: 1.2397441864013672
LOSS: 1.2396382093429565
LOSS: 1.2395342588424683
LOSS: 1.239431619644165
LOSS: 1.239332675933838
LOSS: 1.2392369508743286
LOSS: 1.2391448020935059
LOSS: 1.2390573024749756
LOSS: 1.2389737367630005
LOSS: 1.238895058631897
LOSS: 1.2388204336166382
LOSS: 1.2387492656707764
LOSS: 1.238681435585022
LOSS: 1.2386163473129272
LOSS: 1.2385534048080444
LOSS: 1.238492727279663
LOSS: 1.2384345531463623
LOSS: 1.2383767366409302
LOSS: 1.238319754600525
LOSS: 1.2382620573043823
LOSS: 1.238203763961792
LOSS: 1.2381433248519897
LOSS: 1.2380820512771606
LOSS: 1.2380173206329346
LOSS: 1.2379509210586548
LOSS: 1.2378820180892944
LOSS: 1.2378119230270386
LOSS: 1.2377398014068604
LOSS: 1.2376662492752075
LOSS: 1.2375922203063965
LOSS: 1.2375175952911377
LOSS: 1.237442970275879
LOSS: 1.2373679876327515
LOSS: 1.2372936010360718
LOSS: 1.2372205257415771
LOSS: 1.2371474504470825
LOSS: 1.237076997756958
LOSS: 1.2370080947875977
LOSS: 1.2369407415390015
LOSS: 1.236876130104065
LOSS: 1.3419101238250732
LOSS: 1.3358820676803589
LOSS: 1.3301349878311157
LOSS: 1.3247041702270508
LOSS: 1.3196240663528442
LOSS: 1.3149197101593018
LOSS: 1.3105992078781128
LOSS: 1.3066446781158447
LOSS: 1.3030062913894653
LOSS: 1.2996059656143188
LOSS: 1.2963502407073975
LOSS: 1.2931526899337769
LOSS: 1.2899528741836548
LOSS: 1.286724328994751
LOSS: 1.2834749221801758
LOSS: 1.2802399396896362
LOSS: 1.2770702838897705
LOSS: 1.2740223407745361
LOSS: 1.2711466550827026
LOSS: 1.2684799432754517
LOSS: 1.2660423517227173
LOSS: 1.2638367414474487
LOSS: 1.2618517875671387
LOSS: 1.2600657939910889
LOSS: 1.2584524154663086
LOSS: 1.2569838762283325
LOSS: 1.2556346654891968
LOSS: 1.2543832063674927
LOSS: 1.2532140016555786
LOSS: 1.2521172761917114
LOSS: 1.25108802318573
LOSS: 1.2501235008239746
LOSS: 1.2492244243621826
LOSS: 1.2483901977539062
LOSS: 1.2476202249526978
LOSS: 1.246911644935608
LOSS: 1.246260404586792
LOSS: 1.2456611394882202
LOSS: 1.2451092004776
LOSS: 1.2445993423461914
LOSS: 1.2441282272338867
LOSS: 1.243693470954895
LOSS: 1.2432942390441895
LOSS: 1.2429286241531372
LOSS: 1.2425956726074219
LOSS: 1.2422937154769897
LOSS: 1.2420194149017334
LOSS: 1.2417693138122559
LOSS: 1.241540551185608
LOSS: 1.2413283586502075
LOSS: 1.2411316633224487
LOSS: 1.2409483194351196
LOSS: 1.2407764196395874
LOSS: 1.2406163215637207
LOSS: 1.2404677867889404
LOSS: 1.240329384803772
LOSS: 1.2402005195617676
LOSS: 1.2400788068771362
LOSS: 1.2399635314941406
LOSS: 1.2398523092269897
LOSS: 1.2397441864013672
LOSS: 1.2396382093429565
LOSS: 1.2395342588424683
LOSS: 1.239431619644165
LOSS: 1.239332675933838
LOSS: 1.2392369508743286
LOSS: 1.2391448020935059
LOSS: 1.2390573024749756
LOSS: 1.2389737367630005
LOSS: 1.238895058631897
LOSS: 1.2388204336166382
LOSS: 1.2387492656707764
LOSS: 1.238681435585022
LOSS: 1.2386163473129272
LOSS: 1.2385534048080444
LOSS: 1.238492727279663
LOSS: 1.2384345531463623
LOSS: 1.2383767366409302
LOSS: 1.238319754600525
LOSS: 1.2382620573043823
LOSS: 1.238203763961792
LOSS: 1.2381433248519897
LOSS: 1.2380820512771606
LOSS: 1.2380173206329346
LOSS: 1.2379509210586548
LOSS: 1.2378820180892944
LOSS: 1.2378119230270386
LOSS: 1.2377398014068604
LOSS: 1.2376662492752075
LOSS: 1.2375922203063965
LOSS: 1.2375175952911377
LOSS: 1.237442970275879
LOSS: 1.2373679876327515
LOSS: 1.2372936010360718
LOSS: 1.2372205257415771
LOSS: 1.2371474504470825
LOSS: 1.237076997756958
LOSS: 1.2370080947875977
LOSS: 1.2369407415390015
LOSS: 1.236876130104065
LOSS: 1.352876901626587
LOSS: 1.3472651243209839
LOSS: 1.342022180557251
LOSS: 1.3371961116790771
LOSS: 1.3328262567520142
LOSS: 1.3289334774017334
LOSS: 1.3255081176757812
LOSS: 1.3224973678588867
LOSS: 1.3197990655899048
LOSS: 1.3172755241394043
LOSS: 1.3147846460342407
LOSS: 1.3122206926345825
LOSS: 1.3095368146896362
LOSS: 1.3067418336868286
LOSS: 1.3038822412490845
LOSS: 1.3010183572769165
LOSS: 1.298208475112915
LOSS: 1.2954975366592407
LOSS: 1.2929118871688843
LOSS: 1.2904603481292725
LOSS: 1.2881354093551636
LOSS: 1.2600795030593872
LOSS: 1.2599629163742065
LOSS: 1.2598509788513184
LOSS: 1.2597424983978271
LOSS: 1.2596368789672852
LOSS: 1.2595338821411133
LOSS: 1.2594329118728638
LOSS: 1.2593355178833008
LOSS: 1.259242057800293
LOSS: 1.259151816368103
LOSS: 1.2590656280517578
LOSS: 1.2589837312698364
LOSS: 1.2589048147201538
LOSS: 1.2588286399841309
LOSS: 1.2587558031082153
LOSS: 1.2586852312088013
LOSS: 1.2586175203323364
LOSS: 1.2585521936416626
LOSS: 1.2584885358810425
LOSS: 1.2584278583526611
LOSS: 1.2583694458007812
LOSS: 1.2583129405975342
LOSS: 1.258258581161499
LOSS: 1.2582052946090698
LOSS: 1.2581537961959839
LOSS: 1.258103847503662
LOSS: 1.258054494857788
LOSS: 1.2580071687698364
LOSS: 1.2579609155654907
LOSS: 1.2579164505004883
LOSS: 1.2578727006912231
LOSS: 1.2578303813934326
LOSS: 1.257788896560669
LOSS: 1.2577488422393799
LOSS: 1.2577095031738281
LOSS: 1.2576709985733032
LOSS: 1.2576336860656738
LOSS: 1.3520963191986084
LOSS: 1.3474645614624023
LOSS: 1.343206763267517
LOSS: 1.3393642902374268
LOSS: 1.3359650373458862
LOSS: 1.3330119848251343
LOSS: 1.3304692506790161
LOSS: 1.328250527381897
LOSS: 1.326226830482483
LOSS: 1.3242586851119995
LOSS: 1.3222404718399048
LOSS: 1.320129632949829
LOSS: 1.3179430961608887
LOSS: 1.31573486328125
LOSS: 1.3135675191879272
LOSS: 1.3114935159683228
LOSS: 1.3095433712005615
LOSS: 1.3077244758605957
LOSS: 1.3060250282287598
LOSS: 1.3044219017028809
LOSS: 1.302889108657837
LOSS: 1.3014065027236938
LOSS: 1.2999628782272339
LOSS: 1.2985588312149048
LOSS: 1.2972044944763184
LOSS: 1.2959154844284058
LOSS: 1.2947092056274414
LOSS: 1.2936012744903564
LOSS: 1.2926000356674194
LOSS: 1.2917075157165527
LOSS: 1.2909175157546997
LOSS: 1.2902199029922485
LOSS: 1.2896021604537964
LOSS: 1.2890537977218628
LOSS: 1.2885671854019165
LOSS: 1.288136601448059
LOSS: 1.2877578735351562
LOSS: 1.2874280214309692
LOSS: 1.2871408462524414
LOSS: 1.2868890762329102
LOSS: 1.2866640090942383
LOSS: 1.2864556312561035
LOSS: 1.2862558364868164
LOSS: 1.2860584259033203
LOSS: 1.2858622074127197
LOSS: 1.2856661081314087
LOSS: 1.2854745388031006
LOSS: 1.2852917909622192
LOSS: 1.285121202468872
LOSS: 1.284966230392456
LOSS: 1.2848267555236816
LOSS: 1.2847017049789429
LOSS: 1.2845872640609741
LOSS: 1.284481406211853
LOSS: 1.2843824625015259
LOSS: 1.2842869758605957
LOSS: 1.2841951847076416
LOSS: 1.2841079235076904
LOSS: 1.2840261459350586
LOSS: 1.283948540687561
LOSS: 1.2838746309280396
LOSS: 1.2838048934936523
LOSS: 1.2837356328964233
LOSS: 1.283666729927063
LOSS: 1.283595323562622
LOSS: 1.2835209369659424
LOSS: 1.2834460735321045
LOSS: 1.283371090888977
LOSS: 1.2832953929901123
LOSS: 1.283219337463379
LOSS: 1.2831436395645142
LOSS: 1.283065676689148
LOSS: 1.2829855680465698
LOSS: 1.2829031944274902
LOSS: 1.282817006111145
LOSS: 1.2827270030975342
LOSS: 1.2826353311538696
LOSS: 1.2825404405593872
LOSS: 1.2824434041976929
LOSS: 1.282342553138733
LOSS: 1.2822376489639282
LOSS: 1.2821290493011475
LOSS: 1.282015085220337
LOSS: 1.281898856163025
LOSS: 1.2817796468734741
LOSS: 1.2816556692123413
LOSS: 1.2815313339233398
LOSS: 1.2814027070999146
LOSS: 1.2812731266021729
LOSS: 1.2811403274536133
LOSS: 1.2810052633285522
LOSS: 1.280869960784912
LOSS: 1.2807337045669556
LOSS: 1.280597448348999
LOSS: 1.280461072921753
LOSS: 1.2803243398666382
LOSS: 1.2801902294158936
LOSS: 1.2800568342208862
LOSS: 1.2799261808395386
LOSS: 1.2797985076904297
LOSS: 1.3520963191986084
LOSS: 1.3474645614624023
LOSS: 1.343206763267517
LOSS: 1.3393642902374268
LOSS: 1.3359650373458862
LOSS: 1.3330119848251343
LOSS: 1.3304692506790161
LOSS: 1.328250527381897
LOSS: 1.326226830482483
LOSS: 1.3242586851119995
LOSS: 1.3222404718399048
LOSS: 1.320129632949829
LOSS: 1.3179430961608887
LOSS: 1.31573486328125
LOSS: 1.3135675191879272
LOSS: 1.3114935159683228
LOSS: 1.3095433712005615
LOSS: 1.3077244758605957
LOSS: 1.3060250282287598
LOSS: 1.3044219017028809
LOSS: 1.302889108657837
LOSS: 1.3014065027236938
LOSS: 1.2999628782272339
LOSS: 1.2985588312149048
LOSS: 1.2972044944763184
LOSS: 1.2959154844284058
LOSS: 1.2947092056274414
LOSS: 1.2936012744903564
LOSS: 1.2926000356674194
LOSS: 1.2917075157165527
LOSS: 1.2909175157546997
LOSS: 1.2902199029922485
LOSS: 1.2896021604537964
LOSS: 1.2890537977218628
LOSS: 1.2885671854019165
LOSS: 1.288136601448059
LOSS: 1.2877578735351562
LOSS: 1.2874280214309692
LOSS: 1.2871408462524414
LOSS: 1.2868890762329102
LOSS: 1.2866640090942383
LOSS: 1.2864556312561035
LOSS: 1.2862558364868164
LOSS: 1.2860584259033203
LOSS: 1.2858622074127197
LOSS: 1.2856661081314087
LOSS: 1.2854745388031006
LOSS: 1.2852917909622192
LOSS: 1.285121202468872
LOSS: 1.284966230392456
LOSS: 1.2848267555236816
LOSS: 1.2847017049789429
LOSS: 1.2845872640609741
LOSS: 1.284481406211853
LOSS: 1.2843824625015259
LOSS: 1.2842869758605957
LOSS: 1.2841951847076416
LOSS: 1.2841079235076904
LOSS: 1.2840261459350586
LOSS: 1.283948540687561
LOSS: 1.2838746309280396
LOSS: 1.2838048934936523
LOSS: 1.2837356328964233
LOSS: 1.283666729927063
LOSS: 1.283595323562622
LOSS: 1.2835209369659424
LOSS: 1.2834460735321045
LOSS: 1.283371090888977
LOSS: 1.2832953929901123
LOSS: 1.283219337463379
LOSS: 1.2831436395645142
LOSS: 1.283065676689148
LOSS: 1.2829855680465698
LOSS: 1.2829031944274902
LOSS: 1.282817006111145
LOSS: 1.2827270030975342
LOSS: 1.2826353311538696
LOSS: 1.2825404405593872
LOSS: 1.2824434041976929
LOSS: 1.282342553138733
LOSS: 1.2822376489639282
LOSS: 1.2821290493011475
LOSS: 1.282015085220337
LOSS: 1.281898856163025
LOSS: 1.2817796468734741
LOSS: 1.2816556692123413
LOSS: 1.2815313339233398
LOSS: 1.2814027070999146
LOSS: 1.2812731266021729
LOSS: 1.2811403274536133
LOSS: 1.2810052633285522
LOSS: 1.280869960784912
LOSS: 1.2807337045669556
LOSS: 1.280597448348999
LOSS: 1.280461072921753
LOSS: 1.2803243398666382
LOSS: 1.2801902294158936
LOSS: 1.2800568342208862
LOSS: 1.2799261808395386
LOSS: 1.2797985076904297
LOSS: 1.3599352836608887
LOSS: 1.354947805404663
LOSS: 1.3503750562667847
LOSS: 1.3462576866149902
LOSS: 1.342616081237793
LOSS: 1.339430570602417
LOSS: 1.3366225957870483
LOSS: 1.3340572118759155
LOSS: 1.3315861225128174
LOSS: 1.3291065692901611
LOSS: 1.3265873193740845
LOSS: 1.3240498304367065
LOSS: 1.3215413093566895
LOSS: 1.3191102743148804
LOSS: 1.3167935609817505
LOSS: 1.3146079778671265
LOSS: 1.3125499486923218
LOSS: 1.3105963468551636
LOSS: 1.3087143898010254
LOSS: 1.3068701028823853
LOSS: 1.3050386905670166
LOSS: 1.3032079935073853
LOSS: 1.3013800382614136
LOSS: 1.299566388130188
LOSS: 1.297784686088562
LOSS: 1.2960530519485474
LOSS: 1.294385313987732
LOSS: 1.2927895784378052
LOSS: 1.2912650108337402
LOSS: 1.2898048162460327
LOSS: 1.2883986234664917
LOSS: 1.2870389223098755
LOSS: 1.285722255706787
LOSS: 1.2844513654708862
LOSS: 1.2832344770431519
LOSS: 1.2820820808410645
LOSS: 1.2810020446777344
LOSS: 1.2799986600875854
LOSS: 1.2790697813034058
LOSS: 1.2782100439071655
LOSS: 1.277410626411438
LOSS: 1.2766629457473755
LOSS: 1.2759613990783691
LOSS: 1.275301218032837
LOSS: 1.2746824026107788
LOSS: 1.2741050720214844
LOSS: 1.2735697031021118
LOSS: 1.2730754613876343
LOSS: 1.2726200819015503
LOSS: 1.2721996307373047
LOSS: 1.2718093395233154
LOSS: 1.271445870399475
LOSS: 1.2711057662963867
LOSS: 1.2707892656326294
LOSS: 1.2704944610595703
LOSS: 1.2702223062515259
LOSS: 1.2699699401855469
LOSS: 1.2697383165359497
LOSS: 1.2695231437683105
LOSS: 1.2693203687667847
LOSS: 1.2691290378570557
LOSS: 1.2689478397369385
LOSS: 1.2687745094299316
LOSS: 1.268609881401062
LOSS: 1.2684534788131714
LOSS: 1.268306016921997
LOSS: 1.268165946006775
LOSS: 1.2680329084396362
LOSS: 1.2679064273834229
LOSS: 1.2677844762802124
LOSS: 1.26766836643219
LOSS: 1.2675576210021973
LOSS: 1.267451524734497
LOSS: 1.267350673675537
LOSS: 1.2672545909881592
LOSS: 1.2671629190444946
LOSS: 1.2670737504959106
LOSS: 1.2669909000396729
LOSS: 1.2669100761413574
LOSS: 1.266832709312439
LOSS: 1.2667601108551025
LOSS: 1.2666897773742676
LOSS: 1.2666224241256714
LOSS: 1.2665601968765259
LOSS: 1.2664997577667236
LOSS: 1.2664408683776855
LOSS: 1.2663837671279907
LOSS: 1.266329050064087
LOSS: 1.2662760019302368
LOSS: 1.2662220001220703
LOSS: 1.2661702632904053
LOSS: 1.2661195993423462
LOSS: 1.266069769859314
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 48, in pearson
    r = pearsonr(y_true, y_pred).statistic
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/stats/_stats_py.py", line 4452, in pearsonr
    normym = linalg.norm(ym)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/scipy/linalg/_misc.py", line 146, in norm
    a = np.asarray_chkfinite(a)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/lib/function_base.py", line 627, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 25, in rmse_score
    return mean_squared_error(y_test, y_pred, squared=False)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/metrics/_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 921, in check_array
    _assert_all_finite(
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/utils/validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3256: RuntimeWarning: overflow encountered in power
  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (446). n_quantiles is set to n_samples.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (446). n_quantiles is set to n_samples.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (446). n_quantiles is set to n_samples.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (447). n_quantiles is set to n_samples.
  warnings.warn(
/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (447). n_quantiles is set to n_samples.
  warnings.warn(
Average scores:	 r: 0.77±0.04	 r2: 0.58±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/GP_predictions.csv
Average scores:	 r: nan±nan	 r2: nan±nan
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/GP_predictions.csv
Average scores:	 r: 0.5±0.08	 r2: 0.17±0.11
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/GP_predictions.csv
Average scores:	 r: 0.63±0.06	 r2: 0.38±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/GP_predictions.csv
Average scores:	 r: 0.62±0.05	 r2: 0.37±0.07
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/GP_predictions.csv
n numeric features: 8
Average scores:	 r: 0.64±0.06	 r2: 0.4±0.09
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/GP_predictions.csv
n numeric features: 5
Average scores:	 r: nan±nan	 r2: nan±nan
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/GP_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/GP_predictions.csv
Average scores:	 r: 0.73±0.05	 r2: 0.5±0.1
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_ECFP/NN_predictions.csv
Average scores:	 r: 0.16±0.25	 r2: -8.902955035016204e+131±5.191270253109827e+132
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_mordred/NN_predictions.csv
Average scores:	 r: 0.59±0.05	 r2: 0.31±0.11
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_OHE/NN_predictions.csv
Average scores:	 r: 0.51±0.16	 r2: -7.11±30.42
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SELFIES/NN_predictions.csv
Average scores:	 r: 0.47±0.11	 r2: -1.01±3.52
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_SMILES/NN_predictions.csv
n numeric features: 8
Average scores:	 r: 0.57±0.07	 r2: 0.31±0.08
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_material properties/NN_predictions.csv
n numeric features: 5
Average scores:	 r: 0.24±0.19	 r2: -1.2075498737458894e+74±7.041165224438799e+74
Saved results to:
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/NN_scores.json
/home/chefos/projects/ml_for_opvs/results/target_PCE/features_fabrication only/NN_predictions.csv
Traceback (most recent call last):
  File "/home/chefos/projects/ml_for_opvs/code_/training/train_structure_only.py", line 266, in <module>
    main_representation_model_grid(target_feats=[target], hyperopt=h_opt)
  File "/home/chefos/projects/ml_for_opvs/code_/training/train_structure_only.py", line 219, in main_representation_model_grid
    main_graphs_only(dataset=opv_dataset,
  File "/home/chefos/projects/ml_for_opvs/code_/training/train_structure_only.py", line 65, in main_graphs_only
    scores, predictions = run_graphs_only(dataset=dataset,
  File "/home/chefos/projects/ml_for_opvs/code_/training/training_utils.py", line 338, in run_graphs_only
    return _run_graphs(X, y,
  File "/home/chefos/projects/ml_for_opvs/code_/training/training_utils.py", line 377, in _run_graphs
    scores, predictions = cross_validate_regressor(regressor, X, y, cv_outer)
  File "/home/chefos/projects/ml_for_opvs/code_/training/scoring.py", line 85, in cross_validate_regressor
    scores: dict[str, float] = cross_validate(regressor, X, y,
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 5 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
5 fits failed with the following error:
Traceback (most recent call last):
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/pipeline.py", line 405, in fit
    self._final_estimator.fit(Xt, y, **fit_params_last_step)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/site-packages/sklearn/compose/_target.py", line 262, in fit
    self.regressor_.fit(X, y_trans, **fit_params)
  File "/home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py", line 255, in fit
    self.create_data(x_train, y_train)
  File "/home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py", line 255, in fit
    self.create_data(x_train, y_train)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/bdb.py", line 114, in dispatch_line
    self.user_line(frame)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/pdb.py", line 253, in user_line
    self.interaction(frame, None)
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/pdb.py", line 348, in interaction
    self._cmdloop()
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/pdb.py", line 313, in _cmdloop
    self.cmdloop()
  File "/home/chefos/anaconda3/envs/opv_ml/lib/python3.10/cmd.py", line 126, in cmdloop
    line = input(self.prompt)
RuntimeError: input(): lost sys.stdin

LOSS: 1.2660210132598877
LOSS: 1.2659729719161987
LOSS: 1.2659263610839844
LOSS: 1.2658812999725342
LOSS: 1.2658380270004272
LOSS: 1.2657949924468994
LOSS: 1.2657533884048462
LOSS: 1.352876901626587
LOSS: 1.3472651243209839
LOSS: 1.342022180557251
LOSS: 1.3371961116790771
LOSS: 1.3328262567520142
LOSS: 1.3289334774017334
LOSS: 1.3255081176757812
LOSS: 1.3224973678588867
LOSS: 1.3197990655899048
LOSS: 1.3172755241394043
LOSS: 1.3147846460342407
LOSS: 1.3122206926345825
LOSS: 1.3095368146896362
LOSS: 1.3067418336868286
LOSS: 1.3038822412490845
LOSS: 1.3010183572769165
LOSS: 1.298208475112915
LOSS: 1.2954975366592407
LOSS: 1.2929118871688843
LOSS: 1.2904603481292725
LOSS: 1.2881354093551636
LOSS: 1.2859201431274414
LOSS: 1.2837955951690674
LOSS: 1.2817469835281372
LOSS: 1.2797681093215942
LOSS: 1.2778618335723877
LOSS: 1.2760376930236816
LOSS: 1.2743104696273804
LOSS: 1.2726937532424927
LOSS: 1.2711979150772095
LOSS: 1.2698255777359009
LOSS: 1.2685716152191162
LOSS: 1.2674221992492676
LOSS: 1.266358733177185
LOSS: 1.2653603553771973
LOSS: 1.2644102573394775
LOSS: 1.2634963989257812
LOSS: 1.2626153230667114
LOSS: 1.2617684602737427
LOSS: 1.2609608173370361
LOSS: 1.260199785232544
LOSS: 1.2594882249832153
LOSS: 1.2588274478912354
LOSS: 1.2582151889801025
LOSS: 1.2576452493667603
LOSS: 1.2571107149124146
LOSS: 1.2566053867340088
LOSS: 1.2561240196228027
LOSS: 1.2556647062301636
LOSS: 1.255226731300354
LOSS: 1.2548110485076904
LOSS: 1.2544190883636475
LOSS: 1.2540501356124878
LOSS: 1.2537040710449219
LOSS: 1.2533786296844482
LOSS: 1.2530713081359863
LOSS: 1.2527785301208496
LOSS: 1.2524986267089844
LOSS: 1.252229928970337
LOSS: 1.2519721984863281
LOSS: 1.2517261505126953
LOSS: 1.2514926195144653
LOSS: 1.251272439956665
LOSS: 1.2510654926300049
LOSS: 1.250870943069458
LOSS: 1.250687837600708
LOSS: 1.2505141496658325
LOSS: 1.2503485679626465
LOSS: 1.2501904964447021
LOSS: 1.250038743019104
LOSS: 1.249894380569458
LOSS: 1.249755859375
LOSS: 1.2496229410171509
LOSS: 1.249495267868042
LOSS: 1.2493722438812256
LOSS: 1.2492526769638062
LOSS: 1.249136209487915
LOSS: 1.2490218877792358
LOSS: 1.2489094734191895
LOSS: 1.2487995624542236
LOSS: 1.248692274093628
LOSS: 1.2485872507095337
LOSS: 1.2484849691390991
LOSS: 1.2483859062194824
LOSS: 1.2482889890670776
LOSS: 1.2481938600540161
LOSS: 1.2481015920639038
LOSS: 1.2480109930038452
LOSS: 1.2479225397109985
LOSS: 1.2478357553482056
LOSS: 1.247750997543335
LOSS: 1.247667908668518
LOSS: 1.2475872039794922
LOSS: 1.2475078105926514
LOSS: 1.2474303245544434
LOSS: 1.2473543882369995
LOSS: 1.247279405593872
LOSS: 1.247206449508667
LOSS: 1.2471345663070679
LOSS: 1.247064232826233
LOSS: 1.3481526374816895
LOSS: 1.34260892868042
LOSS: 1.3373496532440186
LOSS: 1.3324167728424072
LOSS: 1.3278474807739258
LOSS: 1.323667287826538
LOSS: 1.3198796510696411
LOSS: 1.3164571523666382
LOSS: 1.313334584236145
LOSS: 1.310414433479309
LOSS: 1.3075897693634033
LOSS: 1.3047740459442139
LOSS: 1.3019243478775024
LOSS: 1.2990469932556152
LOSS: 1.2961839437484741
LOSS: 1.293393850326538
LOSS: 1.290734887123108
LOSS: 1.2882518768310547
LOSS: 1.285970687866211
LOSS: 1.2838969230651855
LOSS: 1.2820197343826294
LOSS: 1.280317783355713
LOSS: 1.2787662744522095
LOSS: 1.27734375
LOSS: 1.2760355472564697
LOSS: 1.2748345136642456
LOSS: 1.2737411260604858
LOSS: 1.2727588415145874
LOSS: 1.271890640258789
LOSS: 1.2711371183395386
LOSS: 1.2704914808273315
LOSS: 1.2699404954910278
LOSS: 1.2694648504257202
LOSS: 1.2690421342849731
LOSS: 1.2686493396759033
LOSS: 1.2682687044143677
LOSS: 1.2678890228271484
LOSS: 1.2675060033798218
LOSS: 1.2671239376068115
LOSS: 1.2667487859725952
LOSS: 1.2663905620574951
LOSS: 1.266057014465332
LOSS: 1.2657530307769775
LOSS: 1.2654789686203003
LOSS: 1.2652331590652466
LOSS: 1.265012502670288
LOSS: 1.2648108005523682
LOSS: 1.2646253108978271
LOSS: 1.2644524574279785
LOSS: 1.264291763305664
LOSS: 1.264142394065857
LOSS: 1.264005184173584
LOSS: 1.2638789415359497
LOSS: 1.2637622356414795
LOSS: 1.26365327835083
LOSS: 1.263549566268921
LOSS: 1.2634482383728027
LOSS: 1.2633484601974487
LOSS: 1.2632486820220947
LOSS: 1.2631492614746094
LOSS: 1.2630499601364136
LOSS: 1.2629523277282715
LOSS: 1.2628560066223145
LOSS: 1.2627620697021484
LOSS: 1.2626711130142212
LOSS: 1.2625815868377686
LOSS: 1.262492299079895
LOSS: 1.2624030113220215
LOSS: 1.262312412261963
LOSS: 1.2622207403182983
LOSS: 1.2621265649795532
LOSS: 1.2620292901992798
LOSS: 1.261928677558899
LOSS: 1.2618223428726196
LOSS: 1.2617100477218628
LOSS: 1.261590600013733
LOSS: 1.261461615562439
LOSS: 1.2613236904144287
LOSS: 1.261176347732544
LOSS: 1.2610212564468384
LOSS: 1.2608593702316284
LOSS: 1.2606935501098633
LOSS: 1.260526180267334
LOSS: 1.2603601217269897
LOSS: 1.2601978778839111
LOSS: 1.2600412368774414
LOSS: 1.259891390800476
LOSS: 1.2597503662109375
LOSS: 1.2596158981323242
LOSS: 1.2594908475875854
LOSS: 1.259373426437378
LOSS: 1.259263515472412
LOSS: 1.2591609954833984
LOSS: 1.2590649127960205
LOSS: 1.2589763402938843
LOSS: 1.258893370628357
LOSS: 1.2588164806365967
LOSS: 1.258744478225708
LOSS: 1.2586779594421387
LOSS: 1.2586147785186768
> /home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py(255)fit()
-> self.create_data(x_train, y_train)
LOSS: 1.2549197673797607
LOSS: 1.2547065019607544
LOSS: 1.254503846168518
LOSS: 1.2543134689331055
LOSS: 1.2541321516036987
LOSS: 1.2539596557617188
LOSS: 1.2537949085235596
LOSS: 1.253636121749878
LOSS: 1.2534823417663574
LOSS: 1.2533332109451294
LOSS: 1.2531890869140625
LOSS: 1.2530510425567627
LOSS: 1.2529189586639404
LOSS: 1.2527917623519897
LOSS: 1.2526700496673584
LOSS: 1.252553105354309
LOSS: 1.252439260482788
LOSS: 1.2523280382156372
LOSS: 1.2522201538085938
LOSS: 1.2521138191223145
LOSS: 1.2520108222961426
LOSS: 1.2519100904464722
LOSS: 1.2518123388290405
LOSS: 1.2517167329788208
LOSS: 1.2516220808029175
LOSS: 1.2515298128128052
LOSS: 1.2514382600784302
LOSS: 1.2513487339019775
LOSS: 1.2512590885162354
LOSS: 1.2511714696884155
LOSS: 1.2510854005813599
LOSS: 1.2510002851486206
LOSS: 1.2509163618087769
LOSS: 1.2508333921432495
LOSS: 1.2507516145706177
LOSS: 1.2506704330444336
LOSS: 1.2505911588668823
LOSS: 1.2505122423171997
LOSS: 1.250434160232544
LOSS: 1.2503572702407837
LOSS: 1.250280737876892
LOSS: 1.250206470489502
LOSS: 1.364822268486023
LOSS: 1.3601505756378174
LOSS: 1.3558233976364136
LOSS: 1.3518733978271484
LOSS: 1.3483139276504517
LOSS: 1.345127820968628
LOSS: 1.3422510623931885
LOSS: 1.3395715951919556
LOSS: 1.3369580507278442
LOSS: 1.3343086242675781
LOSS: 1.3315815925598145
LOSS: 1.3287924528121948
LOSS: 1.3259899616241455
LOSS: 1.3232309818267822
LOSS: 1.3205623626708984
LOSS: 1.318009853363037
LOSS: 1.3155758380889893
LOSS: 1.3132424354553223
LOSS: 1.310982584953308
LOSS: 1.308772087097168
LOSS: 1.306597352027893
LOSS: 1.3044604063034058
LOSS: 1.3023775815963745
LOSS: 1.3003743886947632
LOSS: 1.2984789609909058
LOSS: 1.2967158555984497
LOSS: 1.2951009273529053
LOSS: 1.2936358451843262
LOSS: 1.2923084497451782
LOSS: 1.291094422340393
LOSS: 1.2899634838104248
LOSS: 1.288887619972229
LOSS: 1.2878471612930298
LOSS: 1.2868345975875854
LOSS: 1.2858529090881348
LOSS: 1.2849122285842896
LOSS: 1.2840244770050049
LOSS: 1.2831958532333374
LOSS: 1.2824283838272095
LOSS: 1.2817184925079346
LOSS: 1.281058430671692
LOSS: 1.2804405689239502
LOSS: 1.2798579931259155
LOSS: 1.2793078422546387
LOSS: 1.2787878513336182
LOSS: 1.2782988548278809
LOSS: 1.2778409719467163
LOSS: 1.2774111032485962
LOSS: 1.2770065069198608
LOSS: 1.276623249053955
LOSS: 1.2762563228607178
LOSS: 1.2759028673171997
LOSS: 1.2755612134933472
LOSS: 1.275230050086975
LOSS: 1.2749117612838745
LOSS: 1.2746058702468872
LOSS: 1.2743126153945923
LOSS: 1.2740304470062256
LOSS: 1.2737548351287842
LOSS: 1.2734860181808472
LOSS: 1.2732189893722534
LOSS: 1.2729519605636597
LOSS: 1.2726836204528809
LOSS: 1.2724109888076782
LOSS: 1.2721339464187622
LOSS: 1.2718499898910522
LOSS: 1.271553635597229
LOSS: 1.2712442874908447
LOSS: 1.2709215879440308
LOSS: 1.2705820798873901
LOSS: 1.2702252864837646
LOSS: 1.2698535919189453
LOSS: 1.269468069076538
LOSS: 1.2690739631652832
LOSS: 1.2686727046966553
LOSS: 1.268270492553711
LOSS: 1.267869472503662
LOSS: 1.2674753665924072
LOSS: 1.2670918703079224
LOSS: 1.2667216062545776
LOSS: 1.266369104385376
LOSS: 1.2660350799560547
LOSS: 1.2657209634780884
LOSS: 1.2654274702072144
LOSS: 1.2651543617248535
LOSS: 1.2649013996124268
LOSS: 1.2646657228469849
LOSS: 1.2644481658935547
LOSS: 1.2642464637756348
LOSS: 1.2640585899353027
LOSS: 1.2638850212097168
LOSS: 1.2637240886688232
LOSS: 1.263573169708252
LOSS: 1.2634329795837402
LOSS: 1.2633014917373657
LOSS: 1.2631783485412598
LOSS: 1.2630614042282104
LOSS: 1.2629516124725342
LOSS: 1.2628459930419922
LOSS: 1.2627460956573486
LOSS: 1.3599352836608887
LOSS: 1.354947805404663
LOSS: 1.3503750562667847
LOSS: 1.3462576866149902
LOSS: 1.342616081237793
LOSS: 1.339430570602417
LOSS: 1.3366225957870483
LOSS: 1.3340572118759155
LOSS: 1.3315861225128174
LOSS: 1.3291065692901611
LOSS: 1.3265873193740845
LOSS: 1.3240498304367065
LOSS: 1.3215413093566895
LOSS: 1.3191102743148804
LOSS: 1.3167935609817505
LOSS: 1.3146079778671265
LOSS: 1.3125499486923218
LOSS: 1.3105963468551636
LOSS: 1.3087143898010254
LOSS: 1.3068701028823853
LOSS: 1.3050386905670166
LOSS: 1.3032079935073853
LOSS: 1.3013800382614136
LOSS: 1.299566388130188
LOSS: 1.297784686088562
LOSS: 1.2960530519485474
LOSS: 1.294385313987732
LOSS: 1.2927895784378052
LOSS: 1.2912650108337402
LOSS: 1.2898048162460327
LOSS: 1.2883986234664917
LOSS: 1.2870389223098755
LOSS: 1.285722255706787
LOSS: 1.2844513654708862
LOSS: 1.2832344770431519
LOSS: 1.2820820808410645
LOSS: 1.2810020446777344
LOSS: 1.2799986600875854
LOSS: 1.2790697813034058
LOSS: 1.2782100439071655
LOSS: 1.277410626411438
LOSS: 1.2766629457473755
LOSS: 1.2759613990783691
LOSS: 1.275301218032837
LOSS: 1.2746824026107788
LOSS: 1.2741050720214844
LOSS: 1.2735697031021118
LOSS: 1.2730754613876343
LOSS: 1.2726200819015503
LOSS: 1.2721996307373047
LOSS: 1.2718093395233154
LOSS: 1.271445870399475
LOSS: 1.2711057662963867
LOSS: 1.2707892656326294
LOSS: 1.2704944610595703
LOSS: 1.2702223062515259
LOSS: 1.2699699401855469
LOSS: 1.2697383165359497
LOSS: 1.2695231437683105
LOSS: 1.2693203687667847
LOSS: 1.2691290378570557
LOSS: 1.2689478397369385
LOSS: 1.2687745094299316
LOSS: 1.268609881401062
LOSS: 1.2684534788131714
LOSS: 1.268306016921997
LOSS: 1.268165946006775
LOSS: 1.2680329084396362
LOSS: 1.2679064273834229
LOSS: 1.2677844762802124
LOSS: 1.26766836643219
LOSS: 1.2675576210021973
LOSS: 1.267451524734497
LOSS: 1.267350673675537
LOSS: 1.2672545909881592
LOSS: 1.2671629190444946
LOSS: 1.2670737504959106
LOSS: 1.2669909000396729
LOSS: 1.2669100761413574
LOSS: 1.266832709312439
LOSS: 1.2667601108551025
LOSS: 1.2666897773742676
LOSS: 1.2666224241256714
LOSS: 1.2665601968765259
LOSS: 1.2664997577667236
LOSS: 1.2664408683776855
LOSS: 1.2663837671279907
LOSS: 1.266329050064087
LOSS: 1.2662760019302368
LOSS: 1.2662220001220703
LOSS: 1.2661702632904053
LOSS: 1.2661195993423462
LOSS: 1.266069769859314
LOSS: 1.2660210132598877
LOSS: 1.2659729719161987
LOSS: 1.2659263610839844
LOSS: 1.2658812999725342
LOSS: 1.2658380270004272
LOSS: 1.2657949924468994
LOSS: 1.2657533884048462
> /home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py(255)fit()
-> self.create_data(x_train, y_train)
LOSS: 1.2510854005813599
LOSS: 1.2510002851486206
LOSS: 1.2509163618087769
LOSS: 1.2508333921432495
LOSS: 1.2507516145706177
LOSS: 1.2506704330444336
LOSS: 1.2505911588668823
LOSS: 1.2505122423171997
LOSS: 1.250434160232544
LOSS: 1.2503572702407837
LOSS: 1.250280737876892
LOSS: 1.250206470489502
LOSS: 1.3450164794921875
LOSS: 1.340138554573059
LOSS: 1.3356823921203613
LOSS: 1.3316746950149536
LOSS: 1.3281232118606567
LOSS: 1.3250203132629395
LOSS: 1.3223282098770142
LOSS: 1.3199657201766968
LOSS: 1.3178095817565918
LOSS: 1.3157219886779785
LOSS: 1.3135889768600464
LOSS: 1.311352014541626
LOSS: 1.309009075164795
LOSS: 1.30659818649292
LOSS: 1.304176688194275
LOSS: 1.3018019199371338
LOSS: 1.2995175123214722
LOSS: 1.29734468460083
LOSS: 1.2952826023101807
LOSS: 1.2933157682418823
LOSS: 1.2914222478866577
LOSS: 1.28957998752594
LOSS: 1.2877718210220337
LOSS: 1.2859896421432495
LOSS: 1.2842336893081665
LOSS: 1.2825137376785278
LOSS: 1.2808442115783691
LOSS: 1.2792432308197021
LOSS: 1.277726173400879
LOSS: 1.2763041257858276
LOSS: 1.27498197555542
LOSS: 1.2737559080123901
LOSS: 1.2726163864135742
LOSS: 1.271550178527832
LOSS: 1.270544171333313
LOSS: 1.2695887088775635
LOSS: 1.2686783075332642
LOSS: 1.2678133249282837
LOSS: 1.2669957876205444
LOSS: 1.2662279605865479
LOSS: 1.2655116319656372
LOSS: 1.264844298362732
LOSS: 1.2642199993133545
LOSS: 1.2636293172836304
LOSS: 1.2630608081817627
LOSS: 1.2625041007995605
LOSS: 1.2619504928588867
LOSS: 1.2613928318023682
LOSS: 1.2608267068862915
LOSS: 1.2602506875991821
LOSS: 1.2596646547317505
LOSS: 1.2590676546096802
LOSS: 1.2584604024887085
LOSS: 1.2578436136245728
LOSS: 1.2572177648544312
LOSS: 1.2565854787826538
LOSS: 1.2559504508972168
LOSS: 1.2553175687789917
LOSS: 1.254692792892456
LOSS: 1.2540819644927979
LOSS: 1.2534929513931274
LOSS: 1.252929449081421
LOSS: 1.2523950338363647
LOSS: 1.251890778541565
LOSS: 1.2514194250106812
LOSS: 1.2509781122207642
LOSS: 1.250566005706787
LOSS: 1.2501832246780396
LOSS: 1.2498276233673096
LOSS: 1.2494995594024658
LOSS: 1.249194860458374
LOSS: 1.2489137649536133
LOSS: 1.2486565113067627
LOSS: 1.2484177350997925
LOSS: 1.248198390007019
LOSS: 1.247994065284729
LOSS: 1.2478039264678955
LOSS: 1.2476272583007812
LOSS: 1.2474627494812012
LOSS: 1.2473089694976807
LOSS: 1.2471634149551392
LOSS: 1.247027039527893
LOSS: 1.246897578239441
LOSS: 1.2467749118804932
LOSS: 1.2466589212417603
LOSS: 1.2465485334396362
LOSS: 1.2464417219161987
LOSS: 1.2463411092758179
LOSS: 1.2462458610534668
LOSS: 1.2461555004119873
LOSS: 1.2460708618164062
LOSS: 1.2459907531738281
LOSS: 1.245916724205017
LOSS: 1.2458455562591553
LOSS: 1.2457777261734009
LOSS: 1.2457122802734375
LOSS: 1.2456481456756592
LOSS: 1.2455841302871704
LOSS: 1.2455196380615234
LOSS: 1.2454545497894287
LOSS: 1.3431907892227173
LOSS: 1.3380851745605469
LOSS: 1.3333576917648315
LOSS: 1.3290432691574097
LOSS: 1.3251742124557495
LOSS: 1.3217693567276
LOSS: 1.3188233375549316
LOSS: 1.3162963390350342
LOSS: 1.3141053915023804
LOSS: 1.3121304512023926
LOSS: 1.3102351427078247
LOSS: 1.3083014488220215
LOSS: 1.3062576055526733
LOSS: 1.3040887117385864
LOSS: 1.3018254041671753
LOSS: 1.2995260953903198
LOSS: 1.2972586154937744
LOSS: 1.2950819730758667
LOSS: 1.2930372953414917
LOSS: 1.2911432981491089
LOSS: 1.2893956899642944
LOSS: 1.2877728939056396
LOSS: 1.2862446308135986
LOSS: 1.2847813367843628
LOSS: 1.2833616733551025
LOSS: 1.281976342201233
LOSS: 1.2806274890899658
LOSS: 1.2793272733688354
LOSS: 1.2780930995941162
LOSS: 1.2769429683685303
LOSS: 1.2758933305740356
LOSS: 1.274953007698059
LOSS: 1.274123191833496
LOSS: 1.273395299911499
LOSS: 1.2727539539337158
LOSS: 1.2721797227859497
LOSS: 1.2716546058654785
LOSS: 1.271162748336792
LOSS: 1.2706941366195679
LOSS: 1.2702457904815674
LOSS: 1.269819974899292
LOSS: 1.2694199085235596
LOSS: 1.269049048423767
LOSS: 1.2687113285064697
LOSS: 1.26840341091156
LOSS: 1.2681221961975098
LOSS: 1.267861008644104
LOSS: 1.2676136493682861
LOSS: 1.2673726081848145
LOSS: 1.2671347856521606
LOSS: 1.2668966054916382
LOSS: 1.2666571140289307
LOSS: 1.2664176225662231
LOSS: 1.2661757469177246
LOSS: 1.2659319639205933
LOSS: 1.2656807899475098
LOSS: 1.2654179334640503
LOSS: 1.2651346921920776
LOSS: 1.2648239135742188
LOSS: 1.264479160308838
LOSS: 1.2640949487686157
LOSS: 1.2636678218841553
LOSS: 1.263200283050537
LOSS: 1.262695550918579
LOSS: 1.2621583938598633
LOSS: 1.2615967988967896
LOSS: 1.2610187530517578
LOSS: 1.2604323625564575
LOSS: 1.2598458528518677
LOSS: 1.2592673301696777
LOSS: 1.2587051391601562
LOSS: 1.2581660747528076
LOSS: 1.2576546669006348
LOSS: 1.257176160812378
LOSS: 1.2567315101623535
LOSS: 1.256321907043457
LOSS: 1.255946397781372
LOSS: 1.2556030750274658
LOSS: 1.255289077758789
LOSS: 1.2550016641616821
LOSS: 1.2547388076782227
LOSS: 1.2544981241226196
LOSS: 1.254278302192688
LOSS: 1.254077672958374
LOSS: 1.2538949251174927
LOSS: 1.2537281513214111
LOSS: 1.253575086593628
LOSS: 1.2534323930740356
LOSS: 1.2533007860183716
LOSS: 1.2531801462173462
LOSS: 1.253069281578064
LOSS: 1.2529675960540771
LOSS: 1.2528729438781738
LOSS: 1.252785086631775
LOSS: 1.2527029514312744
LOSS: 1.2526259422302246
LOSS: 1.2525545358657837
LOSS: 1.2524882555007935
LOSS: 1.2524255514144897
LOSS: 1.2523659467697144
LOSS: 1.3431907892227173
LOSS: 1.3380851745605469
LOSS: 1.3333576917648315
LOSS: 1.3290432691574097
LOSS: 1.3251742124557495
LOSS: 1.3217693567276
LOSS: 1.3188233375549316
LOSS: 1.3162963390350342
LOSS: 1.3141053915023804
LOSS: 1.3121304512023926
LOSS: 1.3102351427078247
LOSS: 1.3083014488220215
LOSS: 1.3062576055526733
LOSS: 1.3040887117385864
LOSS: 1.3018254041671753
LOSS: 1.2995260953903198
LOSS: 1.2972586154937744
LOSS: 1.2950819730758667
LOSS: 1.2930372953414917
LOSS: 1.2911432981491089
LOSS: 1.2893956899642944
LOSS: 1.2877728939056396
LOSS: 1.2862446308135986
LOSS: 1.2847813367843628
LOSS: 1.2833616733551025
LOSS: 1.281976342201233
LOSS: 1.2806274890899658
LOSS: 1.2793272733688354
LOSS: 1.2780930995941162
LOSS: 1.2769429683685303
LOSS: 1.2758933305740356
LOSS: 1.274953007698059
LOSS: 1.274123191833496
LOSS: 1.273395299911499
LOSS: 1.2727539539337158
LOSS: 1.2721797227859497
LOSS: 1.2716546058654785
LOSS: 1.271162748336792
LOSS: 1.2706941366195679
LOSS: 1.2702457904815674
LOSS: 1.269819974899292
LOSS: 1.2694199085235596
LOSS: 1.269049048423767
LOSS: 1.2687113285064697
LOSS: 1.26840341091156
LOSS: 1.2681221961975098
LOSS: 1.267861008644104
LOSS: 1.2676136493682861
LOSS: 1.2673726081848145
LOSS: 1.2671347856521606
LOSS: 1.2668966054916382
LOSS: 1.2666571140289307
LOSS: 1.2664176225662231
LOSS: 1.2661757469177246
LOSS: 1.2659319639205933
LOSS: 1.2656807899475098
LOSS: 1.2654179334640503
LOSS: 1.2651346921920776
LOSS: 1.2648239135742188
LOSS: 1.264479160308838
LOSS: 1.2640949487686157
LOSS: 1.2636678218841553
LOSS: 1.263200283050537
LOSS: 1.262695550918579
LOSS: 1.2621583938598633
LOSS: 1.2615967988967896
LOSS: 1.2610187530517578
LOSS: 1.2604323625564575
LOSS: 1.2598458528518677
LOSS: 1.2592673301696777
LOSS: 1.2587051391601562
LOSS: 1.2581660747528076
LOSS: 1.2576546669006348
LOSS: 1.257176160812378
LOSS: 1.2567315101623535
LOSS: 1.256321907043457
LOSS: 1.255946397781372
LOSS: 1.2556030750274658
LOSS: 1.255289077758789
LOSS: 1.2550016641616821
LOSS: 1.2547388076782227
LOSS: 1.2544981241226196
LOSS: 1.254278302192688
LOSS: 1.254077672958374
LOSS: 1.2538949251174927
LOSS: 1.2537281513214111
LOSS: 1.253575086593628
LOSS: 1.2534323930740356
LOSS: 1.2533007860183716
LOSS: 1.2531801462173462
LOSS: 1.253069281578064
LOSS: 1.2529675960540771
LOSS: 1.2528729438781738
LOSS: 1.252785086631775
LOSS: 1.2527029514312744
LOSS: 1.2526259422302246
LOSS: 1.2525545358657837
LOSS: 1.2524882555007935
LOSS: 1.2524255514144897
LOSS: 1.2523659467697144
> /home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py(255)fit()
-> self.create_data(x_train, y_train)
LOSS: 1.2859201431274414
LOSS: 1.2837955951690674
LOSS: 1.2817469835281372
LOSS: 1.2797681093215942
LOSS: 1.2778618335723877
LOSS: 1.2760376930236816
LOSS: 1.2743104696273804
LOSS: 1.2726937532424927
LOSS: 1.2711979150772095
LOSS: 1.2698255777359009
LOSS: 1.2685716152191162
LOSS: 1.2674221992492676
LOSS: 1.266358733177185
LOSS: 1.2653603553771973
LOSS: 1.2644102573394775
LOSS: 1.2634963989257812
LOSS: 1.2626153230667114
LOSS: 1.2617684602737427
LOSS: 1.2609608173370361
LOSS: 1.260199785232544
LOSS: 1.2594882249832153
LOSS: 1.2588274478912354
LOSS: 1.2582151889801025
LOSS: 1.2576452493667603
LOSS: 1.2571107149124146
LOSS: 1.2566053867340088
LOSS: 1.2561240196228027
LOSS: 1.2556647062301636
LOSS: 1.255226731300354
LOSS: 1.2548110485076904
LOSS: 1.2544190883636475
LOSS: 1.2540501356124878
LOSS: 1.2537040710449219
LOSS: 1.2533786296844482
LOSS: 1.2530713081359863
LOSS: 1.2527785301208496
LOSS: 1.2524986267089844
LOSS: 1.252229928970337
LOSS: 1.2519721984863281
LOSS: 1.2517261505126953
LOSS: 1.2514926195144653
LOSS: 1.251272439956665
LOSS: 1.2510654926300049
LOSS: 1.250870943069458
LOSS: 1.250687837600708
LOSS: 1.2505141496658325
LOSS: 1.2503485679626465
LOSS: 1.2501904964447021
LOSS: 1.250038743019104
LOSS: 1.249894380569458
LOSS: 1.249755859375
LOSS: 1.2496229410171509
LOSS: 1.249495267868042
LOSS: 1.2493722438812256
LOSS: 1.2492526769638062
LOSS: 1.249136209487915
LOSS: 1.2490218877792358
LOSS: 1.2489094734191895
LOSS: 1.2487995624542236
LOSS: 1.248692274093628
LOSS: 1.2485872507095337
LOSS: 1.2484849691390991
LOSS: 1.2483859062194824
LOSS: 1.2482889890670776
LOSS: 1.2481938600540161
LOSS: 1.2481015920639038
LOSS: 1.2480109930038452
LOSS: 1.2479225397109985
LOSS: 1.2478357553482056
LOSS: 1.247750997543335
LOSS: 1.247667908668518
LOSS: 1.2475872039794922
LOSS: 1.2475078105926514
LOSS: 1.2474303245544434
LOSS: 1.2473543882369995
LOSS: 1.247279405593872
LOSS: 1.247206449508667
LOSS: 1.2471345663070679
LOSS: 1.247064232826233
LOSS: 1.3481526374816895
LOSS: 1.34260892868042
LOSS: 1.3373496532440186
LOSS: 1.3324167728424072
LOSS: 1.3278474807739258
LOSS: 1.323667287826538
LOSS: 1.3198796510696411
LOSS: 1.3164571523666382
LOSS: 1.313334584236145
LOSS: 1.310414433479309
LOSS: 1.3075897693634033
LOSS: 1.3047740459442139
LOSS: 1.3019243478775024
LOSS: 1.2990469932556152
LOSS: 1.2961839437484741
LOSS: 1.293393850326538
LOSS: 1.290734887123108
LOSS: 1.2882518768310547
LOSS: 1.285970687866211
LOSS: 1.2838969230651855
LOSS: 1.2820197343826294
LOSS: 1.280317783355713
LOSS: 1.2787662744522095
LOSS: 1.27734375
LOSS: 1.2760355472564697
LOSS: 1.2748345136642456
LOSS: 1.2737411260604858
LOSS: 1.2727588415145874
LOSS: 1.271890640258789
LOSS: 1.2711371183395386
LOSS: 1.2704914808273315
LOSS: 1.2699404954910278
LOSS: 1.2694648504257202
LOSS: 1.2690421342849731
LOSS: 1.2686493396759033
LOSS: 1.2682687044143677
LOSS: 1.2678890228271484
LOSS: 1.2675060033798218
LOSS: 1.2671239376068115
LOSS: 1.2667487859725952
LOSS: 1.2663905620574951
LOSS: 1.266057014465332
LOSS: 1.2657530307769775
LOSS: 1.2654789686203003
LOSS: 1.2652331590652466
LOSS: 1.265012502670288
LOSS: 1.2648108005523682
LOSS: 1.2646253108978271
LOSS: 1.2644524574279785
LOSS: 1.264291763305664
LOSS: 1.264142394065857
LOSS: 1.264005184173584
LOSS: 1.2638789415359497
LOSS: 1.2637622356414795
LOSS: 1.26365327835083
LOSS: 1.263549566268921
LOSS: 1.2634482383728027
LOSS: 1.2633484601974487
LOSS: 1.2632486820220947
LOSS: 1.2631492614746094
LOSS: 1.2630499601364136
LOSS: 1.2629523277282715
LOSS: 1.2628560066223145
LOSS: 1.2627620697021484
LOSS: 1.2626711130142212
LOSS: 1.2625815868377686
LOSS: 1.262492299079895
LOSS: 1.2624030113220215
LOSS: 1.262312412261963
LOSS: 1.2622207403182983
LOSS: 1.2621265649795532
LOSS: 1.2620292901992798
LOSS: 1.261928677558899
LOSS: 1.2618223428726196
LOSS: 1.2617100477218628
LOSS: 1.261590600013733
LOSS: 1.261461615562439
LOSS: 1.2613236904144287
LOSS: 1.261176347732544
LOSS: 1.2610212564468384
LOSS: 1.2608593702316284
LOSS: 1.2606935501098633
LOSS: 1.260526180267334
LOSS: 1.2603601217269897
LOSS: 1.2601978778839111
LOSS: 1.2600412368774414
LOSS: 1.259891390800476
LOSS: 1.2597503662109375
LOSS: 1.2596158981323242
LOSS: 1.2594908475875854
LOSS: 1.259373426437378
LOSS: 1.259263515472412
LOSS: 1.2591609954833984
LOSS: 1.2590649127960205
LOSS: 1.2589763402938843
LOSS: 1.258893370628357
LOSS: 1.2588164806365967
LOSS: 1.258744478225708
LOSS: 1.2586779594421387
LOSS: 1.2586147785186768
LOSS: 1.364822268486023
LOSS: 1.3601505756378174
LOSS: 1.3558233976364136
LOSS: 1.3518733978271484
LOSS: 1.3483139276504517
LOSS: 1.345127820968628
LOSS: 1.3422510623931885
LOSS: 1.3395715951919556
LOSS: 1.3369580507278442
LOSS: 1.3343086242675781
LOSS: 1.3315815925598145
LOSS: 1.3287924528121948
LOSS: 1.3259899616241455
LOSS: 1.3232309818267822
LOSS: 1.3205623626708984
LOSS: 1.318009853363037
LOSS: 1.3155758380889893
LOSS: 1.3132424354553223
LOSS: 1.310982584953308
LOSS: 1.308772087097168
LOSS: 1.306597352027893
LOSS: 1.3044604063034058
LOSS: 1.3023775815963745
LOSS: 1.3003743886947632
LOSS: 1.2984789609909058
LOSS: 1.2967158555984497
LOSS: 1.2951009273529053
LOSS: 1.2936358451843262
LOSS: 1.2923084497451782
LOSS: 1.291094422340393
LOSS: 1.2899634838104248
LOSS: 1.288887619972229
LOSS: 1.2878471612930298
LOSS: 1.2868345975875854
LOSS: 1.2858529090881348
LOSS: 1.2849122285842896
LOSS: 1.2840244770050049
LOSS: 1.2831958532333374
LOSS: 1.2824283838272095
LOSS: 1.2817184925079346
LOSS: 1.281058430671692
LOSS: 1.2804405689239502
LOSS: 1.2798579931259155
LOSS: 1.2793078422546387
LOSS: 1.2787878513336182
LOSS: 1.2782988548278809
LOSS: 1.2778409719467163
LOSS: 1.2774111032485962
LOSS: 1.2770065069198608
LOSS: 1.276623249053955
LOSS: 1.2762563228607178
LOSS: 1.2759028673171997
LOSS: 1.2755612134933472
LOSS: 1.275230050086975
LOSS: 1.2749117612838745
LOSS: 1.2746058702468872
LOSS: 1.2743126153945923
LOSS: 1.2740304470062256
LOSS: 1.2737548351287842
LOSS: 1.2734860181808472
LOSS: 1.2732189893722534
LOSS: 1.2729519605636597
LOSS: 1.2726836204528809
LOSS: 1.2724109888076782
LOSS: 1.2721339464187622
LOSS: 1.2718499898910522
LOSS: 1.271553635597229
LOSS: 1.2712442874908447
LOSS: 1.2709215879440308
LOSS: 1.2705820798873901
LOSS: 1.2702252864837646
LOSS: 1.2698535919189453
LOSS: 1.269468069076538
LOSS: 1.2690739631652832
LOSS: 1.2686727046966553
LOSS: 1.268270492553711
LOSS: 1.267869472503662
LOSS: 1.2674753665924072
LOSS: 1.2670918703079224
LOSS: 1.2667216062545776
LOSS: 1.266369104385376
LOSS: 1.2660350799560547
LOSS: 1.2657209634780884
LOSS: 1.2654274702072144
LOSS: 1.2651543617248535
LOSS: 1.2649013996124268
LOSS: 1.2646657228469849
LOSS: 1.2644481658935547
LOSS: 1.2642464637756348
LOSS: 1.2640585899353027
LOSS: 1.2638850212097168
LOSS: 1.2637240886688232
LOSS: 1.263573169708252
LOSS: 1.2634329795837402
LOSS: 1.2633014917373657
LOSS: 1.2631783485412598
LOSS: 1.2630614042282104
LOSS: 1.2629516124725342
LOSS: 1.2628459930419922
LOSS: 1.2627460956573486
> /home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py(255)fit()
-> self.create_data(x_train, y_train)
> /home/chefos/projects/ml_for_opvs/code_/training/pytorch_models.py(255)fit()
-> self.create_data(x_train, y_train)
